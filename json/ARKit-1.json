[
  {
    "title": "ar_device_anchor_query_status_success | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_device_anchor_query_status_t/ar_device_anchor_query_status_success",
    "html": "See Also\nQuery status\nar_device_anchor_query_status_failure\nThe device anchor query failed.\nBeta"
  },
  {
    "title": "ar_plane_detection_provider_set_update_handler_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295902-ar_plane_detection_provider_set_",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_world_tracking_provider_add_anchor_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295921-ar_world_tracking_provider_add_a",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_world_tracking_provider_set_anchor_update_handler_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295923-ar_world_tracking_provider_set_a",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_world_tracking_provider_get_required_authorization_type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4108283-ar_world_tracking_provider_get_r",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_world_tracking_provider_set_anchor_update_handler | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131560-ar_world_tracking_provider_set_a",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_session_error_code_data_provider_failed_to_run | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_session_error_code_t/ar_session_error_code_data_provider_failed_to_run",
    "html": "See Also\nDetermining the cause of session errors\nar_session_error_code_data_provider_not_authorized\nThe error code for when a data provider is missing at least one authorization it needs to run.\nBeta"
  },
  {
    "title": "ar_session_error_code_data_provider_not_authorized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_session_error_code_t/ar_session_error_code_data_provider_not_authorized",
    "html": "See Also\nDetermining the cause of session errors\nar_session_error_code_data_provider_failed_to_run\nThe error code for when a data provider fails to run.\nBeta"
  },
  {
    "title": "ar_world_tracking_error_code_add_anchor_failed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_tracking_error_code_t/ar_world_tracking_error_code_add_anchor_failed",
    "html": "See Also\nDetermining causes for tracking failures\nar_world_tracking_error_code_remove_anchor_failed\nThe error code for when a world tracking provider can’t remove a world anchor.\nBeta\nar_world_tracking_error_code_anchor_max_limit_reached\nThe error code for when a world tracking provider reaches its world anchor limit.\nBeta"
  },
  {
    "title": "ar_world_anchor_create_with_origin_from_anchor_transform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4293490-ar_world_anchor_create_with_orig",
    "html": "See Also\nWorld tracking\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_world_tracking_error_code_remove_anchor_failed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_tracking_error_code_t/ar_world_tracking_error_code_remove_anchor_failed",
    "html": "See Also\nDetermining causes for tracking failures\nar_world_tracking_error_code_add_anchor_failed\nThe error code for when a world tracking provider can’t add a world anchor.\nBeta\nar_world_tracking_error_code_anchor_max_limit_reached\nThe error code for when a world tracking provider reaches its world anchor limit.\nBeta"
  },
  {
    "title": "ar_world_tracking_error_code_anchor_max_limit_reached | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_tracking_error_code_t/ar_world_tracking_error_code_anchor_max_limit_reached",
    "html": "See Also\nDetermining causes for tracking failures\nar_world_tracking_error_code_add_anchor_failed\nThe error code for when a world tracking provider can’t add a world anchor.\nBeta\nar_world_tracking_error_code_remove_anchor_failed\nThe error code for when a world tracking provider can’t remove a world anchor.\nBeta"
  },
  {
    "title": "ar_mesh_classification_door | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_classification_t/ar_mesh_classification_door",
    "html": "See Also\nGetting architecture classifications\nar_mesh_classification_ceiling\nA ceiling.\nBeta\nar_mesh_classification_floor\nA floor.\nBeta\nar_mesh_classification_stairs\nA set of stairs.\nBeta\nar_mesh_classification_wall\nA wall.\nBeta\nar_mesh_classification_window\nA window.\nBeta"
  },
  {
    "title": "ar_mesh_classification_floor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_classification_t/ar_mesh_classification_floor",
    "html": "See Also\nGetting architecture classifications\nar_mesh_classification_ceiling\nA ceiling.\nBeta\nar_mesh_classification_door\nA door.\nBeta\nar_mesh_classification_stairs\nA set of stairs.\nBeta\nar_mesh_classification_wall\nA wall.\nBeta\nar_mesh_classification_window\nA window.\nBeta"
  },
  {
    "title": "ar_world_anchors_enumerate_anchors_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295917-ar_world_anchors_enumerate_ancho",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_world_tracking_provider_is_supported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4218669-ar_world_tracking_provider_is_su",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_world_tracking_provider_add_anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103691-ar_world_tracking_provider_add_a",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_world_tracking_remove_anchor_completion_handler_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_tracking_remove_anchor_completion_handler_t",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_world_tracking_configuration_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131558-ar_world_tracking_configuration_",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_world_tracking_anchor_update_handler_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_tracking_anchor_update_handler_t",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_world_anchors_enumerate_anchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103685-ar_world_anchors_enumerate_ancho",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_world_anchors_get_count | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103687-ar_world_anchors_get_count",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_device_anchor_query_status_failure | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_device_anchor_query_status_t/ar_device_anchor_query_status_failure",
    "html": "See Also\nQuery status\nar_device_anchor_query_status_success\nThe device anchor query succeeded.\nBeta"
  },
  {
    "title": "ar_mesh_classification_window | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_classification_t/ar_mesh_classification_window",
    "html": "See Also\nGetting architecture classifications\nar_mesh_classification_ceiling\nA ceiling.\nBeta\nar_mesh_classification_door\nA door.\nBeta\nar_mesh_classification_floor\nA floor.\nBeta\nar_mesh_classification_stairs\nA set of stairs.\nBeta\nar_mesh_classification_wall\nA wall.\nBeta"
  },
  {
    "title": "ar_mesh_classification_cabinet | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_classification_t/ar_mesh_classification_cabinet",
    "html": "See Also\nGetting furniture classifications\nar_mesh_classification_bed\nA bed.\nBeta\nar_mesh_classification_home_appliance\nA home appliance.\nBeta\nar_mesh_classification_seat\nA seat.\nBeta\nar_mesh_classification_table\nA table.\nBeta"
  },
  {
    "title": "ar_world_tracking_provider_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103692-ar_world_tracking_provider_creat",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_mesh_classification_plant | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_classification_t/ar_mesh_classification_plant",
    "html": "See Also\nGetting decoration classifications\nar_mesh_classification_tv\nA television.\nBeta"
  },
  {
    "title": "ar_mesh_classification_bed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_classification_t/ar_mesh_classification_bed",
    "html": "See Also\nGetting furniture classifications\nar_mesh_classification_cabinet\nA cabinet.\nBeta\nar_mesh_classification_home_appliance\nA home appliance.\nBeta\nar_mesh_classification_seat\nA seat.\nBeta\nar_mesh_classification_table\nA table.\nBeta"
  },
  {
    "title": "ar_mesh_classification_tv | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_classification_t/ar_mesh_classification_tv",
    "html": "See Also\nGetting decoration classifications\nar_mesh_classification_plant\nA plant.\nBeta"
  },
  {
    "title": "ar_world_tracking_provider_query_device_anchor_at_timestamp | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4293491-ar_world_tracking_provider_query",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_session_error_code_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_session_error_code_t",
    "html": "Topics\nDetermining the cause of session errors\nar_session_error_code_data_provider_failed_to_run\nThe error code for when a data provider fails to run.\nar_session_error_code_data_provider_not_authorized\nThe error code for when a data provider is missing at least one authorization it needs to run.\nSee Also\nErrors\nar_error_t\nAn error reported by ARKit.\nBeta\nar_error_code_t\nCodes that identify errors in ARKit.\nBeta\nar_error_domain\nA string that indicates the error domain in Core Foundation.\nBeta\nar_world_tracking_error_code_t\nThe error codes for errors that world tracking providers throw.\nBeta\nar_error_get_error_code\nGets the error code associated with an error.\nBeta\nar_error_copy_cf_error\nCopies a reference to a Core Foundation error object that represents the specified ARKit error.\nBeta"
  },
  {
    "title": "ar_error_get_error_code | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103451-ar_error_get_error_code",
    "html": "See Also\nErrors\nar_error_t\nAn error reported by ARKit.\nBeta\nar_error_code_t\nCodes that identify errors in ARKit.\nBeta\nar_error_domain\nA string that indicates the error domain in Core Foundation.\nBeta\nar_session_error_code_t\nThe error codes for ARKit sessions.\nBeta\nar_world_tracking_error_code_t\nThe error codes for errors that world tracking providers throw.\nBeta\nar_error_copy_cf_error\nCopies a reference to a Core Foundation error object that represents the specified ARKit error.\nBeta"
  },
  {
    "title": "ar_device_anchor_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4293485-ar_device_anchor_create",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_plane_extent_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_extent_t",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_extent_get_height | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4218660-ar_plane_extent_get_height",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_classification_status_unknown | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_classification_t/ar_plane_classification_status_unknown",
    "html": "See Also\nGetting unknown classifications\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta"
  },
  {
    "title": "ar_mesh_classification_seat | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_classification_t/ar_mesh_classification_seat",
    "html": "See Also\nGetting furniture classifications\nar_mesh_classification_bed\nA bed.\nBeta\nar_mesh_classification_cabinet\nA cabinet.\nBeta\nar_mesh_classification_home_appliance\nA home appliance.\nBeta\nar_mesh_classification_table\nA table.\nBeta"
  },
  {
    "title": "ar_mesh_classification_home_appliance | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_classification_t/ar_mesh_classification_home_appliance",
    "html": "See Also\nGetting furniture classifications\nar_mesh_classification_bed\nA bed.\nBeta\nar_mesh_classification_cabinet\nA cabinet.\nBeta\nar_mesh_classification_seat\nA seat.\nBeta\nar_mesh_classification_table\nA table.\nBeta"
  },
  {
    "title": "ar_error_domain | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_error_domain",
    "html": "See Also\nErrors\nar_error_t\nAn error reported by ARKit.\nBeta\nar_error_code_t\nCodes that identify errors in ARKit.\nBeta\nar_session_error_code_t\nThe error codes for ARKit sessions.\nBeta\nar_world_tracking_error_code_t\nThe error codes for errors that world tracking providers throw.\nBeta\nar_error_get_error_code\nGets the error code associated with an error.\nBeta\nar_error_copy_cf_error\nCopies a reference to a Core Foundation error object that represents the specified ARKit error.\nBeta"
  },
  {
    "title": "ar_mesh_classification_table | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_classification_t/ar_mesh_classification_table",
    "html": "See Also\nGetting furniture classifications\nar_mesh_classification_bed\nA bed.\nBeta\nar_mesh_classification_cabinet\nA cabinet.\nBeta\nar_mesh_classification_home_appliance\nA home appliance.\nBeta\nar_mesh_classification_seat\nA seat.\nBeta"
  },
  {
    "title": "ar_plane_anchor_get_plane_classification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103526-ar_plane_anchor_get_plane_classi",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_classification_table | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_classification_t/ar_plane_classification_table",
    "html": "See Also\nGetting known classifications\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta"
  },
  {
    "title": "ar_plane_classification_window | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_classification_t/ar_plane_classification_window",
    "html": "See Also\nGetting known classifications\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta"
  },
  {
    "title": "ar_plane_classification_wall | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_classification_t/ar_plane_classification_wall",
    "html": "See Also\nGetting known classifications\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_window\nA window.\nBeta"
  },
  {
    "title": "ar_plane_anchor_get_geometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103524-ar_plane_anchor_get_geometry",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_anchors_enumerate_anchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103528-ar_plane_anchors_enumerate_ancho",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_anchors_enumerate_anchors_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295900-ar_plane_anchors_enumerate_ancho",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_detection_configuration_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_detection_configuration_t",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_detection_provider_is_supported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4218659-ar_plane_detection_provider_is_s",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_anchors_get_count | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103530-ar_plane_anchors_get_count",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_authorization_results_enumerator_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_results_enumerator_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_plane_detection_provider_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103543-ar_plane_detection_provider_crea",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_data_provider_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_data_provider_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_image_anchors_enumerator_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_image_anchors_enumerator_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_mesh_classification_ceiling | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_classification_t/ar_mesh_classification_ceiling",
    "html": "See Also\nGetting architecture classifications\nar_mesh_classification_door\nA door.\nBeta\nar_mesh_classification_floor\nA floor.\nBeta\nar_mesh_classification_stairs\nA set of stairs.\nBeta\nar_mesh_classification_wall\nA wall.\nBeta\nar_mesh_classification_window\nA window.\nBeta"
  },
  {
    "title": "ar_trackable_anchor_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_trackable_anchor_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_session_data_provider_state_change_handler_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_session_data_provider_state_change_handler_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_update_handler_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_scene_reconstruction_update_handler_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_reference_images_load_reference_images_in_group | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4139339-ar_reference_images_load_referen",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_world_tracking_add_anchor_completion_handler_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_tracking_add_anchor_completion_handler_t",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_mesh_classification_stairs | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_classification_t/ar_mesh_classification_stairs",
    "html": "See Also\nGetting architecture classifications\nar_mesh_classification_ceiling\nA ceiling.\nBeta\nar_mesh_classification_door\nA door.\nBeta\nar_mesh_classification_floor\nA floor.\nBeta\nar_mesh_classification_wall\nA wall.\nBeta\nar_mesh_classification_window\nA window.\nBeta"
  },
  {
    "title": "ar_mesh_classification_wall | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_classification_t/ar_mesh_classification_wall",
    "html": "See Also\nGetting architecture classifications\nar_mesh_classification_ceiling\nA ceiling.\nBeta\nar_mesh_classification_door\nA door.\nBeta\nar_mesh_classification_floor\nA floor.\nBeta\nar_mesh_classification_stairs\nA set of stairs.\nBeta\nar_mesh_classification_window\nA window.\nBeta"
  },
  {
    "title": "ar_authorization_type_none | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_type_t/ar_authorization_type_none",
    "html": "See Also\nRequesting authorization\nar_authorization_type_hand_tracking\nThe authorization for access to detailed hand-tracking data.\nBeta\nar_authorization_type_world_sensing\nThe authorization for access to plane detection, scene reconstruction, and image tracking.\nBeta"
  },
  {
    "title": "ar_authorization_type_world_sensing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_type_t/ar_authorization_type_world_sensing",
    "html": "See Also\nRequesting authorization\nar_authorization_type_hand_tracking\nThe authorization for access to detailed hand-tracking data.\nBeta\nar_authorization_type_none\nNo authorization type.\nBeta"
  },
  {
    "title": "ar_world_tracking_error_code_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_tracking_error_code_t",
    "html": "Topics\nDetermining causes for tracking failures\nar_world_tracking_error_code_add_anchor_failed\nThe error code for when a world tracking provider can’t add a world anchor.\nar_world_tracking_error_code_remove_anchor_failed\nThe error code for when a world tracking provider can’t remove a world anchor.\nar_world_tracking_error_code_anchor_max_limit_reached\nThe error code for when a world tracking provider reaches its world anchor limit.\nSee Also\nErrors\nar_error_t\nAn error reported by ARKit.\nBeta\nar_error_code_t\nCodes that identify errors in ARKit.\nBeta\nar_error_domain\nA string that indicates the error domain in Core Foundation.\nBeta\nar_session_error_code_t\nThe error codes for ARKit sessions.\nBeta\nar_error_get_error_code\nGets the error code associated with an error.\nBeta\nar_error_copy_cf_error\nCopies a reference to a Core Foundation error object that represents the specified ARKit error.\nBeta"
  },
  {
    "title": "ar_world_tracking_provider_remove_anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103694-ar_world_tracking_provider_remov",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_error_copy_cf_error | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131520-ar_error_copy_cf_error",
    "html": "See Also\nErrors\nar_error_t\nAn error reported by ARKit.\nBeta\nar_error_code_t\nCodes that identify errors in ARKit.\nBeta\nar_error_domain\nA string that indicates the error domain in Core Foundation.\nBeta\nar_session_error_code_t\nThe error codes for ARKit sessions.\nBeta\nar_world_tracking_error_code_t\nThe error codes for errors that world tracking providers throw.\nBeta\nar_error_get_error_code\nGets the error code associated with an error.\nBeta"
  },
  {
    "title": "ar_world_tracking_provider_remove_anchor_with_identifier | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4139342-ar_world_tracking_provider_remov",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_mode_default | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_scene_reconstruction_mode_t/ar_scene_reconstruction_mode_default",
    "html": "See Also\nScene reconstruction modes\nar_scene_reconstruction_mode_classification\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_mode_classification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_scene_reconstruction_mode_t/ar_scene_reconstruction_mode_classification",
    "html": "See Also\nScene reconstruction modes\nar_scene_reconstruction_mode_default\nBeta"
  },
  {
    "title": "ar_plane_alignment_vertical | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_alignment_t/ar_plane_alignment_vertical",
    "html": "See Also\nGetting plane alignment\nar_plane_alignment_horizontal\nThe plane is positioned horizontally.\nBeta\nar_plane_alignment_none\nNo plane alignment.\nBeta"
  },
  {
    "title": "ar_plane_alignment_horizontal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_alignment_t/ar_plane_alignment_horizontal",
    "html": "See Also\nGetting plane alignment\nar_plane_alignment_vertical\nThe plane is positioned vertically.\nBeta\nar_plane_alignment_none\nNo plane alignment.\nBeta"
  },
  {
    "title": "ar_plane_extent_get_plane_anchor_from_plane_extent_transform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4278276-ar_plane_extent_get_plane_anchor",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_device_anchor_query_status_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_device_anchor_query_status_t",
    "html": "Topics\nQuery status\nar_device_anchor_query_status_success\nThe device anchor query succeeded.\nar_device_anchor_query_status_failure\nThe device anchor query failed.\nSee Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta"
  },
  {
    "title": "ar_plane_extent_get_width | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4218662-ar_plane_extent_get_width",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta"
  },
  {
    "title": "ar_world_tracking_provider_remove_anchor_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295922-ar_world_tracking_provider_remov",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_plane_alignment_none | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_alignment_t/ar_plane_alignment_none",
    "html": "See Also\nGetting plane alignment\nar_plane_alignment_horizontal\nThe plane is positioned horizontally.\nBeta\nar_plane_alignment_vertical\nThe plane is positioned vertically.\nBeta"
  },
  {
    "title": "ar_world_tracking_configuration_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_tracking_configuration_t",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_device_anchor_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_device_anchor_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_device_anchor_t",
    "html": "See Also\nWorld tracking\nar_world_anchor_create_with_origin_from_anchor_transform\nCreates a world anchor from a position and orientation in world space.\nBeta\nar_world_tracking_add_anchor_completion_handler_t\nA handler to call upon completion of a request to add a world anchor.\nBeta\nar_world_tracking_remove_anchor_completion_handler_t\nA handler to call upon completion of a request to remove a world anchor.\nBeta\nar_world_tracking_anchor_update_handler_t\nA handler for receiving updates to world anchors.\nBeta\nar_world_tracking_configuration_create\nCreates a world tracking configuration.\nBeta\nar_world_anchors_enumerate_anchors\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_enumerate_anchors_f\nEnumerates a collection of world anchors.\nBeta\nar_world_anchors_get_count\nGets the number of world anchors in the collection.\nBeta\nar_world_tracking_provider_create\nCreates a world tracking provider.\nBeta\nar_world_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports world tracking providers.\nBeta\nar_world_tracking_provider_query_device_anchor_at_timestamp\nQueries the predicted pose of the current device at a given time.\nBeta\nar_world_tracking_provider_add_anchor\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_add_anchor_f\nAdds a world anchor you supply to the set of currently tracked anchors.\nBeta\nar_world_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track world anchors.\nBeta\nar_world_tracking_provider_set_anchor_update_handler\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_set_anchor_update_handler_f\nSets the handler for receiving world tracking updates.\nBeta\nar_world_tracking_provider_remove_anchor_with_identifier\nRemoves a world anchor from a world tracking provider based on its ID.\nBeta\nar_world_tracking_provider_remove_anchor\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_provider_remove_anchor_f\nRemoves a world anchor from a world tracking provider.\nBeta\nar_world_tracking_configuration_t\nBeta\nar_device_anchor_create\nBeta\nar_device_anchor_query_status_t\nBeta"
  },
  {
    "title": "ar_plane_geometry_get_mesh_faces | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131536-ar_plane_geometry_get_mesh_faces",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_geometry_get_plane_extent | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4218664-ar_plane_geometry_get_plane_exte",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_geometry_get_mesh_vertices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103551-ar_plane_geometry_get_mesh_verti",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_detection_provider_get_required_authorization_type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4108272-ar_plane_detection_provider_get_",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_anchor_get_alignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4108271-ar_plane_anchor_get_alignment",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_detection_provider_set_update_handler | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131535-ar_plane_detection_provider_set_",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_detection_configuration_set_alignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131533-ar_plane_detection_configuration",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_detection_configuration_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131532-ar_plane_detection_configuration",
    "html": "See Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_geometry_source_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_geometry_source_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_hand_anchor_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_anchor_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_geometry_primitive_type_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_geometry_primitive_type_t",
    "html": "Topics\nPrimitive shapes\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nSee Also\nGeometry\nar_geometry_element_get_buffer\nGets a Metal buffer that contains index data that defines the geometry of an object.\nBeta\nar_geometry_element_get_bytes_per_index\nGets the number of bytes that represent an index value.\nBeta\nar_geometry_element_get_count\nGets the number of primitives in the Metal buffer for a geometry element.\nBeta\nar_geometry_element_get_index_count_per_primitive\nGets the number of indices for each primitive.\nBeta\nar_geometry_element_get_primitive_type\nGets the kind of primitive, lines or triangles, that a geometry element contains.\nBeta\nar_geometry_source_get_buffer\nGets a Metal buffer that contains per-vector data for a geometry source.\nBeta\nar_geometry_source_get_components_per_vector\nGets the number of scalar components in each vector in a geometry source.\nBeta\nar_geometry_source_get_count\nGets the number of vectors in a geometry source.\nBeta\nar_geometry_source_get_format\nGets the vertex format for data in a geometry source’s buffer.\nBeta\nar_geometry_source_get_offset\nGets the offset, in bytes, from the beginning of a geometry source’s buffer.\nBeta\nar_geometry_source_get_stride\nGets the number of bytes between one vector and another in a geometry source’s buffer.\nBeta\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nBeta\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nBeta\nar_pose_t\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta"
  },
  {
    "title": "ar_authorization_results_handler_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_results_handler_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_authorization_results_handler_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_results_handler_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_hand_tracking_update_handler_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_tracking_update_handler_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_hand_tracking_provider_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_tracking_provider_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_mesh_anchor_get_geometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103581-ar_mesh_anchor_get_geometry",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_data_provider_get_state | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131514-ar_data_provider_get_state",
    "html": "See Also\nData providers\nar_data_provider_state_t\nThe possible states of a data provider.\nBeta\nar_data_provider_get_required_authorization_type\nThe kinds of authorization you need to use a particular data provider type.\nBeta\nar_data_providers_enumerator_t\nA handler for enumerating a collection of data providers.\nBeta\nar_data_providers_t\nA collection of data providers.\nBeta\nar_data_providers_add_data_provider\nAdds a data provider to a collection.\nBeta\nar_data_providers_add_data_providers\nAdds multiple data providers to a collection.\nBeta\nar_data_providers_create\nCreates an empty collection of data providers.\nBeta\nar_data_providers_create_with_data_providers\nCreates a collection of data providers that contains the data providers you supply.\nBeta\nar_data_providers_enumerate_data_providers_f\nEnumerates a collection of data providers.\nBeta\nar_data_providers_get_count\nGets the number of data providers in a collection.\nBeta\nar_data_providers_enumerate_data_providers\nEnumerates a collection of data providers.\nBeta\nar_data_providers_remove_data_provider\nRemoves a data provider from a collection.\nBeta\nar_data_providers_remove_data_providers\nRemoves multiple data providers from a collection.\nBeta"
  },
  {
    "title": "ARRaycastTargetAlignmentHorizontal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycasttargetalignment/arraycasttargetalignmenthorizontal",
    "html": "See Also\nChoosing a Target Alignment\nARRaycastTargetAlignmentAny\nThe case that indicates a target may be aligned in any way with respect to gravity.\nARRaycastTargetAlignmentVertical\nThe case that indicates a target is aligned vertically with respect to gravity."
  },
  {
    "title": "ARRaycastTargetExistingPlaneInfinite | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycasttarget/arraycasttargetexistingplaneinfinite",
    "html": "See Also\nTargets\nARRaycastTargetEstimatedPlane\nA raycast target that specifies nonplanar surfaces, or planes about which ARKit can only estimate.\nARRaycastTargetExistingPlaneGeometry\nA raycast target that requires a plane to have a definitive size and shape."
  },
  {
    "title": "ar_trackable_anchor_is_tracked | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103418-ar_trackable_anchor_is_tracked",
    "html": "See Also\nAnchors\nar_anchor_get_identifier\nGets the unique identifier that distinguishes this anchor from all other anchors.\nBeta\nar_anchor_get_timestamp\nGets the timestamp corresponding to the anchor.\nBeta\nar_anchor_get_origin_from_anchor_transform\nGets the transform from the anchor to the origin coordinate system.\nBeta"
  },
  {
    "title": "initWithOrigin:direction:allowingTarget:alignment: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery/3194581-initwithorigin",
    "html": "Parameters\norigin\n\nA 3D position that describes the raycast's starting point.\n\ndirection\n\nA 3D vector that describes the raycast's direction.\n\nallowing\n\nThe type of plane with which you allow the raycast to intersect.\n\nalignment\n\nThe target's alignment with respect to gravity with which you allow the raycast to intersect.\n\nDiscussion\n\nThis creates a query by supplying a 3D starting place and vector. To acquire a raycast query using a screen point and vector that points outward from the user, call raycastQueryFromPoint:allowingTarget:alignment: on ARSCNView."
  },
  {
    "title": "ar_mesh_geometry_get_vertices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103590-ar_mesh_geometry_get_vertices",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta"
  },
  {
    "title": "ar_anchor_get_timestamp | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103415-ar_anchor_get_timestamp",
    "html": "See Also\nAnchors\nar_anchor_get_identifier\nGets the unique identifier that distinguishes this anchor from all other anchors.\nBeta\nar_anchor_get_origin_from_anchor_transform\nGets the transform from the anchor to the origin coordinate system.\nBeta\nar_trackable_anchor_is_tracked\nReturns a Boolean value that indicates whether ARKit is tracking an anchor.\nBeta"
  },
  {
    "title": "ar_session_set_data_provider_state_change_handler_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295914-ar_session_set_data_provider_sta",
    "html": "See Also\nSessions\nar_session_t\nThe main entry point for receiving data from ARKit.\nBeta\nar_session_create\nCreates a new session.\nBeta\nar_session_query_authorization_results\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_query_authorization_results_f\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_request_authorization\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_request_authorization_f\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_run\nRuns a session with the data providers you supply.\nBeta\nar_session_set_authorization_update_handler\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_authorization_update_handler_f\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_data_provider_state_change_handler\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_data_provider_state_change_handler_t\nA handler for receiving updates to data provider states.\nBeta\nar_session_stop\nStops a session.\nBeta"
  },
  {
    "title": "ar_session_query_authorization_results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131546-ar_session_query_authorization_r",
    "html": "See Also\nSessions\nar_session_t\nThe main entry point for receiving data from ARKit.\nBeta\nar_session_create\nCreates a new session.\nBeta\nar_session_query_authorization_results_f\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_request_authorization\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_request_authorization_f\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_run\nRuns a session with the data providers you supply.\nBeta\nar_session_set_authorization_update_handler\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_authorization_update_handler_f\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_data_provider_state_change_handler\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_set_data_provider_state_change_handler_f\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_data_provider_state_change_handler_t\nA handler for receiving updates to data provider states.\nBeta\nar_session_stop\nStops a session.\nBeta"
  },
  {
    "title": "ar_session_set_authorization_update_handler_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295913-ar_session_set_authorization_upd",
    "html": "See Also\nSessions\nar_session_t\nThe main entry point for receiving data from ARKit.\nBeta\nar_session_create\nCreates a new session.\nBeta\nar_session_query_authorization_results\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_query_authorization_results_f\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_request_authorization\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_request_authorization_f\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_run\nRuns a session with the data providers you supply.\nBeta\nar_session_set_authorization_update_handler\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_data_provider_state_change_handler\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_set_data_provider_state_change_handler_f\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_data_provider_state_change_handler_t\nA handler for receiving updates to data provider states.\nBeta\nar_session_stop\nStops a session.\nBeta"
  },
  {
    "title": "ar_authorization_results_enumerator_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_results_enumerator_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_mesh_anchors_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_anchors_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_image_anchor_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_image_anchor_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_reference_images_get_count | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4139338-ar_reference_images_get_count",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_world_tracking_anchor_update_handler_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_tracking_anchor_update_handler_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_reference_image_create_from_cgimage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4108265-ar_reference_image_create_from_c",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_image_tracking_configuration_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131528-ar_image_tracking_configuration_",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_image_tracking_provider_set_update_handler | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131530-ar_image_tracking_provider_set_u",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_image_anchors_get_count | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103488-ar_image_anchors_get_count",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_mesh_anchors_enumerator_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_anchors_enumerator_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_image_tracking_provider_set_update_handler_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295896-ar_image_tracking_provider_set_u",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_release | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103515-ar_release",
    "html": "See Also\nMemory management\nar_retain\nAdds a reference count to an ARKit object.\nBeta"
  },
  {
    "title": "ar_error_code_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_error_code_t",
    "html": "See Also\nErrors\nar_error_t\nAn error reported by ARKit.\nBeta\nar_error_domain\nA string that indicates the error domain in Core Foundation.\nBeta\nar_session_error_code_t\nThe error codes for ARKit sessions.\nBeta\nar_world_tracking_error_code_t\nThe error codes for errors that world tracking providers throw.\nBeta\nar_error_get_error_code\nGets the error code associated with an error.\nBeta\nar_error_copy_cf_error\nCopies a reference to a Core Foundation error object that represents the specified ARKit error.\nBeta"
  },
  {
    "title": "ar_reference_image_create_from_pixel_buffer | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4108266-ar_reference_image_create_from_p",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_reference_image_get_name | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103500-ar_reference_image_get_name",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_anchor_get_identifier | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103414-ar_anchor_get_identifier",
    "html": "See Also\nAnchors\nar_anchor_get_timestamp\nGets the timestamp corresponding to the anchor.\nBeta\nar_anchor_get_origin_from_anchor_transform\nGets the transform from the anchor to the origin coordinate system.\nBeta\nar_trackable_anchor_is_tracked\nReturns a Boolean value that indicates whether ARKit is tracking an anchor.\nBeta"
  },
  {
    "title": "ar_plane_classification_seat | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_classification_t/ar_plane_classification_seat",
    "html": "See Also\nGetting known classifications\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta"
  },
  {
    "title": "ARFrameSemanticPersonSegmentationWithDepth | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframesemantics/arframesemanticpersonsegmentationwithdepth",
    "html": "Discussion\n\nThe ARFrameSemanticPersonSegmentationWithDepth frame semantic specifies that any person ARKit detects in the camera feed should occlude virtual content, depending on the person's depth in the scene.\n\nWhen this option is enabled, ARKit sets the estimatedDepthData and segmentationBuffer properties to serve as a foundation for people occlusion. The standard renderers (ARView, and ARSCNView) use those properties to implement people occlusion for you. See frameSemantics for more information.\n\nSee Also\nOccluding Virtual Content with People\nARFrameSemanticPersonSegmentation\nAn option that indicates that people occlude your app's virtual content."
  },
  {
    "title": "ar_data_provider_state_initialized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_data_provider_state_t/ar_data_provider_state_initialized",
    "html": "See Also\nGetting the status of a data provider\nar_data_provider_state_running\nThe data provider is running.\nBeta\nar_data_provider_state_stopped\nThe data provider is stopped.\nBeta\nar_data_provider_state_paused\nThe data provider is paused.\nBeta"
  },
  {
    "title": "ar_data_provider_state_running | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_data_provider_state_t/ar_data_provider_state_running",
    "html": "See Also\nGetting the status of a data provider\nar_data_provider_state_initialized\nThe data provider has been created.\nBeta\nar_data_provider_state_stopped\nThe data provider is stopped.\nBeta\nar_data_provider_state_paused\nThe data provider is paused.\nBeta"
  },
  {
    "title": "ar_data_provider_state_stopped | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_data_provider_state_t/ar_data_provider_state_stopped",
    "html": "See Also\nGetting the status of a data provider\nar_data_provider_state_initialized\nThe data provider has been created.\nBeta\nar_data_provider_state_running\nThe data provider is running.\nBeta\nar_data_provider_state_paused\nThe data provider is paused.\nBeta"
  },
  {
    "title": "ar_data_provider_state_paused | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_data_provider_state_t/ar_data_provider_state_paused",
    "html": "See Also\nGetting the status of a data provider\nar_data_provider_state_initialized\nThe data provider has been created.\nBeta\nar_data_provider_state_running\nThe data provider is running.\nBeta\nar_data_provider_state_stopped\nThe data provider is stopped.\nBeta"
  },
  {
    "title": "ar_plane_classification_status_not_available | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_classification_t/ar_plane_classification_status_not_available",
    "html": "See Also\nGetting unknown classifications\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta"
  },
  {
    "title": "ar_plane_classification_floor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_classification_t/ar_plane_classification_floor",
    "html": "See Also\nGetting known classifications\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_update_handler_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_scene_reconstruction_update_handler_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_image_tracking_update_handler_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_image_tracking_update_handler_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_mesh_anchor_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_anchor_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_provider_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_scene_reconstruction_provider_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_plane_geometry_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_geometry_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_plane_detection_update_handler_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_detection_update_handler_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_plane_detection_provider_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_detection_provider_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_plane_anchor_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_anchor_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_plane_anchors_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_anchors_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_plane_anchors_enumerator_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_anchors_enumerator_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_mesh_anchors_enumerator_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_anchors_enumerator_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_geometry_element_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_geometry_element_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_image_tracking_provider_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_image_tracking_provider_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_world_tracking_provider_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_tracking_provider_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_authorization_type_hand_tracking | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_type_t/ar_authorization_type_hand_tracking",
    "html": "See Also\nRequesting authorization\nar_authorization_type_world_sensing\nThe authorization for access to plane detection, scene reconstruction, and image tracking.\nBeta\nar_authorization_type_none\nNo authorization type.\nBeta"
  },
  {
    "title": "ar_authorization_update_handler_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_update_handler_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_authorization_result_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_result_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_world_anchor_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_anchor_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_world_anchors_enumerator_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_anchors_enumerator_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_plane_anchors_enumerator_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_anchors_enumerator_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_plane_detection_update_handler_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_detection_update_handler_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_world_anchors_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_anchors_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_reference_images_enumerator_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_reference_images_enumerator_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_world_anchors_enumerator_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_anchors_enumerator_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_image_tracking_update_handler_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_image_tracking_update_handler_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_data_providers_enumerator_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_data_providers_enumerator_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_authorization_update_handler_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_update_handler_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_anchor_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_anchor_t",
    "html": "See Also\nType aliases\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_plane_classification_status_undetermined | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_classification_t/ar_plane_classification_status_undetermined",
    "html": "See Also\nGetting unknown classifications\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta"
  },
  {
    "title": "ar_data_providers_enumerate_data_providers | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4278275-ar_data_providers_enumerate_data",
    "html": "See Also\nData providers\nar_data_provider_state_t\nThe possible states of a data provider.\nBeta\nar_data_provider_get_required_authorization_type\nThe kinds of authorization you need to use a particular data provider type.\nBeta\nar_data_provider_get_state\nGets the current status of data coming from this provider.\nBeta\nar_data_providers_enumerator_t\nA handler for enumerating a collection of data providers.\nBeta\nar_data_providers_t\nA collection of data providers.\nBeta\nar_data_providers_add_data_provider\nAdds a data provider to a collection.\nBeta\nar_data_providers_add_data_providers\nAdds multiple data providers to a collection.\nBeta\nar_data_providers_create\nCreates an empty collection of data providers.\nBeta\nar_data_providers_create_with_data_providers\nCreates a collection of data providers that contains the data providers you supply.\nBeta\nar_data_providers_enumerate_data_providers_f\nEnumerates a collection of data providers.\nBeta\nar_data_providers_get_count\nGets the number of data providers in a collection.\nBeta\nar_data_providers_remove_data_provider\nRemoves a data provider from a collection.\nBeta\nar_data_providers_remove_data_providers\nRemoves multiple data providers from a collection.\nBeta"
  },
  {
    "title": "ar_data_providers_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4169959-ar_data_providers_create",
    "html": "See Also\nData providers\nar_data_provider_state_t\nThe possible states of a data provider.\nBeta\nar_data_provider_get_required_authorization_type\nThe kinds of authorization you need to use a particular data provider type.\nBeta\nar_data_provider_get_state\nGets the current status of data coming from this provider.\nBeta\nar_data_providers_enumerator_t\nA handler for enumerating a collection of data providers.\nBeta\nar_data_providers_t\nA collection of data providers.\nBeta\nar_data_providers_add_data_provider\nAdds a data provider to a collection.\nBeta\nar_data_providers_add_data_providers\nAdds multiple data providers to a collection.\nBeta\nar_data_providers_create_with_data_providers\nCreates a collection of data providers that contains the data providers you supply.\nBeta\nar_data_providers_enumerate_data_providers_f\nEnumerates a collection of data providers.\nBeta\nar_data_providers_get_count\nGets the number of data providers in a collection.\nBeta\nar_data_providers_enumerate_data_providers\nEnumerates a collection of data providers.\nBeta\nar_data_providers_remove_data_provider\nRemoves a data provider from a collection.\nBeta\nar_data_providers_remove_data_providers\nRemoves multiple data providers from a collection.\nBeta"
  },
  {
    "title": "ar_data_providers_remove_data_provider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4169964-ar_data_providers_remove_data_pr",
    "html": "See Also\nData providers\nar_data_provider_state_t\nThe possible states of a data provider.\nBeta\nar_data_provider_get_required_authorization_type\nThe kinds of authorization you need to use a particular data provider type.\nBeta\nar_data_provider_get_state\nGets the current status of data coming from this provider.\nBeta\nar_data_providers_enumerator_t\nA handler for enumerating a collection of data providers.\nBeta\nar_data_providers_t\nA collection of data providers.\nBeta\nar_data_providers_add_data_provider\nAdds a data provider to a collection.\nBeta\nar_data_providers_add_data_providers\nAdds multiple data providers to a collection.\nBeta\nar_data_providers_create\nCreates an empty collection of data providers.\nBeta\nar_data_providers_create_with_data_providers\nCreates a collection of data providers that contains the data providers you supply.\nBeta\nar_data_providers_enumerate_data_providers_f\nEnumerates a collection of data providers.\nBeta\nar_data_providers_get_count\nGets the number of data providers in a collection.\nBeta\nar_data_providers_enumerate_data_providers\nEnumerates a collection of data providers.\nBeta\nar_data_providers_remove_data_providers\nRemoves multiple data providers from a collection.\nBeta"
  },
  {
    "title": "ar_data_providers_add_data_providers | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4169958-ar_data_providers_add_data_provi",
    "html": "See Also\nData providers\nar_data_provider_state_t\nThe possible states of a data provider.\nBeta\nar_data_provider_get_required_authorization_type\nThe kinds of authorization you need to use a particular data provider type.\nBeta\nar_data_provider_get_state\nGets the current status of data coming from this provider.\nBeta\nar_data_providers_enumerator_t\nA handler for enumerating a collection of data providers.\nBeta\nar_data_providers_t\nA collection of data providers.\nBeta\nar_data_providers_add_data_provider\nAdds a data provider to a collection.\nBeta\nar_data_providers_create\nCreates an empty collection of data providers.\nBeta\nar_data_providers_create_with_data_providers\nCreates a collection of data providers that contains the data providers you supply.\nBeta\nar_data_providers_enumerate_data_providers_f\nEnumerates a collection of data providers.\nBeta\nar_data_providers_get_count\nGets the number of data providers in a collection.\nBeta\nar_data_providers_enumerate_data_providers\nEnumerates a collection of data providers.\nBeta\nar_data_providers_remove_data_provider\nRemoves a data provider from a collection.\nBeta\nar_data_providers_remove_data_providers\nRemoves multiple data providers from a collection.\nBeta"
  },
  {
    "title": "ar_data_providers_remove_data_providers | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4169965-ar_data_providers_remove_data_pr",
    "html": "See Also\nData providers\nar_data_provider_state_t\nThe possible states of a data provider.\nBeta\nar_data_provider_get_required_authorization_type\nThe kinds of authorization you need to use a particular data provider type.\nBeta\nar_data_provider_get_state\nGets the current status of data coming from this provider.\nBeta\nar_data_providers_enumerator_t\nA handler for enumerating a collection of data providers.\nBeta\nar_data_providers_t\nA collection of data providers.\nBeta\nar_data_providers_add_data_provider\nAdds a data provider to a collection.\nBeta\nar_data_providers_add_data_providers\nAdds multiple data providers to a collection.\nBeta\nar_data_providers_create\nCreates an empty collection of data providers.\nBeta\nar_data_providers_create_with_data_providers\nCreates a collection of data providers that contains the data providers you supply.\nBeta\nar_data_providers_enumerate_data_providers_f\nEnumerates a collection of data providers.\nBeta\nar_data_providers_get_count\nGets the number of data providers in a collection.\nBeta\nar_data_providers_enumerate_data_providers\nEnumerates a collection of data providers.\nBeta\nar_data_providers_remove_data_provider\nRemoves a data provider from a collection.\nBeta"
  },
  {
    "title": "ar_data_providers_create_with_data_providers | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4278274-ar_data_providers_create_with_da",
    "html": "See Also\nData providers\nar_data_provider_state_t\nThe possible states of a data provider.\nBeta\nar_data_provider_get_required_authorization_type\nThe kinds of authorization you need to use a particular data provider type.\nBeta\nar_data_provider_get_state\nGets the current status of data coming from this provider.\nBeta\nar_data_providers_enumerator_t\nA handler for enumerating a collection of data providers.\nBeta\nar_data_providers_t\nA collection of data providers.\nBeta\nar_data_providers_add_data_provider\nAdds a data provider to a collection.\nBeta\nar_data_providers_add_data_providers\nAdds multiple data providers to a collection.\nBeta\nar_data_providers_create\nCreates an empty collection of data providers.\nBeta\nar_data_providers_enumerate_data_providers_f\nEnumerates a collection of data providers.\nBeta\nar_data_providers_get_count\nGets the number of data providers in a collection.\nBeta\nar_data_providers_enumerate_data_providers\nEnumerates a collection of data providers.\nBeta\nar_data_providers_remove_data_provider\nRemoves a data provider from a collection.\nBeta\nar_data_providers_remove_data_providers\nRemoves multiple data providers from a collection.\nBeta"
  },
  {
    "title": "ar_data_providers_enumerate_data_providers_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295890-ar_data_providers_enumerate_data",
    "html": "See Also\nData providers\nar_data_provider_state_t\nThe possible states of a data provider.\nBeta\nar_data_provider_get_required_authorization_type\nThe kinds of authorization you need to use a particular data provider type.\nBeta\nar_data_provider_get_state\nGets the current status of data coming from this provider.\nBeta\nar_data_providers_enumerator_t\nA handler for enumerating a collection of data providers.\nBeta\nar_data_providers_t\nA collection of data providers.\nBeta\nar_data_providers_add_data_provider\nAdds a data provider to a collection.\nBeta\nar_data_providers_add_data_providers\nAdds multiple data providers to a collection.\nBeta\nar_data_providers_create\nCreates an empty collection of data providers.\nBeta\nar_data_providers_create_with_data_providers\nCreates a collection of data providers that contains the data providers you supply.\nBeta\nar_data_providers_get_count\nGets the number of data providers in a collection.\nBeta\nar_data_providers_enumerate_data_providers\nEnumerates a collection of data providers.\nBeta\nar_data_providers_remove_data_provider\nRemoves a data provider from a collection.\nBeta\nar_data_providers_remove_data_providers\nRemoves multiple data providers from a collection.\nBeta"
  },
  {
    "title": "ar_geometry_element_get_buffer | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103565-ar_geometry_element_get_buffer",
    "html": "See Also\nGeometry\nar_geometry_primitive_type_t\nThe kinds of geometry primitives that a geometry element can contain.\nBeta\nar_geometry_element_get_bytes_per_index\nGets the number of bytes that represent an index value.\nBeta\nar_geometry_element_get_count\nGets the number of primitives in the Metal buffer for a geometry element.\nBeta\nar_geometry_element_get_index_count_per_primitive\nGets the number of indices for each primitive.\nBeta\nar_geometry_element_get_primitive_type\nGets the kind of primitive, lines or triangles, that a geometry element contains.\nBeta\nar_geometry_source_get_buffer\nGets a Metal buffer that contains per-vector data for a geometry source.\nBeta\nar_geometry_source_get_components_per_vector\nGets the number of scalar components in each vector in a geometry source.\nBeta\nar_geometry_source_get_count\nGets the number of vectors in a geometry source.\nBeta\nar_geometry_source_get_format\nGets the vertex format for data in a geometry source’s buffer.\nBeta\nar_geometry_source_get_offset\nGets the offset, in bytes, from the beginning of a geometry source’s buffer.\nBeta\nar_geometry_source_get_stride\nGets the number of bytes between one vector and another in a geometry source’s buffer.\nBeta\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nBeta\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nBeta\nar_pose_t\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta"
  },
  {
    "title": "ar_geometry_element_get_bytes_per_index | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103566-ar_geometry_element_get_bytes_pe",
    "html": "See Also\nGeometry\nar_geometry_primitive_type_t\nThe kinds of geometry primitives that a geometry element can contain.\nBeta\nar_geometry_element_get_buffer\nGets a Metal buffer that contains index data that defines the geometry of an object.\nBeta\nar_geometry_element_get_count\nGets the number of primitives in the Metal buffer for a geometry element.\nBeta\nar_geometry_element_get_index_count_per_primitive\nGets the number of indices for each primitive.\nBeta\nar_geometry_element_get_primitive_type\nGets the kind of primitive, lines or triangles, that a geometry element contains.\nBeta\nar_geometry_source_get_buffer\nGets a Metal buffer that contains per-vector data for a geometry source.\nBeta\nar_geometry_source_get_components_per_vector\nGets the number of scalar components in each vector in a geometry source.\nBeta\nar_geometry_source_get_count\nGets the number of vectors in a geometry source.\nBeta\nar_geometry_source_get_format\nGets the vertex format for data in a geometry source’s buffer.\nBeta\nar_geometry_source_get_offset\nGets the offset, in bytes, from the beginning of a geometry source’s buffer.\nBeta\nar_geometry_source_get_stride\nGets the number of bytes between one vector and another in a geometry source’s buffer.\nBeta\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nBeta\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nBeta\nar_pose_t\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta"
  },
  {
    "title": "ar_geometry_element_get_count | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103567-ar_geometry_element_get_count",
    "html": "See Also\nGeometry\nar_geometry_primitive_type_t\nThe kinds of geometry primitives that a geometry element can contain.\nBeta\nar_geometry_element_get_buffer\nGets a Metal buffer that contains index data that defines the geometry of an object.\nBeta\nar_geometry_element_get_bytes_per_index\nGets the number of bytes that represent an index value.\nBeta\nar_geometry_element_get_index_count_per_primitive\nGets the number of indices for each primitive.\nBeta\nar_geometry_element_get_primitive_type\nGets the kind of primitive, lines or triangles, that a geometry element contains.\nBeta\nar_geometry_source_get_buffer\nGets a Metal buffer that contains per-vector data for a geometry source.\nBeta\nar_geometry_source_get_components_per_vector\nGets the number of scalar components in each vector in a geometry source.\nBeta\nar_geometry_source_get_count\nGets the number of vectors in a geometry source.\nBeta\nar_geometry_source_get_format\nGets the vertex format for data in a geometry source’s buffer.\nBeta\nar_geometry_source_get_offset\nGets the offset, in bytes, from the beginning of a geometry source’s buffer.\nBeta\nar_geometry_source_get_stride\nGets the number of bytes between one vector and another in a geometry source’s buffer.\nBeta\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nBeta\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nBeta\nar_pose_t\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta"
  },
  {
    "title": "ar_geometry_element_get_index_count_per_primitive | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103568-ar_geometry_element_get_index_co",
    "html": "See Also\nGeometry\nar_geometry_primitive_type_t\nThe kinds of geometry primitives that a geometry element can contain.\nBeta\nar_geometry_element_get_buffer\nGets a Metal buffer that contains index data that defines the geometry of an object.\nBeta\nar_geometry_element_get_bytes_per_index\nGets the number of bytes that represent an index value.\nBeta\nar_geometry_element_get_count\nGets the number of primitives in the Metal buffer for a geometry element.\nBeta\nar_geometry_element_get_primitive_type\nGets the kind of primitive, lines or triangles, that a geometry element contains.\nBeta\nar_geometry_source_get_buffer\nGets a Metal buffer that contains per-vector data for a geometry source.\nBeta\nar_geometry_source_get_components_per_vector\nGets the number of scalar components in each vector in a geometry source.\nBeta\nar_geometry_source_get_count\nGets the number of vectors in a geometry source.\nBeta\nar_geometry_source_get_format\nGets the vertex format for data in a geometry source’s buffer.\nBeta\nar_geometry_source_get_offset\nGets the offset, in bytes, from the beginning of a geometry source’s buffer.\nBeta\nar_geometry_source_get_stride\nGets the number of bytes between one vector and another in a geometry source’s buffer.\nBeta\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nBeta\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nBeta\nar_pose_t\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta"
  },
  {
    "title": "ar_geometry_source_get_buffer | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103574-ar_geometry_source_get_buffer",
    "html": "See Also\nGeometry\nar_geometry_primitive_type_t\nThe kinds of geometry primitives that a geometry element can contain.\nBeta\nar_geometry_element_get_buffer\nGets a Metal buffer that contains index data that defines the geometry of an object.\nBeta\nar_geometry_element_get_bytes_per_index\nGets the number of bytes that represent an index value.\nBeta\nar_geometry_element_get_count\nGets the number of primitives in the Metal buffer for a geometry element.\nBeta\nar_geometry_element_get_index_count_per_primitive\nGets the number of indices for each primitive.\nBeta\nar_geometry_element_get_primitive_type\nGets the kind of primitive, lines or triangles, that a geometry element contains.\nBeta\nar_geometry_source_get_components_per_vector\nGets the number of scalar components in each vector in a geometry source.\nBeta\nar_geometry_source_get_count\nGets the number of vectors in a geometry source.\nBeta\nar_geometry_source_get_format\nGets the vertex format for data in a geometry source’s buffer.\nBeta\nar_geometry_source_get_offset\nGets the offset, in bytes, from the beginning of a geometry source’s buffer.\nBeta\nar_geometry_source_get_stride\nGets the number of bytes between one vector and another in a geometry source’s buffer.\nBeta\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nBeta\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nBeta\nar_pose_t\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta"
  },
  {
    "title": "ar_geometry_element_get_primitive_type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103569-ar_geometry_element_get_primitiv",
    "html": "See Also\nGeometry\nar_geometry_primitive_type_t\nThe kinds of geometry primitives that a geometry element can contain.\nBeta\nar_geometry_element_get_buffer\nGets a Metal buffer that contains index data that defines the geometry of an object.\nBeta\nar_geometry_element_get_bytes_per_index\nGets the number of bytes that represent an index value.\nBeta\nar_geometry_element_get_count\nGets the number of primitives in the Metal buffer for a geometry element.\nBeta\nar_geometry_element_get_index_count_per_primitive\nGets the number of indices for each primitive.\nBeta\nar_geometry_source_get_buffer\nGets a Metal buffer that contains per-vector data for a geometry source.\nBeta\nar_geometry_source_get_components_per_vector\nGets the number of scalar components in each vector in a geometry source.\nBeta\nar_geometry_source_get_count\nGets the number of vectors in a geometry source.\nBeta\nar_geometry_source_get_format\nGets the vertex format for data in a geometry source’s buffer.\nBeta\nar_geometry_source_get_offset\nGets the offset, in bytes, from the beginning of a geometry source’s buffer.\nBeta\nar_geometry_source_get_stride\nGets the number of bytes between one vector and another in a geometry source’s buffer.\nBeta\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nBeta\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nBeta\nar_pose_t\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta"
  },
  {
    "title": "ar_geometry_source_get_count | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103576-ar_geometry_source_get_count",
    "html": "See Also\nGeometry\nar_geometry_primitive_type_t\nThe kinds of geometry primitives that a geometry element can contain.\nBeta\nar_geometry_element_get_buffer\nGets a Metal buffer that contains index data that defines the geometry of an object.\nBeta\nar_geometry_element_get_bytes_per_index\nGets the number of bytes that represent an index value.\nBeta\nar_geometry_element_get_count\nGets the number of primitives in the Metal buffer for a geometry element.\nBeta\nar_geometry_element_get_index_count_per_primitive\nGets the number of indices for each primitive.\nBeta\nar_geometry_element_get_primitive_type\nGets the kind of primitive, lines or triangles, that a geometry element contains.\nBeta\nar_geometry_source_get_buffer\nGets a Metal buffer that contains per-vector data for a geometry source.\nBeta\nar_geometry_source_get_components_per_vector\nGets the number of scalar components in each vector in a geometry source.\nBeta\nar_geometry_source_get_format\nGets the vertex format for data in a geometry source’s buffer.\nBeta\nar_geometry_source_get_offset\nGets the offset, in bytes, from the beginning of a geometry source’s buffer.\nBeta\nar_geometry_source_get_stride\nGets the number of bytes between one vector and another in a geometry source’s buffer.\nBeta\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nBeta\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nBeta\nar_pose_t\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta"
  },
  {
    "title": "ar_geometry_source_get_components_per_vector | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103575-ar_geometry_source_get_component",
    "html": "See Also\nGeometry\nar_geometry_primitive_type_t\nThe kinds of geometry primitives that a geometry element can contain.\nBeta\nar_geometry_element_get_buffer\nGets a Metal buffer that contains index data that defines the geometry of an object.\nBeta\nar_geometry_element_get_bytes_per_index\nGets the number of bytes that represent an index value.\nBeta\nar_geometry_element_get_count\nGets the number of primitives in the Metal buffer for a geometry element.\nBeta\nar_geometry_element_get_index_count_per_primitive\nGets the number of indices for each primitive.\nBeta\nar_geometry_element_get_primitive_type\nGets the kind of primitive, lines or triangles, that a geometry element contains.\nBeta\nar_geometry_source_get_buffer\nGets a Metal buffer that contains per-vector data for a geometry source.\nBeta\nar_geometry_source_get_count\nGets the number of vectors in a geometry source.\nBeta\nar_geometry_source_get_format\nGets the vertex format for data in a geometry source’s buffer.\nBeta\nar_geometry_source_get_offset\nGets the offset, in bytes, from the beginning of a geometry source’s buffer.\nBeta\nar_geometry_source_get_stride\nGets the number of bytes between one vector and another in a geometry source’s buffer.\nBeta\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nBeta\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nBeta\nar_pose_t\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_provider_set_update_handler_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295906-ar_scene_reconstruction_provider",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_pose_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_pose_t",
    "html": "See Also\nGeometry\nar_geometry_primitive_type_t\nThe kinds of geometry primitives that a geometry element can contain.\nBeta\nar_geometry_element_get_buffer\nGets a Metal buffer that contains index data that defines the geometry of an object.\nBeta\nar_geometry_element_get_bytes_per_index\nGets the number of bytes that represent an index value.\nBeta\nar_geometry_element_get_count\nGets the number of primitives in the Metal buffer for a geometry element.\nBeta\nar_geometry_element_get_index_count_per_primitive\nGets the number of indices for each primitive.\nBeta\nar_geometry_element_get_primitive_type\nGets the kind of primitive, lines or triangles, that a geometry element contains.\nBeta\nar_geometry_source_get_buffer\nGets a Metal buffer that contains per-vector data for a geometry source.\nBeta\nar_geometry_source_get_components_per_vector\nGets the number of scalar components in each vector in a geometry source.\nBeta\nar_geometry_source_get_count\nGets the number of vectors in a geometry source.\nBeta\nar_geometry_source_get_format\nGets the vertex format for data in a geometry source’s buffer.\nBeta\nar_geometry_source_get_offset\nGets the offset, in bytes, from the beginning of a geometry source’s buffer.\nBeta\nar_geometry_source_get_stride\nGets the number of bytes between one vector and another in a geometry source’s buffer.\nBeta\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nBeta\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nBeta"
  },
  {
    "title": "ar_geometry_source_get_stride | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103579-ar_geometry_source_get_stride",
    "html": "See Also\nGeometry\nar_geometry_primitive_type_t\nThe kinds of geometry primitives that a geometry element can contain.\nBeta\nar_geometry_element_get_buffer\nGets a Metal buffer that contains index data that defines the geometry of an object.\nBeta\nar_geometry_element_get_bytes_per_index\nGets the number of bytes that represent an index value.\nBeta\nar_geometry_element_get_count\nGets the number of primitives in the Metal buffer for a geometry element.\nBeta\nar_geometry_element_get_index_count_per_primitive\nGets the number of indices for each primitive.\nBeta\nar_geometry_element_get_primitive_type\nGets the kind of primitive, lines or triangles, that a geometry element contains.\nBeta\nar_geometry_source_get_buffer\nGets a Metal buffer that contains per-vector data for a geometry source.\nBeta\nar_geometry_source_get_components_per_vector\nGets the number of scalar components in each vector in a geometry source.\nBeta\nar_geometry_source_get_count\nGets the number of vectors in a geometry source.\nBeta\nar_geometry_source_get_format\nGets the vertex format for data in a geometry source’s buffer.\nBeta\nar_geometry_source_get_offset\nGets the offset, in bytes, from the beginning of a geometry source’s buffer.\nBeta\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nBeta\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nBeta\nar_pose_t\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta"
  },
  {
    "title": "ar_mesh_classification_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_classification_t",
    "html": "Topics\nGetting architecture classifications\nar_mesh_classification_ceiling\nA ceiling.\nar_mesh_classification_door\nA door.\nar_mesh_classification_floor\nA floor.\nar_mesh_classification_stairs\nA set of stairs.\nar_mesh_classification_wall\nA wall.\nar_mesh_classification_window\nA window.\nGetting furniture classifications\nar_mesh_classification_bed\nA bed.\nar_mesh_classification_cabinet\nA cabinet.\nar_mesh_classification_home_appliance\nA home appliance.\nar_mesh_classification_seat\nA seat.\nar_mesh_classification_table\nA table.\nGetting decoration classifications\nar_mesh_classification_plant\nA plant.\nar_mesh_classification_tv\nA television.\nGetting unknown classifications\nar_mesh_classification_none\nSee Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_authorization_results_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_results_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_mesh_geometry_get_faces | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103588-ar_mesh_geometry_get_faces",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_world_tracking_add_anchor_completion_handler_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_tracking_add_anchor_completion_handler_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_mesh_geometry_get_classification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103587-ar_mesh_geometry_get_classificat",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_world_tracking_remove_anchor_completion_handler_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_world_tracking_remove_anchor_completion_handler_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta"
  },
  {
    "title": "ar_image_anchors_enumerator_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_image_anchors_enumerator_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_mesh_anchors_get_count | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103585-ar_mesh_anchors_get_count",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ARRaycastTargetAlignmentVertical | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycasttargetalignment/arraycasttargetalignmentvertical",
    "html": "See Also\nChoosing a Target Alignment\nARRaycastTargetAlignmentAny\nThe case that indicates a target may be aligned in any way with respect to gravity.\nARRaycastTargetAlignmentHorizontal\nThe case that indicates a target is aligned horizontally with respect to gravity."
  },
  {
    "title": "ar_image_anchors_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_image_anchors_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ARRaycastTargetExistingPlaneGeometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycasttarget/arraycasttargetexistingplanegeometry",
    "html": "See Also\nTargets\nARRaycastTargetEstimatedPlane\nA raycast target that specifies nonplanar surfaces, or planes about which ARKit can only estimate.\nARRaycastTargetExistingPlaneInfinite\nA raycast target that specifies a detected plane, regardless of its size and shape."
  },
  {
    "title": "ar_error_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_error_t",
    "html": "See Also\nErrors\nar_error_code_t\nCodes that identify errors in ARKit.\nBeta\nar_error_domain\nA string that indicates the error domain in Core Foundation.\nBeta\nar_session_error_code_t\nThe error codes for ARKit sessions.\nBeta\nar_world_tracking_error_code_t\nThe error codes for errors that world tracking providers throw.\nBeta\nar_error_get_error_code\nGets the error code associated with an error.\nBeta\nar_error_copy_cf_error\nCopies a reference to a Core Foundation error object that represents the specified ARKit error.\nBeta"
  },
  {
    "title": "ar_session_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_session_t",
    "html": "See Also\nSessions\nar_session_create\nCreates a new session.\nBeta\nar_session_query_authorization_results\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_query_authorization_results_f\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_request_authorization\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_request_authorization_f\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_run\nRuns a session with the data providers you supply.\nBeta\nar_session_set_authorization_update_handler\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_authorization_update_handler_f\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_data_provider_state_change_handler\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_set_data_provider_state_change_handler_f\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_data_provider_state_change_handler_t\nA handler for receiving updates to data provider states.\nBeta\nar_session_stop\nStops a session.\nBeta"
  },
  {
    "title": "ar_session_request_authorization_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295912-ar_session_request_authorization",
    "html": "See Also\nSessions\nar_session_t\nThe main entry point for receiving data from ARKit.\nBeta\nar_session_create\nCreates a new session.\nBeta\nar_session_query_authorization_results\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_query_authorization_results_f\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_request_authorization\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_run\nRuns a session with the data providers you supply.\nBeta\nar_session_set_authorization_update_handler\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_authorization_update_handler_f\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_data_provider_state_change_handler\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_set_data_provider_state_change_handler_f\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_data_provider_state_change_handler_t\nA handler for receiving updates to data provider states.\nBeta\nar_session_stop\nStops a session.\nBeta"
  },
  {
    "title": "ar_session_set_authorization_update_handler | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131549-ar_session_set_authorization_upd",
    "html": "See Also\nSessions\nar_session_t\nThe main entry point for receiving data from ARKit.\nBeta\nar_session_create\nCreates a new session.\nBeta\nar_session_query_authorization_results\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_query_authorization_results_f\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_request_authorization\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_request_authorization_f\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_run\nRuns a session with the data providers you supply.\nBeta\nar_session_set_authorization_update_handler_f\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_data_provider_state_change_handler\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_set_data_provider_state_change_handler_f\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_data_provider_state_change_handler_t\nA handler for receiving updates to data provider states.\nBeta\nar_session_stop\nStops a session.\nBeta"
  },
  {
    "title": "ar_session_run | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131548-ar_session_run",
    "html": "See Also\nSessions\nar_session_t\nThe main entry point for receiving data from ARKit.\nBeta\nar_session_create\nCreates a new session.\nBeta\nar_session_query_authorization_results\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_query_authorization_results_f\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_request_authorization\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_request_authorization_f\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_set_authorization_update_handler\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_authorization_update_handler_f\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_data_provider_state_change_handler\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_set_data_provider_state_change_handler_f\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_data_provider_state_change_handler_t\nA handler for receiving updates to data provider states.\nBeta\nar_session_stop\nStops a session.\nBeta"
  },
  {
    "title": "ar_authorization_status_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_status_t",
    "html": "Topics\nGetting authorization states\nar_authorization_status_not_determined\nThe user hasn’t yet granted or denied permission.\nar_authorization_status_allowed\nThe user granted your app permission to use the associated kind of ARKit data.\nar_authorization_status_denied\nThe user denied your app permission to use the associated kind of ARKit data.\nSee Also\nAuthorization\nar_authorization_type_t\nThe authorization types you can request from ARKit.\nBeta\nar_authorization_result_get_authorization_type\nGets the authorization type associated with an authorization result.\nBeta\nar_authorization_result_get_status\nGets the authorization status associated with an authorization result.\nBeta\nar_authorization_results_enumerate_results\nEnumerates a collection of authorization results.\nBeta\nar_authorization_results_enumerate_results_f\nEnumerates a collection of authorization results.\nBeta\nar_authorization_results_get_count\nGets the number of authorization results in a collection.\nBeta\nar_authorization_status_allowed\nThe user granted your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_denied\nThe user denied your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_not_determined\nThe user hasn’t yet granted or denied permission.\nBeta"
  },
  {
    "title": "ar_reference_image_get_physical_width | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103502-ar_reference_image_get_physical_",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_image_tracking_provider_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103491-ar_image_tracking_provider_creat",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_image_tracking_provider_get_required_authorization_type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4108264-ar_image_tracking_provider_get_r",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_image_tracking_configuration_add_reference_images | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4139331-ar_image_tracking_configuration_",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_reference_images_enumerate_images_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295898-ar_reference_images_enumerate_im",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_image_tracking_provider_is_supported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4218657-ar_image_tracking_provider_is_su",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_reference_images_enumerate_images | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4139336-ar_reference_images_enumerate_im",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_reference_image_set_name | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103504-ar_reference_image_set_name",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_reference_images_enumerator_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_reference_images_enumerator_t",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_reference_images_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4139335-ar_reference_images_create",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_reference_images_add_image | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4139333-ar_reference_images_add_image",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_reference_images_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_reference_images_t",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_reference_image_get_physical_height | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103501-ar_reference_image_get_physical_",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_retain | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103517-ar_retain",
    "html": "See Also\nMemory management\nar_release\nReleases a reference count on an ARKit object.\nBeta"
  },
  {
    "title": "ar_image_tracking_configuration_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_image_tracking_configuration_t",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_image_anchors_enumerate_anchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103486-ar_image_anchors_enumerate_ancho",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_reference_images_add_images | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4139334-ar_reference_images_add_images",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_reference_image_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_reference_image_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_data_providers_get_count | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4169963-ar_data_providers_get_count",
    "html": "See Also\nData providers\nar_data_provider_state_t\nThe possible states of a data provider.\nBeta\nar_data_provider_get_required_authorization_type\nThe kinds of authorization you need to use a particular data provider type.\nBeta\nar_data_provider_get_state\nGets the current status of data coming from this provider.\nBeta\nar_data_providers_enumerator_t\nA handler for enumerating a collection of data providers.\nBeta\nar_data_providers_t\nA collection of data providers.\nBeta\nar_data_providers_add_data_provider\nAdds a data provider to a collection.\nBeta\nar_data_providers_add_data_providers\nAdds multiple data providers to a collection.\nBeta\nar_data_providers_create\nCreates an empty collection of data providers.\nBeta\nar_data_providers_create_with_data_providers\nCreates a collection of data providers that contains the data providers you supply.\nBeta\nar_data_providers_enumerate_data_providers_f\nEnumerates a collection of data providers.\nBeta\nar_data_providers_enumerate_data_providers\nEnumerates a collection of data providers.\nBeta\nar_data_providers_remove_data_provider\nRemoves a data provider from a collection.\nBeta\nar_data_providers_remove_data_providers\nRemoves multiple data providers from a collection.\nBeta"
  },
  {
    "title": "ar_mesh_geometry_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_mesh_geometry_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_hand_tracking_update_handler_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_configuration_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131538-ar_scene_reconstruction_configur",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_mode_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_scene_reconstruction_mode_t",
    "html": "Topics\nScene reconstruction modes\nar_scene_reconstruction_mode_classification\nar_scene_reconstruction_mode_default\nSee Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_provider_is_supported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4218665-ar_scene_reconstruction_provider",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_geometry_primitive_type_line | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_geometry_primitive_type_t/ar_geometry_primitive_type_line",
    "html": "See Also\nPrimitive shapes\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_configuration_get_scene_reconstruction_mode | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131539-ar_scene_reconstruction_configur",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_configuration_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_scene_reconstruction_configuration_t",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_plane_classification_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_classification_t",
    "html": "Topics\nGetting known classifications\nar_plane_classification_ceiling\nA ceiling.\nar_plane_classification_door\nA door.\nar_plane_classification_floor\nA floor.\nar_plane_classification_seat\nA seat.\nar_plane_classification_table\nA table.\nar_plane_classification_wall\nA wall.\nar_plane_classification_window\nA window.\nGetting unknown classifications\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nSee Also\nPlane detection\nar_plane_alignment_t\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_plane_alignment_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_alignment_t",
    "html": "Topics\nGetting plane alignment\nar_plane_alignment_horizontal\nThe plane is positioned horizontally.\nar_plane_alignment_vertical\nThe plane is positioned vertically.\nar_plane_alignment_none\nNo plane alignment.\nSee Also\nPlane detection\nar_plane_classification_t\nThe kinds of object classification a plane anchor can have.\nBeta\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_status_not_available\nA plane classification is currently unavailable.\nBeta\nar_plane_classification_status_undetermined\nA plane classification hasn’t been determined yet.\nBeta\nar_plane_classification_status_unknown\nA plane classification isn’t one of the known classes.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta\nar_plane_anchor_get_alignment\nGets the alignment — horizontal or vertical — of a plane anchor relative to gravity.\nBeta\nar_plane_anchor_get_geometry\nGets the shape of a plane anchor.\nBeta\nar_plane_anchor_get_plane_classification\nGets the kind of real-world object that ARKit determined this plane anchor might be.\nBeta\nar_plane_anchors_enumerate_anchors\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_enumerate_anchors_f\nEnumerates a collection of plane anchors.\nBeta\nar_plane_anchors_get_count\nGets the number of plane anchors in the collection.\nBeta\nar_plane_detection_configuration_t\nBeta\nar_plane_detection_provider_create\nCreates a plane detection provider for the types of planes you want to detect.\nBeta\nar_plane_detection_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports plane detection providers.\nBeta\nar_plane_geometry_get_plane_extent\nGets the size of a plane.\nBeta\nar_plane_geometry_get_mesh_vertices\nGets the vertices in the mesh that describes a plane.\nBeta\nar_plane_geometry_get_mesh_faces\nGets the faces in the mesh that describes a plane.\nBeta\nar_plane_detection_provider_get_required_authorization_type\nGets the types of authorizations required to detect planes.\nBeta\nar_plane_detection_configuration_create\nCreates a plane detection configuration.\nBeta\nar_plane_detection_configuration_set_alignment\nSets the plane alignments that you want this provider to detect.\nBeta\nar_plane_detection_provider_set_update_handler\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_detection_provider_set_update_handler_f\nSets the handler for receiving plane detection updates.\nBeta\nar_plane_extent_t\nThe size of a plane.\nBeta\nar_plane_extent_get_height\nGets the height of a plane.\nBeta\nar_plane_extent_get_plane_anchor_from_plane_extent_transform\nGet the transform of a plane anchor from its extent transform.\nBeta\nar_plane_extent_get_width\nGets the width of a plane.\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_provider_set_update_handler | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131542-ar_scene_reconstruction_provider",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_plane_classification_ceiling | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_classification_t/ar_plane_classification_ceiling",
    "html": "See Also\nGetting known classifications\nar_plane_classification_door\nA door.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta"
  },
  {
    "title": "ar_geometry_source_get_format | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103577-ar_geometry_source_get_format",
    "html": "See Also\nGeometry\nar_geometry_primitive_type_t\nThe kinds of geometry primitives that a geometry element can contain.\nBeta\nar_geometry_element_get_buffer\nGets a Metal buffer that contains index data that defines the geometry of an object.\nBeta\nar_geometry_element_get_bytes_per_index\nGets the number of bytes that represent an index value.\nBeta\nar_geometry_element_get_count\nGets the number of primitives in the Metal buffer for a geometry element.\nBeta\nar_geometry_element_get_index_count_per_primitive\nGets the number of indices for each primitive.\nBeta\nar_geometry_element_get_primitive_type\nGets the kind of primitive, lines or triangles, that a geometry element contains.\nBeta\nar_geometry_source_get_buffer\nGets a Metal buffer that contains per-vector data for a geometry source.\nBeta\nar_geometry_source_get_components_per_vector\nGets the number of scalar components in each vector in a geometry source.\nBeta\nar_geometry_source_get_count\nGets the number of vectors in a geometry source.\nBeta\nar_geometry_source_get_offset\nGets the offset, in bytes, from the beginning of a geometry source’s buffer.\nBeta\nar_geometry_source_get_stride\nGets the number of bytes between one vector and another in a geometry source’s buffer.\nBeta\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nBeta\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nBeta\nar_pose_t\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_provider_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103596-ar_scene_reconstruction_provider",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_configuration_set_scene_reconstruction_mode | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131540-ar_scene_reconstruction_configur",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_plane_classification_door | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_plane_classification_t/ar_plane_classification_door",
    "html": "See Also\nGetting known classifications\nar_plane_classification_ceiling\nA ceiling.\nBeta\nar_plane_classification_floor\nA floor.\nBeta\nar_plane_classification_seat\nA seat.\nBeta\nar_plane_classification_table\nA table.\nBeta\nar_plane_classification_wall\nA wall.\nBeta\nar_plane_classification_window\nA window.\nBeta"
  },
  {
    "title": "ar_scene_reconstruction_provider_get_required_authorization_type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4108274-ar_scene_reconstruction_provider",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_geometry_source_get_offset | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103578-ar_geometry_source_get_offset",
    "html": "See Also\nGeometry\nar_geometry_primitive_type_t\nThe kinds of geometry primitives that a geometry element can contain.\nBeta\nar_geometry_element_get_buffer\nGets a Metal buffer that contains index data that defines the geometry of an object.\nBeta\nar_geometry_element_get_bytes_per_index\nGets the number of bytes that represent an index value.\nBeta\nar_geometry_element_get_count\nGets the number of primitives in the Metal buffer for a geometry element.\nBeta\nar_geometry_element_get_index_count_per_primitive\nGets the number of indices for each primitive.\nBeta\nar_geometry_element_get_primitive_type\nGets the kind of primitive, lines or triangles, that a geometry element contains.\nBeta\nar_geometry_source_get_buffer\nGets a Metal buffer that contains per-vector data for a geometry source.\nBeta\nar_geometry_source_get_components_per_vector\nGets the number of scalar components in each vector in a geometry source.\nBeta\nar_geometry_source_get_count\nGets the number of vectors in a geometry source.\nBeta\nar_geometry_source_get_format\nGets the vertex format for data in a geometry source’s buffer.\nBeta\nar_geometry_source_get_stride\nGets the number of bytes between one vector and another in a geometry source’s buffer.\nBeta\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nBeta\nar_geometry_primitive_type_triangle\nThree vertices that connect to form a triangle.\nBeta\nar_pose_t\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta"
  },
  {
    "title": "ar_geometry_primitive_type_triangle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_geometry_primitive_type_t/ar_geometry_primitive_type_triangle",
    "html": "See Also\nPrimitive shapes\nar_geometry_primitive_type_line\nTwo vertices that connect to form a line.\nBeta"
  },
  {
    "title": "ar_session_set_data_provider_state_change_handler_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295914-ar_session_set_data_provider_sta",
    "html": "See Also\nSessions\nar_session_t\nThe main entry point for receiving data from ARKit.\nBeta\nar_session_create\nCreates a new session.\nBeta\nar_session_query_authorization_results\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_query_authorization_results_f\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_request_authorization\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_request_authorization_f\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_run\nRuns a session with the data providers you supply.\nBeta\nar_session_set_authorization_update_handler\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_authorization_update_handler_f\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_data_provider_state_change_handler\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_data_provider_state_change_handler_t\nA handler for receiving updates to data provider states.\nBeta\nar_session_stop\nStops a session.\nBeta"
  },
  {
    "title": "ar_session_data_provider_state_change_handler_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_session_data_provider_state_change_handler_t",
    "html": "See Also\nSessions\nar_session_t\nThe main entry point for receiving data from ARKit.\nBeta\nar_session_create\nCreates a new session.\nBeta\nar_session_query_authorization_results\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_query_authorization_results_f\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_request_authorization\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_request_authorization_f\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_run\nRuns a session with the data providers you supply.\nBeta\nar_session_set_authorization_update_handler\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_authorization_update_handler_f\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_data_provider_state_change_handler\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_set_data_provider_state_change_handler_f\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_stop\nStops a session.\nBeta"
  },
  {
    "title": "ar_session_stop | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4278278-ar_session_stop",
    "html": "See Also\nSessions\nar_session_t\nThe main entry point for receiving data from ARKit.\nBeta\nar_session_create\nCreates a new session.\nBeta\nar_session_query_authorization_results\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_query_authorization_results_f\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_request_authorization\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_request_authorization_f\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_run\nRuns a session with the data providers you supply.\nBeta\nar_session_set_authorization_update_handler\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_authorization_update_handler_f\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_data_provider_state_change_handler\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_set_data_provider_state_change_handler_f\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_data_provider_state_change_handler_t\nA handler for receiving updates to data provider states.\nBeta"
  },
  {
    "title": "ar_authorization_result_get_authorization_type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103423-ar_authorization_result_get_auth",
    "html": "See Also\nAuthorization\nar_authorization_status_t\nThe authorization states for a type of ARKit data.\nBeta\nar_authorization_type_t\nThe authorization types you can request from ARKit.\nBeta\nar_authorization_result_get_status\nGets the authorization status associated with an authorization result.\nBeta\nar_authorization_results_enumerate_results\nEnumerates a collection of authorization results.\nBeta\nar_authorization_results_enumerate_results_f\nEnumerates a collection of authorization results.\nBeta\nar_authorization_results_get_count\nGets the number of authorization results in a collection.\nBeta\nar_authorization_status_allowed\nThe user granted your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_denied\nThe user denied your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_not_determined\nThe user hasn’t yet granted or denied permission.\nBeta"
  },
  {
    "title": "ar_authorization_type_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_type_t",
    "html": "Topics\nRequesting authorization\nar_authorization_type_hand_tracking\nThe authorization for access to detailed hand-tracking data.\nar_authorization_type_world_sensing\nThe authorization for access to plane detection, scene reconstruction, and image tracking.\nar_authorization_type_none\nNo authorization type.\nSee Also\nAuthorization\nar_authorization_status_t\nThe authorization states for a type of ARKit data.\nBeta\nar_authorization_result_get_authorization_type\nGets the authorization type associated with an authorization result.\nBeta\nar_authorization_result_get_status\nGets the authorization status associated with an authorization result.\nBeta\nar_authorization_results_enumerate_results\nEnumerates a collection of authorization results.\nBeta\nar_authorization_results_enumerate_results_f\nEnumerates a collection of authorization results.\nBeta\nar_authorization_results_get_count\nGets the number of authorization results in a collection.\nBeta\nar_authorization_status_allowed\nThe user granted your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_denied\nThe user denied your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_not_determined\nThe user hasn’t yet granted or denied permission.\nBeta"
  },
  {
    "title": "ar_mesh_anchors_enumerate_anchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103583-ar_mesh_anchors_enumerate_anchor",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ar_authorization_results_enumerate_results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103427-ar_authorization_results_enumera",
    "html": "See Also\nAuthorization\nar_authorization_status_t\nThe authorization states for a type of ARKit data.\nBeta\nar_authorization_type_t\nThe authorization types you can request from ARKit.\nBeta\nar_authorization_result_get_authorization_type\nGets the authorization type associated with an authorization result.\nBeta\nar_authorization_result_get_status\nGets the authorization status associated with an authorization result.\nBeta\nar_authorization_results_enumerate_results_f\nEnumerates a collection of authorization results.\nBeta\nar_authorization_results_get_count\nGets the number of authorization results in a collection.\nBeta\nar_authorization_status_allowed\nThe user granted your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_denied\nThe user denied your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_not_determined\nThe user hasn’t yet granted or denied permission.\nBeta"
  },
  {
    "title": "ar_authorization_result_get_status | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103424-ar_authorization_result_get_stat",
    "html": "See Also\nAuthorization\nar_authorization_status_t\nThe authorization states for a type of ARKit data.\nBeta\nar_authorization_type_t\nThe authorization types you can request from ARKit.\nBeta\nar_authorization_result_get_authorization_type\nGets the authorization type associated with an authorization result.\nBeta\nar_authorization_results_enumerate_results\nEnumerates a collection of authorization results.\nBeta\nar_authorization_results_enumerate_results_f\nEnumerates a collection of authorization results.\nBeta\nar_authorization_results_get_count\nGets the number of authorization results in a collection.\nBeta\nar_authorization_status_allowed\nThe user granted your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_denied\nThe user denied your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_not_determined\nThe user hasn’t yet granted or denied permission.\nBeta"
  },
  {
    "title": "ar_hand_tracking_update_handler_function_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_tracking_update_handler_function_t",
    "html": "See Also\nType aliases\nar_anchor_t\nThe identity, location, and orientation of an object in world space.\nBeta\nar_authorization_result_t\nAn authorization result.\nBeta\nar_authorization_results_enumerator_t\nA handler for enumerating a collection of authorization results.\nBeta\nar_authorization_results_handler_t\nA handler to call upon completion of an authorization request.\nBeta\nar_authorization_results_t\nA collection of authorization results.\nBeta\nar_authorization_update_handler_t\nA handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_data_provider_t\nA source of live data from ARKit.\nBeta\nar_geometry_element_t\nA container for vertex indices of lines or triangles.\nBeta\nar_geometry_source_t\nA container for geometrical vector data.\nBeta\nar_hand_anchor_t\nA hand’s position in a person’s surroundings.\nBeta\nar_hand_tracking_provider_t\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_update_handler_t\nA handler for receiving updates to hand anchors.\nBeta\nar_image_anchor_t\nA 2D image’s position in a person’s surroundings.\nBeta\nar_image_anchors_t\nA collection of image anchors.\nBeta\nar_image_anchors_enumerator_t\nA handler for enumerating a collection of image anchors.\nBeta\nar_image_tracking_provider_t\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nar_image_tracking_update_handler_t\nA handler for receiving updates to image anchors.\nBeta\nar_mesh_anchor_t\nA surface’s position in a person’s surroundings.\nBeta\nar_mesh_anchors_t\nA collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerator_t\nA handler for enumerating a collection of mesh anchors.\nBeta\nar_mesh_geometry_t\nThe shapes that make up a mesh anchor.\nBeta\nar_plane_anchor_t\nAn anchor that represents horizontal and vertical planes.\nBeta\nar_plane_anchors_t\nA collection of plane anchors.\nBeta\nar_plane_anchors_enumerator_t\nA handler for enumerating a collection of plane anchors.\nBeta\nar_plane_detection_provider_t\nA source of live data about planes in a person’s surroundings.\nBeta\nar_plane_detection_update_handler_t\nA handler for receiving updates to plane anchors.\nBeta\nar_plane_geometry_t\nThe geometry of a plane anchor.\nBeta\nar_reference_image_t\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_t\nA source of live data about the shape of a person’s surroundings.\nBeta\nar_scene_reconstruction_update_handler_t\nA handler for receiving updates to mesh anchors.\nBeta\nar_trackable_anchor_t\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nar_world_anchor_t\nA fixed location in a person’s surroundings.\nBeta\nar_world_anchors_t\nA collection of world anchors.\nBeta\nar_world_anchors_enumerator_t\nA handler for enumerating a collection of world anchors.\nBeta\nar_world_tracking_provider_t\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nar_authorization_results_enumerator_function_t\nBeta\nar_authorization_results_handler_function_t\nBeta\nar_authorization_update_handler_function_t\nBeta\nar_data_providers_enumerator_function_t\nBeta\nar_image_anchors_enumerator_function_t\nBeta\nar_image_tracking_update_handler_function_t\nBeta\nar_mesh_anchors_enumerator_function_t\nBeta\nar_plane_anchors_enumerator_function_t\nBeta\nar_plane_detection_update_handler_function_t\nBeta\nar_reference_images_enumerator_function_t\nBeta\nar_scene_reconstruction_update_handler_function_t\nBeta\nar_session_data_provider_state_change_handler_function_t\nBeta\nar_world_anchors_enumerator_function_t\nBeta\nar_world_tracking_add_anchor_completion_handler_function_t\nBeta\nar_world_tracking_anchor_update_handler_function_t\nBeta\nar_world_tracking_remove_anchor_completion_handler_function_t\nBeta"
  },
  {
    "title": "ar_authorization_results_enumerate_results_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295888-ar_authorization_results_enumera",
    "html": "See Also\nAuthorization\nar_authorization_status_t\nThe authorization states for a type of ARKit data.\nBeta\nar_authorization_type_t\nThe authorization types you can request from ARKit.\nBeta\nar_authorization_result_get_authorization_type\nGets the authorization type associated with an authorization result.\nBeta\nar_authorization_result_get_status\nGets the authorization status associated with an authorization result.\nBeta\nar_authorization_results_enumerate_results\nEnumerates a collection of authorization results.\nBeta\nar_authorization_results_get_count\nGets the number of authorization results in a collection.\nBeta\nar_authorization_status_allowed\nThe user granted your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_denied\nThe user denied your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_not_determined\nThe user hasn’t yet granted or denied permission.\nBeta"
  },
  {
    "title": "ar_authorization_results_get_count | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103429-ar_authorization_results_get_cou",
    "html": "See Also\nAuthorization\nar_authorization_status_t\nThe authorization states for a type of ARKit data.\nBeta\nar_authorization_type_t\nThe authorization types you can request from ARKit.\nBeta\nar_authorization_result_get_authorization_type\nGets the authorization type associated with an authorization result.\nBeta\nar_authorization_result_get_status\nGets the authorization status associated with an authorization result.\nBeta\nar_authorization_results_enumerate_results\nEnumerates a collection of authorization results.\nBeta\nar_authorization_results_enumerate_results_f\nEnumerates a collection of authorization results.\nBeta\nar_authorization_status_allowed\nThe user granted your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_denied\nThe user denied your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_not_determined\nThe user hasn’t yet granted or denied permission.\nBeta"
  },
  {
    "title": "ar_authorization_status_denied | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_status_t/ar_authorization_status_denied",
    "html": "See Also\nGetting authorization states\nar_authorization_status_not_determined\nThe user hasn’t yet granted or denied permission.\nBeta\nar_authorization_status_allowed\nThe user granted your app permission to use the associated kind of ARKit data.\nBeta"
  },
  {
    "title": "ar_authorization_status_allowed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_status_t/ar_authorization_status_allowed",
    "html": "See Also\nGetting authorization states\nar_authorization_status_not_determined\nThe user hasn’t yet granted or denied permission.\nBeta\nar_authorization_status_denied\nThe user denied your app permission to use the associated kind of ARKit data.\nBeta"
  },
  {
    "title": "ar_authorization_status_not_determined | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_authorization_status_t/ar_authorization_status_not_determined",
    "html": "See Also\nGetting authorization states\nar_authorization_status_allowed\nThe user granted your app permission to use the associated kind of ARKit data.\nBeta\nar_authorization_status_denied\nThe user denied your app permission to use the associated kind of ARKit data.\nBeta"
  },
  {
    "title": "ar_image_anchor_get_estimated_scale_factor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103482-ar_image_anchor_get_estimated_sc",
    "html": "See Also\nImage tracking\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_mesh_anchors_enumerate_anchors_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295904-ar_mesh_anchors_enumerate_anchor",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_normals\nGets the normals of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "initWithArchiveURL:error: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/2977513-initwitharchiveurl",
    "html": "Parameters\nurl\n\nThe local file URL containing the reference object to load.\n\nerror\n\nA pointer to an NSError object. If this method returns nil, check this pointer for an error describing the failure.\n\nReturn Value\n\nThe reference object contained in the file, or nil if the file could not be loaded.\n\nDiscussion\n\nTo use the object for detection in a world-tracking AR session, add it to the detectionObjects set in your session configuration.\n\nSee Also\nLoading Reference Objects\n+ referenceObjectsInGroupNamed:bundle:\nLoads all reference objects in the specified AR Resource Group in your Xcode project's asset catalog."
  },
  {
    "title": "ar_image_anchor_get_reference_image | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103484-ar_image_anchor_get_reference_im",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_enumerate_anchors_f\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ar_image_anchors_enumerate_anchors_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295894-ar_image_anchors_enumerate_ancho",
    "html": "See Also\nImage tracking\nar_image_anchor_get_estimated_scale_factor\nGets the estimated scale factor between the tracked image’s physical size and the reference image’s size.\nBeta\nar_image_anchor_get_reference_image\nGets the reference image that this image anchor tracks.\nBeta\nar_image_anchors_enumerate_anchors\nEnumerates a collection of image anchors.\nBeta\nar_image_anchors_get_count\nGets the number of image anchors in the collection.\nBeta\nar_image_tracking_provider_create\nCreates an image tracking provider that tracks the reference images you supply.\nBeta\nar_image_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports image tracking providers.\nBeta\nar_image_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track images.\nBeta\nar_image_tracking_provider_set_update_handler\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_provider_set_update_handler_f\nSets the handler for receiving image tracking updates.\nBeta\nar_image_tracking_configuration_add_reference_images\nAdds reference images to the set to be tracked.\nBeta\nar_image_tracking_configuration_create\nCreates an image tracking configuration.\nBeta\nar_image_tracking_configuration_t\nBeta\nar_reference_images_t\nA collection of reference images.\nBeta\nar_reference_images_enumerator_t\nA handler for enumerating a collection of reference images.\nBeta\nar_reference_images_add_image\nAdds a reference image to a collection.\nBeta\nar_reference_images_add_images\nAdds multiple reference images to a collection.\nBeta\nar_reference_images_create\nCreates an empty collection of reference images.\nBeta\nar_reference_images_enumerate_images\nEnumerates a collection of reference images.\nBeta\nar_reference_images_enumerate_images_f\nEnumerates a collection of reference images.\nBeta\nar_reference_images_get_count\nGets the number of reference images in a collection.\nBeta\nar_reference_images_load_reference_images_in_group\nCreates multiple reference images based on their group name in an asset catalog.\nBeta\nar_reference_image_create_from_cgimage\nCreates a reference image from a Core Graphics image.\nBeta\nar_reference_image_create_from_pixel_buffer\nCreates a reference image from a pixel buffer.\nBeta\nar_reference_image_get_name\nGets the name of a reference image.\nBeta\nar_reference_image_set_name\nSets the name of a reference image.\nBeta\nar_reference_image_get_physical_height\nGets the height, in meters, of a reference image in the real world.\nBeta\nar_reference_image_get_physical_width\nGets the height, in meters, of a reference image in the real world.\nBeta"
  },
  {
    "title": "ARFrameSemanticBodyDetection | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframesemantics/arframesemanticbodydetection",
    "html": "Discussion\n\nWhen you set this option in your configuration's frameSemantics property, ARKit describes the joint positions of a body it detects in the camera image, using normalized coordinates. See detectedBody for more information."
  },
  {
    "title": "ar_data_providers_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_data_providers_t",
    "html": "See Also\nData providers\nar_data_provider_state_t\nThe possible states of a data provider.\nBeta\nar_data_provider_get_required_authorization_type\nThe kinds of authorization you need to use a particular data provider type.\nBeta\nar_data_provider_get_state\nGets the current status of data coming from this provider.\nBeta\nar_data_providers_enumerator_t\nA handler for enumerating a collection of data providers.\nBeta\nar_data_providers_add_data_provider\nAdds a data provider to a collection.\nBeta\nar_data_providers_add_data_providers\nAdds multiple data providers to a collection.\nBeta\nar_data_providers_create\nCreates an empty collection of data providers.\nBeta\nar_data_providers_create_with_data_providers\nCreates a collection of data providers that contains the data providers you supply.\nBeta\nar_data_providers_enumerate_data_providers_f\nEnumerates a collection of data providers.\nBeta\nar_data_providers_get_count\nGets the number of data providers in a collection.\nBeta\nar_data_providers_enumerate_data_providers\nEnumerates a collection of data providers.\nBeta\nar_data_providers_remove_data_provider\nRemoves a data provider from a collection.\nBeta\nar_data_providers_remove_data_providers\nRemoves multiple data providers from a collection.\nBeta"
  },
  {
    "title": "ARFrameSemanticPersonSegmentation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframesemantics/arframesemanticpersonsegmentation",
    "html": "Discussion\n\nThe ARFrameSemanticPersonSegmentation frame semantic specifies that any person ARKit detects in the camera feed occludes virtual content, regardless of the person's depth in the scene.\n\nWhen this option is enabled, ARKit sets the estimatedDepthData and segmentationBuffer properties to serve as a foundation for people occlusion. The standard renderers (ARView, and ARSCNView) use those properties to implement people occlusion for you. See frameSemantics for more information.\n\nSee Also\nOccluding Virtual Content with People\nARFrameSemanticPersonSegmentationWithDepth\nAn option that indicates that people occlude your app's virtual content depending on depth."
  },
  {
    "title": "ar_data_providers_add_data_provider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4169957-ar_data_providers_add_data_provi",
    "html": "See Also\nData providers\nar_data_provider_state_t\nThe possible states of a data provider.\nBeta\nar_data_provider_get_required_authorization_type\nThe kinds of authorization you need to use a particular data provider type.\nBeta\nar_data_provider_get_state\nGets the current status of data coming from this provider.\nBeta\nar_data_providers_enumerator_t\nA handler for enumerating a collection of data providers.\nBeta\nar_data_providers_t\nA collection of data providers.\nBeta\nar_data_providers_add_data_providers\nAdds multiple data providers to a collection.\nBeta\nar_data_providers_create\nCreates an empty collection of data providers.\nBeta\nar_data_providers_create_with_data_providers\nCreates a collection of data providers that contains the data providers you supply.\nBeta\nar_data_providers_enumerate_data_providers_f\nEnumerates a collection of data providers.\nBeta\nar_data_providers_get_count\nGets the number of data providers in a collection.\nBeta\nar_data_providers_enumerate_data_providers\nEnumerates a collection of data providers.\nBeta\nar_data_providers_remove_data_provider\nRemoves a data provider from a collection.\nBeta\nar_data_providers_remove_data_providers\nRemoves multiple data providers from a collection.\nBeta"
  },
  {
    "title": "ar_data_providers_enumerator_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_data_providers_enumerator_t",
    "html": "See Also\nData providers\nar_data_provider_state_t\nThe possible states of a data provider.\nBeta\nar_data_provider_get_required_authorization_type\nThe kinds of authorization you need to use a particular data provider type.\nBeta\nar_data_provider_get_state\nGets the current status of data coming from this provider.\nBeta\nar_data_providers_t\nA collection of data providers.\nBeta\nar_data_providers_add_data_provider\nAdds a data provider to a collection.\nBeta\nar_data_providers_add_data_providers\nAdds multiple data providers to a collection.\nBeta\nar_data_providers_create\nCreates an empty collection of data providers.\nBeta\nar_data_providers_create_with_data_providers\nCreates a collection of data providers that contains the data providers you supply.\nBeta\nar_data_providers_enumerate_data_providers_f\nEnumerates a collection of data providers.\nBeta\nar_data_providers_get_count\nGets the number of data providers in a collection.\nBeta\nar_data_providers_enumerate_data_providers\nEnumerates a collection of data providers.\nBeta\nar_data_providers_remove_data_provider\nRemoves a data provider from a collection.\nBeta\nar_data_providers_remove_data_providers\nRemoves multiple data providers from a collection.\nBeta"
  },
  {
    "title": "ar_data_provider_state_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_data_provider_state_t",
    "html": "Topics\nGetting the status of a data provider\nar_data_provider_state_initialized\nThe data provider has been created.\nar_data_provider_state_running\nThe data provider is running.\nar_data_provider_state_stopped\nThe data provider is stopped.\nar_data_provider_state_paused\nThe data provider is paused.\nSee Also\nData providers\nar_data_provider_get_required_authorization_type\nThe kinds of authorization you need to use a particular data provider type.\nBeta\nar_data_provider_get_state\nGets the current status of data coming from this provider.\nBeta\nar_data_providers_enumerator_t\nA handler for enumerating a collection of data providers.\nBeta\nar_data_providers_t\nA collection of data providers.\nBeta\nar_data_providers_add_data_provider\nAdds a data provider to a collection.\nBeta\nar_data_providers_add_data_providers\nAdds multiple data providers to a collection.\nBeta\nar_data_providers_create\nCreates an empty collection of data providers.\nBeta\nar_data_providers_create_with_data_providers\nCreates a collection of data providers that contains the data providers you supply.\nBeta\nar_data_providers_enumerate_data_providers_f\nEnumerates a collection of data providers.\nBeta\nar_data_providers_get_count\nGets the number of data providers in a collection.\nBeta\nar_data_providers_enumerate_data_providers\nEnumerates a collection of data providers.\nBeta\nar_data_providers_remove_data_provider\nRemoves a data provider from a collection.\nBeta\nar_data_providers_remove_data_providers\nRemoves multiple data providers from a collection.\nBeta"
  },
  {
    "title": "ARFrameSemanticSceneDepth | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframesemantics/arframesemanticscenedepth",
    "html": "Discussion\n\nEnable this option on a world-tracking configuration (ARWorldTrackingConfiguration) to instruct ARKit to provide your app with the distance between the user's device and the real-world objects in the camera feed. ARKit samples this distance using the LiDAR scanner and provides the results through the sceneDepth property on the session's currentFrame.\n\nARKit creates this object from LiDAR readings at same time as the current frame. The data in sceneDepth reflects the distance from the device to real-world objects pictured in the frame's capturedImage. Alternatively, ARKit provides a ARFrameSemanticSmoothedSceneDepth property that minimizes the difference in LiDAR readings across frames.\n\nARKit supports scene depth only on LiDAR-capable devices, so call supportsFrameSemantics: to ensure device support before attempting to enable scene depth.\n\nSee Also\nAccessing Depth\nARFrameSemanticSmoothedSceneDepth\nAn option that provides the distance from the device to real-world objects, averaged across several frames."
  },
  {
    "title": "ar_data_provider_get_required_authorization_type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4278273-ar_data_provider_get_required_au",
    "html": "See Also\nData providers\nar_data_provider_state_t\nThe possible states of a data provider.\nBeta\nar_data_provider_get_state\nGets the current status of data coming from this provider.\nBeta\nar_data_providers_enumerator_t\nA handler for enumerating a collection of data providers.\nBeta\nar_data_providers_t\nA collection of data providers.\nBeta\nar_data_providers_add_data_provider\nAdds a data provider to a collection.\nBeta\nar_data_providers_add_data_providers\nAdds multiple data providers to a collection.\nBeta\nar_data_providers_create\nCreates an empty collection of data providers.\nBeta\nar_data_providers_create_with_data_providers\nCreates a collection of data providers that contains the data providers you supply.\nBeta\nar_data_providers_enumerate_data_providers_f\nEnumerates a collection of data providers.\nBeta\nar_data_providers_get_count\nGets the number of data providers in a collection.\nBeta\nar_data_providers_enumerate_data_providers\nEnumerates a collection of data providers.\nBeta\nar_data_providers_remove_data_provider\nRemoves a data provider from a collection.\nBeta\nar_data_providers_remove_data_providers\nRemoves multiple data providers from a collection.\nBeta"
  },
  {
    "title": "ARFrameSemanticSmoothedSceneDepth | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframesemantics/arframesemanticsmoothedscenedepth",
    "html": "Discussion\n\nEnable this option on a world-tracking configuration (ARWorldTrackingConfiguration) to instruct ARKit to provide your app with the distance between the user’s device and the real-world objects pictured in the frame's capturedImage. ARKit samples this distance using the LiDAR scanner and provides the results through the smoothedSceneDepth property on the session’s currentFrame.\n\nTo minimize the difference in LiDAR readings across frames, ARKit processes the data as an average. The averaged readings reduce flickering to create a smoother motion effect when depicting objects with depth, as demonstrated in Creating a Fog Effect Using Scene Depth. Alternatively, to access a discrete LiDAR reading at the instant the framework creates the current frame, use sceneDepth.\n\nARKit supports scene depth only on LiDAR-capable devices, so call supportsFrameSemantics: to ensure device support before attempting to enable scene depth.\n\nSee Also\nAccessing Depth\nARFrameSemanticSceneDepth\nAn option that provides the distance from the device to real-world objects viewed through the camera."
  },
  {
    "title": "ar_anchor_get_origin_from_anchor_transform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4245759-ar_anchor_get_origin_from_anchor",
    "html": "See Also\nAnchors\nar_anchor_get_identifier\nGets the unique identifier that distinguishes this anchor from all other anchors.\nBeta\nar_anchor_get_timestamp\nGets the timestamp corresponding to the anchor.\nBeta\nar_trackable_anchor_is_tracked\nReturns a Boolean value that indicates whether ARKit is tracking an anchor.\nBeta"
  },
  {
    "title": "ARRaycastTargetAlignmentAny | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycasttargetalignment/arraycasttargetalignmentany",
    "html": "See Also\nChoosing a Target Alignment\nARRaycastTargetAlignmentHorizontal\nThe case that indicates a target is aligned horizontally with respect to gravity.\nARRaycastTargetAlignmentVertical\nThe case that indicates a target is aligned vertically with respect to gravity."
  },
  {
    "title": "ar_mesh_geometry_get_normals | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103589-ar_mesh_geometry_get_normals",
    "html": "See Also\nScene reconstruction\nar_scene_reconstruction_configuration_t\nBeta\nar_scene_reconstruction_mode_t\nThe additional kinds of information you can request about a person’s surroundings.\nBeta\nar_scene_reconstruction_provider_create\nCreates a provider that reconstructs the person’s surroundings.\nBeta\nar_scene_reconstruction_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nBeta\nar_scene_reconstruction_provider_get_required_authorization_type\nGets the types of authorizations needed to run scene reconstruction.\nBeta\nar_scene_reconstruction_configuration_create\nCreates a scene reconstruction configuration.\nBeta\nar_scene_reconstruction_configuration_get_scene_reconstruction_mode\nGets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_configuration_set_scene_reconstruction_mode\nSets the scene reconstruction mode.\nBeta\nar_scene_reconstruction_provider_set_update_handler\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_scene_reconstruction_provider_set_update_handler_f\nSets the handler for receiving scene reconstruction updates.\nBeta\nar_mesh_classification_t\nThe kinds of classification a mesh anchor can have.\nBeta\nar_mesh_anchor_get_geometry\nGets the shape of a mesh anchor.\nBeta\nar_mesh_anchors_enumerate_anchors\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_enumerate_anchors_f\nEnumerates a collection of mesh anchors.\nBeta\nar_mesh_anchors_get_count\nGets the number of mesh anchors in the collection.\nBeta\nar_mesh_geometry_get_classification\nGets the classification of each face in the mesh.\nBeta\nar_mesh_geometry_get_faces\nGets the faces of the mesh.\nBeta\nar_mesh_geometry_get_vertices\nGets the vertices of the mesh.\nBeta"
  },
  {
    "title": "ARRaycastTargetEstimatedPlane | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycasttarget/arraycasttargetestimatedplane",
    "html": "Discussion\n\nA raycast with this target intersects feature points around the ray that ARKit estimates may be a real-world surface.\n\nWhen combined with ARRaycastTargetAlignmentAny, ARKit bases estimated plane alignment on the normal of the surface.\n\nWhen you set your world-tracking configuration's sceneReconstruction to one of the mesh options, ARKit allows a raycast with this target (and target-alignment ARRaycastTargetAlignmentAny) to intersect the scene mesh. Then the raycast result can include points even on nonplanar surfaces or surfaces that have few or no features, such as a white wall. If you set sceneReconstruction to ARSceneReconstructionNone, raycasts ignore the scene mesh.\n\nSee Also\nTargets\nARRaycastTargetExistingPlaneGeometry\nA raycast target that requires a plane to have a definitive size and shape.\nARRaycastTargetExistingPlaneInfinite\nA raycast target that specifies a detected plane, regardless of its size and shape."
  },
  {
    "title": "ar_session_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131545-ar_session_create",
    "html": "See Also\nSessions\nar_session_t\nThe main entry point for receiving data from ARKit.\nBeta\nar_session_query_authorization_results\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_query_authorization_results_f\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_request_authorization\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_request_authorization_f\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_run\nRuns a session with the data providers you supply.\nBeta\nar_session_set_authorization_update_handler\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_authorization_update_handler_f\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_data_provider_state_change_handler\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_set_data_provider_state_change_handler_f\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_data_provider_state_change_handler_t\nA handler for receiving updates to data provider states.\nBeta\nar_session_stop\nStops a session.\nBeta"
  },
  {
    "title": "ar_session_query_authorization_results_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295911-ar_session_query_authorization_r",
    "html": "See Also\nSessions\nar_session_t\nThe main entry point for receiving data from ARKit.\nBeta\nar_session_create\nCreates a new session.\nBeta\nar_session_query_authorization_results\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_request_authorization\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_request_authorization_f\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_run\nRuns a session with the data providers you supply.\nBeta\nar_session_set_authorization_update_handler\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_authorization_update_handler_f\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_data_provider_state_change_handler\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_set_data_provider_state_change_handler_f\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_data_provider_state_change_handler_t\nA handler for receiving updates to data provider states.\nBeta\nar_session_stop\nStops a session.\nBeta"
  },
  {
    "title": "ar_session_request_authorization | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131547-ar_session_request_authorization",
    "html": "See Also\nSessions\nar_session_t\nThe main entry point for receiving data from ARKit.\nBeta\nar_session_create\nCreates a new session.\nBeta\nar_session_query_authorization_results\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_query_authorization_results_f\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_request_authorization_f\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_run\nRuns a session with the data providers you supply.\nBeta\nar_session_set_authorization_update_handler\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_authorization_update_handler_f\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_data_provider_state_change_handler\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_set_data_provider_state_change_handler_f\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_data_provider_state_change_handler_t\nA handler for receiving updates to data provider states.\nBeta\nar_session_stop\nStops a session.\nBeta"
  },
  {
    "title": "ar_session_set_data_provider_state_change_handler | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4241200-ar_session_set_data_provider_sta",
    "html": "See Also\nSessions\nar_session_t\nThe main entry point for receiving data from ARKit.\nBeta\nar_session_create\nCreates a new session.\nBeta\nar_session_query_authorization_results\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_query_authorization_results_f\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nBeta\nar_session_request_authorization\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_request_authorization_f\nRequests authorization from the user to use the specified kinds of ARKit data.\nBeta\nar_session_run\nRuns a session with the data providers you supply.\nBeta\nar_session_set_authorization_update_handler\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_authorization_update_handler_f\nSets the handler for receiving updates in authorization status for a specific authorization type.\nBeta\nar_session_set_data_provider_state_change_handler_f\nSets the handler for responding to a change in the state of one or more data providers.\nBeta\nar_session_data_provider_state_change_handler_t\nA handler for receiving updates to data provider states.\nBeta\nar_session_stop\nStops a session.\nBeta"
  },
  {
    "title": "ARWorldMappingStatusNotAvailable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldmappingstatus/arworldmappingstatusnotavailable",
    "html": "Discussion\n\nWhen the worldMappingStatus of the session's currentFrame is ARWorldMappingStatusNotAvailable, the session has no internal map of the real-world space around the device, nor the scene visible to the camera. Calling getCurrentWorldMapWithCompletionHandler: at this time results in an error.\n\nThis status occurs shortly after starting a new session. To save or share a world map, wait for the user to explore their surroundings and the session's status to change to ARWorldMappingStatusMapped or ARWorldMappingStatusExtending."
  },
  {
    "title": "ARWorldMappingStatusLimited | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldmappingstatus/arworldmappingstatuslimited",
    "html": "Discussion\n\nWhen the worldMappingStatus of the session's currentFrame is ARWorldMappingStatusLimited, the session has not yet fully mapped the real-world space around the device, nor the scene visible to the camera.\n\nAlthough it is possible at this time to save a world map by calling getCurrentWorldMapWithCompletionHandler:, the resulting ARWorldMap is unlikely to be useful for relocalization in the real-world space near the device's current position.\n\nTo produce a higher quality world map, wait for the user to explore more of their surroundings and the session's status to change to ARWorldMappingStatusMapped or ARWorldMappingStatusExtending."
  },
  {
    "title": "ARWorldMappingStatusExtending | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldmappingstatus/arworldmappingstatusextending",
    "html": "Discussion\n\nWhen the worldMappingStatus of the session's currentFrame is ARWorldMappingStatusExtending, the session has produced a high-fidelity internal map of the real-world spaces the device has recently passed thorugh, but is still collecting data to map the area around the device's current position and the scene visible to the camera.\n\nThis status provides moderate to high reliability for relocalizing to a saved world map, provided that:\n\nYou call getCurrentWorldMapWithCompletionHandler: to save the world map while the status of the currentFrame is ARWorldMappingStatusExtending.\n\nWhen you run a new session (later or on another device) from that ARWorldMap, the device running the new session passes through positions and orientations that were visited by the device that saved the session.\n\nSaving or sharing a world map at this time is likely to produce adequate results, but a higher quality world map may be possible if you wait until the user explores more of their surroundings and the status changes to ARWorldMappingStatusMapped."
  },
  {
    "title": "checkAvailabilityAtCoordinate:completionHandler: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration/3571350-checkavailabilityatcoordinate",
    "html": "Parameters\ncoordinate\n\nThe GPS location that the framework checks for availability.\n\ncompletionHandler\n\nCode you supply that runs after the function returns. The closure takes a Boolean argument that indicates whether geotracking is available.\n\nDiscussion\n\nThis function returns NO under the following circumstances:\n\nARKit lacks localization imagery for the argument GPS coordinate.\n\nA network connection is unavailable to download localization imagery.\n\nThe device lacks cellular (GPS) capability.\n\nTo determine availability at the user’s GPS coordinate, use checkAvailabilityWithCompletionHandler: instead.\n\nFor a list of supported areas and cities, see ARGeoTrackingConfiguration.\n\nSee Also\nChecking Availability\n+ checkAvailabilityWithCompletionHandler:\nDetermines if geotracking supports the user’s current location."
  },
  {
    "title": "ar_hand_tracking_provider_get_required_authorization_type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4108263-ar_hand_tracking_provider_get_re",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_tracking_provider_set_update_handler_f | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4295892-ar_hand_tracking_provider_set_up",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ARWorldMappingStatusMapped | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldmappingstatus/arworldmappingstatusmapped",
    "html": "Discussion\n\nWhen the worldMappingStatus of the session's currentFrame is ARWorldMappingStatusMapped, the session has produced a high-fidelity internal map of the real-world space around the device's current position and the scene visible to the camera.\n\nThis status provides the highest reliability for relocalizing to a saved world map, provided that:\n\nYou call getCurrentWorldMapWithCompletionHandler: to save the world map while the status of the currentFrame is ARWorldMappingStatusMapped.\n\nWhen you run a new session (later or on another device) from that ARWorldMap, the device running the new session is at a real-world position and orientation similar to that when the world map was saved."
  },
  {
    "title": "new | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration/3566286-new",
    "html": "See Also\nCreating a Configuration\n- init\nInitializes a new geotracking configuration."
  },
  {
    "title": "ar_hand_chirality_right | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_chirality_t/ar_hand_chirality_right",
    "html": "See Also\nGetting hand chirality\nar_hand_chirality_left\nA left hand.\nBeta"
  },
  {
    "title": "referenceObjectsInGroupNamed:bundle: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/2968202-referenceobjectsingroupnamed",
    "html": "Parameters\nname\n\nThe name of an AR Resource Group from your Xcode project's main asset catalog.\n\nbundle\n\nThe bundle from which to load asset catalog resources, or nil to use your app's main bundle.\n\nReturn Value\n\nA set of all unique reference objects in the specified group.\n\nDiscussion\n\nTo use the objects for detection in a world-tracking AR session, provide this set for your session configuration's detectionObjects property.\n\nSee Also\nLoading Reference Objects\n- initWithArchiveURL:error:\nLoads a reference object from the specified file URL."
  },
  {
    "title": "displayTransformForOrientation:viewportSize: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/2923543-displaytransformfororientation",
    "html": "Parameters\norientation\n\nThe orientation intended for presenting the view.\n\nviewportSize\n\nThe size, in points, of the view intended for rendering the camera image.\n\nReturn Value\n\nA transform matrix that converts from normalized image coordinates in the captured image to normalized image coordinates that account for the specified parameters.\n\nDiscussion\n\nNormalized image coordinates range from (0,0) in the upper left corner of the image to (1,1) in the lower right corner.\n\nThis method creates an affine transform representing the rotation and aspect-fit crop operations necessary to adapt the camera image to the specified orientation and to the aspect ratio of the specified viewport. The affine transform does not scale to the viewport's pixel size.\n\nThe capturedImage pixel buffer is the original image captured by the device camera, and thus not adjusted for device orientation or view aspect ratio.\n\nSee Also\nAccessing scene data\nlightEstimate\nAn estimate of lighting conditions based on the camera image.\nrawFeaturePoints\nThe current intermediate results of the scene analysis ARKit uses to perform world tracking.\ncapturedDepthData\nDepth data captured in front-camera experiences.\ncapturedDepthDataTimestamp\nThe time at which depth data for the frame (if any) was captured.\nsceneDepth\nData on the distance between a device's rear camera and real-world objects in an AR experience.\nsmoothedSceneDepth\nAn average of distance measurements between a device's rear camera and real-world objects that creates smoother visuals in an AR experience."
  },
  {
    "title": "ar_hand_chirality_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_chirality_t",
    "html": "Topics\nGetting hand chirality\nar_hand_chirality_left\nA left hand.\nar_hand_chirality_right\nA right hand.\nSee Also\nHand tracking\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ARPlaneDetectionHorizontal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanedetection/arplanedetectionhorizontal",
    "html": "See Also\nPlane Detection Options\nARPlaneDetectionVertical\nThe session detects surfaces that are parallel to gravity, regardless of other orientation.\nARPlaneDetectionNone\nPlane detection is disabled."
  },
  {
    "title": "AREnvironmentTexturingManual | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arenvironmenttexturing/arenvironmenttexturingmanual",
    "html": "Discussion\n\nWhen you use this environmentTexturing option, you must manually choose when and where to generate environment map textures:\n\nCreate an AREnvironmentProbeAnchor object with a transform indicating its position in the scene.\n\nAdd the probe anchor to the session with the addAnchor: method.\n\nIf you display AR content using ARSCNView, SceneKit automatically retrieves texture maps from probe anchors and uses them to light the scene. Otherwise, use a delegate method such as session:didUpdateAnchors: to find out when the probe anchor's texture has been updated and access the environmentTexture property.\n\nSee Also\nEnvironment Texture Options\nAREnvironmentTexturingNone\nThe framework doesn’t generate environment textures.\nAREnvironmentTexturingAutomatic\nThe framework automatically determines when and where to generate environment textures."
  },
  {
    "title": "AREnvironmentTexturingNone | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arenvironmenttexturing/arenvironmenttexturingnone",
    "html": "See Also\nEnvironment Texture Options\nAREnvironmentTexturingManual\nThe framework generates environment textures only for probe anchors you explicitly add to the session.\nAREnvironmentTexturingAutomatic\nThe framework automatically determines when and where to generate environment textures."
  },
  {
    "title": "ARBlendShapeLocationMouthRollLower | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthrolllower",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "AREnvironmentTexturingAutomatic | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arenvironmenttexturing/arenvironmenttexturingautomatic",
    "html": "Discussion\n\nWhen you use this environmentTexturing option, ARKit automatically chooses positions in the scene to generate environment textures based on the camera imagery it has collected and the other anchors you've placed.\n\nIf you display AR content using ARSCNView, SceneKit automatically retrieves texture maps from probe anchors and uses them to light the scene. Otherwise, use a delegate method such as session:didUpdateAnchors: to find out when the probe anchor's texture has been updated and access the environmentTexture property.\n\nSee Also\nEnvironment Texture Options\nAREnvironmentTexturingNone\nThe framework doesn’t generate environment textures.\nAREnvironmentTexturingManual\nThe framework generates environment textures only for probe anchors you explicitly add to the session."
  },
  {
    "title": "ARFrameSemantics | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframesemantics",
    "html": "Overview\n\nA frame semantic represents 2D information that ARKit extracts from a frame.\n\nTopics\nTracking Bodies in 2D\nARFrameSemanticBodyDetection\nAn option that indicates that 2D body detection is enabled.\nOccluding Virtual Content with People\nARFrameSemanticPersonSegmentation\nAn option that indicates that people occlude your app's virtual content.\nARFrameSemanticPersonSegmentationWithDepth\nAn option that indicates that people occlude your app's virtual content depending on depth.\nAccessing Depth\nARFrameSemanticSceneDepth\nAn option that provides the distance from the device to real-world objects viewed through the camera.\nARFrameSemanticSmoothedSceneDepth\nAn option that provides the distance from the device to real-world objects, averaged across several frames.\nDisabling Features\nARFrameSemanticNone\nAn option that indicates no frame features are enabled.\nSee Also\nEnabling frame features\nframeSemantics\nThe set of active semantics on the frame.\n+ supportsFrameSemantics:\nChecks whether a particular feature is supported."
  },
  {
    "title": "ar_hand_anchor_get_chirality | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103462-ar_hand_anchor_get_chirality",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_anchor_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103460-ar_hand_anchor_create",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_tracking_provider_is_supported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4218656-ar_hand_tracking_provider_is_sup",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_tracking_configuration_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131522-ar_hand_tracking_configuration_c",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ARPlaneDetectionNone | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanedetection/arplanedetectionnone",
    "html": "See Also\nPlane Detection Options\nARPlaneDetectionHorizontal\nThe session detects planar surfaces that are perpendicular to gravity.\nARPlaneDetectionVertical\nThe session detects surfaces that are parallel to gravity, regardless of other orientation."
  },
  {
    "title": "ar_hand_tracking_configuration_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_tracking_configuration_t",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_tracking_provider_set_update_handler | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4131524-ar_hand_tracking_provider_set_up",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ARRaycastTargetAlignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycasttargetalignment",
    "html": "Overview\n\nA raycast ignores potential targets with an alignment different than the one you specify in the raycast query.\n\nTopics\nChoosing a Target Alignment\nARRaycastTargetAlignmentAny\nThe case that indicates a target may be aligned in any way with respect to gravity.\nARRaycastTargetAlignmentHorizontal\nThe case that indicates a target is aligned horizontally with respect to gravity.\nARRaycastTargetAlignmentVertical\nThe case that indicates a target is aligned vertically with respect to gravity.\nSee Also\nSpecifying the Target\ntarget\nA plane type that allows the raycast to terminate if it's encountered.\nARRaycastTarget\nThe types of surface you allow a raycast to intersect with.\ntargetAlignment\nThe target's alignment with respect to gravity."
  },
  {
    "title": "ar_hand_chirality_left | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_chirality_t/ar_hand_chirality_left",
    "html": "See Also\nGetting hand chirality\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_tracking_provider_get_latest_anchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103472-ar_hand_tracking_provider_get_la",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ARRaycastTarget | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycasttarget",
    "html": "Topics\nTargets\nARRaycastTargetEstimatedPlane\nA raycast target that specifies nonplanar surfaces, or planes about which ARKit can only estimate.\nARRaycastTargetExistingPlaneGeometry\nA raycast target that requires a plane to have a definitive size and shape.\nARRaycastTargetExistingPlaneInfinite\nA raycast target that specifies a detected plane, regardless of its size and shape.\nSee Also\nSpecifying the Target\ntarget\nA plane type that allows the raycast to terminate if it's encountered.\ntargetAlignment\nThe target's alignment with respect to gravity.\nARRaycastTargetAlignment\nA specification that indicates a target's alignment with respect to gravity."
  },
  {
    "title": "ar_hand_tracking_provider_create | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/4103471-ar_hand_tracking_provider_create",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ARSegmentationClassNone | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsegmentationclass/arsegmentationclassnone",
    "html": "See Also\nClassifying Pixels\nARSegmentationClassPerson\nA classification of a pixel in the segmentation buffer as part of a person."
  },
  {
    "title": "ARBlendShapeLocationNoseSneerRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationnosesneerright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nARBlendShapeLocationBrowDownLeft\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowDownRight\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationBrowInnerUp\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nARBlendShapeLocationBrowOuterUpLeft\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowOuterUpRight\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationCheekPuff\nThe coefficient describing outward movement of both cheeks.\nARBlendShapeLocationCheekSquintLeft\nThe coefficient describing upward movement of the cheek around and below the left eye.\nARBlendShapeLocationCheekSquintRight\nThe coefficient describing upward movement of the cheek around and below the right eye.\nARBlendShapeLocationNoseSneerLeft\nThe coefficient describing a raising of the left side of the nose around the nostril."
  },
  {
    "title": "ARGeometryPrimitiveType.line | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometryprimitivetype/line",
    "html": "See Also\nType of Connection\ncase triangle\nThree vertices that connect to form a triangle."
  },
  {
    "title": "ARWorldMappingStatus | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldmappingstatus",
    "html": "Topics\nEnumeration Cases\nARWorldMappingStatusNotAvailable\nNo world map is available.\nARWorldMappingStatusLimited\nWorld tracking has not yet sufficiently mapped the area around the current device position.\nARWorldMappingStatusExtending\nWorld tracking has mapped recently visited areas, but is still mapping around the current device position.\nARWorldMappingStatusMapped\nWorld tracking has adequately mapped the visible area.\nSee Also\nChecking world-mapping status\nworldMappingStatus\nThe feasibility of generating or relocalizing a world map for this frame."
  },
  {
    "title": "ARWorldAlignmentGravityAndHeading | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldalignment/arworldalignmentgravityandheading",
    "html": "Discussion\n\nThe y-axis matches the direction of gravity as detected by the device's motion sensing hardware; that is, the vector (0,-1,0) points downward.\n\nThe x- and z-axes match the longitude and latitude directions as measured by Location Services. The vector (0,0,-1) points to true north and the vector (-1,0,0) points west. (That is, the positive x-, y-, and z-axes point east, up, and south, respectively.)\n\nFigure 1 In gravity and heading alignment, all directions are fixed to a real-world reference frame.\n\nAlthough this option fixes the directions of the three coordinate axes to real-world directions, the location of the coordinate system's origin is still relative to the device, matching the device's position as of when the session configuration is first run.\n\nNote\n\nUsing gravity and heading alignment requires tracking the device's geographic location. Your app's Info.plist must include user-facing text for the NSLocationUsageDescription or NSLocationWhenInUseUsageDescription key so that the user can grant your app permission for location tracking.\n\nSee Also\nAlignments\nARWorldAlignmentGravity\nThe coordinate system's y-axis is parallel to gravity, and its origin is the initial position of the device.\nARWorldAlignmentCamera\nThe scene coordinate system is locked to match the orientation of the camera."
  },
  {
    "title": "ARSegmentationClassPerson | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsegmentationclass/arsegmentationclassperson",
    "html": "See Also\nClassifying Pixels\nARSegmentationClassNone\nA classification of a pixel in the segmentation buffer as unidentified."
  },
  {
    "title": "ARBlendShapeLocationTongueOut | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationtongueout",
    "html": "Discussion\n\nA value of 0.0 indicates that the tongue is fully inside the mouth; a value of 1.0 indicates that the tongue is as far out of the mouth as ARKit tracks."
  },
  {
    "title": "ARWorldAlignmentCamera | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldalignment/arworldalignmentcamera",
    "html": "Discussion\n\nCamera alignment defines a coordinate system based on the native sensor orientation of the device camera. Relative to a AVCaptureVideoOrientationLandscapeRight-oriented camera image, the x-axis points to the right, the y-axis points up, and the z-axis points out the front of the device (toward the user).\n\nNote\n\nThis coordinate system is always the same regardless of device or user interface orientation. That is, the x-axis always points along the long axis of the device, even if that direction is \"down\" relative to the user.\n\nWhen this alignment is active, ARKit performs no device motion tracking. That is, world-space positions are effectively always relative to the current position and orientation of the device. (For example, a SceneKit object placed in an ARSCNView will thus maintain the same position on screen, even as the camera image changes while the device moves.)\n\nSee Also\nAlignments\nARWorldAlignmentGravity\nThe coordinate system's y-axis is parallel to gravity, and its origin is the initial position of the device.\nARWorldAlignmentGravityAndHeading\nThe coordinate system's y-axis is parallel to gravity, its x- and z-axes are oriented to compass heading, and its origin is the initial position of the device."
  },
  {
    "title": "ARBlendShapeLocationNoseSneerLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationnosesneerleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nARBlendShapeLocationBrowDownLeft\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowDownRight\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationBrowInnerUp\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nARBlendShapeLocationBrowOuterUpLeft\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowOuterUpRight\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationCheekPuff\nThe coefficient describing outward movement of both cheeks.\nARBlendShapeLocationCheekSquintLeft\nThe coefficient describing upward movement of the cheek around and below the left eye.\nARBlendShapeLocationCheekSquintRight\nThe coefficient describing upward movement of the cheek around and below the right eye.\nARBlendShapeLocationNoseSneerRight\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "ARBlendShapeLocationMouthShrugLower | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthshruglower",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationMouthRollUpper | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthrollupper",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "checkAvailabilityWithCompletionHandler: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration/3571351-checkavailabilitywithcompletionh",
    "html": "Parameters\ncompletionHandler\n\nCode you supply that runs after the function returns. The closure takes a Boolean argument that indicates whether geotracking is available.\n\nDiscussion\n\nThis function returns NO under the following circumstances:\n\nARKit lacks localization imagery for the user’s geographic position.\n\nA network connection is unavailable to download localization imagery.\n\nThe device lacks cellular (GPS) capability.\n\nTo determine availability at a different location than the device’s current location, call checkAvailabilityAtCoordinate:completionHandler: instead.\n\nFor a list of supported areas and cities, see ARGeoTrackingConfiguration.\n\nSee Also\nChecking Availability\n+ checkAvailabilityAtCoordinate:completionHandler:\nDetermines if geotracking supports a particular location."
  },
  {
    "title": "ARWorldAlignmentGravity | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldalignment/arworldalignmentgravity",
    "html": "Discussion\n\nThe y-axis matches the direction of gravity as detected by the device's motion sensing hardware; that is, the vector (0,-1,0) points downward.\n\nThe position and orientation of the device as of when the session configuration is first run determine the rest of the coordinate system: For the z-axis, ARKit chooses a basis vector (0,0,-1) pointing in the direction the device camera faces and perpendicular to the gravity axis. ARKit chooses a x-axis based on the z- and y-axes using the right hand rule—that is, the basis vector (1,0,0) is orthogonal to the other two axes, and (for a viewer looking in the negative-z direction) points toward the right.\n\nFigure 1 In gravity-only alignment, X and Z directions are relative to the device's initial orientation.\n\nSee Also\nAlignments\nARWorldAlignmentGravityAndHeading\nThe coordinate system's y-axis is parallel to gravity, its x- and z-axes are oriented to compass heading, and its origin is the initial position of the device.\nARWorldAlignmentCamera\nThe scene coordinate system is locked to match the orientation of the camera."
  },
  {
    "title": "lightEstimationEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/2923546-lightestimationenabled",
    "html": "Discussion\n\nWhen this value is YES (the default), a running AR session provides scene lighting information in the lightEstimate property of each ARFrame object it captures.\n\nIf you render your own overlay graphics for the AR scene, you can use this information in shading algorithms to help make those graphics match the real-world lighting conditions of the scene captured by the camera. (The ARSCNView class automatically uses this information to configure SceneKit lighting.)\n\nSee Also\nConfiguring the AR session\nworldAlignment\nA value specifying how the session maps real-world device motion into a 3D scene coordinate system.\nARWorldAlignment\nOptions for how ARKit constructs a scene coordinate system based on real-world device motion."
  },
  {
    "title": "ARBlendShapeLocationMouthDimpleRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthdimpleright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "raycastQueryFromPoint:allowingTarget:alignment: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/3194578-raycastqueryfrompoint",
    "html": "Parameters\npoint\n\nA normalized coordinate in the UI system, where 0 is top-left, and 1 is bottom-right.\n\ntarget\n\nThe types of plane you allow this ray cast to intersect with.\n\ntargetAlignment\n\nAn alignment with respect to gravity a plane must have to interset this ray.\n\nDiscussion\n\nTo cast the ray, you pass the resulting query to your current session via raycast: or trackedRaycast:updateHandler:.\n\nSee Also\nTracking and interacting with the real world\nanchors\nThe list of anchors representing positions tracked or objects detected in the scene.\n- hitTest:types:\nSearches for real-world objects or AR anchors in the captured camera image.\nDeprecated"
  },
  {
    "title": "ARBlendShapeLocationEyeLookInLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationeyelookinleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nLeft Eye\nARBlendShapeLocationEyeBlinkLeft\nThe coefficient describing closure of the eyelids over the left eye.\nARBlendShapeLocationEyeLookDownLeft\nThe coefficient describing movement of the left eyelids consistent with a downward gaze.\nARBlendShapeLocationEyeLookOutLeft\nThe coefficient describing movement of the left eyelids consistent with a leftward gaze.\nARBlendShapeLocationEyeLookUpLeft\nThe coefficient describing movement of the left eyelids consistent with an upward gaze.\nARBlendShapeLocationEyeSquintLeft\nThe coefficient describing contraction of the face around the left eye.\nARBlendShapeLocationEyeWideLeft\nThe coefficient describing a widening of the eyelids around the left eye."
  },
  {
    "title": "ARBlendShapeLocationMouthStretchRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthstretchright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "AREnvironmentTexturing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arenvironmenttexturing",
    "html": "Topics\nEnvironment Texture Options\nAREnvironmentTexturingNone\nThe framework doesn’t generate environment textures.\nAREnvironmentTexturingManual\nThe framework generates environment textures only for probe anchors you explicitly add to the session.\nAREnvironmentTexturingAutomatic\nThe framework automatically determines when and where to generate environment textures.\nSee Also\nCreating Realistic Reflections\nenvironmentTexturing\nAn option that determines how the framework generates environment textures.\nAREnvironmentProbeAnchor\nAn object that provides environmental lighting information for a specific area of space in a world-tracking AR session.\nwantsHDREnvironmentTextures\nA flag that instructs the framework to create environment textures in HDR format."
  },
  {
    "title": "ARPlaneDetectionVertical | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanedetection/arplanedetectionvertical",
    "html": "See Also\nPlane Detection Options\nARPlaneDetectionHorizontal\nThe session detects planar surfaces that are perpendicular to gravity.\nARPlaneDetectionNone\nPlane detection is disabled."
  },
  {
    "title": "ARBlendShapeLocationMouthStretchLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthstretchleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationEyeBlinkLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationeyeblinkleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nLeft Eye\nARBlendShapeLocationEyeLookDownLeft\nThe coefficient describing movement of the left eyelids consistent with a downward gaze.\nARBlendShapeLocationEyeLookInLeft\nThe coefficient describing movement of the left eyelids consistent with a rightward gaze.\nARBlendShapeLocationEyeLookOutLeft\nThe coefficient describing movement of the left eyelids consistent with a leftward gaze.\nARBlendShapeLocationEyeLookUpLeft\nThe coefficient describing movement of the left eyelids consistent with an upward gaze.\nARBlendShapeLocationEyeSquintLeft\nThe coefficient describing contraction of the face around the left eye.\nARBlendShapeLocationEyeWideLeft\nThe coefficient describing a widening of the eyelids around the left eye."
  },
  {
    "title": "ARBlendShapeLocationEyeLookDownLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationeyelookdownleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nLeft Eye\nARBlendShapeLocationEyeBlinkLeft\nThe coefficient describing closure of the eyelids over the left eye.\nARBlendShapeLocationEyeLookInLeft\nThe coefficient describing movement of the left eyelids consistent with a rightward gaze.\nARBlendShapeLocationEyeLookOutLeft\nThe coefficient describing movement of the left eyelids consistent with a leftward gaze.\nARBlendShapeLocationEyeLookUpLeft\nThe coefficient describing movement of the left eyelids consistent with an upward gaze.\nARBlendShapeLocationEyeSquintLeft\nThe coefficient describing contraction of the face around the left eye.\nARBlendShapeLocationEyeWideLeft\nThe coefficient describing a widening of the eyelids around the left eye."
  },
  {
    "title": "ARBlendShapeLocationEyeLookOutLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationeyelookoutleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nLeft Eye\nARBlendShapeLocationEyeBlinkLeft\nThe coefficient describing closure of the eyelids over the left eye.\nARBlendShapeLocationEyeLookDownLeft\nThe coefficient describing movement of the left eyelids consistent with a downward gaze.\nARBlendShapeLocationEyeLookInLeft\nThe coefficient describing movement of the left eyelids consistent with a rightward gaze.\nARBlendShapeLocationEyeLookUpLeft\nThe coefficient describing movement of the left eyelids consistent with an upward gaze.\nARBlendShapeLocationEyeSquintLeft\nThe coefficient describing contraction of the face around the left eye.\nARBlendShapeLocationEyeWideLeft\nThe coefficient describing a widening of the eyelids around the left eye."
  },
  {
    "title": "ARBlendShapeLocationEyeLookOutRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationeyelookoutright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nRight Eye\nARBlendShapeLocationEyeBlinkRight\nThe coefficient describing closure of the eyelids over the right eye.\nARBlendShapeLocationEyeLookDownRight\nThe coefficient describing movement of the right eyelids consistent with a downward gaze.\nARBlendShapeLocationEyeLookInRight\nThe coefficient describing movement of the right eyelids consistent with a leftward gaze.\nARBlendShapeLocationEyeLookUpRight\nThe coefficient describing movement of the right eyelids consistent with an upward gaze.\nARBlendShapeLocationEyeSquintRight\nThe coefficient describing contraction of the face around the right eye.\nARBlendShapeLocationEyeWideRight\nThe coefficient describing a widening of the eyelids around the right eye."
  },
  {
    "title": "ARBlendShapeLocationEyeLookInRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationeyelookinright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nRight Eye\nARBlendShapeLocationEyeBlinkRight\nThe coefficient describing closure of the eyelids over the right eye.\nARBlendShapeLocationEyeLookDownRight\nThe coefficient describing movement of the right eyelids consistent with a downward gaze.\nARBlendShapeLocationEyeLookOutRight\nThe coefficient describing movement of the right eyelids consistent with a rightward gaze.\nARBlendShapeLocationEyeLookUpRight\nThe coefficient describing movement of the right eyelids consistent with an upward gaze.\nARBlendShapeLocationEyeSquintRight\nThe coefficient describing contraction of the face around the right eye.\nARBlendShapeLocationEyeWideRight\nThe coefficient describing a widening of the eyelids around the right eye."
  },
  {
    "title": "ARBlendShapeLocationEyeWideRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationeyewideright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nRight Eye\nARBlendShapeLocationEyeBlinkRight\nThe coefficient describing closure of the eyelids over the right eye.\nARBlendShapeLocationEyeLookDownRight\nThe coefficient describing movement of the right eyelids consistent with a downward gaze.\nARBlendShapeLocationEyeLookInRight\nThe coefficient describing movement of the right eyelids consistent with a leftward gaze.\nARBlendShapeLocationEyeLookOutRight\nThe coefficient describing movement of the right eyelids consistent with a rightward gaze.\nARBlendShapeLocationEyeLookUpRight\nThe coefficient describing movement of the right eyelids consistent with an upward gaze.\nARBlendShapeLocationEyeSquintRight\nThe coefficient describing contraction of the face around the right eye."
  },
  {
    "title": "ARBlendShapeLocationEyeSquintRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationeyesquintright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nRight Eye\nARBlendShapeLocationEyeBlinkRight\nThe coefficient describing closure of the eyelids over the right eye.\nARBlendShapeLocationEyeLookDownRight\nThe coefficient describing movement of the right eyelids consistent with a downward gaze.\nARBlendShapeLocationEyeLookInRight\nThe coefficient describing movement of the right eyelids consistent with a leftward gaze.\nARBlendShapeLocationEyeLookOutRight\nThe coefficient describing movement of the right eyelids consistent with a rightward gaze.\nARBlendShapeLocationEyeLookUpRight\nThe coefficient describing movement of the right eyelids consistent with an upward gaze.\nARBlendShapeLocationEyeWideRight\nThe coefficient describing a widening of the eyelids around the right eye."
  },
  {
    "title": "ARBlendShapeLocationEyeWideLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationeyewideleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nLeft Eye\nARBlendShapeLocationEyeBlinkLeft\nThe coefficient describing closure of the eyelids over the left eye.\nARBlendShapeLocationEyeLookDownLeft\nThe coefficient describing movement of the left eyelids consistent with a downward gaze.\nARBlendShapeLocationEyeLookInLeft\nThe coefficient describing movement of the left eyelids consistent with a rightward gaze.\nARBlendShapeLocationEyeLookOutLeft\nThe coefficient describing movement of the left eyelids consistent with a leftward gaze.\nARBlendShapeLocationEyeLookUpLeft\nThe coefficient describing movement of the left eyelids consistent with an upward gaze.\nARBlendShapeLocationEyeSquintLeft\nThe coefficient describing contraction of the face around the left eye."
  },
  {
    "title": "ARBlendShapeLocationEyeBlinkRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationeyeblinkright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nRight Eye\nARBlendShapeLocationEyeLookDownRight\nThe coefficient describing movement of the right eyelids consistent with a downward gaze.\nARBlendShapeLocationEyeLookInRight\nThe coefficient describing movement of the right eyelids consistent with a leftward gaze.\nARBlendShapeLocationEyeLookOutRight\nThe coefficient describing movement of the right eyelids consistent with a rightward gaze.\nARBlendShapeLocationEyeLookUpRight\nThe coefficient describing movement of the right eyelids consistent with an upward gaze.\nARBlendShapeLocationEyeSquintRight\nThe coefficient describing contraction of the face around the right eye.\nARBlendShapeLocationEyeWideRight\nThe coefficient describing a widening of the eyelids around the right eye."
  },
  {
    "title": "ARBlendShapeLocationEyeLookUpRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationeyelookupright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nRight Eye\nARBlendShapeLocationEyeBlinkRight\nThe coefficient describing closure of the eyelids over the right eye.\nARBlendShapeLocationEyeLookDownRight\nThe coefficient describing movement of the right eyelids consistent with a downward gaze.\nARBlendShapeLocationEyeLookInRight\nThe coefficient describing movement of the right eyelids consistent with a leftward gaze.\nARBlendShapeLocationEyeLookOutRight\nThe coefficient describing movement of the right eyelids consistent with a rightward gaze.\nARBlendShapeLocationEyeSquintRight\nThe coefficient describing contraction of the face around the right eye.\nARBlendShapeLocationEyeWideRight\nThe coefficient describing a widening of the eyelids around the right eye."
  },
  {
    "title": "ARBlendShapeLocationEyeLookDownRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationeyelookdownright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nRight Eye\nARBlendShapeLocationEyeBlinkRight\nThe coefficient describing closure of the eyelids over the right eye.\nARBlendShapeLocationEyeLookInRight\nThe coefficient describing movement of the right eyelids consistent with a leftward gaze.\nARBlendShapeLocationEyeLookOutRight\nThe coefficient describing movement of the right eyelids consistent with a rightward gaze.\nARBlendShapeLocationEyeLookUpRight\nThe coefficient describing movement of the right eyelids consistent with an upward gaze.\nARBlendShapeLocationEyeSquintRight\nThe coefficient describing contraction of the face around the right eye.\nARBlendShapeLocationEyeWideRight\nThe coefficient describing a widening of the eyelids around the right eye."
  },
  {
    "title": "ARBlendShapeLocationBrowOuterUpLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationbrowouterupleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nARBlendShapeLocationBrowDownLeft\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowDownRight\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationBrowInnerUp\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nARBlendShapeLocationBrowOuterUpRight\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationCheekPuff\nThe coefficient describing outward movement of both cheeks.\nARBlendShapeLocationCheekSquintLeft\nThe coefficient describing upward movement of the cheek around and below the left eye.\nARBlendShapeLocationCheekSquintRight\nThe coefficient describing upward movement of the cheek around and below the right eye.\nARBlendShapeLocationNoseSneerLeft\nThe coefficient describing a raising of the left side of the nose around the nostril.\nARBlendShapeLocationNoseSneerRight\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "ARBlendShapeLocationMouthLowerDownRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthlowerdownright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationCheekPuff | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationcheekpuff",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nARBlendShapeLocationBrowDownLeft\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowDownRight\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationBrowInnerUp\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nARBlendShapeLocationBrowOuterUpLeft\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowOuterUpRight\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationCheekSquintLeft\nThe coefficient describing upward movement of the cheek around and below the left eye.\nARBlendShapeLocationCheekSquintRight\nThe coefficient describing upward movement of the cheek around and below the right eye.\nARBlendShapeLocationNoseSneerLeft\nThe coefficient describing a raising of the left side of the nose around the nostril.\nARBlendShapeLocationNoseSneerRight\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "ARBlendShapeLocationCheekSquintLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationcheeksquintleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nARBlendShapeLocationBrowDownLeft\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowDownRight\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationBrowInnerUp\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nARBlendShapeLocationBrowOuterUpLeft\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowOuterUpRight\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationCheekPuff\nThe coefficient describing outward movement of both cheeks.\nARBlendShapeLocationCheekSquintRight\nThe coefficient describing upward movement of the cheek around and below the right eye.\nARBlendShapeLocationNoseSneerLeft\nThe coefficient describing a raising of the left side of the nose around the nostril.\nARBlendShapeLocationNoseSneerRight\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "ARBlendShapeLocationMouthUpperUpRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthupperupright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side."
  },
  {
    "title": "ARBlendShapeLocationCheekSquintRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationcheeksquintright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nARBlendShapeLocationBrowDownLeft\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowDownRight\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationBrowInnerUp\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nARBlendShapeLocationBrowOuterUpLeft\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowOuterUpRight\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationCheekPuff\nThe coefficient describing outward movement of both cheeks.\nARBlendShapeLocationCheekSquintLeft\nThe coefficient describing upward movement of the cheek around and below the left eye.\nARBlendShapeLocationNoseSneerLeft\nThe coefficient describing a raising of the left side of the nose around the nostril.\nARBlendShapeLocationNoseSneerRight\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "ARBlendShapeLocationBrowDownRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationbrowdownright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nARBlendShapeLocationBrowDownLeft\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowInnerUp\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nARBlendShapeLocationBrowOuterUpLeft\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowOuterUpRight\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationCheekPuff\nThe coefficient describing outward movement of both cheeks.\nARBlendShapeLocationCheekSquintLeft\nThe coefficient describing upward movement of the cheek around and below the left eye.\nARBlendShapeLocationCheekSquintRight\nThe coefficient describing upward movement of the cheek around and below the right eye.\nARBlendShapeLocationNoseSneerLeft\nThe coefficient describing a raising of the left side of the nose around the nostril.\nARBlendShapeLocationNoseSneerRight\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "ARBlendShapeLocationBrowDownLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationbrowdownleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nARBlendShapeLocationBrowDownRight\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationBrowInnerUp\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nARBlendShapeLocationBrowOuterUpLeft\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowOuterUpRight\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationCheekPuff\nThe coefficient describing outward movement of both cheeks.\nARBlendShapeLocationCheekSquintLeft\nThe coefficient describing upward movement of the cheek around and below the left eye.\nARBlendShapeLocationCheekSquintRight\nThe coefficient describing upward movement of the cheek around and below the right eye.\nARBlendShapeLocationNoseSneerLeft\nThe coefficient describing a raising of the left side of the nose around the nostril.\nARBlendShapeLocationNoseSneerRight\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "ARBlendShapeLocationMouthLowerDownLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthlowerdownleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationMouthUpperUpLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthupperupleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationBrowInnerUp | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationbrowinnerup",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nARBlendShapeLocationBrowDownLeft\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowDownRight\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationBrowOuterUpLeft\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowOuterUpRight\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationCheekPuff\nThe coefficient describing outward movement of both cheeks.\nARBlendShapeLocationCheekSquintLeft\nThe coefficient describing upward movement of the cheek around and below the left eye.\nARBlendShapeLocationCheekSquintRight\nThe coefficient describing upward movement of the cheek around and below the right eye.\nARBlendShapeLocationNoseSneerLeft\nThe coefficient describing a raising of the left side of the nose around the nostril.\nARBlendShapeLocationNoseSneerRight\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "ARBlendShapeLocationMouthPressLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthpressleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationBrowOuterUpRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationbrowouterupright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nARBlendShapeLocationBrowDownLeft\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowDownRight\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationBrowInnerUp\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nARBlendShapeLocationBrowOuterUpLeft\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationCheekPuff\nThe coefficient describing outward movement of both cheeks.\nARBlendShapeLocationCheekSquintLeft\nThe coefficient describing upward movement of the cheek around and below the left eye.\nARBlendShapeLocationCheekSquintRight\nThe coefficient describing upward movement of the cheek around and below the right eye.\nARBlendShapeLocationNoseSneerLeft\nThe coefficient describing a raising of the left side of the nose around the nostril.\nARBlendShapeLocationNoseSneerRight\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "ARBlendShapeLocationMouthPressRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthpressright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationMouthShrugUpper | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthshrugupper",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationJawLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationjawleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationJawForward | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationjawforward",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "exportObjectToURL:previewImage:error: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/2968198-exportobjecttourl",
    "html": "Parameters\nurl\n\nThe file URL at which to write the reference object data.\n\npreviewImage\n\nA thumbnail image to be embedded in the reference object's filesystem representation.\n\nARKit ignores preview images when loading reference objects. Instead, this image helps make reference object files visually identifiable in external tools like Xcode, Finder, and Quick Look.\n\nerror\n\nA pointer to an NSError. On completion, if the method returns NO, the pointer is populated with an object describing the failure.\n\nReturn Value\n\nYES if the operation succeeded. If NO, check the error parameter for failure details.\n\nDiscussion\n\nAfter exporting a reference object from your object-scanning app to a file, you can bundle that reference object into other apps you create by inserting it into an Xcode asset catalog.\n\nSee Also\nSaving Recorded Objects\nARReferenceObjectArchiveExtension\nThe standard filename extension for exported ARReferenceObject instances."
  },
  {
    "title": "ARBlendShapeLocationMouthRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationMouthLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationMouthFunnel | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthfunnel",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationJawRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationjawright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationMouthPucker | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthpucker",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationMouthFrownRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthfrownright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationMouthSmileRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthsmileright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationMouthFrownLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthfrownleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ar_hand_skeleton_joint_name_wrist | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_wrist",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_ring_finger_knuckle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_ring_finger_knuckle",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_ring_finger_tip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_ring_finger_tip",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_thumb_intermediate_tip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_thumb_intermediate_tip",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_thumb_knuckle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_thumb_knuckle",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_thumb_intermediate_base | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_thumb_intermediate_base",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_thumb_tip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_thumb_tip",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_little_finger_tip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_little_finger_tip",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_middle_finger_intermediate_base | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_middle_finger_intermediate_base",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_little_finger_metacarpal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_little_finger_metacarpal",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_little_finger_knuckle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_little_finger_knuckle",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_little_finger_intermediate_tip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_little_finger_intermediate_tip",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_index_finger_tip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_index_finger_tip",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_index_finger_metacarpal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_index_finger_metacarpal",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_index_finger_intermediate_tip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_index_finger_intermediate_tip",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ARErrorCodeSensorFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodesensorfailed",
    "html": "See Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ar_hand_skeleton_joint_name_index_finger_intermediate_base | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_index_finger_intermediate_base",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ARErrorCodeFileIOFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodefileiofailed",
    "html": "See Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARErrorCodeInvalidReferenceImage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodeinvalidreferenceimage",
    "html": "Discussion\n\nThis error occurs when you supply a reference image to the configuration's detectionImages but ARKit determined it's unusable. This can happen when the image data doesn't contain enough features to identify a unique picture, for example, it's all white.\n\nSee Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARErrorCodeGeoTrackingFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodegeotrackingfailed",
    "html": "Discussion\n\nARKit will raise an error with this error code when visual localization is taking too long. This situation indicates that the app has met all requirements for geo tracking except for visual localization. To try again, the app needs to ask the user pan the device around the physical environment to acquire different camera-feed imagery. For more information, see Assisting the User with Visual Localization.\n\nSee Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARErrorCodeCameraUnauthorized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodecameraunauthorized",
    "html": "Discussion\n\nTo use the device's camera:\n\nYour app's Info.plist file must provide a message for the NSCameraUsageDescription key. If this key is missing, any attempt to run an AR session fails with this error.\n\nWhen your app first attempts to run an AR session or otherwise use the camera, iOS automatically shows an alert with your camera usage description message, asking the user to grant camera permission to your app. If the user accepts this request, the session begins; otherwise the session fails with this error.\n\nIf the user has previously denied camera permission for your app, all attempts to run an AR session or otherwise use the camera fail with this error. To grant camera permission, the user must explicitly enable your app in the iOS Settings app, under Privacy > Camera.\n\nSee Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARErrorCodeInvalidWorldMap | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodeinvalidworldmap",
    "html": "See Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "revolutions | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/spinaction/revolutions",
    "html": "Overview\n\nIrrational values translate to partial revolutions. For example, set this property to 1.25 to complete one and a quarter rotations before stopping.\n\nDeclaration\nuniform double revolutions = 1.0\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that spin on an axis.\nduration\nThe amount of time between the start of an action and its end.\naxis\nA vector that describes the axis of rotation."
  },
  {
    "title": "ARGeoTrackingStateReasonNeedLocationPermissions | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatereason/argeotrackingstatereasonneedlocationpermissions",
    "html": "Discussion\n\nThis reason indicates that the user has not given this app permission to access the user’s location. To enable geo tracking, an app needs to ask the user to enable location sharing for this app in Settings.\n\nSee Also\nStatus Reasons\nARGeoTrackingStateReasonNone\nNo issues reported.\nARGeoTrackingStateReasonNotAvailableAtLocation\nThe location doesn't provide geotracking.\nARGeoTrackingStateReasonDevicePointedTooLow\nThe position of the device is too low for geotracking.\nARGeoTrackingStateReasonWorldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\nARGeoTrackingStateReasonWaitingForLocation\nA state in which the framework performs a check for the user's GPS position.\nARGeoTrackingStateReasonWaitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\nARGeoTrackingStateReasonGeoDataNotLoaded\nA state in which the framework downloads localization imagery.\nARGeoTrackingStateReasonVisualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "ARErrorCodeWorldTrackingFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodeworldtrackingfailed",
    "html": "See Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARErrorCodeUnsupportedConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodeunsupportedconfiguration",
    "html": "Discussion\n\nCall isSupported on an ARConfiguration to ensure it's supported before attempting to create and run it on the session with runWithConfiguration:.\n\nSee Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "duration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/spinaction/duration",
    "html": "Overview\n\nThe default value is 1.0.\n\nDeclaration\nuniform double duration = 1.0\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that spin on an axis.\nrevolutions\nThe number rotations to complete.\naxis\nA vector that describes the axis of rotation."
  },
  {
    "title": "ARErrorCodeObjectMergeFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodeobjectmergefailed",
    "html": "See Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARGeoTrackingStateReasonWaitingForLocation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatereason/argeotrackingstatereasonwaitingforlocation",
    "html": "Discussion\n\nWhile in this state, the app needs to wait for the Core Location subsystem to provide the user's GPS location. Inform the user of the check in progress; for instance, present a message alerting them to the geotracking initialization process.\n\nSee Also\nStatus Reasons\nARGeoTrackingStateReasonNone\nNo issues reported.\nARGeoTrackingStateReasonNotAvailableAtLocation\nThe location doesn't provide geotracking.\nARGeoTrackingStateReasonNeedLocationPermissions\nThe location requires user permission for geotracking.\nARGeoTrackingStateReasonDevicePointedTooLow\nThe position of the device is too low for geotracking.\nARGeoTrackingStateReasonWorldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\nARGeoTrackingStateReasonWaitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\nARGeoTrackingStateReasonGeoDataNotLoaded\nA state in which the framework downloads localization imagery.\nARGeoTrackingStateReasonVisualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/spinaction/info_id",
    "html": "Overview\n\nThe default value is Spin.\n\nDeclaration\nuniform token info:id = \"Spin\"\n\n\nSee Also\nProperties\naffectedObjects\nA list of prims that spin on an axis.\nduration\nThe amount of time between the start of an action and its end.\nrevolutions\nThe number rotations to complete.\naxis\nA vector that describes the axis of rotation."
  },
  {
    "title": "ARGeoTrackingAccuracyUndetermined | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingaccuracy/argeotrackingaccuracyundetermined",
    "html": "Discussion\n\nThis value indicates that because the session has not completed visual localization, geo-tracking accuracy is indeterminate.\n\nSee Also\nAccuracies\nARGeoTrackingAccuracyHigh\nGeo-tracking accuracy is high.\nARGeoTrackingAccuracyLow\nGeo-tracking accuracy is low.\nARGeoTrackingAccuracyMedium\nGeo-tracking accuracy is average."
  },
  {
    "title": "ARGeoTrackingAccuracyHigh | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingaccuracy/argeotrackingaccuracyhigh",
    "html": "Discussion\n\nThis value indicates that visual localization is complete and geo-tracking accuracy is very good.\n\nSee Also\nAccuracies\nARGeoTrackingAccuracyUndetermined\nGeo-tracking accuracy is undetermined.\nARGeoTrackingAccuracyLow\nGeo-tracking accuracy is low.\nARGeoTrackingAccuracyMedium\nGeo-tracking accuracy is average."
  },
  {
    "title": "ARGeoTrackingAccuracyLow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingaccuracy/argeotrackingaccuracylow",
    "html": "Discussion\n\nThis value indicates that visual localization is complete and geo-tracking accuracy is low.\n\nOne technique an app can use to deal with low accuracy is to render location anchors with an asset that’s more forgiving, like a large ball. If an app renders the ball further in the air, any offset that results from low accuracy will be less noticeable, and less critical to the user.\n\nSee Also\nAccuracies\nARGeoTrackingAccuracyHigh\nGeo-tracking accuracy is high.\nARGeoTrackingAccuracyUndetermined\nGeo-tracking accuracy is undetermined.\nARGeoTrackingAccuracyMedium\nGeo-tracking accuracy is average."
  },
  {
    "title": "ARGeoTrackingAccuracyMedium | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingaccuracy/argeotrackingaccuracymedium",
    "html": "Discussion\n\nThis value indicates that visual localization is complete and geo-tracking accuracy is average.\n\nSee Also\nAccuracies\nARGeoTrackingAccuracyHigh\nGeo-tracking accuracy is high.\nARGeoTrackingAccuracyUndetermined\nGeo-tracking accuracy is undetermined.\nARGeoTrackingAccuracyLow\nGeo-tracking accuracy is low."
  },
  {
    "title": "autoFocusEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/2942262-autofocusenabled",
    "html": "Discussion\n\nFor apps deployed to iOS 11.3 or later, ARKit enables autofocus by default."
  },
  {
    "title": "ARCollaborationDataPriorityOptional | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcollaborationdatapriority/arcollaborationdatapriorityoptional",
    "html": "Discussion\n\nARKit sets the data priority to ARCollaborationDataPriorityOptional when the data is important and time-sensitive but the session can continue if it's not received.\n\nSee Also\nData Sensitivity\nARCollaborationDataPriorityCritical\nA priority that indicates that collaboration depends on this data."
  },
  {
    "title": "ARAltitudeSourcePrecise | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/araltitudesource/araltitudesourceprecise",
    "html": "See Also\nSources\nARAltitudeSourceCoarse\nThe framework sets the altitude using a coarse digital-elevation model.\nARAltitudeSourceUserDefined\nThe app defines the altitude.\nARAltitudeSourceUnknown\nAltitude isn’t yet set."
  },
  {
    "title": "ARAltitudeSourceCoarse | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/araltitudesource/araltitudesourcecoarse",
    "html": "Discussion\n\nThe accuracy of this altitude is noticeably imprecise at close range, but it’s sufficient from far away. Use this option to save computational resources for anchors that are far off in the distance.\n\nSee Also\nSources\nARAltitudeSourcePrecise\nThe framework sets the altitude using a high-resolution digital-elevation model.\nARAltitudeSourceUserDefined\nThe app defines the altitude.\nARAltitudeSourceUnknown\nAltitude isn’t yet set."
  },
  {
    "title": "ARBlendShapeLocationMouthDimpleLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthdimpleleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARCollaborationDataPriorityCritical | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcollaborationdatapriority/arcollaborationdataprioritycritical",
    "html": "Discussion\n\nARKit sets the data priority to ARCollaborationDataPriorityCritical when it's needed to establish or continue a collaborative session.\n\nSee Also\nData Sensitivity\nARCollaborationDataPriorityOptional\nA priority that indicates that collaboration can continue without this data."
  },
  {
    "title": "ARAltitudeSourceUserDefined | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/araltitudesource/araltitudesourceuserdefined",
    "html": "Discussion\n\nARKit records this altitude source when your app defines a geo anchor’s altitude.\n\nYou may acquire altitude by providing a particular scene coordinate to the session using getGeoLocationForPoint:completionHandler:.\n\nFor example, your app might set a geo anchor’s altitude by raycasting a surface, then adding an arbitrary y-amount to make the anchor more visible from afar.\n\nSee Also\nSources\nARAltitudeSourcePrecise\nThe framework sets the altitude using a high-resolution digital-elevation model.\nARAltitudeSourceCoarse\nThe framework sets the altitude using a coarse digital-elevation model.\nARAltitudeSourceUnknown\nAltitude isn’t yet set."
  },
  {
    "title": "ARPlaneDetection | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanedetection",
    "html": "Overview\n\nUse an empty set literal [] to specify no plane detection.\n\nTopics\nPlane Detection Options\nARPlaneDetectionHorizontal\nThe session detects planar surfaces that are perpendicular to gravity.\nARPlaneDetectionVertical\nThe session detects surfaces that are parallel to gravity, regardless of other orientation.\nARPlaneDetectionNone\nPlane detection is disabled.\nSee Also\nTracking Surfaces\nplaneDetection\nA value that specifies whether and how the session automatically attempts to detect flat surfaces in the camera-captured image.\nsceneReconstruction\nA flag that enables scene reconstruction.\n+ supportsSceneReconstruction:\nChecks if the device supports scene reconstruction."
  },
  {
    "title": "new | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/2923555-new",
    "html": "Discussion\n\nTo use the configuration in an AR experience, pass it as an argument to your app’s runWithConfiguration:options: function.\n\nSee Also\nCreating a Configuration\n- init\nInitializes a new world-tracking configuration.\ninitialWorldMap\nThe state from a previous AR session to attempt to resume with this session configuration."
  },
  {
    "title": "ARBlendShapeLocationEyeLookUpLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationeyelookupleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nLeft Eye\nARBlendShapeLocationEyeBlinkLeft\nThe coefficient describing closure of the eyelids over the left eye.\nARBlendShapeLocationEyeLookDownLeft\nThe coefficient describing movement of the left eyelids consistent with a downward gaze.\nARBlendShapeLocationEyeLookInLeft\nThe coefficient describing movement of the left eyelids consistent with a rightward gaze.\nARBlendShapeLocationEyeLookOutLeft\nThe coefficient describing movement of the left eyelids consistent with a leftward gaze.\nARBlendShapeLocationEyeSquintLeft\nThe coefficient describing contraction of the face around the left eye.\nARBlendShapeLocationEyeWideLeft\nThe coefficient describing a widening of the eyelids around the left eye."
  },
  {
    "title": "ARBlendShapeLocationEyeSquintLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationeyesquintleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nLeft Eye\nARBlendShapeLocationEyeBlinkLeft\nThe coefficient describing closure of the eyelids over the left eye.\nARBlendShapeLocationEyeLookDownLeft\nThe coefficient describing movement of the left eyelids consistent with a downward gaze.\nARBlendShapeLocationEyeLookInLeft\nThe coefficient describing movement of the left eyelids consistent with a rightward gaze.\nARBlendShapeLocationEyeLookOutLeft\nThe coefficient describing movement of the left eyelids consistent with a leftward gaze.\nARBlendShapeLocationEyeLookUpLeft\nThe coefficient describing movement of the left eyelids consistent with an upward gaze.\nARBlendShapeLocationEyeWideLeft\nThe coefficient describing a widening of the eyelids around the left eye."
  },
  {
    "title": "raycastQueryFromPoint:allowingTarget:alignment: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/3194595-raycastqueryfrompoint",
    "html": "Discussion\n\nWhen you call this function, ARKit creates a ray that extends in the positive z-direction from the argument screen space point, to determine if any of the argument targets exist in the physical environment anywhere along the ray. If so, ARKit returns a 3D position where the ray intersects the target.\n\nSee Also\nFinding Real-World Surfaces\n- hitTest:types:\nSearches for real-world objects or AR anchors in the captured camera image corresponding to a point in the SceneKit view.\nDeprecated"
  },
  {
    "title": "anchorForNode: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/2875546-anchorfornode",
    "html": "Parameters\nnode\n\nA SceneKit node in the view's scene.\n\nReturn Value\n\nThe ARAnchor object tracking the node, or nil if the node is not associated with an anchor or not in the view's scene.\n\nSee Also\nMapping Content to Real-World Positions\n- nodeForAnchor:\nReturns the SceneKit node associated with the specified AR anchor, if any.\n- unprojectPoint:ontoPlaneWithTransform:\nReturns the projection of a point from 2D view onto a plane in the 3D world space detected by ARKit."
  },
  {
    "title": "nodeForAnchor: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/2874122-nodeforanchor",
    "html": "Parameters\nanchor\n\nAn anchor in the view's AR session.\n\nReturn Value\n\nThe node whose position in the AR scene the anchor tracks, or nil if the anchor has no associated node or is not in the view's AR session.\n\nSee Also\nMapping Content to Real-World Positions\n- anchorForNode:\nReturns the AR anchor associated with the specified SceneKit node, if any.\n- unprojectPoint:ontoPlaneWithTransform:\nReturns the projection of a point from 2D view onto a plane in the 3D world space detected by ARKit."
  },
  {
    "title": "ARErrorCodeGeoTrackingNotAvailableAtLocation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodegeotrackingnotavailableatlocation",
    "html": "Discussion\n\nThis error code indicates that ARKit does not have the necessary localization imagery to support geo tracking at the user’s current location. See checkAvailabilityWithCompletionHandler: for more information.\n\nIf checkAvailabilityWithCompletionHandler: returns YES and an app begins geo-tracking session, ARKit provides this state reason when the user has moved to an unsupported area.\n\nSee Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARBlendShapeLocationMouthSmileLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthsmileleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationMouthClose | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationmouthclose",
    "html": "Discussion\n\nThis coefficient describes a closing of the lips without relation to the position of the jaw (the ARBlendShapeLocationJawOpen coefficient), so some values of the ARBlendShapeLocationMouthClose coefficient can produce unrealistic facial expressions unless other coefficients are also set to realistic values.\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in three states:\n\nA neutral face (all ARBlendShapeLocation coefficient values at 0.0, including both ARBlendShapeLocationJawOpen and ARBlendShapeLocationMouthClose)\n\nSetting only the ARBlendShapeLocationJawOpen coefficient to 1.0, while keeping all other coefficient values (including ARBlendShapeLocationMouthClose) at 0.0\n\nSetting both the ARBlendShapeLocationJawOpen and ARBlendShapeLocationMouthClose coefficients to 1.0, while keeping all other coefficient values at 0.0\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARBlendShapeLocationJawOpen | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocationjawopen",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARBlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ar_hand_skeleton_joint_name_middle_finger_tip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_middle_finger_tip",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_ring_finger_intermediate_base | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_ring_finger_intermediate_base",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_ring_finger_metacarpal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_ring_finger_metacarpal",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_middle_finger_knuckle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_middle_finger_knuckle",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_middle_finger_metacarpal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_middle_finger_metacarpal",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_ring_finger_intermediate_tip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_ring_finger_intermediate_tip",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_middle_finger_intermediate_tip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_middle_finger_intermediate_tip",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_little_finger_intermediate_base | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_little_finger_intermediate_base",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_index_finger_knuckle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_index_finger_knuckle",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_forearm_wrist | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_forearm_wrist",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_arm\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ar_hand_skeleton_joint_name_forearm_arm | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t/ar_hand_skeleton_joint_name_forearm_arm",
    "html": "See Also\nHand tracking\nar_hand_chirality_t\nThe values identifying hand chirality.\nBeta\nar_hand_anchor_create\nCreates a hand anchor.\nBeta\nar_hand_anchor_get_chirality\nGets the value that indicates whether the hand is a left or right hand.\nBeta\nar_hand_tracking_configuration_t\nBeta\nar_hand_tracking_provider_create\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nar_hand_tracking_provider_is_supported\nReturns a Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nBeta\nar_hand_tracking_provider_get_latest_anchors\nFetches the most recent hand anchors for each hand.\nBeta\nar_hand_tracking_provider_get_required_authorization_type\nGets the types of authorizations required to track hands.\nBeta\nar_hand_tracking_configuration_create\nBeta\nar_hand_tracking_provider_set_update_handler\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_tracking_provider_set_update_handler_f\nSets the handler for receiving hand tracking updates.\nBeta\nar_hand_skeleton_joint_name_forearm_wrist\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_index_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_index_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_index_finger_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_little_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_little_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_little_finger_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_middle_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_middle_finger_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nBeta\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_ring_finger_knuckle\nBeta\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nBeta\nar_hand_skeleton_joint_name_ring_finger_tip\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_base\nBeta\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nBeta\nar_hand_skeleton_joint_name_thumb_knuckle\nBeta\nar_hand_skeleton_joint_name_thumb_tip\nBeta\nar_hand_skeleton_joint_name_wrist\nBeta\nar_hand_chirality_left\nA left hand.\nBeta\nar_hand_chirality_right\nA right hand.\nBeta"
  },
  {
    "title": "ARErrorCodeInsufficientFeatures | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodeinsufficientfeatures",
    "html": "Discussion\n\nFor more information about a session’s feature requirements, see Managing Session Life Cycle and Tracking Quality.\n\nSee Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARErrorCodeInvalidCollaborationData | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodeinvalidcollaborationdata",
    "html": "Discussion\n\nARKit produces this error when an app passes invalid data into the updateWithCollaborationData: function.\n\nSee Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARErrorCodeInvalidConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodeinvalidconfiguration",
    "html": "See Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARErrorCodeMicrophoneUnauthorized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodemicrophoneunauthorized",
    "html": "Discussion\n\nWhen this error occurs, the app needs to prompt the user to give permission to use the microphone in Settings.\n\nSee Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARErrorCodeInvalidReferenceObject | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodeinvalidreferenceobject",
    "html": "See Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARErrorCodeSensorUnavailable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodesensorunavailable",
    "html": "See Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARGeoTrackingStateReasonDevicePointedTooLow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatereason/argeotrackingstatereasondevicepointedtoolow",
    "html": "Discussion\n\nARKit provides the app with this reason when the app is in state ARGeoTrackingStateLocalizing and the device is not capturing enough of the necessary live-camera imagery needed for visual localization because the user is pointing the camera too low. To resolve the issue, the app needs to instruct the user to raise the device and follow the guidance in Assisting the User with Visual Localization.\n\nSee Also\nStatus Reasons\nARGeoTrackingStateReasonNone\nNo issues reported.\nARGeoTrackingStateReasonNotAvailableAtLocation\nThe location doesn't provide geotracking.\nARGeoTrackingStateReasonNeedLocationPermissions\nThe location requires user permission for geotracking.\nARGeoTrackingStateReasonWorldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\nARGeoTrackingStateReasonWaitingForLocation\nA state in which the framework performs a check for the user's GPS position.\nARGeoTrackingStateReasonWaitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\nARGeoTrackingStateReasonGeoDataNotLoaded\nA state in which the framework downloads localization imagery.\nARGeoTrackingStateReasonVisualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "ARGeoTrackingStateReasonNone | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatereason/argeotrackingstatereasonnone",
    "html": "Discussion\n\nThis reason indicates that there is no user action currently needed to improve the geo-tracking state.\n\nSee Also\nStatus Reasons\nARGeoTrackingStateReasonNotAvailableAtLocation\nThe location doesn't provide geotracking.\nARGeoTrackingStateReasonNeedLocationPermissions\nThe location requires user permission for geotracking.\nARGeoTrackingStateReasonDevicePointedTooLow\nThe position of the device is too low for geotracking.\nARGeoTrackingStateReasonWorldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\nARGeoTrackingStateReasonWaitingForLocation\nA state in which the framework performs a check for the user's GPS position.\nARGeoTrackingStateReasonWaitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\nARGeoTrackingStateReasonGeoDataNotLoaded\nA state in which the framework downloads localization imagery.\nARGeoTrackingStateReasonVisualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "ARGeoTrackingStateReasonNotAvailableAtLocation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatereason/argeotrackingstatereasonnotavailableatlocation",
    "html": "Discussion\n\nThis reason indicates that ARKit does not have the necessary landscape data to support geo tracking at the user’s current location. See checkAvailabilityWithCompletionHandler: for more information.\n\nIf checkAvailabilityWithCompletionHandler: returns YES and an app begins a geo-tracking session, ARKit provides this state reason when the user has moved to an unsupported area.\n\nSee Also\nStatus Reasons\nARGeoTrackingStateReasonNone\nNo issues reported.\nARGeoTrackingStateReasonNeedLocationPermissions\nThe location requires user permission for geotracking.\nARGeoTrackingStateReasonDevicePointedTooLow\nThe position of the device is too low for geotracking.\nARGeoTrackingStateReasonWorldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\nARGeoTrackingStateReasonWaitingForLocation\nA state in which the framework performs a check for the user's GPS position.\nARGeoTrackingStateReasonWaitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\nARGeoTrackingStateReasonGeoDataNotLoaded\nA state in which the framework downloads localization imagery.\nARGeoTrackingStateReasonVisualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "ARGeoTrackingStateReasonWorldTrackingUnstable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatereason/argeotrackingstatereasonworldtrackingunstable",
    "html": "Discussion\n\nThis reason indicates that ARKit’s local-space tracking is functioning at a limited capacity. To retrieve more information about the cause, an app needs to refer to the camera’s trackingState. For the possible causes of this state, see ARTrackingState and ARTrackingStateReason.\n\nSee Also\nStatus Reasons\nARGeoTrackingStateReasonNone\nNo issues reported.\nARGeoTrackingStateReasonNotAvailableAtLocation\nThe location doesn't provide geotracking.\nARGeoTrackingStateReasonNeedLocationPermissions\nThe location requires user permission for geotracking.\nARGeoTrackingStateReasonDevicePointedTooLow\nThe position of the device is too low for geotracking.\nARGeoTrackingStateReasonWaitingForLocation\nA state in which the framework performs a check for the user's GPS position.\nARGeoTrackingStateReasonWaitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\nARGeoTrackingStateReasonGeoDataNotLoaded\nA state in which the framework downloads localization imagery.\nARGeoTrackingStateReasonVisualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "ARGeoTrackingStateReasonGeoDataNotLoaded | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatereason/argeotrackingstatereasongeodatanotloaded",
    "html": "Discussion\n\nARKit provides this reason in state ARGeoTrackingStateLocalizing when the session is actively attempting to download localization imagery (see Refine the User's Position with Imagery).\n\nIf this state persists for too long, it may indicate a network issue. If a reasonable amount of time elapses in this state reason, the app may consider requesting that the user check their internet connection.\n\nSee Also\nStatus Reasons\nARGeoTrackingStateReasonNone\nNo issues reported.\nARGeoTrackingStateReasonNotAvailableAtLocation\nThe location doesn't provide geotracking.\nARGeoTrackingStateReasonNeedLocationPermissions\nThe location requires user permission for geotracking.\nARGeoTrackingStateReasonDevicePointedTooLow\nThe position of the device is too low for geotracking.\nARGeoTrackingStateReasonWorldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\nARGeoTrackingStateReasonWaitingForLocation\nA state in which the framework performs a check for the user's GPS position.\nARGeoTrackingStateReasonWaitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\nARGeoTrackingStateReasonVisualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "ARGeoTrackingStateReasonVisualLocalizationFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatereason/argeotrackingstatereasonvisuallocalizationfailed",
    "html": "Discussion\n\nARKit provides this reason when visual localization is taking too long. This indicates that the app has met all requirements for geo tracking, except for visual localization. In this situation, the app needs to ask the user to pan the device around the physical environment to acquire more camera-feed imagery. For more information, see Assisting the User with Visual Localization.\n\nSee Also\nStatus Reasons\nARGeoTrackingStateReasonNone\nNo issues reported.\nARGeoTrackingStateReasonNotAvailableAtLocation\nThe location doesn't provide geotracking.\nARGeoTrackingStateReasonNeedLocationPermissions\nThe location requires user permission for geotracking.\nARGeoTrackingStateReasonDevicePointedTooLow\nThe position of the device is too low for geotracking.\nARGeoTrackingStateReasonWorldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\nARGeoTrackingStateReasonWaitingForLocation\nA state in which the framework performs a check for the user's GPS position.\nARGeoTrackingStateReasonWaitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\nARGeoTrackingStateReasonGeoDataNotLoaded\nA state in which the framework downloads localization imagery."
  },
  {
    "title": "ARGeoTrackingStateReasonWaitingForAvailabilityCheck | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatereason/argeotrackingstatereasonwaitingforavailabilitycheck",
    "html": "Discussion\n\nWhile in this state, the app waits for the geotracking subsystem to determine geotracking availability at the user's GPS location. Inform the user of the check in progress; for instance, present a message alerting them to the geotracking initialization process.\n\nSee Also\nStatus Reasons\nARGeoTrackingStateReasonNone\nNo issues reported.\nARGeoTrackingStateReasonNotAvailableAtLocation\nThe location doesn't provide geotracking.\nARGeoTrackingStateReasonNeedLocationPermissions\nThe location requires user permission for geotracking.\nARGeoTrackingStateReasonDevicePointedTooLow\nThe position of the device is too low for geotracking.\nARGeoTrackingStateReasonWorldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\nARGeoTrackingStateReasonWaitingForLocation\nA state in which the framework performs a check for the user's GPS position.\nARGeoTrackingStateReasonGeoDataNotLoaded\nA state in which the framework downloads localization imagery.\nARGeoTrackingStateReasonVisualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "offset | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometrysource/3516919-offset",
    "html": "See Also\nGetting Geometry Information\nvar componentsPerVector: Int\nThe number of scalar components in each vector.\nvar count: Int\nThe number of vectors in the buffer.\nvar format: MTLVertexFormat\nThe type of vector data in the buffer.\nvar stride: Int\nThe length, in bytes, of the start of one vector in the buffer to the start of the next vector."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/transformaction/info_id",
    "html": "Overview\n\nThe default value is Transform.\n\nDeclaration\nuniform token info:id = \"Transform\"\n\n\nSee Also\nProperties\naffectedObjects\nA list of prims to which a transform applies.\nxformTarget\nA prim that provides the transform to which this action animates.\nduration\nThe amount of time between the start of an action and its end.\ntype\nAn option that determines if a transform is based on another, source transform.\neaseType\nAn option that describes the animation’s change in pace over time."
  },
  {
    "title": "ARErrorCodeLocationUnauthorized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodelocationunauthorized",
    "html": "Discussion\n\nTo resolve this issue, the app needs to ask the user to enable location access for this app in Settings.\n\nSee Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "referenceObjectByMergingObject:error: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/3019548-referenceobjectbymergingobject",
    "html": "Parameters\nobject\n\nThe other reference object with which to combine this reference object.\n\nerror\n\nA pointer to an NSError. On completion, if the method returns nil, this pointer references an object describing the failure.\n\nReturn Value\n\nA new ARReferenceObject that includes the spatial information from both objects. If the two objects cannot be merged, this method returns nil and the error parameter describes the failure.\n\nDiscussion\n\nThe accuracy of 3D object detection depends on similarity of lighting and environmental conditions between when you scan a real object (producing an ARReferenceObject) and when a user of your app attempts to detect that object. If, for example, you scan an object in a bright environment, then a user attempts to detect it in a dark environment, ARKit may fail to recognize that the real object matches the reference object, or may not detect the object quickly.\n\nTo make a reference object that is more robust in a wide variety of detection conditions, scan the same real-world object multiple times: For each scan, vary the lighting conditions or the background environment to capture the variety of situations in which your app might attempt to detect the same real object. Then, use this method to combine those scan results into a single ARReferenceObject incorporating recognition information for all the conditions you scanned in.\n\nSee Also\nCreating Derivative Reference Objects\n- referenceObjectByApplyingTransform:\nReturns a new reference object created by applying the specified transform to this reference object's geometric data."
  },
  {
    "title": "affectedObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/transformanimationaction/affectedobjects",
    "html": "Overview\n\nAdd one or more Xformable prims to this list.\n\nDeclaration\nrel affectedObjects\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\nanimation\nA prim that contains a transform animation."
  },
  {
    "title": "ARWorldAlignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldalignment",
    "html": "Topics\nAlignments\nARWorldAlignmentGravity\nThe coordinate system's y-axis is parallel to gravity, and its origin is the initial position of the device.\nARWorldAlignmentGravityAndHeading\nThe coordinate system's y-axis is parallel to gravity, its x- and z-axes are oriented to compass heading, and its origin is the initial position of the device.\nARWorldAlignmentCamera\nThe scene coordinate system is locked to match the orientation of the camera.\nSee Also\nConfiguring the AR session\nlightEstimationEnabled\nA Boolean value specifying whether ARKit analyzes scene lighting in captured camera images.\nworldAlignment\nA value specifying how the session maps real-world device motion into a 3D scene coordinate system."
  },
  {
    "title": "ARErrorCodeHighResolutionFrameCaptureFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodehighresolutionframecapturefailed",
    "html": "See Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARErrorCodeHighResolutionFrameCaptureInProgress | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcodehighresolutionframecaptureinprogress",
    "html": "See Also\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline."
  },
  {
    "title": "ARReferenceObjectArchiveExtension | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobjectarchiveextension",
    "html": "Discussion\n\nUse this filename extension when constructing a URL to save a reference object file with the exportObjectToURL:previewImage:error: method.\n\nSee Also\nSaving Recorded Objects\n- exportObjectToURL:previewImage:error:\nWrites a binary representation of the object to the specified file URL."
  },
  {
    "title": "ARVideoFormat | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arvideoformat",
    "html": "Overview\n\nThis class is immutable; to set the frame rate and video resolution for an AR session, set your configuration's videoFormat property to one of the formats in the supportedVideoFormats array.\n\nTopics\nAccessing format information\nframesPerSecond\nThe rate at which the session captures video and provides AR frame information.\nimageResolution\nThe size, in pixels, of video images captured in the session.\nisRecommendedForHighResolutionFrameCapturing\nDetermines whether the framework considers a format suitable for high-resolution frame capture.\nvideoHDRSupported\nDetermines whether the format supports high dynamic range (HDR).\nInspecting the video source\ncaptureDevicePosition\nThe position of the capture device.\nAVCaptureDevicePosition\nConstants that indicate the physical position of a capture device.\ncaptureDeviceType\nThe camera that supplies the video format.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nSee Also\nManaging video capture options\nvideoFormat\nA camera type, resolution, and frame rate for an AR session.\nsupportedVideoFormats\nThe set of video capture formats available on the current device.\nvideoHDRAllowed\nEnables high dynamic range (HDR) for the session's camera feed.\nconfigurableCaptureDeviceForPrimaryCamera\nAn object that enables you to alter the appearance of a frame's captured image.\nrecommendedVideoFormatFor4KResolution\nProvides a 4K video format if the device and configuration support it.\nrecommendedVideoFormatForHighResolutionFrameCapturing\nReturns a video format that the framework recommends for high-resolution-still-image capture."
  },
  {
    "title": "captureDeviceType | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arvideoformat/3738412-capturedevicetype",
    "html": "Discussion\n\nTo specify a particular video format, select from your configuration's supportedVideoFormats and set the desired format to the configuration's videoFormat property.\n\nFor example, to specify the ultra-wide camera in a face-tracking session, search the supported video formats for the AVCaptureDeviceTypeBuiltInUltraWideCamera capture device.\n\nlet config = ARFaceTrackingConfiguration()\nfor videoFormat in ARFaceTrackingConfiguration.supportedVideoFormats {\n    if videoFormat.captureDeviceType == .builtInUltraWideCamera {\n        config.videoFormat = videoFormat\n        break\n    }\n}\nsession.run(config)\n\n\nImportant\n\nAR frames only contain depth data (capturedDepthData) in face-tracking sessions that use the TrueDepth camera.\n\nSee Also\nInspecting the video source\ncaptureDevicePosition\nThe position of the capture device.\nAVCaptureDevicePosition\nConstants that indicate the physical position of a capture device."
  },
  {
    "title": "framesPerSecond | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arvideoformat/2942259-framespersecond",
    "html": "See Also\nAccessing format information\nimageResolution\nThe size, in pixels, of video images captured in the session.\nisRecommendedForHighResolutionFrameCapturing\nDetermines whether the framework considers a format suitable for high-resolution frame capture.\nvideoHDRSupported\nDetermines whether the format supports high dynamic range (HDR)."
  },
  {
    "title": "captureDevicePosition | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arvideoformat/3112799-capturedeviceposition",
    "html": "See Also\nInspecting the video source\nAVCaptureDevicePosition\nConstants that indicate the physical position of a capture device.\ncaptureDeviceType\nThe camera that supplies the video format."
  },
  {
    "title": "imageResolution | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arvideoformat/2942257-imageresolution",
    "html": "Discussion\n\nVideo format sizes are relative to the native sensor orientation of the device camera, and as such are always landscape-oriented.\n\nSee Also\nAccessing format information\nframesPerSecond\nThe rate at which the session captures video and provides AR frame information.\nisRecommendedForHighResolutionFrameCapturing\nDetermines whether the framework considers a format suitable for high-resolution frame capture.\nvideoHDRSupported\nDetermines whether the format supports high dynamic range (HDR)."
  },
  {
    "title": "videoHDRSupported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arvideoformat/3930053-videohdrsupported",
    "html": "Discussion\n\nCall this function before setting videoHDRAllowed to true to first check whether a video format supports HDR.\n\nSee Also\nAccessing format information\nframesPerSecond\nThe rate at which the session captures video and provides AR frame information.\nimageResolution\nThe size, in pixels, of video images captured in the session.\nisRecommendedForHighResolutionFrameCapturing\nDetermines whether the framework considers a format suitable for high-resolution frame capture."
  },
  {
    "title": "ARGeometryPrimitiveType.triangle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometryprimitivetype/triangle",
    "html": "See Also\nType of Connection\ncase line\nA line segment in which a line connects two vertices."
  },
  {
    "title": "subscript(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometryelement/3601240-subscript",
    "html": "Discussion\n\nThis subscript operates on geometry elements of type Int32 with 4 bytesPerIndex. In the case of the faces property, this operator returns an array of size 3. The contents of the array are vertex indices that compose a triangle primitive, which represents the face identified by the argument face index.\n\nSee Also\nAccessing Index Data\nvar buffer: MTLBuffer\nA Metal buffer containing primitive data."
  },
  {
    "title": "buffer | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometryelement/3516912-buffer",
    "html": "See Also\nAccessing Index Data\nsubscript(Int) -> [Int32]\nProvides an array of vertex indices that respresents the geometric primitive at the subscripted index."
  },
  {
    "title": "primitiveType | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometryelement/3521379-primitivetype",
    "html": "See Also\nGetting Index Information\nvar bytesPerIndex: Int\nThe number of bytes for each index.\nvar count: Int\nThe number of primitives in the buffer.\nvar indexCountPerPrimitive: Int\nThe number of indices for each primitive.\nenum ARGeometryPrimitiveType\nThe kind of connection between vertices."
  },
  {
    "title": "bytesPerIndex | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometryelement/3516913-bytesperindex",
    "html": "See Also\nGetting Index Information\nvar count: Int\nThe number of primitives in the buffer.\nvar indexCountPerPrimitive: Int\nThe number of indices for each primitive.\nvar primitiveType: ARGeometryPrimitiveType\nThe geometry's type of data (triangle, or line).\nenum ARGeometryPrimitiveType\nThe kind of connection between vertices."
  },
  {
    "title": "indexCountPerPrimitive | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometryelement/3521378-indexcountperprimitive",
    "html": "Discussion\n\nThe value of this property relates to the primitiveType. For ARGeometryPrimitiveType.triangle, the value is 3. For more information, see ARGeometryPrimitiveType.\n\nSee Also\nGetting Index Information\nvar bytesPerIndex: Int\nThe number of bytes for each index.\nvar count: Int\nThe number of primitives in the buffer.\nvar primitiveType: ARGeometryPrimitiveType\nThe geometry's type of data (triangle, or line).\nenum ARGeometryPrimitiveType\nThe kind of connection between vertices."
  },
  {
    "title": "ARGeometryPrimitiveType | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometryprimitivetype",
    "html": "Overview\n\nWhen you enable sceneReconstruction on a world-tracking configuration, ARKit provies a wireframe mesh that models the shape of the real world using a collection of connected vertices. ARKit uses ARGeometryPrimitiveType to indicate how a particular property of that mesh is interpreted. For example, a mesh geometry's faces property specifies that each face within the geometry is of type ARGeometryPrimitiveType.triangle.\n\nTopics\nType of Connection\ncase line\nA line segment in which a line connects two vertices.\ncase triangle\nThree vertices that connect to form a triangle.\nRelationships\nConforms To\nSendable\nSee Also\nGetting Index Information\nvar bytesPerIndex: Int\nThe number of bytes for each index.\nvar count: Int\nThe number of primitives in the buffer.\nvar indexCountPerPrimitive: Int\nThe number of indices for each primitive.\nvar primitiveType: ARGeometryPrimitiveType\nThe geometry's type of data (triangle, or line)."
  },
  {
    "title": "ARSceneReconstruction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscenereconstruction",
    "html": "Overview\n\nWhen you set one of the these values onto a world-tracking configuration's sceneReconstruction property, ARKit provides you with a mesh that models the real-world surrounding the user.\n\nTopics\nModeling the Environment\nARSceneReconstructionMesh\nA polygonal mesh approximation of the physical environment.\nARSceneReconstructionMeshWithClassification\nAn approximate shape of the physical environment, including classification of the real-world objects within it.\nARSceneReconstructionNone\nDisables the scene reconstruction feature."
  },
  {
    "title": "ARSceneReconstructionMesh | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscenereconstruction/arscenereconstructionmesh",
    "html": "See Also\nModeling the Environment\nARSceneReconstructionMeshWithClassification\nAn approximate shape of the physical environment, including classification of the real-world objects within it.\nARSceneReconstructionNone\nDisables the scene reconstruction feature."
  },
  {
    "title": "count | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometryelement/3516914-count",
    "html": "See Also\nGetting Index Information\nvar bytesPerIndex: Int\nThe number of bytes for each index.\nvar indexCountPerPrimitive: Int\nThe number of indices for each primitive.\nvar primitiveType: ARGeometryPrimitiveType\nThe geometry's type of data (triangle, or line).\nenum ARGeometryPrimitiveType\nThe kind of connection between vertices."
  },
  {
    "title": "ARSceneReconstructionMeshWithClassification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscenereconstruction/arscenereconstructionmeshwithclassification",
    "html": "See Also\nModeling the Environment\nARSceneReconstructionMesh\nA polygonal mesh approximation of the physical environment.\nARSceneReconstructionNone\nDisables the scene reconstruction feature."
  },
  {
    "title": "ARSegmentationClass | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsegmentationclass",
    "html": "Overview\n\nARKit applies the categories defined in this class based on its interpretation of the camera feed's pixel data. Only people are identified in a camera feed, and therefore the available pixel classifications are either ARSegmentationClassPerson or ARSegmentationClassNone.\n\nTopics\nClassifying Pixels\nARSegmentationClassPerson\nA classification of a pixel in the segmentation buffer as part of a person.\nARSegmentationClassNone\nA classification of a pixel in the segmentation buffer as unidentified.\nSee Also\nChecking for people\ndetectedBody\nThe screen position information of a body that ARKit recognizes in the camera image.\nARBody2D\nThe screen-space representation of a person ARKit recognizes in the camera feed.\nsegmentationBuffer\nA buffer that contains pixel information identifying the shape of objects from the camera feed that you use to occlude virtual content.\nestimatedDepthData\nA buffer that represents the estimated depth values from the camera feed that you use to occlude virtual content."
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/status/3238108",
    "html": "Parameters\na\n\nThe first status to compare.\n\nb\n\nThe second status to compare.\n\nReturn Value\n\ntrue if the two statuses are equal; otherwise, false.\n\nRelationships\nFrom Protocol\nEquatable\nSee Also\nHashes of a Classification Status\nfunc hash(into: inout Hasher)\nHashes the status by passing it to the given hash function.\nstatic func != (ARPlaneAnchor.Classification.Status, ARPlaneAnchor.Classification.Status) -> Bool\nReturns a Boolean value indicating whether two values are not equal.\nvar hashValue: Int\nA value that identifies an object uniquely as compared to other instances of the same type."
  },
  {
    "title": "ARGeoTrackingState | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstate",
    "html": "Overview\n\nFor any state in a frame’s geoTrackingStatus, ARKit provides a stateReason. A given geo-tracking status may intermix states and reasons, so the reasons are not tied to specific states.\n\nTopics\nStates\nARGeoTrackingStateInitializing\nThe session is initializing geo tracking.\nARGeoTrackingStateLocalized\nGeo tracking is localized.\nARGeoTrackingStateLocalizing\nGeo tracking is attempting to localize against a map.\nARGeoTrackingStateNotAvailable\nGeo tracking is not available.\nSee Also\nChecking State\nstate\nA value that describes the session’s current geo-tracking state."
  },
  {
    "title": "ARBlendShapeLocation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arblendshapelocation",
    "html": "Discussion\n\nThe blendShapes dictionary provided by an ARFaceAnchor object describes the facial expression of a detected face in terms of the movements of specific facial features. For each key in the dictionary, the corresponding value is a floating point number indicating the current position of that feature relative to its neutral configuration, ranging from 0.0 (neutral) to 1.0 (maximum movement).\n\nARKit provides many blend shape coefficients, resulting in a detailed model of a facial expression; however, you can use as many or as few of the coefficients as you desire to create a visual effect. For example, you might animate a simple cartoon character using only the ARBlendShapeLocationJawOpen, ARBlendShapeLocationEyeBlinkLeft, and ARBlendShapeLocationEyeBlinkRight coefficients. A professional 3D artist could create a detailed character model rigged for realistic animation using a larger set, or the entire set, of coefficients.\n\nNote\n\nIn the naming of blend shape coefficients, the left and right directions are relative to the face. That is, the ARBlendShapeLocationEyeBlinkRight coefficient refers to the face's right eye. ARKit views running a face-tracking session mirror the camera image, so the face's right eye appears on the right side in the view.\n\nTopics\nLeft Eye\nARBlendShapeLocationEyeBlinkLeft\nThe coefficient describing closure of the eyelids over the left eye.\nARBlendShapeLocationEyeLookDownLeft\nThe coefficient describing movement of the left eyelids consistent with a downward gaze.\nARBlendShapeLocationEyeLookInLeft\nThe coefficient describing movement of the left eyelids consistent with a rightward gaze.\nARBlendShapeLocationEyeLookOutLeft\nThe coefficient describing movement of the left eyelids consistent with a leftward gaze.\nARBlendShapeLocationEyeLookUpLeft\nThe coefficient describing movement of the left eyelids consistent with an upward gaze.\nARBlendShapeLocationEyeSquintLeft\nThe coefficient describing contraction of the face around the left eye.\nARBlendShapeLocationEyeWideLeft\nThe coefficient describing a widening of the eyelids around the left eye.\nRight Eye\nARBlendShapeLocationEyeBlinkRight\nThe coefficient describing closure of the eyelids over the right eye.\nARBlendShapeLocationEyeLookDownRight\nThe coefficient describing movement of the right eyelids consistent with a downward gaze.\nARBlendShapeLocationEyeLookInRight\nThe coefficient describing movement of the right eyelids consistent with a leftward gaze.\nARBlendShapeLocationEyeLookOutRight\nThe coefficient describing movement of the right eyelids consistent with a rightward gaze.\nARBlendShapeLocationEyeLookUpRight\nThe coefficient describing movement of the right eyelids consistent with an upward gaze.\nARBlendShapeLocationEyeSquintRight\nThe coefficient describing contraction of the face around the right eye.\nARBlendShapeLocationEyeWideRight\nThe coefficient describing a widening of the eyelids around the right eye.\nMouth and Jaw\nARBlendShapeLocationJawForward\nThe coefficient describing forward movement of the lower jaw.\nARBlendShapeLocationJawLeft\nThe coefficient describing leftward movement of the lower jaw.\nARBlendShapeLocationJawRight\nThe coefficient describing rightward movement of the lower jaw.\nARBlendShapeLocationJawOpen\nThe coefficient describing an opening of the lower jaw.\nARBlendShapeLocationMouthClose\nThe coefficient describing closure of the lips independent of jaw position.\nARBlendShapeLocationMouthFunnel\nThe coefficient describing contraction of both lips into an open shape.\nARBlendShapeLocationMouthPucker\nThe coefficient describing contraction and compression of both closed lips.\nARBlendShapeLocationMouthLeft\nThe coefficient describing leftward movement of both lips together.\nARBlendShapeLocationMouthRight\nThe coefficient describing rightward movement of both lips together.\nARBlendShapeLocationMouthSmileLeft\nThe coefficient describing upward movement of the left corner of the mouth.\nARBlendShapeLocationMouthSmileRight\nThe coefficient describing upward movement of the right corner of the mouth.\nARBlendShapeLocationMouthFrownLeft\nThe coefficient describing downward movement of the left corner of the mouth.\nARBlendShapeLocationMouthFrownRight\nThe coefficient describing downward movement of the right corner of the mouth.\nARBlendShapeLocationMouthDimpleLeft\nThe coefficient describing backward movement of the left corner of the mouth.\nARBlendShapeLocationMouthDimpleRight\nThe coefficient describing backward movement of the right corner of the mouth.\nARBlendShapeLocationMouthStretchLeft\nThe coefficient describing leftward movement of the left corner of the mouth.\nARBlendShapeLocationMouthStretchRight\nThe coefficient describing rightward movement of the left corner of the mouth.\nARBlendShapeLocationMouthRollLower\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nARBlendShapeLocationMouthRollUpper\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nARBlendShapeLocationMouthShrugLower\nThe coefficient describing outward movement of the lower lip.\nARBlendShapeLocationMouthShrugUpper\nThe coefficient describing outward movement of the upper lip.\nARBlendShapeLocationMouthPressLeft\nThe coefficient describing upward compression of the lower lip on the left side.\nARBlendShapeLocationMouthPressRight\nThe coefficient describing upward compression of the lower lip on the right side.\nARBlendShapeLocationMouthLowerDownLeft\nThe coefficient describing downward movement of the lower lip on the left side.\nARBlendShapeLocationMouthLowerDownRight\nThe coefficient describing downward movement of the lower lip on the right side.\nARBlendShapeLocationMouthUpperUpLeft\nThe coefficient describing upward movement of the upper lip on the left side.\nARBlendShapeLocationMouthUpperUpRight\nThe coefficient describing upward movement of the upper lip on the right side.\nEyebrows, Cheeks, and Nose\nARBlendShapeLocationBrowDownLeft\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowDownRight\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationBrowInnerUp\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nARBlendShapeLocationBrowOuterUpLeft\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nARBlendShapeLocationBrowOuterUpRight\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nARBlendShapeLocationCheekPuff\nThe coefficient describing outward movement of both cheeks.\nARBlendShapeLocationCheekSquintLeft\nThe coefficient describing upward movement of the cheek around and below the left eye.\nARBlendShapeLocationCheekSquintRight\nThe coefficient describing upward movement of the cheek around and below the right eye.\nARBlendShapeLocationNoseSneerLeft\nThe coefficient describing a raising of the left side of the nose around the nostril.\nARBlendShapeLocationNoseSneerRight\nThe coefficient describing a raising of the right side of the nose around the nostril.\nTongue\nARBlendShapeLocationTongueOut\nThe coefficient describing extension of the tongue.\nSee Also\nUsing Blend Shapes\nblendShapes\nA dictionary of named coefficients representing the detected facial expression in terms of the movement of specific facial features."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/status/3020716",
    "html": "Parameters\nlhs\n\nA value to compare.\n\nrhs\n\nAnother value to compare.\n\nDiscussion\n\nInequality is the inverse of equality. For any values a and b, a != b implies that a == b is false.\n\nThis is the default implementation of the not-equal-to operator (!=) for any type that conforms to Equatable.\n\nSee Also\nHashes of a Classification Status\nfunc hash(into: inout Hasher)\nHashes the status by passing it to the given hash function.\nstatic func == (ARPlaneAnchor.Classification.Status, ARPlaneAnchor.Classification.Status) -> Bool\nIndicates whether two statuses are equal.\nvar hashValue: Int\nA value that identifies an object uniquely as compared to other instances of the same type."
  },
  {
    "title": "ARGeoTrackingStateNotAvailable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstate/argeotrackingstatenotavailable",
    "html": "Discussion\n\nThis state occurs when ARKit doesn’t have the landscape data necessary for visual localization at the user’s location. For more information, see ARGeoTrackingStateLocalizing.\n\nSee Also\nStates\nARGeoTrackingStateInitializing\nThe session is initializing geo tracking.\nARGeoTrackingStateLocalized\nGeo tracking is localized.\nARGeoTrackingStateLocalizing\nGeo tracking is attempting to localize against a map."
  },
  {
    "title": "ARGeoTrackingStateLocalizing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstate/argeotrackingstatelocalizing",
    "html": "Discussion\n\nIn ARGeoTrackingStateLocalizing, the session downloads localization imagery for the user’s geographic location and compares it with captures from the device’s camera. This process is referred to as visual localization. When ARKit succeeds in matching this imagery with captures from the camera, the state moves to ARGeoTrackingStateLocalized and the app is free to create location anchors. For more information about localization imagery, see Refine the User's Position with Imagery.\n\nAssisting the User with Visual Localization\n\nTo establish visual localization, the user must move the camera so it acquires the captures that ARKit needs. To elicit the right user movements in ARGeoTrackingStateLocalizing, the app needs to advise the user to:\n\nPoint the camera at buildings and other visual landmarks to help ARKit match the live camera data with its preexisting landscape-data.\n\nAvoid pointing the device at objects that are too general, like trees. It’s better to focus on distinct visuals, like structures, or signs.\n\nAvoid pointing the device at real-world objects that are transient, like parked cars, or a construction site.\n\nBecause lighting conditions can affect visual localization, avoid geo tracking at night.\n\nSee Also\nStates\nARGeoTrackingStateInitializing\nThe session is initializing geo tracking.\nARGeoTrackingStateLocalized\nGeo tracking is localized.\nARGeoTrackingStateNotAvailable\nGeo tracking is not available."
  },
  {
    "title": "ARGeoTrackingStateInitializing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstate/argeotrackingstateinitializing",
    "html": "Discussion\n\nIn this state, the session is preparing tracking and an app has the opportunity to onboard users to the experience. The app watches for changes in stateReason and coaches the user accordingly to expedite initialization.\n\nSee Also\nStates\nARGeoTrackingStateLocalized\nGeo tracking is localized.\nARGeoTrackingStateLocalizing\nGeo tracking is attempting to localize against a map.\nARGeoTrackingStateNotAvailable\nGeo tracking is not available."
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/status/3238110-hashvalue",
    "html": "Discussion\n\nImportant\n\nhashValue is deprecated as a Hashable requirement. To conform to Hashable, implement the hash(into:) requirement instead.\n\nHash values are not guaranteed to be equal across different executions of your program. Do not save hash values to use during a future execution.\n\nRelationships\nFrom Protocol\nHashable\nSee Also\nHashes of a Classification Status\nfunc hash(into: inout Hasher)\nHashes the status by passing it to the given hash function.\nstatic func == (ARPlaneAnchor.Classification.Status, ARPlaneAnchor.Classification.Status) -> Bool\nIndicates whether two statuses are equal.\nstatic func != (ARPlaneAnchor.Classification.Status, ARPlaneAnchor.Classification.Status) -> Bool\nReturns a Boolean value indicating whether two values are not equal."
  },
  {
    "title": "ARSkeletonJointNameRoot | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletonjointnameroot",
    "html": "Discussion\n\nThe root skeletal joint is at the hip, and is located at the origin of its associated body anchor.\n\nSee Also\nIdentifying Joints\nARSkeletonJointNameHead\nA skeletal joint that ARKit tracks representing the head.\nARSkeletonJointNameLeftFoot\nA skeletal joint that ARKit tracks representing the left foot.\nARSkeletonJointNameLeftHand\nA skeletal joint that ARKit tracks representing the left hand.\nARSkeletonJointNameLeftShoulder\nA skeletal joint that ARKit tracks representing the left shoulder.\nARSkeletonJointNameRightFoot\nA skeletal joint that ARKit tracks representing the right foot.\nARSkeletonJointNameRightHand\nA skeletal joint that ARKit tracks representing the right hand.\nARSkeletonJointNameRightShoulder\nA skeletal joint that ARKit tracks representing the right shoulder."
  },
  {
    "title": "ar_hand_skeleton_joint_name_t | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_hand_skeleton_joint_name_t",
    "html": "Topics\nConstants\nar_hand_skeleton_joint_name_forearm_arm\nar_hand_skeleton_joint_name_forearm_wrist\nar_hand_skeleton_joint_name_index_finger_intermediate_base\nar_hand_skeleton_joint_name_index_finger_intermediate_tip\nar_hand_skeleton_joint_name_index_finger_knuckle\nar_hand_skeleton_joint_name_index_finger_metacarpal\nar_hand_skeleton_joint_name_index_finger_tip\nar_hand_skeleton_joint_name_little_finger_intermediate_base\nar_hand_skeleton_joint_name_little_finger_intermediate_tip\nar_hand_skeleton_joint_name_little_finger_knuckle\nar_hand_skeleton_joint_name_little_finger_metacarpal\nar_hand_skeleton_joint_name_little_finger_tip\nar_hand_skeleton_joint_name_middle_finger_intermediate_base\nar_hand_skeleton_joint_name_middle_finger_intermediate_tip\nar_hand_skeleton_joint_name_middle_finger_knuckle\nar_hand_skeleton_joint_name_middle_finger_metacarpal\nar_hand_skeleton_joint_name_middle_finger_tip\nar_hand_skeleton_joint_name_ring_finger_intermediate_base\nar_hand_skeleton_joint_name_ring_finger_intermediate_tip\nar_hand_skeleton_joint_name_ring_finger_knuckle\nar_hand_skeleton_joint_name_ring_finger_metacarpal\nar_hand_skeleton_joint_name_ring_finger_tip\nar_hand_skeleton_joint_name_thumb_intermediate_base\nar_hand_skeleton_joint_name_thumb_intermediate_tip\nar_hand_skeleton_joint_name_thumb_knuckle\nar_hand_skeleton_joint_name_thumb_tip\nar_hand_skeleton_joint_name_wrist"
  },
  {
    "title": "jointCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton/3229918-jointcount",
    "html": "Discussion\n\nSee Also\nGetting Joint Information\ndefinition\nThe particular configuration of joints that define a body's current state.\n- isJointTracked:\nTells you whether ARKit tracks a joint at a particular index.\nARSkeletonJointName\nA name identifier for a joint."
  },
  {
    "title": "ARSkeletonJointNameForRecognizedPointKey | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/3601095-arskeletonjointnameforrecognized",
    "html": "Parameters\nrecognizedPointKey\n\nThe argument key point.\n\nDiscussion\n\nThis function matches human body key points defined by the Vision framework with joint names defined by ARKit. This function may return nil if the key point doesn't map to a joint name. For more information about key points, see Detecting Human Body Poses in Images."
  },
  {
    "title": "ARSkeletonJointNameLeftFoot | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletonjointnameleftfoot",
    "html": "See Also\nIdentifying Joints\nARSkeletonJointNameRoot\nA skeletal joint that's the root of all other joints.\nARSkeletonJointNameHead\nA skeletal joint that ARKit tracks representing the head.\nARSkeletonJointNameLeftHand\nA skeletal joint that ARKit tracks representing the left hand.\nARSkeletonJointNameLeftShoulder\nA skeletal joint that ARKit tracks representing the left shoulder.\nARSkeletonJointNameRightFoot\nA skeletal joint that ARKit tracks representing the right foot.\nARSkeletonJointNameRightHand\nA skeletal joint that ARKit tracks representing the right hand.\nARSkeletonJointNameRightShoulder\nA skeletal joint that ARKit tracks representing the right shoulder."
  },
  {
    "title": "ARSkeletonJointNameHead | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletonjointnamehead",
    "html": "See Also\nIdentifying Joints\nARSkeletonJointNameRoot\nA skeletal joint that's the root of all other joints.\nARSkeletonJointNameLeftFoot\nA skeletal joint that ARKit tracks representing the left foot.\nARSkeletonJointNameLeftHand\nA skeletal joint that ARKit tracks representing the left hand.\nARSkeletonJointNameLeftShoulder\nA skeletal joint that ARKit tracks representing the left shoulder.\nARSkeletonJointNameRightFoot\nA skeletal joint that ARKit tracks representing the right foot.\nARSkeletonJointNameRightHand\nA skeletal joint that ARKit tracks representing the right hand.\nARSkeletonJointNameRightShoulder\nA skeletal joint that ARKit tracks representing the right shoulder."
  },
  {
    "title": "ARSkeletonJointNameRightFoot | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletonjointnamerightfoot",
    "html": "See Also\nIdentifying Joints\nARSkeletonJointNameRoot\nA skeletal joint that's the root of all other joints.\nARSkeletonJointNameHead\nA skeletal joint that ARKit tracks representing the head.\nARSkeletonJointNameLeftFoot\nA skeletal joint that ARKit tracks representing the left foot.\nARSkeletonJointNameLeftHand\nA skeletal joint that ARKit tracks representing the left hand.\nARSkeletonJointNameLeftShoulder\nA skeletal joint that ARKit tracks representing the left shoulder.\nARSkeletonJointNameRightHand\nA skeletal joint that ARKit tracks representing the right hand.\nARSkeletonJointNameRightShoulder\nA skeletal joint that ARKit tracks representing the right shoulder."
  },
  {
    "title": "ARSkeletonJointNameRightShoulder | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletonjointnamerightshoulder",
    "html": "See Also\nIdentifying Joints\nARSkeletonJointNameRoot\nA skeletal joint that's the root of all other joints.\nARSkeletonJointNameHead\nA skeletal joint that ARKit tracks representing the head.\nARSkeletonJointNameLeftFoot\nA skeletal joint that ARKit tracks representing the left foot.\nARSkeletonJointNameLeftHand\nA skeletal joint that ARKit tracks representing the left hand.\nARSkeletonJointNameLeftShoulder\nA skeletal joint that ARKit tracks representing the left shoulder.\nARSkeletonJointNameRightFoot\nA skeletal joint that ARKit tracks representing the right foot.\nARSkeletonJointNameRightHand\nA skeletal joint that ARKit tracks representing the right hand."
  },
  {
    "title": "ARSkeletonJointNameRightHand | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletonjointnamerighthand",
    "html": "See Also\nIdentifying Joints\nARSkeletonJointNameRoot\nA skeletal joint that's the root of all other joints.\nARSkeletonJointNameHead\nA skeletal joint that ARKit tracks representing the head.\nARSkeletonJointNameLeftFoot\nA skeletal joint that ARKit tracks representing the left foot.\nARSkeletonJointNameLeftHand\nA skeletal joint that ARKit tracks representing the left hand.\nARSkeletonJointNameLeftShoulder\nA skeletal joint that ARKit tracks representing the left shoulder.\nARSkeletonJointNameRightFoot\nA skeletal joint that ARKit tracks representing the right foot.\nARSkeletonJointNameRightShoulder\nA skeletal joint that ARKit tracks representing the right shoulder."
  },
  {
    "title": "ARSkeletonJointNameLeftHand | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletonjointnamelefthand",
    "html": "See Also\nIdentifying Joints\nARSkeletonJointNameRoot\nA skeletal joint that's the root of all other joints.\nARSkeletonJointNameHead\nA skeletal joint that ARKit tracks representing the head.\nARSkeletonJointNameLeftFoot\nA skeletal joint that ARKit tracks representing the left foot.\nARSkeletonJointNameLeftShoulder\nA skeletal joint that ARKit tracks representing the left shoulder.\nARSkeletonJointNameRightFoot\nA skeletal joint that ARKit tracks representing the right foot.\nARSkeletonJointNameRightHand\nA skeletal joint that ARKit tracks representing the right hand.\nARSkeletonJointNameRightShoulder\nA skeletal joint that ARKit tracks representing the right shoulder."
  },
  {
    "title": "ARSkeletonJointNameLeftShoulder | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletonjointnameleftshoulder",
    "html": "See Also\nIdentifying Joints\nARSkeletonJointNameRoot\nA skeletal joint that's the root of all other joints.\nARSkeletonJointNameHead\nA skeletal joint that ARKit tracks representing the head.\nARSkeletonJointNameLeftFoot\nA skeletal joint that ARKit tracks representing the left foot.\nARSkeletonJointNameLeftHand\nA skeletal joint that ARKit tracks representing the left hand.\nARSkeletonJointNameRightFoot\nA skeletal joint that ARKit tracks representing the right foot.\nARSkeletonJointNameRightHand\nA skeletal joint that ARKit tracks representing the right hand.\nARSkeletonJointNameRightShoulder\nA skeletal joint that ARKit tracks representing the right shoulder."
  },
  {
    "title": "ARErrorCodeRequestFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode/arerrorcoderequestfailed",
    "html": "See Also\nErrors\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARAltitudeSourceUnknown | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/araltitudesource/araltitudesourceunknown",
    "html": "See Also\nSources\nARAltitudeSourcePrecise\nThe framework sets the altitude using a high-resolution digital-elevation model.\nARAltitudeSourceCoarse\nThe framework sets the altitude using a coarse digital-elevation model.\nARAltitudeSourceUserDefined\nThe app defines the altitude."
  },
  {
    "title": "subscript(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometrysource/3601241-subscript",
    "html": "Discussion\n\nThis subscript operates on three-component geometry sources with a format of MTLVertexFormat.float3. This operator returns an (x, y, z) offset relative to its parent anchor's position that corresponds to the subscripted vertex position in vertices and to the subscripted normal vector in normals.\n\nSee Also\nAccessing Geometry\nsubscript(Int32) -> CUnsignedChar\nProvides the number at the subscripted index.\nvar buffer: MTLBuffer\nA Metal buffer that contains a list of vectors."
  },
  {
    "title": "multiplePerformOperation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/preliminary_action/multipleperformoperation",
    "html": "Overview\n\nThe runtime doesn’t react to this property for all actions.\n\nAdditional Invocation Options\nallow\n\nRestarts the action by playing it over again.\n\nignore\n\nContinues running the current action, ignoring the additional invocation.\n\nstop\n\nStops the current action.\n\nDeclaration\nuniform token multiplePerformOperation= \"ignore\" (\n    allowedTokens = [\"ignore\", \"allow\", \"stop\"]\n)\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action."
  },
  {
    "title": "ARPlaneAnchorAlignmentHorizontal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchoralignment/arplaneanchoralignmenthorizontal",
    "html": "Discussion\n\nThe transform property for a horizontal plane anchor includes no rotation about the x- or z-axis. Thus, using this anchor's transform to place a 3D model asset in your scene results in the model appearing \"right side up\".\n\nSee Also\nAlignment Values\nARPlaneAnchorAlignmentVertical\nThe plane is parallel to gravity."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/preliminary_action/info_id",
    "html": "Overview\n\nThe default value is blank. Every prim with the Preliminary_Action type defines a unique value for this property.\n\nDeclaration\nuniform token info:id\n\n\nSee Also\nProperties\nmultiplePerformOperation\nAn option that indicates how an action handles an additional invocation while running."
  },
  {
    "title": "ARPlaneAnchorAlignmentVertical | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchoralignment/arplaneanchoralignmentvertical",
    "html": "Discussion\n\nThe transform property for a vertical plane anchor includes a rotation component. That is, the transform matrix represents the result of rotating a horizontal plane to match the orientation of the detected surface.\n\nSee Also\nAlignment Values\nARPlaneAnchorAlignmentHorizontal\nThe plane is perpendicular to gravity."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/orbitaction/info_id",
    "html": "Overview\n\nThe default value is Orbit.\n\nDeclaration\nuniform token info:id = \"Orbit\"\n\n\nSee Also\nProperties\naffectedObjects\nA list of objects in orbit.\ncenter\nA prim around which the affected objects orbit.\nduration\nThe amount of time between the start of an action and its end.\nrevolutions\nThe number of rotations to complete.\naxis\nA vector that describes the axis of rotation.\nalignToPath\nAn option that controls the prim’s orientation as it revolves."
  },
  {
    "title": "affectedObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/transformaction/affectedobjects",
    "html": "Overview\n\nPrims aquire the transform that TransformAction animates by inheriting Xformable. Therefore, add only Xformable prims to this list.\n\nDeclaration\nrel affectedObjects\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\nxformTarget\nA prim that provides the transform to which this action animates.\nduration\nThe amount of time between the start of an action and its end.\ntype\nAn option that determines if a transform is based on another, source transform.\neaseType\nAn option that describes the animation’s change in pace over time."
  },
  {
    "title": "center | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/orbitaction/center",
    "html": "Overview\n\nAssign this property a prim not contained by affectedObjects.\n\nTo reside at an arbitrary location in its parent’s coordinate space, a prim needs the translation component of a transform. Therefore, assign this property an Xformable prim.\n\nDeclaration\nrel center\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of objects in orbit.\nduration\nThe amount of time between the start of an action and its end.\nrevolutions\nThe number of rotations to complete.\naxis\nA vector that describes the axis of rotation.\nalignToPath\nAn option that controls the prim’s orientation as it revolves."
  },
  {
    "title": "<(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfidencelevel/3601238",
    "html": "Relationships\nFrom Protocol\nComparable"
  },
  {
    "title": "duration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/emphasizeaction/duration",
    "html": "Overview\n\nThe default value is 1.0.\n\nDeclaration\nuniform double duration = 1.0\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that the runtime emphasizes when the action’s trigger fires.\nstyle\nAn option that implements different kinds of animation timing.\nmotionType\nAn option that implements animation effects."
  },
  {
    "title": "ARPlaneClassificationStatusUndetermined | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneclassificationstatus/arplaneclassificationstatusundetermined",
    "html": "Discussion\n\nThis status occurs when ARKit is still in the process of plane classification. To be notified when ARKit produces a classification, observe the same plane anchor in a later frame (for example, in the session:didUpdateAnchors: or renderer:didUpdateNode:forAnchor: delegate method).\n\nSee Also\nClassification Status\nARPlaneClassificationStatusNotAvailable\nARKit cannot currently provide plane classification information.\nARPlaneClassificationStatusUnknown\nARKit has completed its classification process for the plane anchor, but the result is inconclusive.\nARPlaneClassificationStatusKnown\nARKit has completed its classfication process for the plane anchor."
  },
  {
    "title": "subscript(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometrysource/3601242-subscript",
    "html": "Discussion\n\nThis subscript operates on one-component geometry sources with a format of MTLVertexFormat.uchar, such as classification.\n\nSee Also\nAccessing Geometry\nsubscript(Int32) -> (Float, Float, Float)\nProvides the source float triplet at the subscripted index.\nvar buffer: MTLBuffer\nA Metal buffer that contains a list of vectors."
  },
  {
    "title": "type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/visibilityaction/type",
    "html": "Overview\n\nThe default value is show.\n\nVisibility Types\nshow\n\nReveals the prim with a transform animation.\n\nhide\n\nHides the prim with a transform animation.\n\nDeclaration\nuniform token type = \"show\" (\n    allowedTokens = [\"show\", \"hide\"]\n)\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims to show or hide.\nstyle\nAn option that implements different kinds of animation timing.\nmotionType\nAn option that determines how the action displays or hides a prim.\neaseType\nAn option that describes the animation’s change in pace over time.\nmoveDistance\nThe distance that this action moves the target prims."
  },
  {
    "title": "affectedObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/visibilityaction/affectedobjects",
    "html": "Overview\n\nAdd one or more Xformable prims to this list.\n\nDeclaration\nrel affectedObjects\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\ntype\nAn option that determines the target prim’s visibility when the action finishes.\nstyle\nAn option that implements different kinds of animation timing.\nmotionType\nAn option that determines how the action displays or hides a prim.\neaseType\nAn option that describes the animation’s change in pace over time.\nmoveDistance\nThe distance that this action moves the target prims."
  },
  {
    "title": "format | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometrysource/3516918-format",
    "html": "Discussion\n\nSee Also\nGetting Geometry Information\nvar componentsPerVector: Int\nThe number of scalar components in each vector.\nvar count: Int\nThe number of vectors in the buffer.\nvar offset: Int\nThe offset, in bytes, from the beginning of the buffer.\nvar stride: Int\nThe length, in bytes, of the start of one vector in the buffer to the start of the next vector."
  },
  {
    "title": "buffer | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometrysource/3516916-buffer",
    "html": "Discussion\n\nEach vector in the buffer is of the type defined by format. Every vector may itself contain multiple scalars, defined by componentsPerVector.\n\nThe buffer's storageMode is MTLStorageMode.shared which allows it to be accessed on both, the CPU, and GPU.\n\nSee Also\nAccessing Geometry\nsubscript(Int32) -> (Float, Float, Float)\nProvides the source float triplet at the subscripted index.\nsubscript(Int32) -> CUnsignedChar\nProvides the number at the subscripted index."
  },
  {
    "title": "count | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometrysource/3516917-count",
    "html": "See Also\nGetting Geometry Information\nvar componentsPerVector: Int\nThe number of scalar components in each vector.\nvar format: MTLVertexFormat\nThe type of vector data in the buffer.\nvar offset: Int\nThe offset, in bytes, from the beginning of the buffer.\nvar stride: Int\nThe length, in bytes, of the start of one vector in the buffer to the start of the next vector."
  },
  {
    "title": "affectedObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/spinaction/affectedobjects",
    "html": "Overview\n\nThe runtime spins a prim by animating the rotation component of its transform. Therefore, add only Xformable prims to this list.\n\nDeclaration\nrel affectedObjects\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\nduration\nThe amount of time between the start of an action and its end.\nrevolutions\nThe number rotations to complete.\naxis\nA vector that describes the axis of rotation."
  },
  {
    "title": "style | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/visibilityaction/style",
    "html": "Overview\n\nThe default value is basic.\n\nStyles\nbasic\n\nAnimates with steady motion.\n\nplayful\n\nAnimates with whimsical motion.\n\nwild\n\nAnimates with sporadic motion.\n\nDeclaration\nuniform token style = \"basic\" (\n    allowedTokens = [\"basic\", \"playful\", \"wild\"]\n)\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims to show or hide.\ntype\nAn option that determines the target prim’s visibility when the action finishes.\nmotionType\nAn option that determines how the action displays or hides a prim.\neaseType\nAn option that describes the animation’s change in pace over time.\nmoveDistance\nThe distance that this action moves the target prims."
  },
  {
    "title": "duration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/waitaction/duration",
    "html": "Overview\n\nThe default value is 1.0.\n\nDeclaration\nuniform double duration = 1.0\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action."
  },
  {
    "title": "axis | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/spinaction/axis",
    "html": "Overview\n\nThe object spins in a plane perpendicular to the value of this property. To reverse a spin direction, invert the value of this property. The default value points positively in the y-direction.\n\nDeclaration\nuniform vector3d axis = (0.0, 1.0, 0.0)\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that spin on an axis.\nduration\nThe amount of time between the start of an action and its end.\nrevolutions\nThe number rotations to complete."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/visibilityaction/info_id",
    "html": "Overview\n\nThe default value is Visibility.\n\nDeclaration\nuniform token info:id = \"Visibility\"\n\n\nSee Also\nProperties\naffectedObjects\nA list of prims to show or hide.\ntype\nAn option that determines the target prim’s visibility when the action finishes.\nstyle\nAn option that implements different kinds of animation timing.\nmotionType\nAn option that determines how the action displays or hides a prim.\neaseType\nAn option that describes the animation’s change in pace over time.\nmoveDistance\nThe distance that this action moves the target prims."
  },
  {
    "title": "ARPlaneAnchor.Classification.Status.unknown | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/status/unknown",
    "html": "Discussion\n\nARKit attempts to classify detected planes using a finite set of common categories. However, a detected plane may not be a real object fitting any of those categories, or the plane classification process may not be able to recognize it. In such cases, the plane anchor's classification is ARPlaneAnchor.Classification.none(_:) with an associated value of ARPlaneAnchor.Classification.Status.unknown.\n\nSee Also\nClassification Status\ncase notAvailable\nARKit cannot currently provide plane classification information.\ncase undetermined\nARKit has not yet produced a classification for the plane anchor."
  },
  {
    "title": "ARGeoTrackingStateReason | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatereason",
    "html": "Overview\n\nThese possible values of stateReason provide more information about a geotracking session's current state.\n\nTopics\nStatus Reasons\nARGeoTrackingStateReasonNone\nNo issues reported.\nARGeoTrackingStateReasonNotAvailableAtLocation\nThe location doesn't provide geotracking.\nARGeoTrackingStateReasonNeedLocationPermissions\nThe location requires user permission for geotracking.\nARGeoTrackingStateReasonDevicePointedTooLow\nThe position of the device is too low for geotracking.\nARGeoTrackingStateReasonWorldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\nARGeoTrackingStateReasonWaitingForLocation\nA state in which the framework performs a check for the user's GPS position.\nARGeoTrackingStateReasonWaitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\nARGeoTrackingStateReasonGeoDataNotLoaded\nA state in which the framework downloads localization imagery.\nARGeoTrackingStateReasonVisualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera.\nSee Also\nDetermining the Reason\nstateReason\nThe reason for the frame’s geo-tracking state."
  },
  {
    "title": "ARPlaneAnchor.Classification.Status.notAvailable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/status/notavailable",
    "html": "Discussion\n\nPlane classification is available only on iPhone XS and iPhone XS Max devices. On other devices, all plane anchors always indicate a classification status of ARPlaneAnchor.Classification.Status.notAvailable.\n\nA classification status of ARPlaneAnchor.Classification.Status.notAvailable can also occur if the plane classification process is temporarily unavilable.\n\nSee Also\nClassification Status\ncase undetermined\nARKit has not yet produced a classification for the plane anchor.\ncase unknown\nARKit has completed its classification process for the plane anchor, but the result is inconclusive."
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/status/3238109-hash",
    "html": "Parameters\nhasher\n\nThe hash function to use to hash the status.\n\nRelationships\nFrom Protocol\nHashable\nSee Also\nHashes of a Classification Status\nstatic func == (ARPlaneAnchor.Classification.Status, ARPlaneAnchor.Classification.Status) -> Bool\nIndicates whether two statuses are equal.\nstatic func != (ARPlaneAnchor.Classification.Status, ARPlaneAnchor.Classification.Status) -> Bool\nReturns a Boolean value indicating whether two values are not equal.\nvar hashValue: Int\nA value that identifies an object uniquely as compared to other instances of the same type."
  },
  {
    "title": "ARPlaneAnchor.Classification.Status.undetermined | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/status/undetermined",
    "html": "Discussion\n\nThis status occurs when ARKit is still in the process of plane classification. To be notified when ARKit produces a classification, observe the same plane anchor in a later frame (for example, in the session(_:didUpdate:) or renderer(_:didUpdate:for:) delegate method).\n\nSee Also\nClassification Status\ncase notAvailable\nARKit cannot currently provide plane classification information.\ncase unknown\nARKit has completed its classification process for the plane anchor, but the result is inconclusive."
  },
  {
    "title": "collaborationEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/3152987-collaborationenabled",
    "html": "Discussion\n\nThe default value of this property is NO. When you enable collaboration, ARKit invokes session:didOutputCollaborationData: periodically, providing you with collaboration data to share with peers. Collaboration data contains information about the real-world surfaces ARKit detects, your position in relation to them, and any anchors you may have created.\n\nMultiple users sharing collaboration data with each other results in an AR experience in which the users interact by sharing and manipulating anchors. By including information that describes a user's unique view of the world, collaboration data enhances ARKit's understanding of the layout of the physical environment much more quickly than is possible with only one user.\n\nFor more information, see Creating a Collaborative Session.\n\nImportant\n\nCollaborative sessions work best with up to four participants.\n\nSharing Collaboration Data Over the Network\n\nYou are responsible for sending collaboration data over the network, including choosing the network framework and implementing the code. See Creating a Multiuser AR Experience for an example app that shares a world map among users via Multipeer Connectivity. Although Creating a Multiuser AR Experience demonstrates sharing world data among peer users, it does so using a host-guest model. The primary advantage of collaboration data is that it enables you to share world data peer-to-peer.\n\nThe data you send is a serialized version of the ARCollaborationData object provided by your session. You serialize it using NSKeyedArchiver.\n\nfunc session(_ session: ARSession, didOutputCollaborationData data: ARSession.CollaborationData) {    \n    if let collaborationDataEncoded = try? NSKeyedArchiver.archivedData(withRootObject: data, requiringSecureCoding: true) {\n        multipeerSession.sendToAllPeers(collaborationDataEncoded)\n    } else {\n        fatalError(\"An error occurred while encoding collaboration data.\")\n    }\n}\n\n\nUpdating Your Session with Collaboration Data\n\nWhen you receive collaboration data from other users, you instantiate an ARCollaborationData object with it, and pass the object to your session via updateWithCollaborationData:.\n\nfunc receivedData(_ data: Data) {        \n    if let collaborationData = try? NSKeyedUnarchiver.unarchivedObject(ofClass: ARSession.CollaborationData.self, from: data) {\n        session.update(with: collaborationData)\n    } else {\n        fatalError(\"An error occurred while decoding collaboration data.\")\n    }\n}\n\n\n"
  },
  {
    "title": "ARGeoTrackingAccuracy | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingaccuracy",
    "html": "Overview\n\nTo ensure the best possible user experience, an app must monitor and react to the geo-tracking accuracy. When accuracy is ARGeoTrackingAccuracyLow, the app needs to show content that’s more forgiving if ARKit is off by a small distance.\n\nFor example, if accuracy is ARGeoTrackingAccuracyLow, rendering a location anchor as a large ball several meters in the air is more appropriate than rendering an arrow that rests its point on a real-world surface. Because a larger ball isn’t meant to mark a precise location, any offset that results from low accuracy will be less noticeable to the user.\n\nApps that need higher-precision location anchors need to wait for ARGeoTrackingAccuracyMedium or ARGeoTrackingAccuracyHigh accuracy before revealing rendered location-anchors, or dismissing user instructions.\n\nTopics\nAccuracies\nARGeoTrackingAccuracyHigh\nGeo-tracking accuracy is high.\nARGeoTrackingAccuracyUndetermined\nGeo-tracking accuracy is undetermined.\nARGeoTrackingAccuracyLow\nGeo-tracking accuracy is low.\nARGeoTrackingAccuracyMedium\nGeo-tracking accuracy is average.\nSee Also\nJudging Accuracy\naccuracy\nThe accuracy of geo tracking at the time the session captured the frame."
  },
  {
    "title": "unprojectPoint:ontoPlaneWithTransform: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/2984228-unprojectpoint",
    "html": "Parameters\npoint\n\nThe point in 2D view space to project onto a plane.\n\nThe coordinate space for this point has its origin is in the upper left corner and a size matching the viewportSize parameter.\n\nplaneTransform\n\nA transform matrix specifying the position and orientation of a plane (with infinite extent) in 3D world space. The plane is the xz-plane of the local coordinate space this transform defines.\n\nReturn Value\n\nThe 3D point in world space where a ray projected from the specified 2D point intersects the specified plane. If the ray does not intersect the plane, this method returns a float3 vector where all elements are NaN.\n\nSee Also\nMapping Content to Real-World Positions\n- anchorForNode:\nReturns the AR anchor associated with the specified SceneKit node, if any.\n- nodeForAnchor:\nReturns the SceneKit node associated with the specified AR anchor, if any."
  },
  {
    "title": "priority | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcollaborationdata/3340466-priority",
    "html": "Discussion\n\nIf you have reliability options in the network protocol you choose to transport ARCollaborationData among peers, the priority property gives you a hint about which reliability option to choose for a given collaboration data instance. For example, if you use MultipeerConnectivity to send collaboration data over the network, choose MCSessionSendDataReliable when calling sendData:toPeers:withMode:error: after ARKit gives you a collaboration data instance with priority ARCollaborationDataPriorityCritical.\n\nSee Also\nObserving Priority\nARCollaborationDataPriority\nOptions that help you choose the appropriate network protocol or settings for a given data instance."
  },
  {
    "title": "ARCollaborationDataPriority | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcollaborationdatapriority",
    "html": "Overview\n\nWhen you send ARCollaborationData over the network by using a protocol that allows you to specify varying reliability, this property provides you with a hint about which reliability setting to use for a given collaboration data instance. Depending on its priority, you may also choose to send a given collaboration data instance using different protocols.\n\nTopics\nData Sensitivity\nARCollaborationDataPriorityCritical\nA priority that indicates that collaboration depends on this data.\nARCollaborationDataPriorityOptional\nA priority that indicates that collaboration can continue without this data.\nSee Also\nObserving Priority\npriority\nA property that gives you a hint about how to send a given data instance over the network."
  },
  {
    "title": "distance | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/proximitytocameratrigger/distance",
    "html": "Overview\n\nThis distance is in stage units (see Encoding Stage Linear Units) and measures from the center of each prim in affectedObjects, to the device’s camera.\n\nDeclaration\nuniform double distance = 0.0\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the trigger.\naffectedObjects\nA list of prims for which the runtime checks proximity."
  },
  {
    "title": "vertexCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanegeometry/2941053-vertexcount",
    "html": "See Also\nAccessing Mesh Data\nvertices\nA buffer of vertex positions for each point in the plane mesh.\ntextureCoordinates\nA buffer of texture coordinate values for each point in the plane mesh.\ntextureCoordinateCount\nThe number of elements in the textureCoordinates buffer.\ntriangleIndices\nA buffer of indices describing the triangle mesh formed by the plane geometry's vertex data.\ntriangleCount\nThe number of triangles described by the triangleIndices buffer."
  },
  {
    "title": "xformTarget | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/transformaction/xformtarget",
    "html": "Overview\n\nTo provide a transform, assign an Xformable prim to this property.\n\nThe prims in the list of affectedObjects animate from their current transform to the transform that this prim specifies. Include in the prim the transformational operations that implement the transform animation. For more information, see xformOp.\n\nDeclaration\nrel xformTarget\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims to which a transform applies.\nduration\nThe amount of time between the start of an action and its end.\ntype\nAn option that determines if a transform is based on another, source transform.\neaseType\nAn option that describes the animation’s change in pace over time."
  },
  {
    "title": "easeType | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/transformaction/easetype",
    "html": "Overview\n\nThe default type is none, which is synonymous with linear timing.\n\nEase Types\nnone\n\nPaces the action at a constant rate.\n\nin\n\nPaces the action slower at the beginning.\n\nout\n\nPaces the action slower at the end.\n\ninout\n\nPaces the action slower at the beginning and end.\n\nDeclaration\nuniform token easeType = \"none\"\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims to which a transform applies.\nxformTarget\nA prim that provides the transform to which this action animates.\nduration\nThe amount of time between the start of an action and its end.\ntype\nAn option that determines if a transform is based on another, source transform."
  },
  {
    "title": "duration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/transformaction/duration",
    "html": "Overview\n\nThe default value is 1.0.\n\nDeclaration\nuniform double duration = 1.0\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims to which a transform applies.\nxformTarget\nA prim that provides the transform to which this action animates.\ntype\nAn option that determines if a transform is based on another, source transform.\neaseType\nAn option that describes the animation’s change in pace over time."
  },
  {
    "title": "type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/transformaction/type",
    "html": "Overview\n\nThe default value is relative.\n\nTransform Types\nrelative\n\nTakes into account the source transform when the action applies the target transform such that translation is additive, rotation is cumulative, and scale is multiplicative.\n\nabsolute\n\nReplaces the affected object’s transform with the target transform.\n\nDeclaration\nuniform token type = \"relative\" (\n    allowedTokens = [\"relative\", \"absolute\"]\n)\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims to which a transform applies.\nxformTarget\nA prim that provides the transform to which this action animates.\nduration\nThe amount of time between the start of an action and its end.\neaseType\nAn option that describes the animation’s change in pace over time."
  },
  {
    "title": "affectedObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/orbitaction/affectedobjects",
    "html": "Overview\n\nTo orbit a point, a prim needs the translation component of a transform. Therefore, add only Xformable prims to this list.\n\nDeclaration\nrel affectedObjects\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\ncenter\nA prim around which the affected objects orbit.\nduration\nThe amount of time between the start of an action and its end.\nrevolutions\nThe number of rotations to complete.\naxis\nA vector that describes the axis of rotation.\nalignToPath\nAn option that controls the prim’s orientation as it revolves."
  },
  {
    "title": "duration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/orbitaction/duration",
    "html": "Overview\n\nThe default value is 1.0.\n\nDeclaration\nuniform double duration = 1.0\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of objects in orbit.\ncenter\nA prim around which the affected objects orbit.\nrevolutions\nThe number of rotations to complete.\naxis\nA vector that describes the axis of rotation.\nalignToPath\nAn option that controls the prim’s orientation as it revolves."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/emphasizeaction/info_id",
    "html": "Overview\n\nThe default value is “Emphasize”.\n\nDeclaration\nuniform token info:id = \"Emphasize\"\n\n\nSee Also\nProperties\naffectedObjects\nA list of prims that the runtime emphasizes when the action’s trigger fires.\nduration\nThe amount of time between the start of an action and its end.\nstyle\nAn option that implements different kinds of animation timing.\nmotionType\nAn option that implements animation effects."
  },
  {
    "title": "axis | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/orbitaction/axis",
    "html": "Overview\n\nThe object orbits in a plane perpendicular to the value of this property. To reverse an orbit direction, invert the value of this property. The default value points positively in the y-direction.\n\nDeclaration\nuniform vector3d axis = (0.0, 1.0, 0.0)\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of objects in orbit.\ncenter\nA prim around which the affected objects orbit.\nduration\nThe amount of time between the start of an action and its end.\nrevolutions\nThe number of rotations to complete.\nalignToPath\nAn option that controls the prim’s orientation as it revolves."
  },
  {
    "title": "revolutions | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/orbitaction/revolutions",
    "html": "Overview\n\nIrrational values translate to partial revolutions. For example, set this property to 1.25 to complete one and a quarter rotations before stopping.\n\nDeclaration\nuniform double revolutions = 1.0\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of objects in orbit.\ncenter\nA prim around which the affected objects orbit.\nduration\nThe amount of time between the start of an action and its end.\naxis\nA vector that describes the axis of rotation.\nalignToPath\nAn option that controls the prim’s orientation as it revolves."
  },
  {
    "title": "alignToPath | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/orbitaction/aligntopath",
    "html": "Overview\n\nThe default value is false, in which the object maintains its orientation as it travels along the orbit path. Set a value of true to reorient the object to face the center as it orbits.\n\nDeclaration\nuniform bool alignToPath = false\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of objects in orbit.\ncenter\nA prim around which the affected objects orbit.\nduration\nThe amount of time between the start of an action and its end.\nrevolutions\nThe number of rotations to complete.\naxis\nA vector that describes the axis of rotation."
  },
  {
    "title": "affectedObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/lookatcameraaction/affectedobjects",
    "html": "Overview\n\nOnly add Xformable prims to this list that share a front vector.\n\nDeclaration\nrel affectedObjects\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\nduration\nThe amount of time that the objects face the camera.\nfront\nA vector that’s perpendicular to, and points outward from, the object’s face.\nupVector\nA vector around which the runtime rotates the object."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/lookatcameraaction/info_id",
    "html": "Overview\n\nThe default value is LookAtCamera.\n\nDeclaration\nuniform token info:id = \"LookAtCamera\"\n\n\nSee Also\nProperties\naffectedObjects\nA list of prims that face the camera when this action executes.\nduration\nThe amount of time that the objects face the camera.\nfront\nA vector that’s perpendicular to, and points outward from, the object’s face.\nupVector\nA vector around which the runtime rotates the object."
  },
  {
    "title": "style | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/emphasizeaction/style",
    "html": "Overview\n\nThe default value is “basic”.\n\nStyles\nbasic\n\nAnimates with steady motion.\n\nplayful\n\nAnimates with whimsical motion.\n\nwild\n\nAnimates with sporadic motion.\n\nDeclaration\nuniform token style = \"basic\" (\n    allowedTokens = [\"basic\", \"playful\", \"wild\"]\n)\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that the runtime emphasizes when the action’s trigger fires.\nduration\nThe amount of time between the start of an action and its end.\nmotionType\nAn option that implements animation effects."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/groupaction/info_id",
    "html": "Overview\n\nThe default value is Group.\n\nDeclaration\nuniform token info:id = \"Group\"\n\n\nSee Also\nProperties\ntype\nAn option that controls the order in which the actions execute.\nloops\nA Boolean value indicating whether the group loops.\nperformCount\nA value that specifies the number of times the group’s actions repeat.\nactions\nA list of actions that make up the group."
  },
  {
    "title": "affectedObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/emphasizeaction/affectedobjects",
    "html": "Overview\n\nAn asset needs to add one or more prims to this property.\n\nDeclaration\nrel affectedObjects\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\nduration\nThe amount of time between the start of an action and its end.\nstyle\nAn option that implements different kinds of animation timing.\nmotionType\nAn option that implements animation effects."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/impulseaction/info_id",
    "html": "Overview\n\nThe default value is Impulse.\n\nDeclaration\nuniform token info:id = \"Impulse\"\n\n\nSee Also\nProperties\naffectedObjects\nA list of prims to deliver an impulse.\nvelocity\nThe amount of velocity the impulse adds to the target prims."
  },
  {
    "title": "motionType | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/emphasizeaction/motiontype",
    "html": "Overview\n\nThe default value is pop.\n\nMotion Types\npop\n\nGrows like an inflating bubble before shrinking as if popped.\n\nblink\n\nScales vertically, mimicking an eye blinking.\n\nbounce\n\nRaises up and then drops, appearing to bounce on the ground.\n\nflip\n\nRaises up and rotates around a central axis before returning to its original position.\n\nfloat\n\nRaises up and levitates for a moment before returning to its original position.\n\njiggle\n\nOscillates back and forth before returning to its original place.\n\npulse\n\nScales larger and smaller like a heartbeat.\n\nspin\n\nRotates around its central y-axis and returns to its original pose.\n\nDeclaration\nuniform token motionType = \"pop\" (\n        allowedTokens = [\"pop\", \"blink\", \"bounce\", \"flip\", \"float\", \"jiggle\", \"pulse\", \"spin\"]\n)\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that the runtime emphasizes when the action’s trigger fires.\nduration\nThe amount of time between the start of an action and its end.\nstyle\nAn option that implements different kinds of animation timing."
  },
  {
    "title": "velocity | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/impulseaction/velocity",
    "html": "Overview\n\nThe value and sign of each component in the triplet determine the impulse’s direction along each respective axis. The magnitude of each component determines the speed along each the respective axis. This property is in local space, so it applies to the coordinate space of the target prims.\n\nDeclaration\nuniform vector3d velocity = (0.0, 0.0, 0.0)\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims to deliver an impulse."
  },
  {
    "title": "performCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/groupaction/performcount",
    "html": "Overview\n\nThe default value is 1, which execute the actions only once. If the value is 0 and loops is set to true, the actions execute infinitely. If the value is a number (n) and the type is serial, the actions execute n times in sequence. And if the value is set to a number (n) and the type is parallel, the actions execute n times independently.\n\nDeclaration\nuniform uint performCount = 1\n\n\nSee Also\nProperties\ninfo:id\nThe action’s unique identifier.\ntype\nAn option that controls the order in which the actions execute.\nloops\nA Boolean value indicating whether the group loops.\nactions\nA list of actions that make up the group."
  },
  {
    "title": "affectedObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/impulseaction/affectedobjects",
    "html": "Overview\n\nOnly add rigid-body prims to this list. For more information, see Simulating a Physical Interaction.\n\nDeclaration\nrel affectedObjects\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\nvelocity\nThe amount of velocity the impulse adds to the target prims."
  },
  {
    "title": "actions | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/groupaction/actions",
    "html": "Overview\n\nOnly add Preliminary_Action prims to this list.\n\nDeclaration\nrel actions\n\n\nSee Also\nProperties\ninfo:id\nThe action’s unique identifier.\ntype\nAn option that controls the order in which the actions execute.\nloops\nA Boolean value indicating whether the group loops.\nperformCount\nA value that specifies the number of times the group’s actions repeat."
  },
  {
    "title": "type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/groupaction/type",
    "html": "Overview\n\nThe default value is serial.\n\nOrder Options\nserial\n\nExecutes in order with each action waiting for the prior action to complete before starting.\n\nparallel\n\nExecutes all actions concurrently.\n\nDeclaration\nuniform token type = \"serial\" (\n        allowedTokens = [\"serial\", \"parallel\"]\n)\n\n\nSee Also\nProperties\ninfo:id\nThe action’s unique identifier.\nloops\nA Boolean value indicating whether the group loops.\nperformCount\nA value that specifies the number of times the group’s actions repeat.\nactions\nA list of actions that make up the group."
  },
  {
    "title": "loops | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/groupaction/loops",
    "html": "Overview\n\nThe default value is false, in which the group executes its actions performCount number of times and then stops.\n\nWhen true and type is serial, the group restarts its action sequence with the first action after the last action completes. When type is parallel, the runtime repeats each action independently.\n\nDeclaration\nuniform bool loops = false\n\n\nSee Also\nProperties\ninfo:id\nThe action’s unique identifier.\ntype\nAn option that controls the order in which the actions execute.\nperformCount\nA value that specifies the number of times the group’s actions repeat.\nactions\nA list of actions that make up the group."
  },
  {
    "title": "audio | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/audioaction/audio",
    "html": "Overview\n\nThe default value is blank. Each instance of AudioAction must supply its own value for this property and an accompanying music file by the same name within the asset.\n\nDeclaration\nuniform asset audio\n\n\nDefine an Audio File\n\nThe following line shows how to define a path to an audio file named backgroundMusic.m4a:\n\nuniform asset audio = @backgroundMusic.m4a@\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that play audio.\ntype\nThe type of command to send the audio.\ngain\nA value that controls the audio volume.\nauralMode\nAn option that controls the audio signal’s spacial dynamics."
  },
  {
    "title": "init(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton/jointname/3601095-init",
    "html": "Parameters\nrecognizedPointKey\n\nThe argument key point.\n\nDiscussion\n\nThis function matches human body key points defined by the Vision framework with joint names defined by ARKit. This function may return nil if the key point doesn't map to a joint name. For more information about key points, see Detecting Human Body Poses in Images.\n\nSee Also\nCreating a Joint Name\ninit(rawValue: String)\nCreates a new joint name.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys."
  },
  {
    "title": "leftFoot | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton/jointname/3255197-leftfoot",
    "html": "See Also\nIdentifying Joints\nstatic let root: ARSkeleton.JointName\nA skeletal joint that's the root of all other joints.\nstatic let head: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the head.\nstatic let leftHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left hand.\nstatic let leftShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left shoulder.\nstatic let rightFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right foot.\nstatic let rightHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right hand.\nstatic let rightShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right shoulder."
  },
  {
    "title": "root | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton/jointname/3255203-root",
    "html": "Discussion\n\nThe root skeletal joint is at the hip, and is located at the origin of its associated body anchor.\n\nSee Also\nIdentifying Joints\nstatic let head: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the head.\nstatic let leftFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left foot.\nstatic let leftHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left hand.\nstatic let leftShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left shoulder.\nstatic let rightFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right foot.\nstatic let rightHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right hand.\nstatic let rightShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right shoulder."
  },
  {
    "title": "type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/audioaction/type",
    "html": "Overview\n\nThe default value is blank. Each AudioAction must specify a type.\n\nAudio Commands\nplay\n\nBegins playing the audio.\n\npause\n\nPauses the audio so a subsequent play action can resume audio from the paused position.\n\nstop\n\nStops playing the audio.\n\nDeclaration\nuniform token type (\n    allowedTokens = [\"play\", \"pause\", \"stop\"]\n)\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that play audio.\naudio\nThe location of an audio file.\ngain\nA value that controls the audio volume.\nauralMode\nAn option that controls the audio signal’s spacial dynamics."
  },
  {
    "title": "gain | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/audioaction/gain",
    "html": "Overview\n\nThe runtime multiplies this value by the incoming audio to produce the output signal. An asset may use this action to raise or lower the audio’s original volume.\n\nThe default value of 1.0 matches the audio’s original volume. The runtime clamps negative values to 0.0, which mutes the audio.\n\nDeclaration\nuniform double gain = 1.0\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that play audio.\naudio\nThe location of an audio file.\ntype\nThe type of command to send the audio.\nauralMode\nAn option that controls the audio signal’s spacial dynamics."
  },
  {
    "title": "head | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton/jointname/3255196-head",
    "html": "See Also\nIdentifying Joints\nstatic let root: ARSkeleton.JointName\nA skeletal joint that's the root of all other joints.\nstatic let leftFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left foot.\nstatic let leftHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left hand.\nstatic let leftShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left shoulder.\nstatic let rightFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right foot.\nstatic let rightHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right hand.\nstatic let rightShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right shoulder."
  },
  {
    "title": "auralMode | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/audioaction/auralmode",
    "html": "Overview\n\nThe default value is blank. Each instance of AudioAction must specify an auralMode.\n\nAural Modes\nspatial\n\nPlays the audio in 3D space at the position of the affected object. If the device doesn’t support spatial audio, it falls back to mono. For best results in spatial mode, supply audio files in mono.\n\nnonSpatial\n\nPlays the audio without regard to the affected object’s 3D position. If the audio media contains any form of stereo or other multichannel sound, the runtime attempts to match this to the device’s output capabilities. Use this mode when an audio action doesn’t use the listener’s position to affect the music’s playback.\n\nambient\n\nPlays the audio in a spatial configuration that tracks rotation, but does not attenuate with distance.\n\nDeclaration\nuniform token auralMode\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that play audio.\naudio\nThe location of an audio file.\ntype\nThe type of command to send the audio.\ngain\nA value that controls the audio volume."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton/jointname/3255210-init",
    "html": "See Also\nCreating a Joint Name\ninit?(VNRecognizedPointKey)\nReturns a joint name that corresponds to a key point defined in a human body pose.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys."
  },
  {
    "title": "leftShoulder | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton/jointname/3255199-leftshoulder",
    "html": "See Also\nIdentifying Joints\nstatic let root: ARSkeleton.JointName\nA skeletal joint that's the root of all other joints.\nstatic let head: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the head.\nstatic let leftFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left foot.\nstatic let leftHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left hand.\nstatic let rightFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right foot.\nstatic let rightHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right hand.\nstatic let rightShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right shoulder."
  },
  {
    "title": "leftHand | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton/jointname/3255198-lefthand",
    "html": "See Also\nIdentifying Joints\nstatic let root: ARSkeleton.JointName\nA skeletal joint that's the root of all other joints.\nstatic let head: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the head.\nstatic let leftFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left foot.\nstatic let leftShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left shoulder.\nstatic let rightFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right foot.\nstatic let rightHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right hand.\nstatic let rightShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right shoulder."
  },
  {
    "title": "rightFoot | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton/jointname/3255200-rightfoot",
    "html": "See Also\nIdentifying Joints\nstatic let root: ARSkeleton.JointName\nA skeletal joint that's the root of all other joints.\nstatic let head: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the head.\nstatic let leftFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left foot.\nstatic let leftHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left hand.\nstatic let leftShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left shoulder.\nstatic let rightHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right hand.\nstatic let rightShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right shoulder."
  },
  {
    "title": "ARPlaneClassificationStatusKnown | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneclassificationstatus/arplaneclassificationstatusknown",
    "html": "Discussion\n\nSee the classification property to identify the detected surface.\n\nSee Also\nClassification Status\nARPlaneClassificationStatusNotAvailable\nARKit cannot currently provide plane classification information.\nARPlaneClassificationStatusUndetermined\nARKit has not yet produced a classification for the plane anchor.\nARPlaneClassificationStatusUnknown\nARKit has completed its classification process for the plane anchor, but the result is inconclusive."
  },
  {
    "title": "rightHand | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton/jointname/3255201-righthand",
    "html": "See Also\nIdentifying Joints\nstatic let root: ARSkeleton.JointName\nA skeletal joint that's the root of all other joints.\nstatic let head: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the head.\nstatic let leftFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left foot.\nstatic let leftHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left hand.\nstatic let leftShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left shoulder.\nstatic let rightFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right foot.\nstatic let rightShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right shoulder."
  },
  {
    "title": "rightShoulder | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton/jointname/3255202-rightshoulder",
    "html": "See Also\nIdentifying Joints\nstatic let root: ARSkeleton.JointName\nA skeletal joint that's the root of all other joints.\nstatic let head: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the head.\nstatic let leftFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left foot.\nstatic let leftHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left hand.\nstatic let leftShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left shoulder.\nstatic let rightFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right foot.\nstatic let rightHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right hand."
  },
  {
    "title": "ARPlaneClassificationWall | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneclassification/arplaneclassificationwall",
    "html": "See Also\nPlane Classifications\nARPlaneClassificationNone\nNo classification is available for the plane anchor.\nARPlaneClassificationFloor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\nARPlaneClassificationCeiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\nARPlaneClassificationTable\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\nARPlaneClassificationSeat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\nARPlaneClassificationDoor\nThe plane anchor represents a real-world door, or similar archway.\nARPlaneClassificationWindow\nThe plane anchor fits the description of a real-world window."
  },
  {
    "title": "ARPlaneClassificationFloor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneclassification/arplaneclassificationfloor",
    "html": "See Also\nPlane Classifications\nARPlaneClassificationNone\nNo classification is available for the plane anchor.\nARPlaneClassificationWall\nThe plane anchor represents a real-world wall or similar large vertical surface.\nARPlaneClassificationCeiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\nARPlaneClassificationTable\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\nARPlaneClassificationSeat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\nARPlaneClassificationDoor\nThe plane anchor represents a real-world door, or similar archway.\nARPlaneClassificationWindow\nThe plane anchor fits the description of a real-world window."
  },
  {
    "title": "ARPlaneClassificationTable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneclassification/arplaneclassificationtable",
    "html": "See Also\nPlane Classifications\nARPlaneClassificationNone\nNo classification is available for the plane anchor.\nARPlaneClassificationWall\nThe plane anchor represents a real-world wall or similar large vertical surface.\nARPlaneClassificationFloor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\nARPlaneClassificationCeiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\nARPlaneClassificationSeat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\nARPlaneClassificationDoor\nThe plane anchor represents a real-world door, or similar archway.\nARPlaneClassificationWindow\nThe plane anchor fits the description of a real-world window."
  },
  {
    "title": "ARPlaneClassificationCeiling | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneclassification/arplaneclassificationceiling",
    "html": "See Also\nPlane Classifications\nARPlaneClassificationNone\nNo classification is available for the plane anchor.\nARPlaneClassificationWall\nThe plane anchor represents a real-world wall or similar large vertical surface.\nARPlaneClassificationFloor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\nARPlaneClassificationTable\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\nARPlaneClassificationSeat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\nARPlaneClassificationDoor\nThe plane anchor represents a real-world door, or similar archway.\nARPlaneClassificationWindow\nThe plane anchor fits the description of a real-world window."
  },
  {
    "title": "ARPlaneClassificationDoor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneclassification/arplaneclassificationdoor",
    "html": "See Also\nPlane Classifications\nARPlaneClassificationNone\nNo classification is available for the plane anchor.\nARPlaneClassificationWall\nThe plane anchor represents a real-world wall or similar large vertical surface.\nARPlaneClassificationFloor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\nARPlaneClassificationCeiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\nARPlaneClassificationTable\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\nARPlaneClassificationSeat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\nARPlaneClassificationWindow\nThe plane anchor fits the description of a real-world window."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/notificationaction/info_id",
    "html": "Overview\n\nThe default value is Notification.\n\nDeclaration\nuniform token info:id = \"Notification\"\n\n\nSee Also\nProperties\naffectedObjects\nA list of prims that respond to the notification.\nidentifier\nA string value that identifies the app-specific notification."
  },
  {
    "title": "affectedObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/notificationaction/affectedobjects",
    "html": "Overview\n\nThe action provides this list of prims to the app, on which it decides the operation to perform to achieve the desired effect.\n\nDeclaration\nrel affectedObjects\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\nidentifier\nA string value that identifies the app-specific notification."
  },
  {
    "title": "actions | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/preliminary_behavior/actions",
    "html": "Overview\n\nWhen this array contains multiple actions, the runtime executes each action, one after the other. Only insert Preliminary_Action prims in this array.\n\nDeclaration\nrel actions\n\n\nSee Also\nProperties\ntriggers\nA list of prims that execute a behavior’s actions.\nexclusive\nA Boolean value that determines if a behavior executes exclusively."
  },
  {
    "title": "triggers | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/preliminary_behavior/triggers",
    "html": "Overview\n\nThe runtime executes all actions in the actions array if the conditions of any trigger in triggers are satisfied. Only insert Preliminary_Trigger prims in this array.\n\nDeclaration\nrel triggers\n\n\nSee Also\nProperties\nactions\nA list of prims that a behavior’s triggers invoke.\nexclusive\nA Boolean value that determines if a behavior executes exclusively."
  },
  {
    "title": "stride | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometrysource/3516920-stride",
    "html": "Discussion\n\nStride may contain padding, so it's not always equal to the vector-format's total size in bytes.\n\nSee Also\nGetting Geometry Information\nvar componentsPerVector: Int\nThe number of scalar components in each vector.\nvar count: Int\nThe number of vectors in the buffer.\nvar format: MTLVertexFormat\nThe type of vector data in the buffer.\nvar offset: Int\nThe offset, in bytes, from the beginning of the buffer."
  },
  {
    "title": "componentsPerVector | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometrysource/3521383-componentspervector",
    "html": "Discussion\n\nIn the case that componentsPerVector is greater than 1, the element type of the geometry-source array is itself, a sequence.\n\nSee Also\nGetting Geometry Information\nvar count: Int\nThe number of vectors in the buffer.\nvar format: MTLVertexFormat\nThe type of vector data in the buffer.\nvar offset: Int\nThe offset, in bytes, from the beginning of the buffer.\nvar stride: Int\nThe length, in bytes, of the start of one vector in the buffer to the start of the next vector."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/collidetrigger/info_id",
    "html": "Overview\n\nThe default value is “Collide”.\n\nDeclaration\nuniform token info:id = \"Collide\"\n\n\nSee Also\nProperties\naffectedObjects\nA list of prims that may come in contact with colliders.\ncolliders\nA list of prims that interact with objects to create a collision."
  },
  {
    "title": "affectedObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/collidetrigger/affectedobjects",
    "html": "Overview\n\nAdd prims that implement physics to this list. For more information about enabling physics on prims, see Simulating a Physical Interaction.\n\nDeclaration\nrel affectedObjects\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the trigger.\ncolliders\nA list of prims that interact with objects to create a collision."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/waitaction/info_id",
    "html": "Overview\n\nThe default value is Wait.\n\nDeclaration\nuniform token info:id = \"Wait\"\n\n\nSee Also\nProperties\nduration\nThe amount of time to wait before finishing the action."
  },
  {
    "title": "colliders | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/collidetrigger/colliders",
    "html": "Overview\n\nAdd prims that implement physics to this list. For more information about enabling physics on prims, see Simulating a Physical Interaction.\n\nDeclaration\nrel colliders\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the trigger.\naffectedObjects\nA list of prims that may come in contact with colliders."
  },
  {
    "title": "easeType | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/visibilityaction/easetype",
    "html": "Overview\n\nThe default type is none.\n\nEase Types\nnone\n\nPaces the action at a constant rate.\n\nin\n\nPaces the action slower at the beginning.\n\nout\n\nPaces the action slower at the end.\n\ninout\n\nPaces the action slower at the beginning and end.\n\nDeclaration\nuniform token easeType = \"none\" (\n        allowedTokens = [\"none\", \"in\", \"out\", \"inout\"]\n)\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims to show or hide.\ntype\nAn option that determines the target prim’s visibility when the action finishes.\nstyle\nAn option that implements different kinds of animation timing.\nmotionType\nAn option that determines how the action displays or hides a prim.\nmoveDistance\nThe distance that this action moves the target prims."
  },
  {
    "title": "moveDistance | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/visibilityaction/movedistance",
    "html": "Overview\n\nThe default value is zero. This property is in stage linear units; for more information, see Encoding Stage Linear Units.\n\nDeclaration\nuniform double moveDistance = 0.0\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims to show or hide.\ntype\nAn option that determines the target prim’s visibility when the action finishes.\nstyle\nAn option that implements different kinds of animation timing.\nmotionType\nAn option that determines how the action displays or hides a prim.\neaseType\nAn option that describes the animation’s change in pace over time."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/transformanimationaction/info_id",
    "html": "Overview\n\nThe default value is TransformAnimation.\n\nDeclaration\nuniform token info:id = \"TransformAnimation\"\n\n\nSee Also\nProperties\naffectedObjects\nA list of prims on which to play a transform animation.\nanimation\nA prim that contains a transform animation."
  },
  {
    "title": "animation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/transformanimationaction/animation",
    "html": "Overview\n\nAssign this property a prim that contains a transform animation.\n\nDeclaration\nrel xformTarget\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims on which to play a transform animation."
  },
  {
    "title": "ARGeoTrackingStatus.StateReason.waitingForAvailabilityCheck | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/statereason/waitingforavailabilitycheck",
    "html": "Discussion\n\nWhile in this state, the app waits for the geotracking subsystem to determine geotracking availability at the user's GPS location. Inform the user of the check in progress; for instance, present a message alerting them to the geotracking initialization process.\n\nSee Also\nStatus Reasons\ncase none\nNo issues reported.\ncase notAvailableAtLocation\nThe location doesn't provide geotracking.\ncase needLocationPermissions\nThe location requires user permission for geotracking.\ncase devicePointedTooLow\nThe position of the device is too low for geotracking.\ncase worldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\ncase waitingForLocation\nA state in which the framework performs a check for the user's GPS position.\ncase geoDataNotLoaded\nA state in which the framework downloads localization imagery.\ncase visualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "ARGeoAnchor.AltitudeSource.precise | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/altitudesource/precise",
    "html": "See Also\nSources\ncase coarse\nThe framework sets the altitude using a coarse digital-elevation model.\ncase userDefined\nThe app defines the altitude.\ncase unknown\nAltitude isn’t yet set."
  },
  {
    "title": "altitude | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/3551716-altitude",
    "html": "Discussion\n\nNegative values indicate below sea level. This property is valid only when altitudeSource is a value other than ARAltitudeSourceUnknown.\n\nSee Also\nDefining Altitude\naltitudeSource\nA record of the source from which an altitude came.\nARAltitudeSource\nOptions for setting a location anchor’s altitude."
  },
  {
    "title": "ARAltitudeSource | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/araltitudesource",
    "html": "Overview\n\nEach altitude source has unique performance and accuracy characteristics.\n\nTopics\nSources\nARAltitudeSourcePrecise\nThe framework sets the altitude using a high-resolution digital-elevation model.\nARAltitudeSourceCoarse\nThe framework sets the altitude using a coarse digital-elevation model.\nARAltitudeSourceUserDefined\nThe app defines the altitude.\nARAltitudeSourceUnknown\nAltitude isn’t yet set.\nSee Also\nDefining Altitude\naltitude\nVertical distance, in meters, between this anchor and sea level.\naltitudeSource\nA record of the source from which an altitude came."
  },
  {
    "title": "ARGeoAnchor.AltitudeSource.coarse | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/altitudesource/coarse",
    "html": "Discussion\n\nThe accuracy of this altitude is noticeably imprecise at close range, but it’s sufficient from far away. Use this option to save computational resources for anchors that are far off in the distance.\n\nSee Also\nSources\ncase precise\nThe framework sets the altitude using a high-resolution digital-elevation model.\ncase userDefined\nThe app defines the altitude.\ncase unknown\nAltitude isn’t yet set."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/tapgesturetrigger/info_id",
    "html": "Overview\n\nThe default value is “TapGesture”.\n\nDeclaration\nuniform token info:id = \"TapGesture\"\n\n\nSee Also\nProperties\naffectedObjects\nA list of prims that fire the trigger."
  },
  {
    "title": "ARMeshClassification.ceiling | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshclassification/ceiling",
    "html": "See Also\nOptions\ncase door\nThe face is a part of a real-world door.\ncase floor\nThe face is a part of a real-world floor.\ncase none\nA face ARKit can't classify.\ncase seat\nThe face is a part of a real-world seat.\ncase table\nThe face is a part of a real-world table.\ncase wall\nThe face is a part of a real-world wall.\ncase window\nThe face is a part of a real-world window."
  },
  {
    "title": "vertices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanegeometry/2941050-vertices",
    "html": "Discussion\n\nEach float3 value in this buffer represents the position of a vertex in the mesh. The owning plane anchor's transform matrix defines the coordinate system for these points.\n\nThe vertexCount property provides the number of elements in the buffer.\n\nThis buffer, together with the triangleIndices buffer, describes a mesh covering the entire surface of the plane. Use this mesh for purposes that involve the filled shape, such as rendering a solid 3D representation of the surface. If, instead, you only need to know the outline of the shape, see the boundaryVertices property.\n\nSee Also\nAccessing Mesh Data\nvertexCount\nThe number of elements in the vertices buffer.\ntextureCoordinates\nA buffer of texture coordinate values for each point in the plane mesh.\ntextureCoordinateCount\nThe number of elements in the textureCoordinates buffer.\ntriangleIndices\nA buffer of indices describing the triangle mesh formed by the plane geometry's vertex data.\ntriangleCount\nThe number of triangles described by the triangleIndices buffer."
  },
  {
    "title": "identifier | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/notificationtrigger/identifier",
    "html": "Overview\n\nThe runtime fires a trigger when an app provides a notification that matches this property’s value.\n\nDeclaration\nuniform string identifier\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the trigger."
  },
  {
    "title": "affectedObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/tapgesturetrigger/affectedobjects",
    "html": "Overview\n\nThe runtime fires this TapGestureTrigger for each prim in this list when the user taps one of them.\n\nDeclaration\nrel affectedObjects\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the trigger."
  },
  {
    "title": "affectedObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/proximitytocameratrigger/affectedobjects",
    "html": "Overview\n\nA trigger fires if the user’s camera crosses the distance threshold of the prims in this list.\n\nDeclaration\nrel affectedObjects\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the trigger.\ndistance\nA threshold that measures the user’s proximity to one or more prims."
  },
  {
    "title": "ARPlaneClassification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneclassification",
    "html": "Overview\n\nYou get values of this type from a plane anchor's classification property, identifying the likely type of real-world surface for a detected plane anchor.\n\nTopics\nPlane Classifications\nARPlaneClassificationNone\nNo classification is available for the plane anchor.\nARPlaneClassificationWall\nThe plane anchor represents a real-world wall or similar large vertical surface.\nARPlaneClassificationFloor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\nARPlaneClassificationCeiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\nARPlaneClassificationTable\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\nARPlaneClassificationSeat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\nARPlaneClassificationDoor\nThe plane anchor represents a real-world door, or similar archway.\nARPlaneClassificationWindow\nThe plane anchor fits the description of a real-world window.\nSee Also\nClassifying a Plane\nclassificationSupported\nA Boolean value that indicates whether plane classification is available on the current device.\nclassification\nA general characterization of what kind of real-world surface the plane anchor represents.\nclassificationStatus\nThe current state of ARKit's process for classifying the plane anchor.\nARPlaneClassificationStatus\nPossible states of ARKit's process for classifying plane anchors."
  },
  {
    "title": "estimatedVerticalPlane | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arhittestresult/resulttype/2887455-estimatedverticalplane",
    "html": "Discussion\n\nARKit provides two ways to locate real-world flat surfaces in a scene. Plane detection (enabled with planeDetection on your session configuration) is an ongoing process, continuously analyzing the scene to accurately map the position and extent of any planes in view. Because plane detection takes time, you can fall back to plane estimation to get an instant, but less accurate, indication of whether a 2D point in the camera image corresponds to a real-world flat surface.\n\nBecause plane detection results are more accurate than plane estimation results, ARKit prefers the former when searching for both. If your hit-test search includes both estimatedVerticalPlane and one or more existingPlane types, and the search finds any already detected plane anchors, the search returns only the existing plane(s) and no estimated plane.\n\nAn estimated plane search returns at most one result—the best estimate for a vertical plane intersecting the hit-test ray.\n\nSee Also\nResult Types\nstatic var featurePoint: ARHitTestResult.ResultType\nA point on a surface detected by ARKit, but not part of any detected planes.\nDeprecated\nstatic var estimatedHorizontalPlane: ARHitTestResult.ResultType\nA point on a real-world planar surface detected during the search, whose orientation is perpendicular to gravity.\nDeprecated\nstatic var existingPlane: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), without considering the plane's size.\nDeprecated\nstatic var existingPlaneUsingExtent: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), respecting the plane's estimated size.\nDeprecated\nstatic var existingPlaneUsingGeometry: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), respecting the plane's estimated size and shape.\nDeprecated"
  },
  {
    "title": "existingPlane | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arhittestresult/resulttype/2875738-existingplane",
    "html": "Discussion\n\nWhen searching for this result type, ARKit can return any point coplanar with an already detected plane, regardless of whether that plane's already detected extent or geometry includes that point. (That is, this result type searches the infinite extensions of detected planes.)\n\nAn existing plane search can return any number of results, depending on how many already-detected planes the hit test ray intersects (if any).\n\nSee Also\nResult Types\nstatic var featurePoint: ARHitTestResult.ResultType\nA point on a surface detected by ARKit, but not part of any detected planes.\nDeprecated\nstatic var estimatedHorizontalPlane: ARHitTestResult.ResultType\nA point on a real-world planar surface detected during the search, whose orientation is perpendicular to gravity.\nDeprecated\nstatic var estimatedVerticalPlane: ARHitTestResult.ResultType\nA point on a real-world planar surface detected during the search, whose orientation is parallel to gravity.\nDeprecated\nstatic var existingPlaneUsingExtent: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), respecting the plane's estimated size.\nDeprecated\nstatic var existingPlaneUsingGeometry: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), respecting the plane's estimated size and shape.\nDeprecated"
  },
  {
    "title": "featurePoint | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arhittestresult/resulttype/2875708-featurepoint",
    "html": "Discussion\n\nDuring a world-tracking AR session, ARKit builds a coarse point cloud representing its rough understanding of the 3D world around the user (see rawFeaturePoints). Individual feature points represent parts of the camera image likely to be part of a real-world surface, but not necessarily a planar surface.\n\nWhen you search using this hit-test option, ARKit finds the feature point nearest to the hit-test ray (the extension of the 2D hit-test point into 3D world space), then returns the point on the ray nearest to that feature point.\n\nSee Also\nResult Types\nstatic var estimatedHorizontalPlane: ARHitTestResult.ResultType\nA point on a real-world planar surface detected during the search, whose orientation is perpendicular to gravity.\nDeprecated\nstatic var estimatedVerticalPlane: ARHitTestResult.ResultType\nA point on a real-world planar surface detected during the search, whose orientation is parallel to gravity.\nDeprecated\nstatic var existingPlane: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), without considering the plane's size.\nDeprecated\nstatic var existingPlaneUsingExtent: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), respecting the plane's estimated size.\nDeprecated\nstatic var existingPlaneUsingGeometry: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), respecting the plane's estimated size and shape.\nDeprecated"
  },
  {
    "title": "existingPlaneUsingExtent | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arhittestresult/resulttype/2887459-existingplaneusingextent",
    "html": "Discussion\n\nWhen searching for this result type, ARKit returns a point coplanar with an already detected plane only if that point lies within the area defined by the plane's center and extent properties. Those properties (together with the anchor's transform) define the smallest rectangular area that includes all regions ARKit estimates to be a part of the plane.\n\nHowever, that rectangular area may contain regions that are not part of the same real-world surface. There may also be parts of the same real-world surface that lie outside the rectangle because ARKit has not yet recognized them as part of the same plane. You can get a more precise estimate of the plane area ARKit has recognized by testing with the existingPlaneUsingGeometry type, or extend your hit test to an infinite plane with the existingPlane type.\n\nAn existing plane search can return any number of results, depending on how many already-detected planes the hit test ray intersects (if any).\n\nSee Also\nResult Types\nstatic var featurePoint: ARHitTestResult.ResultType\nA point on a surface detected by ARKit, but not part of any detected planes.\nDeprecated\nstatic var estimatedHorizontalPlane: ARHitTestResult.ResultType\nA point on a real-world planar surface detected during the search, whose orientation is perpendicular to gravity.\nDeprecated\nstatic var estimatedVerticalPlane: ARHitTestResult.ResultType\nA point on a real-world planar surface detected during the search, whose orientation is parallel to gravity.\nDeprecated\nstatic var existingPlane: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), without considering the plane's size.\nDeprecated\nstatic var existingPlaneUsingGeometry: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), respecting the plane's estimated size and shape.\nDeprecated"
  },
  {
    "title": "existingPlaneUsingGeometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arhittestresult/resulttype/2942264-existingplaneusinggeometry",
    "html": "Discussion\n\nWhen searching for this result type, ARKit returns a point coplanar with an already detected plane only if that point lies within the area defined by the plane's geometry. That property (together with the anchor's transform) defines the smallest polygonal area that includes all regions ARKit estimates to be a part of the plane.\n\nBecause that polygon is always convex, it may contain regions that are not part of the same real-world surface. (It does, however, provide a more precise esitmate than a bounding rectangle provided by the existingPlaneUsingExtent type.) There may also be parts of the same real-world surface that lie outside the polygon because ARKit has not yet recognized them as part of the same plane. You extend your hit test to an infinite plane by using the existingPlane type.\n\nAn existing plane search can return any number of results, depending on how many already-detected planes the hit test ray intersects (if any).\n\nSee Also\nResult Types\nstatic var featurePoint: ARHitTestResult.ResultType\nA point on a surface detected by ARKit, but not part of any detected planes.\nDeprecated\nstatic var estimatedHorizontalPlane: ARHitTestResult.ResultType\nA point on a real-world planar surface detected during the search, whose orientation is perpendicular to gravity.\nDeprecated\nstatic var estimatedVerticalPlane: ARHitTestResult.ResultType\nA point on a real-world planar surface detected during the search, whose orientation is parallel to gravity.\nDeprecated\nstatic var existingPlane: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), without considering the plane's size.\nDeprecated\nstatic var existingPlaneUsingExtent: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), respecting the plane's estimated size.\nDeprecated"
  },
  {
    "title": "ARTrackingStateNotAvailable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/artrackingstate/artrackingstatenotavailable",
    "html": "See Also\nTracking States\nARTrackingStateLimited\nTracking is available, but the quality of results is questionable.\nARTrackingStateNormal\nCamera position tracking is providing optimal results."
  },
  {
    "title": "Preliminary_Action | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/preliminary_action",
    "html": "Overview\n\nBecause it inherits Typed, this schema declares a Preliminary_Action as a type of prim. For more information about typed schemas, see USD Specification > Typed.\n\nWhen a behavior executes an action, the behavior modifies the state of the scene dynamically. For example, an action might start an animation, change the location of a prim, or start playing audio.\n\nDeclaration\nclass \"Preliminary_Action\" (\n    inherits = </Typed>\n)\n\n\nDefine an action that slides a cube\n\nThe following example shows an action prim called PushCube that affects an impulse feature.\n\n#usda 1.0\n\n\ndef Cube \"Cube\" {}\n\n\ndef Preliminary_Action \"PushCube\" {    \n    uniform token info:id = \"Impulse\"\n    rel affectedObjects = [ </Cube> ]\n    uniform vector3d velocity = (1.0, 0.0, 0.0)\n}\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the action.\nmultiplePerformOperation\nAn option that indicates how an action handles an additional invocation while running.\nSee Also\nActions\nAudioAction\nAn action that plays audio.\nChangeSceneAction\nAn action that transitions from one scene to another.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nGroupAction\nAn action that runs a list of other actions.\nImpulseAction\nAn action that adds velocity to an prim.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nOrbitAction\nAn action that orbits a set of prims around another.\nSpinAction\nAn action that spins a prim.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAction\nAn action that animates from one transform to another.\nTransformAnimationAction\nAn action that plays a transform animation.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nWaitAction\nAn action that performs a delay.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "front | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/lookatcameraaction/front",
    "html": "Overview\n\nFrom the value of this property, the runtime calculates how much to spin the target object about the upVector. This property is in local space, and so it’s relative to the target object’s coordinate space. The default value faces positively in the x-direction.\n\nDeclaration\nuniform vector3d front = (1.0, 0.0, 0.0)\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that face the camera when this action executes.\nduration\nThe amount of time that the objects face the camera.\nupVector\nA vector around which the runtime rotates the object."
  },
  {
    "title": "upVector | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/lookatcameraaction/upvector",
    "html": "Overview\n\nThis property defines the axis around which the runtime rotates the target to create the effect of a prim looking at the user. Normally, an asset defines this value to match the stage’s upAxis. The default value points positively in the y-direction.\n\nDeclaration\nuniform vector3d upVector = (0.0, 1.0, 0.0)\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that face the camera when this action executes.\nduration\nThe amount of time that the objects face the camera.\nfront\nA vector that’s perpendicular to, and points outward from, the object’s face."
  },
  {
    "title": "duration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/lookatcameraaction/duration",
    "html": "Overview\n\nThe default value is 1.0. The prims in affectedObjects face the camera immediately when this action executes.\n\nDeclaration\nuniform double duration = 1.0\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that face the camera when this action executes.\nfront\nA vector that’s perpendicular to, and points outward from, the object’s face.\nupVector\nA vector around which the runtime rotates the object."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/audioaction/info_id",
    "html": "Overview\n\nThe default value is “Audio”.\n\nDeclaration\nuniform token info:id = \"Audio\"\n\n\nSee Also\nProperties\naffectedObjects\nA list of prims that play audio.\naudio\nThe location of an audio file.\ntype\nThe type of command to send the audio.\ngain\nA value that controls the audio volume.\nauralMode\nAn option that controls the audio signal’s spacial dynamics."
  },
  {
    "title": "affectedObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/audioaction/affectedobjects",
    "html": "Overview\n\nThis action plays audio located in 3D space to the position of the affected objects when the action’s auralMode is set to spatial.\n\nThe runtime requires a separate AudioAction for each prim from which an asset intends to play audio. As a result, the runtime plays audio only from the first prim in a list.\n\nDeclaration\nrel affectedObjects\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naudio\nThe location of an audio file.\ntype\nThe type of command to send the audio.\ngain\nA value that controls the audio volume.\nauralMode\nAn option that controls the audio signal’s spacial dynamics."
  },
  {
    "title": "scene | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/changesceneaction/scene",
    "html": "Overview\n\nAn asset should assign this relationship a scene prim, as defined through the sceneLibrary kind metadata.\n\nDeclaration\nrel scene\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/changesceneaction/info_id",
    "html": "Overview\n\nThe default value is “ChangeScene”.\n\nDeclaration\nuniform token info:id = \"ChangeScene\"\n\n\nSee Also\nProperties\nscene\nThe scene to which the action transitions."
  },
  {
    "title": "start | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/startanimationaction/start",
    "html": "Overview\n\nThis property is a normalized percentage. The default value is 0.0, which corresponds to the beginning of the animation. The maximum value is 1.0, which represents the end of the animation.\n\nDeclaration\nuniform double start = 0.0\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of objects to animate.\nduration\nThe amount of time between the start of an action and its end.\nreversed\nA Boolean value that determines the clip playback direction.\nanimationSpeed\nA factor to apply to the animation speed.\nreverses\nA Boolean value that indicates whether the animation plays from beginning to end, then again from end to beginning."
  },
  {
    "title": "ARPlaneClassificationSeat | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneclassification/arplaneclassificationseat",
    "html": "See Also\nPlane Classifications\nARPlaneClassificationNone\nNo classification is available for the plane anchor.\nARPlaneClassificationWall\nThe plane anchor represents a real-world wall or similar large vertical surface.\nARPlaneClassificationFloor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\nARPlaneClassificationCeiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\nARPlaneClassificationTable\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\nARPlaneClassificationDoor\nThe plane anchor represents a real-world door, or similar archway.\nARPlaneClassificationWindow\nThe plane anchor fits the description of a real-world window."
  },
  {
    "title": "ARPlaneClassificationWindow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneclassification/arplaneclassificationwindow",
    "html": "See Also\nPlane Classifications\nARPlaneClassificationNone\nNo classification is available for the plane anchor.\nARPlaneClassificationWall\nThe plane anchor represents a real-world wall or similar large vertical surface.\nARPlaneClassificationFloor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\nARPlaneClassificationCeiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\nARPlaneClassificationTable\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\nARPlaneClassificationSeat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\nARPlaneClassificationDoor\nThe plane anchor represents a real-world door, or similar archway."
  },
  {
    "title": "ARPlaneClassificationStatusUnknown | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneclassificationstatus/arplaneclassificationstatusunknown",
    "html": "Discussion\n\nARKit attempts to classify detected planes using a finite set of common categories. However, a detected plane may not be a real object fitting any of those categories, or the plane classification process may not be able to recognize it. In such cases, the plane anchor's classification is ARPlaneClassificationNone and its classificationStatus is ARPlaneClassificationStatusUnknown.\n\nSee Also\nClassification Status\nARPlaneClassificationStatusNotAvailable\nARKit cannot currently provide plane classification information.\nARPlaneClassificationStatusUndetermined\nARKit has not yet produced a classification for the plane anchor.\nARPlaneClassificationStatusKnown\nARKit has completed its classfication process for the plane anchor."
  },
  {
    "title": "reversed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/startanimationaction/reversed",
    "html": "Overview\n\nThe default value is false, which doesn’t reverse the animation. Set this property to true to play the animation from the end to the beginning.\n\nDeclaration\nuniform bool reversed = false\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of objects to animate.\nstart\nThe moment to begin an animation.\nduration\nThe amount of time between the start of an action and its end.\nanimationSpeed\nA factor to apply to the animation speed.\nreverses\nA Boolean value that indicates whether the animation plays from beginning to end, then again from end to beginning."
  },
  {
    "title": "animationSpeed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/startanimationaction/animationspeed",
    "html": "Overview\n\nThe default value is 1.0, which doesn’t change the animation speed.\n\nDeclaration\nuniform double animationSpeed = 1.0\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of objects to animate.\nstart\nThe moment to begin an animation.\nduration\nThe amount of time between the start of an action and its end.\nreversed\nA Boolean value that determines the clip playback direction.\nreverses\nA Boolean value that indicates whether the animation plays from beginning to end, then again from end to beginning."
  },
  {
    "title": "duration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/startanimationaction/duration",
    "html": "Overview\n\nThe default value is 1.0.\n\nDeclaration\nuniform double duration = 1.0\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of objects to animate.\nstart\nThe moment to begin an animation.\nreversed\nA Boolean value that determines the clip playback direction.\nanimationSpeed\nA factor to apply to the animation speed.\nreverses\nA Boolean value that indicates whether the animation plays from beginning to end, then again from end to beginning."
  },
  {
    "title": "affectedObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/startanimationaction/affectedobjects",
    "html": "Overview\n\nAdd one or more prims to this list.\n\nDeclaration\nrel affectedObjects\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\nstart\nThe moment to begin an animation.\nduration\nThe amount of time between the start of an action and its end.\nreversed\nA Boolean value that determines the clip playback direction.\nanimationSpeed\nA factor to apply to the animation speed.\nreverses\nA Boolean value that indicates whether the animation plays from beginning to end, then again from end to beginning."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/startanimationaction/info_id",
    "html": "Overview\n\nThe default value is StartAnimation.\n\nDeclaration\nuniform token info:id = \"StartAnimation\"\n\n\nSee Also\nProperties\naffectedObjects\nA list of objects to animate.\nstart\nThe moment to begin an animation.\nduration\nThe amount of time between the start of an action and its end.\nreversed\nA Boolean value that determines the clip playback direction.\nanimationSpeed\nA factor to apply to the animation speed.\nreverses\nA Boolean value that indicates whether the animation plays from beginning to end, then again from end to beginning."
  },
  {
    "title": "reverses | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/startanimationaction/reverses",
    "html": "Overview\n\nThe default value is false, which doesn’t reverse the animation. Set this property to true to play the animation once to the end, and once more in reverse.\n\nDeclaration\nuniform bool reverses = false\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of objects to animate.\nstart\nThe moment to begin an animation.\nduration\nThe amount of time between the start of an action and its end.\nreversed\nA Boolean value that determines the clip playback direction.\nanimationSpeed\nA factor to apply to the animation speed."
  },
  {
    "title": "ARPlaneAnchorAlignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchoralignment",
    "html": "Topics\nAlignment Values\nARPlaneAnchorAlignmentHorizontal\nThe plane is perpendicular to gravity.\nARPlaneAnchorAlignmentVertical\nThe plane is parallel to gravity.\nSee Also\nOrientation\nalignment\nThe general orientation of the detected plane with respect to gravity."
  },
  {
    "title": "exclusive | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/preliminary_behavior/exclusive",
    "html": "Overview\n\nThe default value is false, which indicates that other behaviors’ actions run concurrently with the behavior. If the value is true, other exclusive behaviors stop performing actions when the runtime actives a trigger in the behavior.\n\nDeclaration\nuniform bool exclusive = false\n\n\nSee Also\nProperties\ntriggers\nA list of prims that execute a behavior’s actions.\nactions\nA list of prims that a behavior’s triggers invoke."
  },
  {
    "title": "identifier | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/notificationaction/identifier",
    "html": "Overview\n\nThe runtime provides this identifier to send the notification to the app. Set a custom value for this property for each unique notification your asset uses.\n\nDeclaration\nuniform string identifier\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that respond to the notification."
  },
  {
    "title": "ARPlaneAnchor.Classification.wall | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/wall",
    "html": "See Also\nPlane Classifications\ncase floor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\ncase ceiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\ncase table\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\ncase seat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\ncase door\nThe plane anchor represents a real-world door or similar vertical surface.\ncase window\nThe plane anchor represents a real-world window or similar vertical surface."
  },
  {
    "title": "ARPlaneAnchor.Classification.none(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/none",
    "html": "Discussion\n\nPlane classification can take longer than plane detection, and ARKit reports classifications only for planes where it has a high confidence in the result. See the associated ARPlaneAnchor.Classification.Status value for the reason a plane anchor reports no classification.\n\nSee Also\nMissing Classification Status\nenum ARPlaneAnchor.Classification.Status\nReasons ARKit is unable to classify a plane."
  },
  {
    "title": "ARGeoTrackingStatus.StateReason.none | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/statereason/none",
    "html": "Discussion\n\nThis reason indicates that there is no user action currently needed to improve the geo-tracking state.\n\nSee Also\nStatus Reasons\ncase notAvailableAtLocation\nThe location doesn't provide geotracking.\ncase needLocationPermissions\nThe location requires user permission for geotracking.\ncase devicePointedTooLow\nThe position of the device is too low for geotracking.\ncase worldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\ncase waitingForLocation\nA state in which the framework performs a check for the user's GPS position.\ncase waitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\ncase geoDataNotLoaded\nA state in which the framework downloads localization imagery.\ncase visualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "width | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/geometry/extent/4218769-width",
    "html": "See Also\nInspecting a plane extent\nvar height: Float\nThe height of a plane.\nvar anchorFromExtentTransform: simd_float4x4\nThe transform from the plane extent to the plane anchor’s coordinate system.\nvar description: String\nA textual description of the size of a plane."
  },
  {
    "title": "ARGeoTrackingStatus.Accuracy.high | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/accuracy/high",
    "html": "Discussion\n\nThis value indicates that visual localization is complete and geo-tracking accuracy is very good.\n\nSee Also\nAccuracies\ncase undetermined\nGeo-tracking accuracy is undetermined.\ncase low\nGeo-tracking accuracy is low.\ncase medium\nGeo-tracking accuracy is average."
  },
  {
    "title": "ARGeoTrackingStatus.Accuracy.undetermined | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/accuracy/undetermined",
    "html": "Discussion\n\nThis value indicates that because the session has not completed visual localization, geo-tracking accuracy is indeterminate.\n\nSee Also\nAccuracies\ncase high\nGeo-tracking accuracy is high.\ncase low\nGeo-tracking accuracy is low.\ncase medium\nGeo-tracking accuracy is average."
  },
  {
    "title": "ARGeoTrackingStatus.StateReason.needLocationPermissions | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/statereason/needlocationpermissions",
    "html": "Discussion\n\nThis reason indicates that the user has not given this app permission to access the user’s location. To enable geo tracking, an app needs to ask the user to enable location sharing for this app in Settings.\n\nSee Also\nStatus Reasons\ncase none\nNo issues reported.\ncase notAvailableAtLocation\nThe location doesn't provide geotracking.\ncase devicePointedTooLow\nThe position of the device is too low for geotracking.\ncase worldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\ncase waitingForLocation\nA state in which the framework performs a check for the user's GPS position.\ncase waitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\ncase geoDataNotLoaded\nA state in which the framework downloads localization imagery.\ncase visualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "ARGeoTrackingStatus.StateReason.devicePointedTooLow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/statereason/devicepointedtoolow",
    "html": "Discussion\n\nARKit provides the app with this reason when the app is in state ARGeoTrackingStatus.State.localizing and the device is not capturing enough of the necessary live-camera imagery needed for visual localization because the user is pointing the camera too low. To resolve the issue, the app needs to instruct the user to raise the device and follow the guidance in Assisting the User with Visual Localization.\n\nSee Also\nStatus Reasons\ncase none\nNo issues reported.\ncase notAvailableAtLocation\nThe location doesn't provide geotracking.\ncase needLocationPermissions\nThe location requires user permission for geotracking.\ncase worldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\ncase waitingForLocation\nA state in which the framework performs a check for the user's GPS position.\ncase waitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\ncase geoDataNotLoaded\nA state in which the framework downloads localization imagery.\ncase visualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "ARGeoTrackingStatus.StateReason.waitingForLocation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/statereason/waitingforlocation",
    "html": "Discussion\n\nWhile in this state, the app needs to wait for the Core Location subsystem to provide the user's GPS location. Inform the user of the check in progress; for instance, present a message alerting them to the geotracking initialization process.\n\nSee Also\nStatus Reasons\ncase none\nNo issues reported.\ncase notAvailableAtLocation\nThe location doesn't provide geotracking.\ncase needLocationPermissions\nThe location requires user permission for geotracking.\ncase devicePointedTooLow\nThe position of the device is too low for geotracking.\ncase worldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\ncase waitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\ncase geoDataNotLoaded\nA state in which the framework downloads localization imagery.\ncase visualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "ARGeoTrackingStatus.Accuracy.medium | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/accuracy/medium",
    "html": "Discussion\n\nThis value indicates that visual localization is complete and geo-tracking accuracy is average.\n\nSee Also\nAccuracies\ncase high\nGeo-tracking accuracy is high.\ncase undetermined\nGeo-tracking accuracy is undetermined.\ncase low\nGeo-tracking accuracy is low."
  },
  {
    "title": "ARAppClipCodeAnchor.URLDecodingState.failed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arappclipcodeanchor/urldecodingstate/failed",
    "html": "Discussion\n\nThe decoding of an App Clip Code URL fails under these circumstances:\n\nThe full app or App Clip is missing the associated-domains entitlement from its code signature.\n\nThe host of the resource identified by the App Clip Code URL doesn't vend an Apple App Site Association (AASA) file.\n\nThe AASA file hosted at the App Clip Code URL's domain lacks the App Clip's fully-qualified application identifier.\n\nThe App Clip Code is not associated to the App Clip in an App Clip experience in App Store Connect.\n\nFor more information, see Configuring the launch experience of your App Clip.\n\nSee Also\nStates\ncase decoded\nA state that indicates the completed decoding of an App Clip Code URL.\ncase decoding\nA state that indicates the continuing process of decoding an App Clip Code's URL."
  },
  {
    "title": "ARGeoTrackingStatus.StateReason.visualLocalizationFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/statereason/visuallocalizationfailed",
    "html": "Discussion\n\nARKit provides this reason when visual localization is taking too long. This indicates that the app has met all requirements for geo tracking, except for visual localization. In this situation, the app needs to ask the user to pan the device around the physical environment to acquire more camera-feed imagery. For more information, see Assisting the User with Visual Localization.\n\nSee Also\nStatus Reasons\ncase none\nNo issues reported.\ncase notAvailableAtLocation\nThe location doesn't provide geotracking.\ncase needLocationPermissions\nThe location requires user permission for geotracking.\ncase devicePointedTooLow\nThe position of the device is too low for geotracking.\ncase worldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\ncase waitingForLocation\nA state in which the framework performs a check for the user's GPS position.\ncase waitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\ncase geoDataNotLoaded\nA state in which the framework downloads localization imagery."
  },
  {
    "title": "ARErrorCode | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrorcode",
    "html": "Topics\nErrors\nARErrorCodeRequestFailed\nAn error that indicates a request fails.\nARErrorCodeCameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\nARErrorCodeFileIOFailed\nAn error that indicates a file access fails to read or write.\nARErrorCodeInsufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\nARErrorCodeInvalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nARErrorCodeInvalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\nARErrorCodeInvalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\nARErrorCodeInvalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\nARErrorCodeInvalidWorldMap\nAn error that indicates the framework fails to process a world map.\nARErrorCodeMicrophoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\nARErrorCodeObjectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\nARErrorCodeSensorFailed\nAn error that indicates a sensor fails to provide required input.\nARErrorCodeSensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\nARErrorCodeUnsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\nARErrorCodeWorldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\nARErrorCodeGeoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nARErrorCodeGeoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\nARErrorCodeLocationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\nARErrorCodeHighResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\nARErrorCodeHighResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "referenceObjectByApplyingTransform: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/2977514-referenceobjectbyapplyingtransfo",
    "html": "Parameters\ntransform\n\nA transform matrix in the local coordinate space of the reference object.\n\nReturn Value\n\nThe transformed reference object.\n\nDiscussion\n\nYou define the local coordinate space of a reference object when you extract it from an ARWorldMap. If an existing reference object has a local coordinate origin that doesn't fit well with the object's intended use, call this method to change the reference object's origin with respect to the physical object it represents.\n\nWhen ARKit detects a reference object, the transform of the resulting ARObjectAnchor is based on the orgin of the reference object's coordinate system. For example, if a reference object represents a physical item that sits on a horizontal surface, virtual content should appear to sit on whatever surface the physical object does. As such, it's typically useful to align a reference object's coordinate origin with the bottom of the physical object.\n\nSee Also\nCreating Derivative Reference Objects\n- referenceObjectByMergingObject:error:\nReturns a new reference object that combines spatial information from both this reference object and another."
  },
  {
    "title": "ARRaycastQuery.TargetAlignment.horizontal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery/targetalignment/horizontal",
    "html": "See Also\nChoosing a Target Alignment\ncase any\nThe case that indicates a target may be aligned in any way with respect to gravity.\ncase vertical\nThe case that indicates a target is aligned vertically with respect to gravity."
  },
  {
    "title": "initWithName:coordinate: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/3551720-initwithname",
    "html": "Parameters\nname\n\nName of the anchor.\n\ncoordinate\n\nLattitude and longitude of the anchor’s geographic location.\n\nDiscussion\n\nBecause this initializer does not take an altitude argument, ARKit will determine (and set) the anchor’s altitude at runtime.\n\nSee Also\nCreating a Geo Anchor\n- initWithCoordinate:\nInitializes a new location anchor with the given coordinates.\n- initWithCoordinate:altitude:\nInitializes a location anchor with the given coordinate and altitude.\n- initWithName:coordinate:altitude:\nInitializes a named location anchor with the given coordinates and altitude."
  },
  {
    "title": "ARRaycastQuery.TargetAlignment.vertical | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery/targetalignment/vertical",
    "html": "See Also\nChoosing a Target Alignment\ncase any\nThe case that indicates a target may be aligned in any way with respect to gravity.\ncase horizontal\nThe case that indicates a target is aligned horizontally with respect to gravity."
  },
  {
    "title": "ARAppClipCodeAnchor.URLDecodingState.decoding | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arappclipcodeanchor/urldecodingstate/decoding",
    "html": "Discussion\n\nThe App Clip Code anchor's url is nil in this state.\n\nSee Also\nStates\ncase decoded\nA state that indicates the completed decoding of an App Clip Code URL.\ncase failed\nA state that indicates the failure to decode an App Clip Code's URL."
  },
  {
    "title": "initWithName:coordinate:altitude: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/3551721-initwithname",
    "html": "Parameters\nname\n\nName of the anchor.\n\ncoordinate\n\nLattitude and longitude of the anchor’s geographic location.\n\naltitude\n\nVertical distance, in meters, between this anchor and sea level.\n\nSee Also\nCreating a Geo Anchor\n- initWithCoordinate:\nInitializes a new location anchor with the given coordinates.\n- initWithCoordinate:altitude:\nInitializes a location anchor with the given coordinate and altitude.\n- initWithName:coordinate:\nInitializes a named location anchor with the given coordinates."
  },
  {
    "title": "isRecommendedForHighResolutionFrameCapturing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arvideoformat/3930052-isrecommendedforhighresolutionfr",
    "html": "See Also\nAccessing format information\nframesPerSecond\nThe rate at which the session captures video and provides AR frame information.\nimageResolution\nThe size, in pixels, of video images captured in the session.\nvideoHDRSupported\nDetermines whether the format supports high dynamic range (HDR)."
  },
  {
    "title": "ARKit Functions | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_functions",
    "html": "Topics\nFunctions\nar_hand_anchor_get_hand_skeleton\nBeta\nar_hand_skeleton_create\nBeta\nar_hand_skeleton_enumerate_joints\nBeta\nar_hand_skeleton_enumerate_joints_f\nBeta\nar_hand_skeleton_get_joint_count\nBeta\nar_hand_skeleton_get_joint_named\nBeta\nar_skeleton_joint_get_anchor_from_joint_transform\nBeta\nar_skeleton_joint_get_index\nBeta\nar_skeleton_joint_get_parent\nBeta\nar_skeleton_joint_get_parent_from_joint_transform\nBeta\nar_skeleton_joint_is_tracked\nBeta"
  },
  {
    "title": "ARGeoAnchor.AltitudeSource.userDefined | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/altitudesource/userdefined",
    "html": "Discussion\n\nARKit records this altitude source when your app defines a geo anchor’s altitude.\n\nYou may acquire altitude by providing a particular scene coordinate to the session using getGeoLocation(forPoint:completionHandler:).\n\nFor example, your app might set a geo anchor’s altitude by raycasting a surface, then adding an arbitrary y-amount to make the anchor more visible from afar.\n\nSee Also\nSources\ncase precise\nThe framework sets the altitude using a high-resolution digital-elevation model.\ncase coarse\nThe framework sets the altitude using a coarse digital-elevation model.\ncase unknown\nAltitude isn’t yet set."
  },
  {
    "title": "ARAppClipCodeAnchor.URLDecodingState.decoded | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arappclipcodeanchor/urldecodingstate/decoded",
    "html": "Discussion\n\nThe App Clip Code anchor’s url is defined in this state.\n\nSee Also\nStates\ncase decoding\nA state that indicates the continuing process of decoding an App Clip Code's URL.\ncase failed\nA state that indicates the failure to decode an App Clip Code's URL."
  },
  {
    "title": "ARMeshClassification.door | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshclassification/door",
    "html": "See Also\nOptions\ncase ceiling\nThe face is a part of a real-world ceiling.\ncase floor\nThe face is a part of a real-world floor.\ncase none\nA face ARKit can't classify.\ncase seat\nThe face is a part of a real-world seat.\ncase table\nThe face is a part of a real-world table.\ncase wall\nThe face is a part of a real-world wall.\ncase window\nThe face is a part of a real-world window."
  },
  {
    "title": "ARMeshClassification.floor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshclassification/floor",
    "html": "See Also\nOptions\ncase ceiling\nThe face is a part of a real-world ceiling.\ncase door\nThe face is a part of a real-world door.\ncase none\nA face ARKit can't classify.\ncase seat\nThe face is a part of a real-world seat.\ncase table\nThe face is a part of a real-world table.\ncase wall\nThe face is a part of a real-world wall.\ncase window\nThe face is a part of a real-world window."
  },
  {
    "title": "ARGeometryElement | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometryelement",
    "html": "Overview\n\nARMeshGeometry uses geometry-elements to store face data (see faces). Each face is defined by the primitive type, for example, ARGeometryPrimitiveType.triangle.\n\nTo demonstrate, an ARMeshGeometry instance with two triangle-type faces results in the following configuration:\n\nfaces count = 2\n\nfaces indexCountPerPrimitive = 3 (because primitiveType is ARGeometryPrimitiveType.triangle)\n\nfaces bytesPerIndex = 4 (because vertex indices are the type UInt32)\n\nThe buffer's total size in bytes = count * indexCountPerPrimitive * bytesPerIndex(which in this case, is 2 * 3 * 4 = 24 bytes)\n\nTopics\nAccessing Index Data\nsubscript(Int) -> [Int32]\nProvides an array of vertex indices that respresents the geometric primitive at the subscripted index.\nvar buffer: MTLBuffer\nA Metal buffer containing primitive data.\nGetting Index Information\nvar bytesPerIndex: Int\nThe number of bytes for each index.\nvar count: Int\nThe number of primitives in the buffer.\nvar indexCountPerPrimitive: Int\nThe number of indices for each primitive.\nvar primitiveType: ARGeometryPrimitiveType\nThe geometry's type of data (triangle, or line).\nenum ARGeometryPrimitiveType\nThe kind of connection between vertices.\nRelationships\nInherits From\nNSObject\nConforms To\nNSSecureCoding\nSee Also\nGetting Geometry Information\nvar classification: ARGeometrySource?\nClassification for each face in the mesh.\nenum ARMeshClassification\nEnumeration of different classes of real-world objects that ARKit can identify.\nvar faces: ARGeometryElement\nAn object that contains a buffer of vertex indices of the geometry's faces.\nvar normals: ARGeometrySource\nRays that define which direction is outside for each face."
  },
  {
    "title": "ARKit Data Types | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_data_types",
    "html": "Topics\nData Types\nar_hand_skeleton_t\nBeta\nar_skeleton_joint_enumerator_function_t\nBeta\nar_skeleton_joint_enumerator_t\nBeta\nar_skeleton_joint_t\nBeta"
  },
  {
    "title": "ARMatteGenerator.Resolution.full | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armattegenerator/resolution/full",
    "html": "See Also\nChoosing a Resolution Option\ncase half\nAn option that specifies half of the camera image resolution."
  },
  {
    "title": "ARSceneReconstructionNone | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscenereconstruction/arscenereconstructionnone",
    "html": "See Also\nModeling the Environment\nARSceneReconstructionMesh\nA polygonal mesh approximation of the physical environment.\nARSceneReconstructionMeshWithClassification\nAn approximate shape of the physical environment, including classification of the real-world objects within it."
  },
  {
    "title": "ARKit Enumerations | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_enumerations",
    "html": "Topics\nEnumerations\nar_hand_skeleton_joint_name_t\nBeta"
  },
  {
    "title": "faces | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshgeometry/3516922-faces",
    "html": "Discussion\n\nEach element of the buffer-based array is a three-index combination that forms a unique triangle, or face. The index refers to that vertex's position in the vertices array. The count of this property represents the number of faces.\n\nThe following code demonstrates getting the vertices of a particular face:\n\nextension ARMeshGeometry {\n    func vertexIndicesOf(faceWithIndex index: Int) -> [Int] {\n        let indicesPerFace = faces.indexCountPerPrimitive\n        let facesPointer = faces.buffer.contents()\n        var vertexIndices = [Int]()\n        for offset in 0..<indicesPerFace {\n            let vertexIndexAddress = facesPointer.advanced(by: (index * indicesPerFace + offset) * MemoryLayout<UInt32>.size)\n            vertexIndices.append(Int(vertexIndexAddress.assumingMemoryBound(to: UInt32.self).pointee))\n        }\n        return vertexIndices\n    }\n}\n\n\nSee Also\nGetting Geometry Information\nvar classification: ARGeometrySource?\nClassification for each face in the mesh.\nenum ARMeshClassification\nEnumeration of different classes of real-world objects that ARKit can identify.\nclass ARGeometryElement\nA container for index data, such as vertex indices of a face.\nvar normals: ARGeometrySource\nRays that define which direction is outside for each face."
  },
  {
    "title": "ARGeoTrackingStateLocalized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstate/argeotrackingstatelocalized",
    "html": "Discussion\n\nIn ARGeoTrackingStateLocalized, ARKit has completed visual localization and the app is free to place location anchors (ARGeoAnchor).\n\nSee Also\nStates\nARGeoTrackingStateInitializing\nThe session is initializing geo tracking.\nARGeoTrackingStateLocalizing\nGeo tracking is attempting to localize against a map.\nARGeoTrackingStateNotAvailable\nGeo tracking is not available."
  },
  {
    "title": "normals | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshgeometry/3516923-normals",
    "html": "See Also\nGetting Geometry Information\nvar classification: ARGeometrySource?\nClassification for each face in the mesh.\nenum ARMeshClassification\nEnumeration of different classes of real-world objects that ARKit can identify.\nvar faces: ARGeometryElement\nAn object that contains a buffer of vertex indices of the geometry's faces.\nclass ARGeometryElement\nA container for index data, such as vertex indices of a face."
  },
  {
    "title": "ARTrackingStateReason | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/artrackingstatereason",
    "html": "Topics\nReason Values\nARTrackingStateReasonNone\nThe current tracking state is not limited.\nARTrackingStateReasonInitializing\nThe AR session has not yet gathered enough camera or motion data to provide tracking information.\nARTrackingStateReasonRelocalizing\nThe AR session is attempting to resume after an interruption.\nARTrackingStateReasonExcessiveMotion\nThe device is moving too fast for accurate image-based position tracking.\nARTrackingStateReasonInsufficientFeatures\nThe scene visible to the camera does not contain enough distinguishable features for image-based position tracking.\nSee Also\nHandling Tracking Status\ntrackingState\nThe general quality of position tracking available when the camera captured a frame.\nARTrackingState\nPossible values for position-tracking quality.\ntrackingStateReason\nA possible diagnosis for limited position-tracking quality as of when the camera captured a frame."
  },
  {
    "title": "ARTrackingStateLimited | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/artrackingstate/artrackingstatelimited",
    "html": "Discussion\n\nIn this state, the positions and transforms of anchors in the scene (especially detected planes) may not be accurate or consistent from one captured frame to the next.\n\nSee the associated ARTrackingStateReason value for information you can present to the user for improving tracking quality.\n\nSee Also\nTracking States\nARTrackingStateNotAvailable\nCamera position tracking is not available.\nARTrackingStateNormal\nCamera position tracking is providing optimal results."
  },
  {
    "title": "ARTrackingState | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/artrackingstate",
    "html": "Topics\nTracking States\nARTrackingStateNotAvailable\nCamera position tracking is not available.\nARTrackingStateLimited\nTracking is available, but the quality of results is questionable.\nARTrackingStateNormal\nCamera position tracking is providing optimal results.\nSee Also\nHandling Tracking Status\ntrackingState\nThe general quality of position tracking available when the camera captured a frame.\ntrackingStateReason\nA possible diagnosis for limited position-tracking quality as of when the camera captured a frame.\nARTrackingStateReason\nPossible causes for limited position-tracking quality."
  },
  {
    "title": "trackingStateReason | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/2880261-trackingstatereason",
    "html": "See Also\nHandling Tracking Status\ntrackingState\nThe general quality of position tracking available when the camera captured a frame.\nARTrackingState\nPossible values for position-tracking quality.\nARTrackingStateReason\nPossible causes for limited position-tracking quality."
  },
  {
    "title": "ARCollaborationData | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcollaborationdata",
    "html": "Overview\n\nTo create a multiuser AR experience, you enable collaboration on a world tracking session. ARKit regularly outputs ARCollaborationData that users share with each other, which enables everyone to view the same virtual content from their own perspective. For more information, see collaborationEnabled.\n\nTopics\nObserving Priority\npriority\nA property that gives you a hint about how to send a given data instance over the network.\nARCollaborationDataPriority\nOptions that help you choose the appropriate network protocol or settings for a given data instance.\nRelationships\nInherits From\nNSObject\nConforms To\nNSSecureCoding\nSee Also\nShared Experiences\nStreaming an AR Experience\nControl an AR experience remotely by transferring sensor and user input over the network.\nCreating a Collaborative Session\nEnable nearby devices to share an AR experience by using a peer-to-peer multiuser strategy.\nCreating a Multiuser AR Experience\nEnable nearby devices to share an AR experience by using a host-guest multiuser strategy.\nSwiftShot: Creating a Game for Augmented Reality\nSee how Apple built the featured demo for WWDC18, and get tips for making your own multiplayer games using ARKit, SceneKit, and Swift.\nARParticipantAnchor\nAn anchor for another user in multiuser augmented reality experiences."
  },
  {
    "title": "next() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/iterator/4218724-next",
    "html": "Relationships\nFrom Protocol\nAsyncIteratorProtocol"
  },
  {
    "title": "ARTrackingStateReasonNone | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/artrackingstatereason/artrackingstatereasonnone",
    "html": "Discussion\n\nThis value occurs when the trackingState property is ARTrackingStateNotAvailable or ARTrackingStateNormal.\n\nSee Also\nReason Values\nARTrackingStateReasonInitializing\nThe AR session has not yet gathered enough camera or motion data to provide tracking information.\nARTrackingStateReasonRelocalizing\nThe AR session is attempting to resume after an interruption.\nARTrackingStateReasonExcessiveMotion\nThe device is moving too fast for accurate image-based position tracking.\nARTrackingStateReasonInsufficientFeatures\nThe scene visible to the camera does not contain enough distinguishable features for image-based position tracking."
  },
  {
    "title": "updateWithCollaborationData: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/3214029-updatewithcollaborationdata",
    "html": "Discussion\n\nCall this function to update a session when the app receives collaboration data from other users that are participating in a multiuser AR experience. Your app receives this data when multiple users scan different parts of an environment and share that information with your app over the network. For more information, see collaborationEnabled.\n\nCollaboration is supported for world tracking configurations only.\n\nSee Also\nManaging collaboration\nARCollaborationData\nAn object that holds information that a user has collected about the physical environment."
  },
  {
    "title": "ARTrackingStateReasonInitializing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/artrackingstatereason/artrackingstatereasoninitializing",
    "html": "Discussion\n\nThis value occurs temporarily after starting a new AR session or changing configurations.\n\nSee Also\nReason Values\nARTrackingStateReasonNone\nThe current tracking state is not limited.\nARTrackingStateReasonRelocalizing\nThe AR session is attempting to resume after an interruption.\nARTrackingStateReasonExcessiveMotion\nThe device is moving too fast for accurate image-based position tracking.\nARTrackingStateReasonInsufficientFeatures\nThe scene visible to the camera does not contain enough distinguishable features for image-based position tracking."
  },
  {
    "title": "ARTrackingStateReasonExcessiveMotion | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/artrackingstatereason/artrackingstatereasonexcessivemotion",
    "html": "See Also\nReason Values\nARTrackingStateReasonNone\nThe current tracking state is not limited.\nARTrackingStateReasonInitializing\nThe AR session has not yet gathered enough camera or motion data to provide tracking information.\nARTrackingStateReasonRelocalizing\nThe AR session is attempting to resume after an interruption.\nARTrackingStateReasonInsufficientFeatures\nThe scene visible to the camera does not contain enough distinguishable features for image-based position tracking."
  },
  {
    "title": "projectionMatrixForOrientation:viewportSize:zNear:zFar: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/2923539-projectionmatrixfororientation",
    "html": "Parameters\norientation\n\nThe orientation in which the camera image is to be presented.\n\nviewportSize\n\nThe size, in points, of the view in which the camera image is to be presented.\n\nzNear\n\nThe distance from the camera to the near clipping plane.\n\nzFar\n\nThe distance from the camera to the far clipping plane.\n\nReturn Value\n\nA projection matrix that provides an aspect fill and rotation for the provided viewport size and orientation.\n\nDiscussion\n\nThis method has no effect on ARKit, and the zNear and zFar parameters have no relationships to ARKit camera state. Instead, this method uses those parameters as well as the camera's state to construct a projection matrix for use in your own rendering code.\n\nSee Also\nApplying Camera Geometry\nprojectionMatrix\nA transform matrix appropriate for rendering 3D content to match the image captured by the camera.\n- viewMatrixForOrientation:\nReturns a transform matrix for converting from world space to camera space.\n- projectPoint:orientation:viewportSize:\nReturns the projection of a point from the 3D world space detected by ARKit into the 2D space of a view rendering the scene.\n- unprojectPoint:ontoPlaneWithTransform:orientation:viewportSize:\nReturns the projection of a point from the 2D space of a view rendering the scene onto a plane in the 3D world space detected by ARKit."
  },
  {
    "title": "viewMatrixForOrientation: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/2921672-viewmatrixfororientation",
    "html": "Parameters\norientation\n\nThe orientation in which the camera image is to be presented.\n\nReturn Value\n\nA view matrix appropriate for the camera with the specified orientation.\n\nDiscussion\n\nThis method has no effect on ARKit. Instead, this method uses the orientation parameter and the camera's state to construct a view matrix for your own rendering code.\n\nSee Also\nApplying Camera Geometry\nprojectionMatrix\nA transform matrix appropriate for rendering 3D content to match the image captured by the camera.\n- projectionMatrixForOrientation:viewportSize:zNear:zFar:\nReturns a transform matrix appropriate for rendering 3D content to match the image captured by the camera, using the specified parameters.\n- projectPoint:orientation:viewportSize:\nReturns the projection of a point from the 3D world space detected by ARKit into the 2D space of a view rendering the scene.\n- unprojectPoint:ontoPlaneWithTransform:orientation:viewportSize:\nReturns the projection of a point from the 2D space of a view rendering the scene onto a plane in the 3D world space detected by ARKit."
  },
  {
    "title": "ARTrackingStateReasonInsufficientFeatures | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/artrackingstatereason/artrackingstatereasoninsufficientfeatures",
    "html": "See Also\nReason Values\nARTrackingStateReasonNone\nThe current tracking state is not limited.\nARTrackingStateReasonInitializing\nThe AR session has not yet gathered enough camera or motion data to provide tracking information.\nARTrackingStateReasonRelocalizing\nThe AR session is attempting to resume after an interruption.\nARTrackingStateReasonExcessiveMotion\nThe device is moving too fast for accurate image-based position tracking."
  },
  {
    "title": "type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/scenetransitiontrigger/type",
    "html": "Overview\n\nThe option enter defines a trigger the runtime fires after loading a scene.\n\nDeclaration\nuniform token type = \"enter\" (       \n    allowedTokens = [\"enter\"]\n)\n\n\n\n\nSee Also\nProperties\ninfo:id\nA unique identifier for the trigger."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/scenetransitiontrigger/info_id",
    "html": "Overview\n\nThe default value is “SceneTransition”.\n\nDeclaration\nuniform token info:id = \"SceneTransition\"\n\n\nSee Also\nProperties\ntype\nAn option that indicates the scene transition that activates the trigger."
  },
  {
    "title": "ARPlaneClassificationStatus | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneclassificationstatus",
    "html": "Overview\n\nYou get values of this type from a plane anchor's classificationStatus property, describing the state of ARKit's plane classification process.\n\nTopics\nClassification Status\nARPlaneClassificationStatusNotAvailable\nARKit cannot currently provide plane classification information.\nARPlaneClassificationStatusUndetermined\nARKit has not yet produced a classification for the plane anchor.\nARPlaneClassificationStatusUnknown\nARKit has completed its classification process for the plane anchor, but the result is inconclusive.\nARPlaneClassificationStatusKnown\nARKit has completed its classfication process for the plane anchor.\nSee Also\nClassifying a Plane\nclassificationSupported\nA Boolean value that indicates whether plane classification is available on the current device.\nclassification\nA general characterization of what kind of real-world surface the plane anchor represents.\nARPlaneClassification\nPossible characterizations of real-world surfaces represented by plane anchors.\nclassificationStatus\nThe current state of ARKit's process for classifying the plane anchor."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/notificationtrigger/info_id",
    "html": "Overview\n\nThe default value is “Notification”.\n\nDeclaration\nuniform token info:id = \"Notification\"\n\n\nSee Also\nProperties\nidentifier\nA string value that identifies an app-specific notification."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/proximitytocameratrigger/info_id",
    "html": "Overview\n\nThe default value is “ProximityToCamera”.\n\nDeclaration\nuniform token info:id = \"ProximityToCamera\"\n\n\nSee Also\nProperties\naffectedObjects\nA list of prims for which the runtime checks proximity.\ndistance\nA threshold that measures the user’s proximity to one or more prims."
  },
  {
    "title": "ARConfidenceLevel.low | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfidencelevel/low",
    "html": "See Also\nLevels\ncase medium\nDepth-value accuracy in which the framework is moderately confident.\ncase high\nDepth-value accuracy in which the framework is fairly confident."
  },
  {
    "title": "textureCoordinates | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanegeometry/2941055-texturecoordinates",
    "html": "Discussion\n\nEach float2 value in this buffer represents the UV texture coordinates for the vertex at the corresponding index in the vertices buffer.\n\nSee Also\nAccessing Mesh Data\nvertices\nA buffer of vertex positions for each point in the plane mesh.\nvertexCount\nThe number of elements in the vertices buffer.\ntextureCoordinateCount\nThe number of elements in the textureCoordinates buffer.\ntriangleIndices\nA buffer of indices describing the triangle mesh formed by the plane geometry's vertex data.\ntriangleCount\nThe number of triangles described by the triangleIndices buffer."
  },
  {
    "title": "textureCoordinateCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanegeometry/2941049-texturecoordinatecount",
    "html": "See Also\nAccessing Mesh Data\nvertices\nA buffer of vertex positions for each point in the plane mesh.\nvertexCount\nThe number of elements in the vertices buffer.\ntextureCoordinates\nA buffer of texture coordinate values for each point in the plane mesh.\ntriangleIndices\nA buffer of indices describing the triangle mesh formed by the plane geometry's vertex data.\ntriangleCount\nThe number of triangles described by the triangleIndices buffer."
  },
  {
    "title": "boundaryVertices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanegeometry/2941052-boundaryvertices",
    "html": "Discussion\n\nEach float3 value in this buffer represents the position of a vertex along the boundary polygon of the estimated plane. The owning plane anchor's transform matrix defines the coordinate system for these points.\n\nThe boundaryVertexCount property provides the number of elements in the buffer.\n\nThis buffer defines the boundary polygon of the plane. Use it for purposes that require only that polygon's definition, such as rendering an outline of the plane's estimated shape or testing whether a point is inside the bounded region. If, instead, you need the filled shape (for example, to render a solid 3D representation of the surface), see the vertices property.\n\nSee Also\nFinding Boundary Points\nboundaryVertexCount\nThe number of elements in the boundaryVertices buffer."
  },
  {
    "title": "boundaryVertexCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanegeometry/2941054-boundaryvertexcount",
    "html": "See Also\nFinding Boundary Points\nboundaryVertices\nA buffer of vertex positions for each point along the plane's boundary."
  },
  {
    "title": "ARConfidenceLevel.medium | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfidencelevel/medium",
    "html": "See Also\nLevels\ncase low\nDepth-value accuracy in which the framework is less confident.\ncase high\nDepth-value accuracy in which the framework is fairly confident."
  },
  {
    "title": "classificationSupported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/2990938-classificationsupported",
    "html": "Discussion\n\nPlane classification is available on iOS devices with A12 or later GPU.\n\nOn devices without plane classification support, all plane anchors report a classification value of ARPlaneClassificationNone and a classificationStatus value of ARPlaneClassificationStatusNotAvailable.\n\nSee Also\nClassifying a Plane\nclassification\nA general characterization of what kind of real-world surface the plane anchor represents.\nARPlaneClassification\nPossible characterizations of real-world surfaces represented by plane anchors.\nclassificationStatus\nThe current state of ARKit's process for classifying the plane anchor.\nARPlaneClassificationStatus\nPossible states of ARKit's process for classifying plane anchors."
  },
  {
    "title": "info:id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/preliminary_trigger/info_id",
    "html": "Overview\n\nTriggers with different purposes define a different value for this property. For example, a runtime fires a trigger with an info:id of TapGesture when the user taps. A runtime executes a trigger with an info:id of ProximityToCamera when the user’s device crosses a distance threshhold that the trigger defines.\n\nDeclaration\nuniform token info:id\n"
  },
  {
    "title": "init(device:fillMesh:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnfacegeometry/2928202-init",
    "html": "Parameters\ndevice\n\nThe Metal device to use for rendering the geometry.\n\nfillMesh\n\nIf false, the mesh doesn’t include the eye and mouth areas of the face topology—as a result, if you render the face geometry for a face anchor atop the camera feed in an AR session, the user’s eyes and the inside of the user’s mouth (when open) are visible through the gaps in the 3D model.\n\nIf true, the mesh fills in the gaps for the eyes and mouth. This option can be useful when using the face geometry for occlusion only—that is, to prevent other 3D content from rendering behind it, while still showing the camera image beneath.\n\nReturn Value\n\nA new SceneKit face geometry, or nil if the Metal device is unavailable or ARKit face tracking is not supported on the current device.\n\nDiscussion\n\nA newly created ARSCNFaceGeometry instance represents a neutral, generic face; use the update(from:) method to deform the geometry to match a specific facial expression or face shape.\n\nIf the fillMesh parameter is false, the geometry contains a single geometry element. If the fillMesh parameter is true, the geometry contains four elements in the following order: the face itself, the left and right eyes, and the mouth. (When a SceneKit geometry contains multiple elements, the number and order of materials you assign to its materials array determines whether the entire surface has a uniform appearance or different appearances for each geometry element.)\n\nSee Also\nCreating a Geometry\ninit?(device: MTLDevice)\nCreates a SceneKit face geometry for rendering with the specified Metal device object."
  },
  {
    "title": "ARKitSession.Error.Code.dataProviderNotAuthorized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/code/dataprovidernotauthorized",
    "html": "See Also\nDetermining the cause of session errors\ncase dataProviderFailedToRun\nThe error code for when a data provider fails to run."
  },
  {
    "title": "unprojectPoint:ontoPlaneWithTransform:orientation:viewportSize: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/2977504-unprojectpoint",
    "html": "Parameters\npoint\n\nThe point in 2D view space to project onto a plane.\n\nThe coordinate space for this point has its origin is in the upper left corner and a size matching the viewportSize parameter.\n\nplaneTransform\n\nA transform matrix specifying the position and orientation of a plane (with infinite extent) in 3D world space. The plane is the xz-plane of the local coordinate space this transform defines.\n\norientation\n\nThe orientation in which the camera image is to be presented.\n\nviewportSize\n\nThe size, in points, of the view in which the camera image is to be presented.\n\nReturn Value\n\nThe 3D point in world space where a ray projected from the specified 2D point intersects the specified plane. If the ray does not intersect the plane, this method returns a float3 vector where all elements are NaN.\n\nDiscussion\n\nIf you display AR content with SceneKit, the ARSCNView class provides an otherwise equivalent unprojectPoint:ontoPlaneWithTransform: method that requires fewer parameters (because the view can infer its orientation and size).\n\nSee Also\nApplying Camera Geometry\nprojectionMatrix\nA transform matrix appropriate for rendering 3D content to match the image captured by the camera.\n- projectionMatrixForOrientation:viewportSize:zNear:zFar:\nReturns a transform matrix appropriate for rendering 3D content to match the image captured by the camera, using the specified parameters.\n- viewMatrixForOrientation:\nReturns a transform matrix for converting from world space to camera space.\n- projectPoint:orientation:viewportSize:\nReturns the projection of a point from the 3D world space detected by ARKit into the 2D space of a view rendering the scene."
  },
  {
    "title": "pointSize | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_text/pointsize",
    "html": "Overview\n\nThe default value is 144 points.\n\nDeclaration\nfloat pointSize = 144.0\n\n\nSee Also\nProperties\ncontent\nThe characters that the text displays.\nfont\nAn array of font names.\nwidth\nThe width of the text’s bounding box.\nheight\nThe height of the text’s bounding box.\ndepth\nA value that defines the depth, in scene units, of the text’s extrusion.\nwrapMode\nAn option that determines the flow of the text.\nhorizontalAlignment\nAn option that controls the text’s horizontal placement within its bounding box.\nverticalAlignment\nAn option that controls the text’s vertical placement within its bounding rectangle."
  },
  {
    "title": "height | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneextent/3950860-height",
    "html": "See Also\nInspecting Plane Size\nvar width: Float\nThe estimated width of the plane."
  },
  {
    "title": "ARPlaneAnchor.Classification.Status | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/status",
    "html": "Overview\n\nARPlaneAnchor.Classification.Status tells you why a plane anchor's classification is ARPlaneAnchor.Classification.none(_:).\n\nTopics\nClassification Status\ncase notAvailable\nARKit cannot currently provide plane classification information.\ncase undetermined\nARKit has not yet produced a classification for the plane anchor.\ncase unknown\nARKit has completed its classification process for the plane anchor, but the result is inconclusive.\nHashes of a Classification Status\nfunc hash(into: inout Hasher)\nHashes the status by passing it to the given hash function.\nstatic func == (ARPlaneAnchor.Classification.Status, ARPlaneAnchor.Classification.Status) -> Bool\nIndicates whether two statuses are equal.\nstatic func != (ARPlaneAnchor.Classification.Status, ARPlaneAnchor.Classification.Status) -> Bool\nReturns a Boolean value indicating whether two values are not equal.\nvar hashValue: Int\nA value that identifies an object uniquely as compared to other instances of the same type.\nSee Also\nMissing Classification Status\ncase none(ARPlaneAnchor.Classification.Status)\nNo classification is available for the plane anchor."
  },
  {
    "title": "ARPlaneAnchor.Classification.ceiling | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/ceiling",
    "html": "See Also\nPlane Classifications\ncase wall\nThe plane anchor represents a real-world wall or similar large vertical surface.\ncase floor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\ncase table\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\ncase seat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\ncase door\nThe plane anchor represents a real-world door or similar vertical surface.\ncase window\nThe plane anchor represents a real-world window or similar vertical surface."
  },
  {
    "title": "WorldTrackingProvider.Error.Code.removeWorldAnchorFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/code/removeworldanchorfailed",
    "html": "See Also\nDetermining causes for tracking failures\ncase addWorldAnchorFailed\nThe error code for when a world tracking provider can’t add a world anchor.\ncase worldAnchorLimitReached\nThe error code for when a world tracking provider reaches its world anchor limit."
  },
  {
    "title": "ARPlaneAnchor.Classification.seat | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/seat",
    "html": "See Also\nPlane Classifications\ncase wall\nThe plane anchor represents a real-world wall or similar large vertical surface.\ncase floor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\ncase ceiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\ncase table\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\ncase door\nThe plane anchor represents a real-world door or similar vertical surface.\ncase window\nThe plane anchor represents a real-world window or similar vertical surface."
  },
  {
    "title": "ARPlaneAnchor.Classification.table | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/table",
    "html": "See Also\nPlane Classifications\ncase wall\nThe plane anchor represents a real-world wall or similar large vertical surface.\ncase floor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\ncase ceiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\ncase seat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\ncase door\nThe plane anchor represents a real-world door or similar vertical surface.\ncase window\nThe plane anchor represents a real-world window or similar vertical surface."
  },
  {
    "title": "ARPlaneAnchor.Classification.door | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/door",
    "html": "See Also\nPlane Classifications\ncase wall\nThe plane anchor represents a real-world wall or similar large vertical surface.\ncase floor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\ncase ceiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\ncase table\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\ncase seat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\ncase window\nThe plane anchor represents a real-world window or similar vertical surface."
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/code/4241486-hashvalue",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nInspecting tracking failures\nvar description: String\nA textual description of the error code.\nfunc hash(into: inout Hasher)\nstatic func == (WorldTrackingProvider.Error.Code, WorldTrackingProvider.Error.Code) -> Bool\nstatic func != (WorldTrackingProvider.Error.Code, WorldTrackingProvider.Error.Code) -> Bool"
  },
  {
    "title": "ARPlaneAnchor.Classification.window | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/window",
    "html": "See Also\nPlane Classifications\ncase wall\nThe plane anchor represents a real-world wall or similar large vertical surface.\ncase floor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\ncase ceiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\ncase table\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\ncase seat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\ncase door\nThe plane anchor represents a real-world door or similar vertical surface."
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/code/4241482",
    "html": "See Also\nInspecting tracking failures\nvar description: String\nA textual description of the error code.\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func != (WorldTrackingProvider.Error.Code, WorldTrackingProvider.Error.Code) -> Bool"
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/3930615",
    "html": "Parameters\nlhs\n\nThe left-argument plane classification.\n\nrhs\n\nThe right-argument plane classification.\n\nReturn Value\n\nReturns true if the plane classifications are equal. Otherwise, returns false.\n\nRelationships\nFrom Protocol\nEquatable\nSee Also\nComparing Classifications\nstatic func != (ARPlaneAnchor.Classification, ARPlaneAnchor.Classification) -> Bool\nReturns a Boolean value indicating whether two plane classifications aren't equal."
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/code/4241485-hash",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nInspecting tracking failures\nvar description: String\nA textual description of the error code.\nvar hashValue: Int\nstatic func == (WorldTrackingProvider.Error.Code, WorldTrackingProvider.Error.Code) -> Bool\nstatic func != (WorldTrackingProvider.Error.Code, WorldTrackingProvider.Error.Code) -> Bool"
  },
  {
    "title": "GeometryElement.Primitive.triangle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement/primitive/triangle",
    "html": "See Also\nPrimitive shapes\ncase line\nTwo vertices that connect to form a line."
  },
  {
    "title": "ARSessionRunOptionResetTracking | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionrunoptions/arsessionrunoptionresettracking",
    "html": "Discussion\n\nBy default, when you call the runWithConfiguration:options: method on a session that has run before or is already running, the session resumes device position tracking from its last known state. (For example, an ARAnchor object keeps its apparent position relative to the camera.) When you call the runWithConfiguration:options: method with a configuration of the same type as the session's current configuration, you can add this option to force device position tracking to return to its initial state.\n\nWhen you call the runWithConfiguration:options: method with a configuration of a different type than the session's current configuration, the session always resets tracking (that is, this option is implicitly enabled).\n\nIn either case, when you reset tracking, ARKit also removes any existing anchors from the session.\n\nSee Also\nRun Options\nARSessionRunOptionRemoveExistingAnchors\nAn option to remove any anchor objects associated with the session's previous run.\nARSessionRunOptionStopTrackedRaycasts\nAn option to stop all active tracked raycasts.\nARSessionRunOptionResetSceneReconstruction\nAn option to reset the scene mesh."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/3930614",
    "html": "Parameters\nlhs\n\nThe left-argument plane classification.\n\nrhs\n\nThe right-argument plane classification.\n\nReturn Value\n\nReturns true if the plane classifications aren't equal. Otherwise, returns false.\n\nDiscussion\n\nInequality is the inverse of equality. For any values a and b, a != b implies that a == b is false.\n\nThis is the default implementation of the not-equal-to operator (!=) for any type that conforms to Equatable.\n\nSee Also\nComparing Classifications\nstatic func == (ARPlaneAnchor.Classification, ARPlaneAnchor.Classification) -> Bool\nDetermines whether two plane classifications are equal."
  },
  {
    "title": "localTransformForJointName: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton3d/3229925-localtransformforjointname",
    "html": "Discussion\n\nLocal space refers to the joints position relative to its parent joint. If an invalid joint name is passed the returned matrix will be filled with NaN values.\n\nSee Also\nGetting a Joint's Pose\njointLocalTransforms\nThe local space transforms for each joint.\njointModelTransforms\nThe model space transforms for each joint.\n- modelTransformForJointName:\nReturns the model transform for a joint with a given name."
  },
  {
    "title": "anchorFromExtentTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/geometry/extent/4293521-anchorfromextenttransform",
    "html": "See Also\nInspecting a plane extent\nvar width: Float\nThe width of a plane.\nvar height: Float\nThe height of a plane.\nvar description: String\nA textual description of the size of a plane."
  },
  {
    "title": "modelTransformForJointName: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton3d/3229926-modeltransformforjointname",
    "html": "Discussion\n\nModel space refers to the joint's position relative to its hip joint. If an invalid joint name is passed in, the returned matrix will be filled with NaN values.\n\nSee Also\nGetting a Joint's Pose\njointLocalTransforms\nThe local space transforms for each joint.\njointModelTransforms\nThe model space transforms for each joint.\n- localTransformForJointName:\nReturns the local transform for a joint with a given name."
  },
  {
    "title": "initWithBlendShapes: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacegeometry/2928204-initwithblendshapes",
    "html": "Parameters\nblendShapes\n\nA dictionary of blend shape coefficients describing a facial expression in terms of the positions of specific facial features. For any coefficient not specified in this dictionary, ARKit assumes a value of 0.0.\n\nReturn Value\n\nA face geometry object, or nil if ARKit face tracking is not supported on the current device.\n\nDiscussion\n\nEach key in the blendShapes dictionary is an ARBlendShapeLocation constant identifying a facial feature. The corresponding value is the position of that feature relative to its neutral configuration, ranging from 0.0 (neutral) to 1.0 (maximum movement).\n\nThe format of this dictionary is identical to that provided by the ARFaceAnchor blendShapes property. You can use that property and this initializer to efficiently save and restore facial expression data; the serialized form of a blend shapes dictionary is more portable than that of the face mesh those coefficients describe."
  },
  {
    "title": "height | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/geometry/extent/4218767-height",
    "html": "See Also\nInspecting a plane extent\nvar width: Float\nThe width of a plane.\nvar anchorFromExtentTransform: simd_float4x4\nThe transform from the plane extent to the plane anchor’s coordinate system.\nvar description: String\nA textual description of the size of a plane."
  },
  {
    "title": "ARSkeletonJointName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletonjointname",
    "html": "Discussion\n\nYou use this class to access information about a named joint, such as its index in a skeleton's array of joints, or its position on the screen or in the physical environment.\n\nWhen you're tracking a body in 2D space, you get the screen-space position of a named joint by using the landmark(for:) function.\n\nWhen you're tracking a body in 3D space, you get a named joint's position in either local or model space by using the localTransform(for:) or modelTransform(for:) functions, respectively.\n\nTopics\nCreating a Joint Name\nARSkeletonJointNameForRecognizedPointKey\nReturns a joint name that corresponds to a key point defined in a human body pose.\nIdentifying Joints\nARSkeletonJointNameRoot\nA skeletal joint that's the root of all other joints.\nARSkeletonJointNameHead\nA skeletal joint that ARKit tracks representing the head.\nARSkeletonJointNameLeftFoot\nA skeletal joint that ARKit tracks representing the left foot.\nARSkeletonJointNameLeftHand\nA skeletal joint that ARKit tracks representing the left hand.\nARSkeletonJointNameLeftShoulder\nA skeletal joint that ARKit tracks representing the left shoulder.\nARSkeletonJointNameRightFoot\nA skeletal joint that ARKit tracks representing the right foot.\nARSkeletonJointNameRightHand\nA skeletal joint that ARKit tracks representing the right hand.\nARSkeletonJointNameRightShoulder\nA skeletal joint that ARKit tracks representing the right shoulder.\nSee Also\nGetting Joint Information\ndefinition\nThe particular configuration of joints that define a body's current state.\njointCount\nThe skeleton's total number of joints.\n- isJointTracked:\nTells you whether ARKit tracks a joint at a particular index."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/geometry/extent/4218766-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting a plane extent\nvar width: Float\nThe width of a plane.\nvar height: Float\nThe height of a plane.\nvar anchorFromExtentTransform: simd_float4x4\nThe transform from the plane extent to the plane anchor’s coordinate system."
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement/primitive/4131695",
    "html": "See Also\nInspecting geometry primitives\nvar description: String\nA textual description of a geometry primitive.\nvar hashValue: Int\nvar indexCount: Int\nfunc hash(into: inout Hasher)\nstatic func != (GeometryElement.Primitive, GeometryElement.Primitive) -> Bool"
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement/primitive/4131697-hashvalue",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nInspecting geometry primitives\nvar description: String\nA textual description of a geometry primitive.\nvar indexCount: Int\nfunc hash(into: inout Hasher)\nstatic func == (GeometryElement.Primitive, GeometryElement.Primitive) -> Bool\nstatic func != (GeometryElement.Primitive, GeometryElement.Primitive) -> Bool"
  },
  {
    "title": "GeometryElement.Primitive.line | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement/primitive/line",
    "html": "See Also\nPrimitive shapes\ncase triangle\nThree vertices that connect to form a triangle."
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement/primitive/4131696-hash",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nInspecting geometry primitives\nvar description: String\nA textual description of a geometry primitive.\nvar hashValue: Int\nvar indexCount: Int\nstatic func == (GeometryElement.Primitive, GeometryElement.Primitive) -> Bool\nstatic func != (GeometryElement.Primitive, GeometryElement.Primitive) -> Bool"
  },
  {
    "title": "defaultBody3DSkeletonDefinition | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletondefinition/3229928-defaultbody3dskeletondefinition",
    "html": "Discussion\n\nThe default height of this skeleton is 1.66 meters.\n\nSee Also\nLocating in the Physical Environment\nneutralBodySkeleton3D\nThe 3D skeleton in neutral pose."
  },
  {
    "title": "indexCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement/primitive/4131698-indexcount",
    "html": "See Also\nInspecting geometry primitives\nvar description: String\nA textual description of a geometry primitive.\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (GeometryElement.Primitive, GeometryElement.Primitive) -> Bool\nstatic func != (GeometryElement.Primitive, GeometryElement.Primitive) -> Bool"
  },
  {
    "title": "ARMeshClassification.table | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshclassification/table",
    "html": "See Also\nOptions\ncase ceiling\nThe face is a part of a real-world ceiling.\ncase door\nThe face is a part of a real-world door.\ncase floor\nThe face is a part of a real-world floor.\ncase none\nA face ARKit can't classify.\ncase seat\nThe face is a part of a real-world seat.\ncase wall\nThe face is a part of a real-world wall.\ncase window\nThe face is a part of a real-world window."
  },
  {
    "title": "ARMeshClassification.seat | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshclassification/seat",
    "html": "See Also\nOptions\ncase ceiling\nThe face is a part of a real-world ceiling.\ncase door\nThe face is a part of a real-world door.\ncase floor\nThe face is a part of a real-world floor.\ncase none\nA face ARKit can't classify.\ncase table\nThe face is a part of a real-world table.\ncase wall\nThe face is a part of a real-world wall.\ncase window\nThe face is a part of a real-world window."
  },
  {
    "title": "ARMeshClassification.window | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshclassification/window",
    "html": "See Also\nOptions\ncase ceiling\nThe face is a part of a real-world ceiling.\ncase door\nThe face is a part of a real-world door.\ncase floor\nThe face is a part of a real-world floor.\ncase none\nA face ARKit can't classify.\ncase seat\nThe face is a part of a real-world seat.\ncase table\nThe face is a part of a real-world table.\ncase wall\nThe face is a part of a real-world wall."
  },
  {
    "title": "ARMeshClassification.wall | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshclassification/wall",
    "html": "See Also\nOptions\ncase ceiling\nThe face is a part of a real-world ceiling.\ncase door\nThe face is a part of a real-world door.\ncase floor\nThe face is a part of a real-world floor.\ncase none\nA face ARKit can't classify.\ncase seat\nThe face is a part of a real-world seat.\ncase table\nThe face is a part of a real-world table.\ncase window\nThe face is a part of a real-world window."
  },
  {
    "title": "ARRaycastQuery.Target.existingPlaneGeometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery/target/existingplanegeometry",
    "html": "See Also\nTargets\ncase estimatedPlane\nA raycast target that specifies nonplanar surfaces, or planes about which ARKit can only estimate.\ncase existingPlaneInfinite\nA raycast target that specifies a detected plane, regardless of its size and shape."
  },
  {
    "title": "ARMeshClassification.none | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshclassification/none",
    "html": "See Also\nOptions\ncase ceiling\nThe face is a part of a real-world ceiling.\ncase door\nThe face is a part of a real-world door.\ncase floor\nThe face is a part of a real-world floor.\ncase seat\nThe face is a part of a real-world seat.\ncase table\nThe face is a part of a real-world table.\ncase wall\nThe face is a part of a real-world wall.\ncase window\nThe face is a part of a real-world window."
  },
  {
    "title": "ARRaycastQuery.Target.existingPlaneInfinite | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery/target/existingplaneinfinite",
    "html": "See Also\nTargets\ncase estimatedPlane\nA raycast target that specifies nonplanar surfaces, or planes about which ARKit can only estimate.\ncase existingPlaneGeometry\nA raycast target that requires a plane to have a definitive size and shape."
  },
  {
    "title": "ARMatteGenerator.Resolution.half | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armattegenerator/resolution/half",
    "html": "See Also\nChoosing a Resolution Option\ncase full\nAn option that specifies the full camera image resolution."
  },
  {
    "title": "ARGeometrySource | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeometrysource",
    "html": "Overview\n\nMesh-anchor geometry (ARMeshGeometry) uses geometry sources to hold 3D data like vertices, and normals, in an efficent, array-like format. A Metal buffer wraps the data, and other properties specify how to interpret that data.\n\nIn the case that componentsPerVector is greater than 1, the element type of the geometry-source array is itself a sequence (pairs, triplets, and so on).\n\nTopics\nAccessing Geometry\nsubscript(Int32) -> (Float, Float, Float)\nProvides the source float triplet at the subscripted index.\nsubscript(Int32) -> CUnsignedChar\nProvides the number at the subscripted index.\nvar buffer: MTLBuffer\nA Metal buffer that contains a list of vectors.\nGetting Geometry Information\nvar componentsPerVector: Int\nThe number of scalar components in each vector.\nvar count: Int\nThe number of vectors in the buffer.\nvar format: MTLVertexFormat\nThe type of vector data in the buffer.\nvar offset: Int\nThe offset, in bytes, from the beginning of the buffer.\nvar stride: Int\nThe length, in bytes, of the start of one vector in the buffer to the start of the next vector.\nRelationships\nInherits From\nNSObject\nConforms To\nNSSecureCoding\nSee Also\nAccessing Geometry Data\nvar vertices: ARGeometrySource\nThe vertices of the mesh."
  },
  {
    "title": "vertices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshgeometry/3516924-vertices",
    "html": "Discussion\n\nThe count equals the total number of vertices. Since each vertex is the type SIMD3<Float>, componentsPerVector is three.\n\nThe following code demonstrates retrieving a vertex at a particular index.\n\nextension ARMeshGeometry { \n    func vertex(at index: UInt32) -> SIMD3<Float> {\n        assert(vertices.format == MTLVertexFormat.float3, \"Expected three floats (twelve bytes) per vertex.\")\n        let vertexPointer = vertices.buffer.contents().advanced(by: vertices.offset + (vertices.stride * Int(index)))\n        let vertex = vertexPointer.assumingMemoryBound(to: SIMD3<Float>.self).pointee\n        return vertex\n    }\n}\n\n\nSee Also\nAccessing Geometry Data\nclass ARGeometrySource\nMesh data in a buffer-based array."
  },
  {
    "title": "classification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshgeometry/3521393-classification",
    "html": "Discussion\n\nEach element of the array (ARGeometrySource) is a classification that corresponds to one face in the geometry. The count of this property represents the number of faces in the geometry. The default value at each index is 0, –– the raw value for ARMeshClassification.none.\n\nThe following code demonstrates retrieving a classification for a particular face:\n\nextension ARMeshGeometry {\n    func classificationOf(faceWithIndex index: Int) -> ARMeshClassification {\n        guard let classification = classification else { return .none }\n        let classificationAddress = classification.buffer.contents().advanced(by: index)\n        let classificationValue = Int(classificationAddress.assumingMemoryBound(to: UInt8.self).pointee)\n        return ARMeshClassification(rawValue: classificationValue) ?? .none\n    }\n}\n\n\nFor a sample app that demonstrates classification, see Visualizing and Interacting with a Reconstructed Scene.\n\nSee Also\nGetting Geometry Information\nenum ARMeshClassification\nEnumeration of different classes of real-world objects that ARKit can identify.\nvar faces: ARGeometryElement\nAn object that contains a buffer of vertex indices of the geometry's faces.\nclass ARGeometryElement\nA container for index data, such as vertex indices of a face.\nvar normals: ARGeometrySource\nRays that define which direction is outside for each face."
  },
  {
    "title": "targetAlignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery/3194584-targetalignment",
    "html": "Discussion\n\nThe available options are horizontal, vertical, or any.\n\nSee Also\nSpecifying the Target\nvar target: ARRaycastQuery.Target\nA plane type that allows the raycast to terminate if it's encountered.\nenum ARRaycastQuery.Target\nThe types of surface you allow a raycast to intersect with.\nenum ARRaycastQuery.TargetAlignment\nA specification that indicates a target's alignment with respect to gravity."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/code/4241445",
    "html": "See Also\nInspecting session errors\nvar description: String\nA textual description of the error code.\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (ARKitSession.Error.Code, ARKitSession.Error.Code) -> Bool"
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/code/4241446",
    "html": "See Also\nInspecting session errors\nvar description: String\nA textual description of the error code.\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func != (ARKitSession.Error.Code, ARKitSession.Error.Code) -> Bool"
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/code/4241451-hash",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nInspecting session errors\nvar description: String\nA textual description of the error code.\nvar hashValue: Int\nstatic func == (ARKitSession.Error.Code, ARKitSession.Error.Code) -> Bool\nstatic func != (ARKitSession.Error.Code, ARKitSession.Error.Code) -> Bool"
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/code/4241452-hashvalue",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nInspecting session errors\nvar description: String\nA textual description of the error code.\nfunc hash(into: inout Hasher)\nstatic func == (ARKitSession.Error.Code, ARKitSession.Error.Code) -> Bool\nstatic func != (ARKitSession.Error.Code, ARKitSession.Error.Code) -> Bool"
  },
  {
    "title": "WaitAction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/waitaction",
    "html": "Overview\n\nUse this action to wait before performing other actions in a GroupAction.\n\nDeclaration\nclass Preliminary_Action \"WaitAction\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the action.\nduration\nThe amount of time to wait before finishing the action.\nSee Also\nActions\nPreliminary_Action\nA specific task that a trigger performs.\nAudioAction\nAn action that plays audio.\nChangeSceneAction\nAn action that transitions from one scene to another.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nGroupAction\nAn action that runs a list of other actions.\nImpulseAction\nAn action that adds velocity to an prim.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nOrbitAction\nAn action that orbits a set of prims around another.\nSpinAction\nAn action that spins a prim.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAction\nAn action that animates from one transform to another.\nTransformAnimationAction\nAn action that plays a transform animation.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/code/4241450-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting session errors\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (ARKitSession.Error.Code, ARKitSession.Error.Code) -> Bool\nstatic func != (ARKitSession.Error.Code, ARKitSession.Error.Code) -> Bool"
  },
  {
    "title": "SpinAction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/spinaction",
    "html": "Overview\n\nThe default configuration of this action spins an object horizontally on its axis, like a top.\n\nDeclaration\nclass Preliminary_Action \"SpinAction\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that spin on an axis.\nduration\nThe amount of time between the start of an action and its end.\nrevolutions\nThe number rotations to complete.\naxis\nA vector that describes the axis of rotation.\nSee Also\nActions\nPreliminary_Action\nA specific task that a trigger performs.\nAudioAction\nAn action that plays audio.\nChangeSceneAction\nAn action that transitions from one scene to another.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nGroupAction\nAn action that runs a list of other actions.\nImpulseAction\nAn action that adds velocity to an prim.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nOrbitAction\nAn action that orbits a set of prims around another.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAction\nAn action that animates from one transform to another.\nTransformAnimationAction\nAn action that plays a transform animation.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nWaitAction\nAn action that performs a delay.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "VisibilityAction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/visibilityaction",
    "html": "Overview\n\nUse this action to display or hide a prim with a transform animation. This action is distinct from UsdGeomImageable because the animation doesn’t alter the visible property.\n\nDeclaration\nclass Preliminary_Action \"VisibilityAction\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims to show or hide.\ntype\nAn option that determines the target prim’s visibility when the action finishes.\nstyle\nAn option that implements different kinds of animation timing.\nmotionType\nAn option that determines how the action displays or hides a prim.\neaseType\nAn option that describes the animation’s change in pace over time.\nmoveDistance\nThe distance that this action moves the target prims.\nSee Also\nActions\nPreliminary_Action\nA specific task that a trigger performs.\nAudioAction\nAn action that plays audio.\nChangeSceneAction\nAn action that transitions from one scene to another.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nGroupAction\nAn action that runs a list of other actions.\nImpulseAction\nAn action that adds velocity to an prim.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nOrbitAction\nAn action that orbits a set of prims around another.\nSpinAction\nAn action that spins a prim.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAction\nAn action that animates from one transform to another.\nTransformAnimationAction\nAn action that plays a transform animation.\nWaitAction\nAn action that performs a delay.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "TransformAnimationAction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/transformanimationaction",
    "html": "Overview\n\nThis action applies the transform animation defined by animation on the prims in affectedObjects.\n\nDeclaration\nclass Preliminary_Action \"TransformAnimationAction\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims on which to play a transform animation.\nanimation\nA prim that contains a transform animation.\nSee Also\nActions\nPreliminary_Action\nA specific task that a trigger performs.\nAudioAction\nAn action that plays audio.\nChangeSceneAction\nAn action that transitions from one scene to another.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nGroupAction\nAn action that runs a list of other actions.\nImpulseAction\nAn action that adds velocity to an prim.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nOrbitAction\nAn action that orbits a set of prims around another.\nSpinAction\nAn action that spins a prim.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAction\nAn action that animates from one transform to another.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nWaitAction\nAn action that performs a delay.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "TransformAction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/transformaction",
    "html": "Overview\n\nThis action animates from the target prim’s current transform to the xformTarget transform.\n\nDeclaration\nclass Preliminary_Action \"TransformAction\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims to which a transform applies.\nxformTarget\nA prim that provides the transform to which this action animates.\nduration\nThe amount of time between the start of an action and its end.\ntype\nAn option that determines if a transform is based on another, source transform.\neaseType\nAn option that describes the animation’s change in pace over time.\nSee Also\nActions\nPreliminary_Action\nA specific task that a trigger performs.\nAudioAction\nAn action that plays audio.\nChangeSceneAction\nAn action that transitions from one scene to another.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nGroupAction\nAn action that runs a list of other actions.\nImpulseAction\nAn action that adds velocity to an prim.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nOrbitAction\nAn action that orbits a set of prims around another.\nSpinAction\nAn action that spins a prim.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAnimationAction\nAn action that plays a transform animation.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nWaitAction\nAn action that performs a delay.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "OrbitAction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/orbitaction",
    "html": "Overview\n\nThis action sends the list of prims defined by affectedObjects into orbit at a fixed distance around its center prim, on a plane perpendicular to the axis of rotation. The runtime caculates the fixed distance as a straight line between the respective origins of the affectedObjects and the center.\n\nDeclaration\nclass Preliminary_Action \"OrbitAction\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of objects in orbit.\ncenter\nA prim around which the affected objects orbit.\nduration\nThe amount of time between the start of an action and its end.\nrevolutions\nThe number of rotations to complete.\naxis\nA vector that describes the axis of rotation.\nalignToPath\nAn option that controls the prim’s orientation as it revolves.\nSee Also\nActions\nPreliminary_Action\nA specific task that a trigger performs.\nAudioAction\nAn action that plays audio.\nChangeSceneAction\nAn action that transitions from one scene to another.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nGroupAction\nAn action that runs a list of other actions.\nImpulseAction\nAn action that adds velocity to an prim.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nSpinAction\nAn action that spins a prim.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAction\nAn action that animates from one transform to another.\nTransformAnimationAction\nAn action that plays a transform animation.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nWaitAction\nAn action that performs a delay.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "LookAtCameraAction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/lookatcameraaction",
    "html": "Overview\n\nBy default, the duration specifies the amount of time the prim looks at the camera. To look at the camera indefinitely, place this action in a GroupAction that repeats infinitely.\n\nDeclaration\nclass Preliminary_Action \"LookAtCameraAction\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that face the camera when this action executes.\nduration\nThe amount of time that the objects face the camera.\nfront\nA vector that’s perpendicular to, and points outward from, the object’s face.\nupVector\nA vector around which the runtime rotates the object.\nSee Also\nActions\nPreliminary_Action\nA specific task that a trigger performs.\nAudioAction\nAn action that plays audio.\nChangeSceneAction\nAn action that transitions from one scene to another.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nGroupAction\nAn action that runs a list of other actions.\nImpulseAction\nAn action that adds velocity to an prim.\nOrbitAction\nAn action that orbits a set of prims around another.\nSpinAction\nAn action that spins a prim.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAction\nAn action that animates from one transform to another.\nTransformAnimationAction\nAn action that plays a transform animation.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nWaitAction\nAn action that performs a delay.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "EmphasizeAction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/emphasizeaction",
    "html": "Overview\n\nInstead of specifying your own animation, choose from the preexisting options in motionType.\n\nDeclaration\nclass Preliminary_Action \"EmphasizeAction\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that the runtime emphasizes when the action’s trigger fires.\nduration\nThe amount of time between the start of an action and its end.\nstyle\nAn option that implements different kinds of animation timing.\nmotionType\nAn option that implements animation effects.\nSee Also\nActions\nPreliminary_Action\nA specific task that a trigger performs.\nAudioAction\nAn action that plays audio.\nChangeSceneAction\nAn action that transitions from one scene to another.\nGroupAction\nAn action that runs a list of other actions.\nImpulseAction\nAn action that adds velocity to an prim.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nOrbitAction\nAn action that orbits a set of prims around another.\nSpinAction\nAn action that spins a prim.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAction\nAn action that animates from one transform to another.\nTransformAnimationAction\nAn action that plays a transform animation.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nWaitAction\nAn action that performs a delay.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "ImpulseAction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/impulseaction",
    "html": "Overview\n\nUse this action to speed up, slow down, or throw a stationary prim.\n\nDeclaration\nclass Preliminary_Action \"ImpulseAction\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims to deliver an impulse.\nvelocity\nThe amount of velocity the impulse adds to the target prims.\nSee Also\nActions\nPreliminary_Action\nA specific task that a trigger performs.\nAudioAction\nAn action that plays audio.\nChangeSceneAction\nAn action that transitions from one scene to another.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nGroupAction\nAn action that runs a list of other actions.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nOrbitAction\nAn action that orbits a set of prims around another.\nSpinAction\nAn action that spins a prim.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAction\nAn action that animates from one transform to another.\nTransformAnimationAction\nAn action that plays a transform animation.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nWaitAction\nAn action that performs a delay.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "GroupAction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/groupaction",
    "html": "Overview\n\nThis action defines how the runtime executes each action in the actions array. When type is serial, the runtime performs actions one after the other. When type is concurrent, the runtime starts each action at the same time.\n\nWhen this action’s type is serial, you can specify a delay between two actions by placing a WaitAction between them.\n\nDeclaration\nclass Preliminary_Action \"GroupAction\"\n\n\nCreate a sequential or looping group action\n\nThe following example defines a group of actions that run sequentially. The group contains a flip action, a wait action, and a hide action.\n\ndef Preliminary_Action \"SimpleGroup\" (\n    inherits = </GroupAction>\n)\n{\n    rel actions = [ <Flip>, <Wait>, <Hide> ]\n    uniform bool loops = false\n    uniform uint performCount = 1\n\n\n    def Action \"Flip\" (\n        inherits = </EmphasizeAction>\n    )\n    {\n        uniform token motionType = \"flip\"\n    }\n\n\n    def Action \"Wait\" (\n        inherits = </WaitAction>\n    )\n    {\n    }\n\n\n    def Action \"Hide\" (\n        inherits = </VisibilityAction>\n    )\n    {\n        uniform token type = \"hide\"\n    }\n}\n\n\nThe following group named EndlessLoop repeats a set of actions indefinitely because performCount is 0.\n\ndef Action \"EndlessLoop\" (\n    inherits = </GroupAction>\n)\n{\n    rel actions = [...]\n    uniform bool loops = true\n    uniform uint performCount = 0\n}\n\n\nTopics\nProperties\ninfo:id\nThe action’s unique identifier.\ntype\nAn option that controls the order in which the actions execute.\nloops\nA Boolean value indicating whether the group loops.\nperformCount\nA value that specifies the number of times the group’s actions repeat.\nactions\nA list of actions that make up the group.\nSee Also\nActions\nPreliminary_Action\nA specific task that a trigger performs.\nAudioAction\nAn action that plays audio.\nChangeSceneAction\nAn action that transitions from one scene to another.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nImpulseAction\nAn action that adds velocity to an prim.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nOrbitAction\nAn action that orbits a set of prims around another.\nSpinAction\nAn action that spins a prim.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAction\nAn action that animates from one transform to another.\nTransformAnimationAction\nAn action that plays a transform animation.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nWaitAction\nAn action that performs a delay.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "ChangeSceneAction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/changesceneaction",
    "html": "Overview\n\nTo define multiple scenes in a USDZ file, use the def and over specifiers. For more information, see sceneLibrary.\n\nDeclaration\nclass Preliminary_Action \"ChangeSceneAction\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the action.\nscene\nThe scene to which the action transitions.\nSee Also\nActions\nPreliminary_Action\nA specific task that a trigger performs.\nAudioAction\nAn action that plays audio.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nGroupAction\nAn action that runs a list of other actions.\nImpulseAction\nAn action that adds velocity to an prim.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nOrbitAction\nAn action that orbits a set of prims around another.\nSpinAction\nAn action that spins a prim.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAction\nAn action that animates from one transform to another.\nTransformAnimationAction\nAn action that plays a transform animation.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nWaitAction\nAn action that performs a delay.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "AudioAction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/audioaction",
    "html": "Overview\n\nThis prim implements one of two ways to play audio in a scene; to play audio without AudioAction, use SpatialAudio.\n\nDeclaration\nclass Preliminary_Action \"AudioAction\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that play audio.\naudio\nThe location of an audio file.\ntype\nThe type of command to send the audio.\ngain\nA value that controls the audio volume.\nauralMode\nAn option that controls the audio signal’s spacial dynamics.\nSee Also\nActions\nPreliminary_Action\nA specific task that a trigger performs.\nChangeSceneAction\nAn action that transitions from one scene to another.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nGroupAction\nAn action that runs a list of other actions.\nImpulseAction\nAn action that adds velocity to an prim.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nOrbitAction\nAn action that orbits a set of prims around another.\nSpinAction\nAn action that spins a prim.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAction\nAn action that animates from one transform to another.\nTransformAnimationAction\nAn action that plays a transform animation.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nWaitAction\nAn action that performs a delay.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "textureCoordinates | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanegeometry/2949171-texturecoordinates",
    "html": "Discussion\n\nEach float2 value in this array represents the UV texture coordinates for the vertex at the corresponding index in the vertices array.\n\nSee Also\nAccessing Mesh Data\nvar vertices: [simd_float3]\nAn array of vertex positions for each point in the plane mesh.\nvar triangleCount: Int\nThe number of triangles described by the triangleIndices buffer.\nvar triangleIndices: [Int16]\nAn array of indices describing the triangle mesh formed by the plane geometry's vertex data."
  },
  {
    "title": "triangleCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanegeometry/2941058-trianglecount",
    "html": "Discussion\n\nEach set of three indices forms a triangle, so the number of indices in the triangleIndices buffer is three times the triangleCount value.\n\nSee Also\nAccessing Mesh Data\nvar vertices: [simd_float3]\nAn array of vertex positions for each point in the plane mesh.\nvar textureCoordinates: [vector_float2]\nAn array of texture coordinate values for each point in the plane mesh.\nvar triangleIndices: [Int16]\nAn array of indices describing the triangle mesh formed by the plane geometry's vertex data."
  },
  {
    "title": "ARSkeleton.JointName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton/jointname",
    "html": "Overview\n\nYou use this class to access information about a named joint, such as its index in a skeleton's array of joints, or its position on the screen or in the physical environment.\n\nWhen you're tracking a body in 2D space, you get the screen-space position of a named joint by using the landmark(for:) function.\n\nWhen you're tracking a body in 3D space, you get a named joint's position in either local or model space by using the localTransform(for:) or modelTransform(for:) functions, respectively.\n\nTopics\nCreating a Joint Name\ninit(rawValue: String)\nCreates a new joint name.\ninit?(VNRecognizedPointKey)\nReturns a joint name that corresponds to a key point defined in a human body pose.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys.\nIdentifying Joints\nstatic let root: ARSkeleton.JointName\nA skeletal joint that's the root of all other joints.\nstatic let head: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the head.\nstatic let leftFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left foot.\nstatic let leftHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left hand.\nstatic let leftShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the left shoulder.\nstatic let rightFoot: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right foot.\nstatic let rightHand: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right hand.\nstatic let rightShoulder: ARSkeleton.JointName\nA skeletal joint that ARKit tracks representing the right shoulder.\nRelationships\nConforms To\nHashable\nRawRepresentable\nSendable\nSee Also\nGetting Joint Information\nvar definition: ARSkeletonDefinition\nThe particular configuration of joints that define a body's current state.\nfunc isJointTracked(Int) -> Bool\nTells you whether ARKit tracks a joint at a particular index."
  },
  {
    "title": "ARPlaneClassificationStatusNotAvailable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneclassificationstatus/arplaneclassificationstatusnotavailable",
    "html": "Discussion\n\nPlane classification is available only on iPhone XS, iPhone XS Max, and iPhone XR. On other devices, all plane anchors always indicate a classification status of ARPlaneClassificationStatusNotAvailable.\n\nA classification status of ARPlaneClassificationStatusNotAvailable can also occur if the plane classification process is temporarily unavilable.\n\nSee Also\nClassification Status\nARPlaneClassificationStatusUndetermined\nARKit has not yet produced a classification for the plane anchor.\nARPlaneClassificationStatusUnknown\nARKit has completed its classification process for the plane anchor, but the result is inconclusive.\nARPlaneClassificationStatusKnown\nARKit has completed its classfication process for the plane anchor."
  },
  {
    "title": "triangleIndices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanegeometry/2949168-triangleindices",
    "html": "Discussion\n\nEach 16-bit integer value in this array represents an index into the vertices and textureCoordinates arrays. Each set of three indices identifies the vertices that form a single triangle in the mesh. You can use array as an index buffer for a triangle mesh in GPU-based rendering or to create 3D model asset files.\n\nEach set of three indices forms a triangle, so the number of indices in the triangleIndices array is three times the triangleCount value.\n\nSee Also\nAccessing Mesh Data\nvar vertices: [simd_float3]\nAn array of vertex positions for each point in the plane mesh.\nvar textureCoordinates: [vector_float2]\nAn array of texture coordinate values for each point in the plane mesh.\nvar triangleCount: Int\nThe number of triangles described by the triangleIndices buffer."
  },
  {
    "title": "boundaryVertices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanegeometry/2949169-boundaryvertices",
    "html": "Discussion\n\nEach float3 value in this array represents the position of a vertex along the boundary polygon of the estimated plane. The owning plane anchor's transform matrix defines the coordinate system for these points.\n\nThis array defines the boundary polygon of the plane. Use it for purposes that require only that polygon's definition, such as rendering an outline of the plane's estimated shape or testing whether a point is inside the bounded region. If, instead, you need the filled shape (for example, to render a solid 3D representation of the surface), see the vertices property."
  },
  {
    "title": "classificationStatus | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/2990937-classificationstatus",
    "html": "Discussion\n\nOn supported devices, ARKit automatically attempts to characterize each detected plane, describing it as a real-world surface such as as a wall, floor, or table. You can then use this information to enhance the realism of your AR experience; for example, by placing certain virtual content only on floors.\n\nWhen this property's value is ARPlaneClassificationStatusKnown, the classification property represents ARKit's characterization of the real-world surface corresponding to the plane anchor.\n\nPlane classification can take longer than plane detection, and ARKit reports classifications only for planes where it has a high confidence in the result, so you can use other values of this property to keep track of ARKit's classification process:\n\nIf ARKit is still working to classify a plane, the classification is ARPlaneClassificationNone and the status is ARPlaneClassificationStatusUndetermined.\n\nIf ARKit cannot characterize a plane with high confidence, the classification is ARPlaneClassificationNone and the status is ARPlaneClassificationStatusUnknown.\n\nPlane classification is available only on iPhone XS, iPhone XS Max, and iPhone XR. Before using classification results, check the classificationSupported class property to make sure you're on a supported device.\n\nSee Also\nClassifying a Plane\nclassificationSupported\nA Boolean value that indicates whether plane classification is available on the current device.\nclassification\nA general characterization of what kind of real-world surface the plane anchor represents.\nARPlaneClassification\nPossible characterizations of real-world surfaces represented by plane anchors.\nARPlaneClassificationStatus\nPossible states of ARKit's process for classifying plane anchors."
  },
  {
    "title": "update(from:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnplanegeometry/2941057-update",
    "html": "Parameters\nplane\n\nA coarse mesh representation of a detected plane's estimated shape.\n\nDiscussion\n\nTo update a SceneKit model of a plane actively tracked in an AR session, call this method in your ARSCNViewDelegate object’s renderer(_:didUpdate:for:) callback, passing the geometry property from the ARPlaneAnchor object that callback provides."
  },
  {
    "title": "width | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneextent/3950862-width",
    "html": "See Also\nInspecting Plane Size\nvar height: Float\nThe estimated height of the plane."
  },
  {
    "title": "ARPlaneClassificationNone | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneclassification/arplaneclassificationnone",
    "html": "Discussion\n\nWhen a plane anchor has no classification, the classificationStatus property indicates the reason: the classification process might be unavailable or incomplete, or ARKit may not have been able to conclusively identify the plane.\n\nSee Also\nPlane Classifications\nARPlaneClassificationWall\nThe plane anchor represents a real-world wall or similar large vertical surface.\nARPlaneClassificationFloor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\nARPlaneClassificationCeiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\nARPlaneClassificationTable\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\nARPlaneClassificationSeat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\nARPlaneClassificationDoor\nThe plane anchor represents a real-world door, or similar archway.\nARPlaneClassificationWindow\nThe plane anchor fits the description of a real-world window."
  },
  {
    "title": "rotationOnYAxis | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneextent/3950861-rotationonyaxis",
    "html": "Discussion\n\nAs the session runs, the framework may update the plane’s y-rotation to better fit its rectangular area in the environment. In iOS 15 and earlier, the framework rotates the plane anchor according to that angle. In iOS 16, the framework doesn’t rotate the anchor automatically and its transform matrix remains unchanged. Instead, the framework exposes the angle in rotationOnYAxis that you apply to any plane extent geometry in your app.\n\nImportant\n\nApps that run on iOS 16 with a deployment target less than iOS 16 preserve the prior y-axis rotation behavior."
  },
  {
    "title": "classification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/2990936-classification",
    "html": "Discussion\n\nOn supported devices, ARKit automatically attempts to characterize each detected plane, describing it as a real-world surface such as as a wall, floor, or table. You can then use this information to enhance the realism of your AR experience; for example, by placing certain virtual content only on floors.\n\nPlane classification can take longer than plane detection, and ARKit reports classifications only for planes where it has a high confidence in the result. Use this property together with the classificationStatus property to keep track of ARKit's classification process:\n\nIf ARKit is still working to classify a plane, the classification is ARPlaneClassificationNone and the status is ARPlaneClassificationStatusUndetermined.\n\nIf ARKit cannot characterize a plane with high confidence, the classification is ARPlaneClassificationNone and the status is ARPlaneClassificationStatusUnknown.\n\nPlane classification is available only on iPhone XS, iPhone XS Max, and iPhone XR. Before using classification results, check the classificationSupported class property to make sure you're on a supported device.\n\nSee Also\nClassifying a Plane\nclassificationSupported\nA Boolean value that indicates whether plane classification is available on the current device.\nARPlaneClassification\nPossible characterizations of real-world surfaces represented by plane anchors.\nclassificationStatus\nThe current state of ARKit's process for classifying the plane anchor.\nARPlaneClassificationStatus\nPossible states of ARKit's process for classifying plane anchors."
  },
  {
    "title": "next() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/iterator/4180398-next",
    "html": "Relationships\nFrom Protocol\nAsyncIteratorProtocol\nSee Also\nPerforming sequence iterator operations\ntypealias AnchorUpdateSequence.Iterator.Element"
  },
  {
    "title": "init(device:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnplanegeometry/2941060-init",
    "html": "Parameters\ndevice\n\nThe Metal device to use for rendering the geometry.\n\nReturn Value\n\nA new SceneKit plane geometry, or nil if the Metal device is unavailable.\n\nDiscussion\n\nA newly created ARSCNPlaneGeometry instance does not represent any specific plane; use the update(from:) method to make the geometry match the estimated shape of a specific plane anchor.\n\nThe geometry contains a single geometry element; as such, assigning more than one material has no visible effect (see the inherited materials property)."
  },
  {
    "title": "verticalAlignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_text/verticalalignment",
    "html": "Overview\n\nThe runtime handles each option of this property differently depending on whether the text displays with line breaks. For more information, see wrapMode.\n\nDeclaration\ntoken verticalAlignment = \"center\" (\n    allowedTokens = [\"top\", \"middle\", \"lowerMiddle\", \"baseline\", \"bottom\"]\n)\n\n\nVertical Alignments for Single-Line Text\n\nFor a single line of text, the vertical alignment is relative to font features.\n\ntop\n\nAligns the line of text vertically with the ascender.\n\nmiddle\n\nAligns the line of text vertically with the center of capital letters.\n\nlowerMiddle\n\nAligns the line of text vertically with the center of lowercase letters.\n\nbaseline\n\nAligns the line of text vertically with the baseline.\n\nbottom\n\nAligns the line of text vertically with a descender.\n\nVertical Alignments for Multiline Text\n\nFor multiline text, each line of text bases its vertical alignment on the text’s bounding box.\n\ntop\n\nAligns each line of text vertically with the top.\n\nmiddle, lowerMiddle\n\nAligns each line of text in the center with equal space above and below the line of text.\n\nbaseline, bottom\n\nAligns each line of text vertically with the bottom.\n\nSee Also\nProperties\ncontent\nThe characters that the text displays.\nfont\nAn array of font names.\npointSize\nThe size of the text’s font.\nwidth\nThe width of the text’s bounding box.\nheight\nThe height of the text’s bounding box.\ndepth\nA value that defines the depth, in scene units, of the text’s extrusion.\nwrapMode\nAn option that determines the flow of the text.\nhorizontalAlignment\nAn option that controls the text’s horizontal placement within its bounding box."
  },
  {
    "title": "AnchorUpdateSequence.Iterator.Element | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/iterator/element",
    "html": "See Also\nPerforming sequence iterator operations\nfunc next() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>.Element?"
  },
  {
    "title": "isJointTracked(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton/3229917-isjointtracked",
    "html": "Discussion\n\nUse this function to determine which joints ARKit tracks using a particular index.\n\nSee Also\nGetting Joint Information\nvar definition: ARSkeletonDefinition\nThe particular configuration of joints that define a body's current state.\nstruct ARSkeleton.JointName\nA name identifier for a joint."
  },
  {
    "title": "definition | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton/3229916-definition",
    "html": "See Also\nGetting Joint Information\nfunc isJointTracked(Int) -> Bool\nTells you whether ARKit tracks a joint at a particular index.\nstruct ARSkeleton.JointName\nA name identifier for a joint."
  },
  {
    "title": "horizontalAlignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_text/horizontalalignment",
    "html": "Overview\n\nThe default value is center.\n\nHorizontal Alignments\nleft\n\nLeft-aligns each line of text.\n\ncenter\n\nCenter-aligns each line of text.\n\nright\n\nRight-aligns each line of text.\n\njustified\n\nLeft- and right-aligns the text by adding additional spaces between words.\n\nDeclaration\ntoken horizontalAlignment = \"center\" (\n    allowedTokens = [\"left\", \"center\", \"right\", \"justified\"]\n)\n\n\nSee Also\nProperties\ncontent\nThe characters that the text displays.\nfont\nAn array of font names.\npointSize\nThe size of the text’s font.\nwidth\nThe width of the text’s bounding box.\nheight\nThe height of the text’s bounding box.\ndepth\nA value that defines the depth, in scene units, of the text’s extrusion.\nwrapMode\nAn option that determines the flow of the text.\nverticalAlignment\nAn option that controls the text’s vertical placement within its bounding rectangle."
  },
  {
    "title": "jointLandmarks | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton2d/3229920-jointlandmarks",
    "html": "Discussion\n\nThe joint landmarks are normalized within the range [0..1] in the coordinate space of the current frame's camera image, where 0 is the upper left, and 1 is the bottom right.\n\nSee Also\nGetting Joint Landmarks\n- landmarkForJointNamed:\nReturns the location of a joint with a given name."
  },
  {
    "title": "wrapMode | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_text/wrapmode",
    "html": "Overview\n\nThe default value is flowing.\n\nWrap Modes\nsingleLine\n\nDisplays text in a single line.\n\nhardBreaks\n\nBreaks the text only at the text string’s line breaks.\n\nflowing\n\nBreaks the text as needed to fit within the bounding box.\n\nDeclaration\ntoken wrapMode = \"flowing\" (\n    allowedTokens = [\"singleLine\", \"hardBreaks\", \"flowing\"]\n)\n\n\nSee Also\nProperties\ncontent\nThe characters that the text displays.\nfont\nAn array of font names.\npointSize\nThe size of the text’s font.\nwidth\nThe width of the text’s bounding box.\nheight\nThe height of the text’s bounding box.\ndepth\nA value that defines the depth, in scene units, of the text’s extrusion.\nhorizontalAlignment\nAn option that controls the text’s horizontal placement within its bounding box.\nverticalAlignment\nAn option that controls the text’s vertical placement within its bounding rectangle."
  },
  {
    "title": "width | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_text/width",
    "html": "Overview\n\nSupply a positive value for this property in stage linear units. For more information, see Encoding Stage Linear Units.\n\nThe runtime ignores this value if wrapMode is singleLine.\n\nDeclaration\nfloat width\n\n\nSee Also\nProperties\ncontent\nThe characters that the text displays.\nfont\nAn array of font names.\npointSize\nThe size of the text’s font.\nheight\nThe height of the text’s bounding box.\ndepth\nA value that defines the depth, in scene units, of the text’s extrusion.\nwrapMode\nAn option that determines the flow of the text.\nhorizontalAlignment\nAn option that controls the text’s horizontal placement within its bounding box.\nverticalAlignment\nAn option that controls the text’s vertical placement within its bounding rectangle."
  },
  {
    "title": "depth | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_text/depth",
    "html": "Overview\n\nThis value must be positive. The runtime always displays this text’s geometry from both sides even if this property is zero.\n\nDeclaration\nfloat depth\n\n\nSee Also\nProperties\ncontent\nThe characters that the text displays.\nfont\nAn array of font names.\npointSize\nThe size of the text’s font.\nwidth\nThe width of the text’s bounding box.\nheight\nThe height of the text’s bounding box.\nwrapMode\nAn option that determines the flow of the text.\nhorizontalAlignment\nAn option that controls the text’s horizontal placement within its bounding box.\nverticalAlignment\nAn option that controls the text’s vertical placement within its bounding rectangle."
  },
  {
    "title": "font | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_text/font",
    "html": "Overview\n\nThe runtime scans this list in ascending order and chooses the first font that the runtime can provide. Font names provide the font family, then the font style, such as “Gill Sans.” You can reference font names using Font Book on macOS.\n\nIf the system doesn’t support your chosen font, you can fall back to a generic font by providing one or more generic font specifiers at the end of the font list: serif, san-serif, monospaced, or cursive.\n\nDeclaration\nstring[] font\n\n\nSee Also\nProperties\ncontent\nThe characters that the text displays.\npointSize\nThe size of the text’s font.\nwidth\nThe width of the text’s bounding box.\nheight\nThe height of the text’s bounding box.\ndepth\nA value that defines the depth, in scene units, of the text’s extrusion.\nwrapMode\nAn option that determines the flow of the text.\nhorizontalAlignment\nAn option that controls the text’s horizontal placement within its bounding box.\nverticalAlignment\nAn option that controls the text’s vertical placement within its bounding rectangle."
  },
  {
    "title": "height | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_text/height",
    "html": "Overview\n\nSupply a positive value for this property in stage linear units. For more information, see Encoding Stage Linear Units.\n\nThe runtime ignores this value if wrapMode is singleLine.\n\nDeclaration\nfloat height\n\n\nSee Also\nProperties\ncontent\nThe characters that the text displays.\nfont\nAn array of font names.\npointSize\nThe size of the text’s font.\nwidth\nThe width of the text’s bounding box.\ndepth\nA value that defines the depth, in scene units, of the text’s extrusion.\nwrapMode\nAn option that determines the flow of the text.\nhorizontalAlignment\nAn option that controls the text’s horizontal placement within its bounding box.\nverticalAlignment\nAn option that controls the text’s vertical placement within its bounding rectangle."
  },
  {
    "title": "content | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_text/content",
    "html": "Overview\n\nYou can include line breaks in the value of this property.\n\nDeclaration\nstring content = \"\"\n\n\nSee Also\nProperties\nfont\nAn array of font names.\npointSize\nThe size of the text’s font.\nwidth\nThe width of the text’s bounding box.\nheight\nThe height of the text’s bounding box.\ndepth\nA value that defines the depth, in scene units, of the text’s extrusion.\nwrapMode\nAn option that determines the flow of the text.\nhorizontalAlignment\nAn option that controls the text’s horizontal placement within its bounding box.\nverticalAlignment\nAn option that controls the text’s vertical placement within its bounding rectangle."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/code/4241481",
    "html": "See Also\nInspecting tracking failures\nvar description: String\nA textual description of the error code.\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (WorldTrackingProvider.Error.Code, WorldTrackingProvider.Error.Code) -> Bool"
  },
  {
    "title": "WorldTrackingProvider.Error.Code.addWorldAnchorFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/code/addworldanchorfailed",
    "html": "See Also\nDetermining causes for tracking failures\ncase removeWorldAnchorFailed\nThe error code for when a world tracking provider can’t remove a world anchor.\ncase worldAnchorLimitReached\nThe error code for when a world tracking provider reaches its world anchor limit."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/code/4241484-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting tracking failures\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (WorldTrackingProvider.Error.Code, WorldTrackingProvider.Error.Code) -> Bool\nstatic func != (WorldTrackingProvider.Error.Code, WorldTrackingProvider.Error.Code) -> Bool"
  },
  {
    "title": "ARPlaneAnchor.Classification.floor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification/floor",
    "html": "See Also\nPlane Classifications\ncase wall\nThe plane anchor represents a real-world wall or similar large vertical surface.\ncase ceiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\ncase table\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\ncase seat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\ncase door\nThe plane anchor represents a real-world door or similar vertical surface.\ncase window\nThe plane anchor represents a real-world window or similar vertical surface."
  },
  {
    "title": "physicalWidth | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_referenceimage/physicalwidth",
    "html": "Overview\n\nThis property informs the runtime how wide the image is in the physical environment. The runtime calculates the height based on the image’s aspect ratio.\n\nBecause this property describes a real-world width, the prim’s transform hierarchy doesn’t modify this property’s value.\n\nDeclaration\nuniform double physicalWidth\n\n\nDefine a reference image’s width\n\nTo recognize an image in the real world, the runtime requires a prim to specify how wide the image is in the physical environment.\n\ndef Preliminary_ReferenceImage \"ImageReference\"\n{\n    uniform double physicalWidth = 12\n    ...\n}\n\n\nSee Also\nProperties\nimage\nAn image file for which the runtime should search."
  },
  {
    "title": "WorldTrackingProvider.Error.Code.worldAnchorLimitReached | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/code/worldanchorlimitreached",
    "html": "See Also\nDetermining causes for tracking failures\ncase addWorldAnchorFailed\nThe error code for when a world tracking provider can’t add a world anchor.\ncase removeWorldAnchorFailed\nThe error code for when a world tracking provider can’t remove a world anchor."
  },
  {
    "title": "ARGeoAnchor.AltitudeSource.unknown | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/altitudesource/unknown",
    "html": "See Also\nSources\ncase precise\nThe framework sets the altitude using a high-resolution digital-elevation model.\ncase coarse\nThe framework sets the altitude using a coarse digital-elevation model.\ncase userDefined\nThe app defines the altitude."
  },
  {
    "title": "image | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_referenceimage/image",
    "html": "Overview\n\nAssign this property a path to a PNG or JPEG file.\n\nDeclaration\nuniform asset image\n\n\nDefine a reference image\n\nThe following Preliminary_ReferenceImage assigns this property a file named image.png.\n\ndef Preliminary_ReferenceImage \"ImageReference\"\n{\n    uniform asset image = @image.png@\n    ...\n}\n\n\nSee Also\nProperties\nphysicalWidth\nAn image’s width in centimeters."
  },
  {
    "title": "ARSessionRunOptionRemoveExistingAnchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionrunoptions/arsessionrunoptionremoveexistinganchors",
    "html": "Discussion\n\nBy default, when you call the runWithConfiguration:options: method on a session that has run before or is already running, the session keeps any ARAnchor objects that you previously added. That is, objects in the AR scene keep their apparent real-world positions relative to the device (unless you enable the ARSessionRunOptionResetTracking option).\n\nEnable the ARSessionRunOptionRemoveExistingAnchors option if changing session configurations should invalidate the apparent real-world positions of objects in the AR scene. For example, if you've added virtual content to the AR scene whose positions are correlated to real-world objects, remove those anchors so you can reevaluate appropriate real-world positions. On the other hand, if the virtual content in your scene needs to track real-world positions only when that content first appears and can move freely thereafter, you can disable this option to keep the anchors.\n\nSee Also\nRun Options\nARSessionRunOptionResetTracking\nAn option to reset the device's position from the session's previous run.\nARSessionRunOptionStopTrackedRaycasts\nAn option to stop all active tracked raycasts.\nARSessionRunOptionResetSceneReconstruction\nAn option to reset the scene mesh."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement/primitive/4169987-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting geometry primitives\nvar hashValue: Int\nvar indexCount: Int\nfunc hash(into: inout Hasher)\nstatic func == (GeometryElement.Primitive, GeometryElement.Primitive) -> Bool\nstatic func != (GeometryElement.Primitive, GeometryElement.Primitive) -> Bool"
  },
  {
    "title": "ARSessionRunOptionStopTrackedRaycasts | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionrunoptions/arsessionrunoptionstoptrackedraycasts",
    "html": "Discussion\n\nBy default, when you call the runWithConfiguration:options: method on a session that is running or has run before, the session keeps tracking any ARTrackedRaycast objects that you previously added by calling trackedRaycast:updateHandler:.\n\nUse ARSessionRunOptionStopTrackedRaycasts if you want to stop all active tracked raycasts. Alternatively, you can stop individual raycasts by calling stopTracking on individual raycasts.\n\nSee Also\nRun Options\nARSessionRunOptionResetTracking\nAn option to reset the device's position from the session's previous run.\nARSessionRunOptionRemoveExistingAnchors\nAn option to remove any anchor objects associated with the session's previous run.\nARSessionRunOptionResetSceneReconstruction\nAn option to reset the scene mesh."
  },
  {
    "title": "ARSessionRunOptionResetSceneReconstruction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionrunoptions/arsessionrunoptionresetscenereconstruction",
    "html": "Discussion\n\nWhen you reset scene reconstruction, ARKit removes any existing mesh anchors (ARMeshAnchor) from the session.\n\nSee Also\nRun Options\nARSessionRunOptionResetTracking\nAn option to reset the device's position from the session's previous run.\nARSessionRunOptionRemoveExistingAnchors\nAn option to remove any anchor objects associated with the session's previous run.\nARSessionRunOptionStopTrackedRaycasts\nAn option to stop all active tracked raycasts."
  },
  {
    "title": "ARCoachingOverlayView.Goal.horizontalPlane | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayview/goal/horizontalplane",
    "html": "Discussion\n\nWhen you use this goal, coaching overlay won't hide until the user has moved their device in a way that facilitates ARKit finding at least one horizontal surface.\n\nSee Also\nDefining a Goal\ncase anyPlane\nA goal that specifies your app requires a plane of any type.\ncase tracking\nA goal that specifies your app requires basic world tracking.\ncase verticalPlane\nA goal that specifies your app requires a vertical plane.\ncase geoTracking\nA goal that specifies your app requires a precise geographic location."
  },
  {
    "title": "ARCoachingOverlayView.Goal.verticalPlane | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayview/goal/verticalplane",
    "html": "Discussion\n\nWhen you use this goal, coaching overlay won't hide until the user has moved their device in a way that facilitates ARKit finding at least one vertical surface.\n\nSee Also\nDefining a Goal\ncase anyPlane\nA goal that specifies your app requires a plane of any type.\ncase horizontalPlane\nA goal that specifies your app requires a horizontal plane.\ncase tracking\nA goal that specifies your app requires basic world tracking.\ncase geoTracking\nA goal that specifies your app requires a precise geographic location."
  },
  {
    "title": "depthMap | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ardepthdata/3566296-depthmap",
    "html": "Discussion\n\nFor custom renderers, if you create a texture to send depth data to the GPU, choose a MTLPixelFormat according to the depthMap pixel format. Call CVPixelBufferGetPixelFormatType(_:) on the depthMap to get its format. For example, if at runtime the depthMap format is kCVPixelFormatType_DepthFloat32 (OSType `fdep`), use MTLPixelFormat.r32Float.\n\nSee Also\nDepth Information\nvar confidenceMap: CVPixelBuffer?\nThe framework’s confidence in the accuracy of the depth-map data.\nenum ARConfidenceLevel\nDegrees to which the framework is confident about depth-data accuracy."
  },
  {
    "title": "identifiers | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arpointcloud/2927813-identifiers",
    "html": "Discussion\n\nEach identifier in this list corresponds to the point vector at the same index in the points array.\n\nSee Also\nIdentifying Feature Points\nvar points: [simd_float3]\nThe list of detected points."
  },
  {
    "title": "coachingOverlayViewDidDeactivate(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayviewdelegate/3152983-coachingoverlayviewdiddeactivate",
    "html": "Discussion\n\nImplement this function to do any custom actions your app requires to begin the AR experience. For example, when coaching is deactivated, your app might restore custom UI.\n\nWhen the coaching overlay is deactivating, isActive is false. If the animated property of setActive(_:animated:) is true, isActive and isHidden are false while the coaching overlay is fading out. When the coaching overlay is deactivated without animation, or when the animation finishes, ARKit sends a coachingOverlayViewDidDeactivate(_:) notification.\n\nSee Also\nEnabling Coaching\nfunc coachingOverlayViewWillActivate(ARCoachingOverlayView)\nTells you when the coaching overlay view activates."
  },
  {
    "title": "points | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arpointcloud/2927814-points",
    "html": "See Also\nIdentifying Feature Points\nvar identifiers: [UInt64]\nA list of unique identifiers corresponding to detected feature points."
  },
  {
    "title": "noseSneerRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928239-nosesneerright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nstatic let browDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nstatic let browDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nstatic let browInnerUp: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nstatic let browOuterUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nstatic let browOuterUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nstatic let cheekPuff: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of both cheeks.\nstatic let cheekSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the left eye.\nstatic let cheekSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the right eye.\nstatic let noseSneerLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the left side of the nose around the nostril."
  },
  {
    "title": "browDownLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928223-browdownleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nstatic let browDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nstatic let browInnerUp: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nstatic let browOuterUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nstatic let browOuterUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nstatic let cheekPuff: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of both cheeks.\nstatic let cheekSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the left eye.\nstatic let cheekSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the right eye.\nstatic let noseSneerLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the left side of the nose around the nostril.\nstatic let noseSneerRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "cheekSquintLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928226-cheeksquintleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nstatic let browDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nstatic let browDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nstatic let browInnerUp: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nstatic let browOuterUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nstatic let browOuterUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nstatic let cheekPuff: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of both cheeks.\nstatic let cheekSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the right eye.\nstatic let noseSneerLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the left side of the nose around the nostril.\nstatic let noseSneerRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "init(blendShapes:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacegeometry/2928204-init",
    "html": "Parameters\nblendShapes\n\nA dictionary of blend shape coefficients describing a facial expression in terms of the positions of specific facial features. For any coefficient not specified in this dictionary, ARKit assumes a value of 0.0.\n\nReturn Value\n\nA face geometry object, or nil if ARKit face tracking is not supported on the current device.\n\nDiscussion\n\nEach key in the blendShapes dictionary is an ARFaceAnchor.BlendShapeLocation constant identifying a facial feature. The corresponding value is the position of that feature relative to its neutral configuration, ranging from 0.0 (neutral) to 1.0 (maximum movement).\n\nThe format of this dictionary is identical to that provided by the ARFaceAnchor blendShapes property. You can use that property and this initializer to efficiently save and restore facial expression data; the serialized form of a blend shapes dictionary is more portable than that of the face mesh those coefficients describe."
  },
  {
    "title": "trackingState | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/2880259-trackingstate",
    "html": "See Also\nHandling Tracking Status\nARTrackingState\nPossible values for position-tracking quality.\ntrackingStateReason\nA possible diagnosis for limited position-tracking quality as of when the camera captured a frame.\nARTrackingStateReason\nPossible causes for limited position-tracking quality."
  },
  {
    "title": "StartAnimationAction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/startanimationaction",
    "html": "Overview\n\nIf the asset defines an animation, this action runs it on every prim in affectedObjects.\n\nDeclaration\nclass Preliminary_Action \"StartAnimationAction\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of objects to animate.\nstart\nThe moment to begin an animation.\nduration\nThe amount of time between the start of an action and its end.\nreversed\nA Boolean value that determines the clip playback direction.\nanimationSpeed\nA factor to apply to the animation speed.\nreverses\nA Boolean value that indicates whether the animation plays from beginning to end, then again from end to beginning.\nSee Also\nActions\nPreliminary_Action\nA specific task that a trigger performs.\nAudioAction\nAn action that plays audio.\nChangeSceneAction\nAn action that transitions from one scene to another.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nGroupAction\nAn action that runs a list of other actions.\nImpulseAction\nAn action that adds velocity to an prim.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nOrbitAction\nAn action that orbits a set of prims around another.\nSpinAction\nAn action that spins a prim.\nTransformAction\nAn action that animates from one transform to another.\nTransformAnimationAction\nAn action that plays a transform animation.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nWaitAction\nAn action that performs a delay.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "origin | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery/3194582-origin",
    "html": "See Also\nInterpreting the Ray\nvar direction: simd_float3\nA vector that describes the ray's trajectory in 3D space."
  },
  {
    "title": "direction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery/3194580-direction",
    "html": "See Also\nInterpreting the Ray\nvar origin: simd_float3\nA 3D coordinate that defines the ray's starting place."
  },
  {
    "title": "ARConfidenceLevel.high | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfidencelevel/high",
    "html": "See Also\nLevels\ncase low\nDepth-value accuracy in which the framework is less confident.\ncase medium\nDepth-value accuracy in which the framework is moderately confident."
  },
  {
    "title": "init(origin:direction:allowing:alignment:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery/3194581-init",
    "html": "Parameters\norigin\n\nA 3D position that describes the raycast's starting point.\n\ndirection\n\nA 3D vector that describes the raycast's direction.\n\nallowing\n\nThe type of plane with which you allow the raycast to intersect.\n\nalignment\n\nThe target's alignment with respect to gravity with which you allow the raycast to intersect.\n\nDiscussion\n\nThis creates a query by supplying a 3D starting place and vector. To acquire a raycast query using a screen point and vector that points outward from the user, call raycastQuery(from:allowing:alignment:) on ARSCNView."
  },
  {
    "title": "preliminary:imageAnchoring:referenceImage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_anchoringapi/preliminary_imageanchoring_referenceimage",
    "html": "Overview\n\nAn asset assigns this property a prim with type Preliminary_ReferenceImage.\n\nThe runtime searches for an image described by this property in the physical environment. If the runtime finds a match, it places a prim at the image’s real-world location. When a runtime places a prim in the physical environment, the prim’s children also attach to that location.\n\nThe runtime employs the information contained in this property only when preliminary:anchoring:type is image.\n\nDeclaration\nrel preliminary:imageAnchoring:referenceImage\n\n\nAnchor a cube to an image\n\nThe following example demonstrates how an asset defines a reference image (ImageReference) that the runtime should look for in the physical environment. The asset includes a cube that assigns an Preliminary_ReferenceImage to its preliminary:imageAnchoring:referenceImage property, instructing the runtime to anchor it to a real-world object that matches the image criteria.\n\ndef Cube \"ImageAnchoredCube\" (\n    prepend apiSchemas = [ \"Preliminary_AnchoringAPI\" ]\n)\n{\n    uniform token preliminary:anchoring:type = \"image\"\n    rel preliminary:imageAnchoring:referenceImage = <ImageReference>\n    ...\n\n\n    def Preliminary_ReferenceImage \"ImageReference\"\n    {\n      uniform asset image = @image.png@\n      uniform double physicalWidth = 0.12\n    }\n}\n\n\nSee Also\nProperties\npreliminary:anchoring:type\nA option that specifies the type of anchor.\npreliminary:planeAnchoring:alignment\nAn option that specifies the orientation of a plane.\nRelated Documentation"
  },
  {
    "title": "target | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery/3194583-target",
    "html": "Discussion\n\nThe available target types are plane, infinite plane, and estimated plane.\n\nSee Also\nSpecifying the Target\nenum ARRaycastQuery.Target\nThe types of surface you allow a raycast to intersect with.\nvar targetAlignment: ARRaycastQuery.TargetAlignment\nThe target's alignment with respect to gravity.\nenum ARRaycastQuery.TargetAlignment\nA specification that indicates a target's alignment with respect to gravity."
  },
  {
    "title": "preliminary:planeAnchoring:alignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_anchoringapi/preliminary_planeanchoring_alignment",
    "html": "Overview\n\nThis property is active only for the preliminary:anchoring:type value plane. The runtime recognizes real-word surfaces such as floors, tables, ceilings as horizontal planes. Vertical planes include walls, doors, and windows.\n\nDeclaration\nuniform token preliminary:planeAnchoring:alignment (\n        allowedTokens = [\"horizontal\", \"vertical\", \"any\"]\n)\n\n\nPlane anchor types\nhorizontal\n\nRequests that the runtime anchor the prim on a floor, table, ceiling, or other flat surface.\n\nvertical\n\nRequests that the runtime anchor the prim on a wall, door, window, or other vertical surface.\n\nany\n\nRequests that the runtime anchor the prim on the first horizontal or vertical surface detected.\n\nAnchor a prim to a horizontal plane\n\nThe following asset definition requests that the runtime anchor this prim to the first surface the runtime detects that occupies a horizontal orientation in relation to the camera.\n\ndef Cube \"PlaneAnchoredCube\" (\n    prepend apiSchemas = [ \"Preliminary_AnchoringAPI\" ]\n)\n{\n    uniform token preliminary:anchoring:type = \"plane\"\n    uniform token preliminary:planeAnchoring:alignment = \"horizontal\"\n    ...\n}\n\n\nSee Also\nProperties\npreliminary:anchoring:type\nA option that specifies the type of anchor.\npreliminary:imageAnchoring:referenceImage\nThe characteristics of an image the runtime should scan for in order to attach a prim."
  },
  {
    "title": "Preliminary_Behavior | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/preliminary_behavior",
    "html": "Overview\n\nBecause it inherits Typed, this schema declares a Preliminary_Behavior as a type of prim. For more information about typed schemas, see USD Specification > Typed.\n\nTo run actions based on a trigger, an asset defines a prim of this type and sets its triggers and actions.\n\nDeclaration\nclass Preliminary_Behavior \"Preliminary_Behavior\" (\n    inherits = </Typed>\n)\n\n\nTrigger animation for a tapped cube\n\nThe following example demonstrates a behavior that applies an EmphasizeAction to a cube to flip it. Because the cube defines a tap trigger, the runtime performs the flip when a user taps the cube in an AR experience.\n\n#usda 1.0\n\n\ndef Preliminary_Behavior \"TapAndFlip\"\n{\n    rel triggers = [ <Tap> ]\n    rel actions = [ <Entry> ]\n\n\n    def Preliminary_Trigger \"Tap\" ( inherits = </TapGestureTrigger> )\n    {\n        rel affectedObjects = [ </Cube> ]\n    }\n\n\n    def Preliminary_Action \"Entry\" ( inherits = </GroupAction> )\n    {\n        uniform token type = \"parallel\"\n        rel actions = [ <Flip> ]\n    }\n\n\n    def Preliminary_Action \"Flip\" ( inherits = </EmphasizeAction> )\n    {\n        rel affectedObjects = [ </Cube> ]\n        uniform token motionType = \"flip\"\n    }\n}\n\n\ndef Cube \"Cube\" { }\n\n\nTopics\nProperties\ntriggers\nA list of prims that execute a behavior’s actions.\nactions\nA list of prims that a behavior’s triggers invoke.\nexclusive\nA Boolean value that determines if a behavior executes exclusively."
  },
  {
    "title": "preliminary:anchoring:type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_anchoringapi/preliminary_anchoring_type",
    "html": "Overview\n\nThe Preliminary_AnchoringAPI specifies an anchor’s center as the prim’s origin, and the top of the anchor as its normal vector points. The runtime requires an asset to supply a value for this property.\n\nDeclaration\nuniform token preliminary:anchoring:type (\n    allowedTokens = [\"plane\", \"image\", \"face\", \"none\"]\n)\n\n\nAnchor Types\n\nplane\n\nRequests that the runtime center the prim on top of a surface.\n\nimage\n\nRequests that the runtime center the prim on top of an image.\n\nface\n\nRequests that the runtime center the prim on a detected face.\n\nnone\n\nRequests that the runtime doesn’t anchor the prim. This option has the same effect as omitting the anchoring schema.\n\nAnchor a cube to a real-world surface\n\nBy adding the anchoring schema and defining preliminary:anchoring:type of plane, the following cube instructs the runtime to place it on the first horizontal surface the runtime detects in an AR experience.\n\ndef Cube \"PlaneAnchoredCube\" (\n    prepend apiSchemas = [ \"Preliminary_AnchoringAPI\" ]\n)\n{\n    uniform token preliminary:anchoring:type = \"plane\"\n    ...\n}\n\n\nSee Also\nProperties\npreliminary:planeAnchoring:alignment\nAn option that specifies the orientation of a plane.\npreliminary:imageAnchoring:referenceImage\nThe characteristics of an image the runtime should scan for in order to attach a prim."
  },
  {
    "title": "modelTransform(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton3d/3295994-modeltransform",
    "html": "Discussion\n\nModel space refers to the joint's position relative to its hip joint. If an invalid joint name is passed in, the returned matrix will be filled with NaN values.\n\nSee Also\nGetting a Joint's Pose\nvar jointLocalTransforms: [simd_float4x4]\nThe local space transforms for each joint.\nvar jointModelTransforms: [simd_float4x4]\nThe model space transforms for each joint.\nfunc localTransform(for: ARSkeleton.JointName) -> simd_float4x4?\nReturns the local transform for a joint with a given name."
  },
  {
    "title": "localTransform(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton3d/3295993-localtransform",
    "html": "Discussion\n\nLocal space refers to the joints position relative to its parent joint. If an invalid joint name is passed the returned matrix will be filled with NaN values.\n\nSee Also\nGetting a Joint's Pose\nvar jointLocalTransforms: [simd_float4x4]\nThe local space transforms for each joint.\nvar jointModelTransforms: [simd_float4x4]\nThe model space transforms for each joint.\nfunc modelTransform(for: ARSkeleton.JointName) -> simd_float4x4?\nReturns the model transform for a joint with a given name."
  },
  {
    "title": "init(device:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnfacegeometry/2928198-init",
    "html": "Parameters\ndevice\n\nThe Metal device to use for rendering the geometry.\n\nReturn Value\n\nA new SceneKit face geometry, or nil if the Metal device is unavailable or ARKit face tracking is not supported on the current device.\n\nDiscussion\n\nA newly created ARSCNFaceGeometry instance represents a neutral, generic face; use the update(from:) method to deform the geometry to match a specific facial expression or face shape.\n\nThe geometry contains a single geometry element; as such, assigning more than one material has no visible effect (see the inherited materials property).\n\nCalling this initializer is equivalent to calling the init(device:fillMesh:) initializer and passing false for the fillMesh parameter.\n\nSee Also\nCreating a Geometry\ninit?(device: MTLDevice, fillMesh: Bool)\nCreates a SceneKit face geometry, optionally filling in gaps in the mesh for the eyes and mouth."
  },
  {
    "title": "extent | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arenvironmentprobeanchor/2977512-extent",
    "html": "Discussion\n\nRendering reflective objects may involve projecting the environmentTexture onto a proxy geometry centered on the anchor's position, then sampling from the projected texture.\n\nAn environment probe anchor may have an infinite extent, which indicates that its texture is a global lighting environment, or a finite extent, which indicates that its texture represents the local lighting conditions in a specific area of the scene."
  },
  {
    "title": "jointModelTransforms | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton3d/3295992-jointmodeltransforms",
    "html": "Discussion\n\nModel space refers to a joint's position relative to the hip joint. Note, the hip joint is located at the body anchor's origin.\n\nSee Also\nGetting a Joint's Pose\nvar jointLocalTransforms: [simd_float4x4]\nThe local space transforms for each joint.\nfunc localTransform(for: ARSkeleton.JointName) -> simd_float4x4?\nReturns the local transform for a joint with a given name.\nfunc modelTransform(for: ARSkeleton.JointName) -> simd_float4x4?\nReturns the model transform for a joint with a given name."
  },
  {
    "title": "jointLocalTransforms | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton3d/3295991-jointlocaltransforms",
    "html": "Discussion\n\nLocal space refers to a joint's position relative to its parent joint.\n\nSee Also\nGetting a Joint's Pose\nvar jointModelTransforms: [simd_float4x4]\nThe model space transforms for each joint.\nfunc localTransform(for: ARSkeleton.JointName) -> simd_float4x4?\nReturns the local transform for a joint with a given name.\nfunc modelTransform(for: ARSkeleton.JointName) -> simd_float4x4?\nReturns the model transform for a joint with a given name."
  },
  {
    "title": "landmark(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton2d/3295990-landmark",
    "html": "Discussion\n\nJoint landmarks are normalized within the range [0..1] and are in the coordinate space of the current frame's camera image, where 0 is the upper left, and 1 is the bottom right.\n\nSee Also\nGetting Joint Landmarks\nvar jointLandmarks: [simd_float2]\nThe joint landmarks in normalized coordinates."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement/primitive/4131694",
    "html": "See Also\nInspecting geometry primitives\nvar description: String\nA textual description of a geometry primitive.\nvar hashValue: Int\nvar indexCount: Int\nfunc hash(into: inout Hasher)\nstatic func == (GeometryElement.Primitive, GeometryElement.Primitive) -> Bool"
  },
  {
    "title": "init(name:transform:extent:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arenvironmentprobeanchor/2984224-init",
    "html": "Parameters\nname\n\nA descriptive name for the anchor. ARKit doesn’t display the name to users, but your app can use it to identify anchors for debugging.\n\ntransform\n\nA matrix that encodes the position, orientation, and scale of the anchor, relative to the world coordinate space of the AR session in which you place the anchor.\n\nWorld coordinate space in ARKit always follows a right-handed convention, but is oriented based on the session configuration. For details, see Understanding World Tracking.\n\nextent\n\nThe area around the anchor's position that contains the textrure.\n\nAn environment probe anchor may have an infinite extent, which indicates that its texture is a global lighting environment, or a finite extent, which indicates that its texture represents the local lighting conditions in a specific area of the scene.\n\nDiscussion\n\nUse the add(anchor:) method to begin tracking your custom anchor in an AR session. After you add an environment probe anchor to the scene, ARKit begins generating environment textures for it. To be notified when the anchor has a new environmentTexture, implement the session(_:didUpdate:), renderer(_:didUpdate:for:), or view(_:didUpdate:for:) delegate method.\n\nSee Also\nCreating Probe Anchors\ninit(transform: simd_float4x4, extent: simd_float3)\nCreates a new environment probe anchor."
  },
  {
    "title": "jointLandmarks | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton2d/3295989-jointlandmarks",
    "html": "Discussion\n\nThe joint landmarks are normalized within the range [0..1] in the coordinate space of the current frame's camera image, where 0 is the upper left, and 1 is the bottom right.\n\nSee Also\nGetting Joint Landmarks\nfunc landmark(for: ARSkeleton.JointName) -> simd_float2?\nReturns the location of a joint with a given name."
  },
  {
    "title": "init(transform:extent:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arenvironmentprobeanchor/2984225-init",
    "html": "Parameters\ntransform\n\nA matrix that encodes the position, orientation, and scale of the anchor, relative to the world coordinate space of the AR session in which you place the anchor.\n\nWorld coordinate space in ARKit always follows a right-handed convention, but is oriented based on the session configuration. For details, see Understanding World Tracking.\n\nDiscussion\n\nUse the add(anchor:) method to begin tracking your custom anchor in an AR session. After you add an environment probe anchor to the scene, ARKit begins generating environment textures for it. To be notified when the anchor has a new environmentTexture, implement the session(_:didUpdate:), renderer(_:didUpdate:for:), or view(_:didUpdate:for:) delegate method.\n\nSee Also\nCreating Probe Anchors\ninit(name: String, transform: simd_float4x4, extent: simd_float3)\nCreates a new anchor object with a descriptive name."
  },
  {
    "title": "NotificationAction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/notificationaction",
    "html": "Overview\n\nUse this action to perform an app-provided operation on a list of prims.\n\nWhen this action executes, it posts a notification for which a custom app scans to perform a specific operation.\n\nDeclaration\nclass Preliminary_Action \"NotificationAction\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the action.\naffectedObjects\nA list of prims that respond to the notification.\nidentifier\nA string value that identifies the app-specific notification.\nSee Also\nActions\nPreliminary_Action\nA specific task that a trigger performs.\nAudioAction\nAn action that plays audio.\nChangeSceneAction\nAn action that transitions from one scene to another.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nGroupAction\nAn action that runs a list of other actions.\nImpulseAction\nAn action that adds velocity to an prim.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nOrbitAction\nAn action that orbits a set of prims around another.\nSpinAction\nAn action that spins a prim.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAction\nAn action that animates from one transform to another.\nTransformAnimationAction\nAn action that plays a transform animation.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nWaitAction\nAn action that performs a delay."
  },
  {
    "title": "ARKitSession.Error.Code.dataProviderFailedToRun | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/code/dataproviderfailedtorun",
    "html": "See Also\nDetermining the cause of session errors\ncase dataProviderNotAuthorized\nThe error code for when a data provider is missing at least one authorization it needs to run."
  },
  {
    "title": "CollideTrigger | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/collidetrigger",
    "html": "Overview\n\nThe runtime fires this trigger when any of the prims in the list of affectedObjects collide with any of the prims in colliders.\n\nIf all objects can collide with each other, define affectedObjects and colliders as the same list.\n\nDeclaration\nclass Preliminary_Trigger \"CollideTrigger\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the trigger.\naffectedObjects\nA list of prims that may come in contact with colliders.\ncolliders\nA list of prims that interact with objects to create a collision.\nSee Also\nTriggers\nPreliminary_Trigger\nA condition that, when met, performs an action.\nProximityToCameraTrigger\nA trigger that fires when the camera crosses the distance threshold of an object.\nSceneTransitionTrigger\nA trigger that fires during scene transitions.\nTapGestureTrigger\nA trigger that fires when the user taps.\nNotificationTrigger\nA trigger that fires when an app posts a notification."
  },
  {
    "title": "SceneTransitionTrigger | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/scenetransitiontrigger",
    "html": "Overview\n\nUse this trigger to perform actions after a scene loads. For more information about scenes, see sceneLibrary.\n\nDeclaration\nclass Preliminary_Trigger \"SceneTransitionTrigger\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the trigger.\ntype\nAn option that indicates the scene transition that activates the trigger.\nSee Also\nTriggers\nPreliminary_Trigger\nA condition that, when met, performs an action.\nCollideTrigger\nA trigger that activates when specified objects collide.\nProximityToCameraTrigger\nA trigger that fires when the camera crosses the distance threshold of an object.\nTapGestureTrigger\nA trigger that fires when the user taps.\nNotificationTrigger\nA trigger that fires when an app posts a notification."
  },
  {
    "title": "vertices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanegeometry/2949170-vertices",
    "html": "Discussion\n\nEach float3 value in this array represents the position of a vertex in the mesh. The owning plane anchor's transform matrix defines the coordinate system for these points.\n\nThis array, together with the triangleIndices array, describes a mesh covering the entire surface of the plane. Use this mesh for purposes that involve the filled shape, such as rendering a solid 3D representation of the surface. If, instead, you only need to know the outline of the shape, see the boundaryVertices property.\n\nSee Also\nAccessing Mesh Data\nvar textureCoordinates: [vector_float2]\nAn array of texture coordinate values for each point in the plane mesh.\nvar triangleCount: Int\nThe number of triangles described by the triangleIndices buffer.\nvar triangleIndices: [Int16]\nAn array of indices describing the triangle mesh formed by the plane geometry's vertex data."
  },
  {
    "title": "TapGestureTrigger | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/tapgesturetrigger",
    "html": "Overview\n\nFor an example, see Trigger animation for a tapped cube.\n\nDeclaration\nclass Preliminary_Trigger \"TapGestureTrigger\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the trigger.\naffectedObjects\nA list of prims that fire the trigger.\nSee Also\nTriggers\nPreliminary_Trigger\nA condition that, when met, performs an action.\nCollideTrigger\nA trigger that activates when specified objects collide.\nProximityToCameraTrigger\nA trigger that fires when the camera crosses the distance threshold of an object.\nSceneTransitionTrigger\nA trigger that fires during scene transitions.\nNotificationTrigger\nA trigger that fires when an app posts a notification."
  },
  {
    "title": "NotificationTrigger | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/notificationtrigger",
    "html": "Overview\n\nThis class enables an asset to fire a trigger as indicated by an app. The app defines the criteria for when the runtime executes the trigger’s action.\n\nDeclaration\nclass Preliminary_Trigger \"NotificationTrigger\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the trigger.\nidentifier\nA string value that identifies an app-specific notification.\nSee Also\nTriggers\nPreliminary_Trigger\nA condition that, when met, performs an action.\nCollideTrigger\nA trigger that activates when specified objects collide.\nProximityToCameraTrigger\nA trigger that fires when the camera crosses the distance threshold of an object.\nSceneTransitionTrigger\nA trigger that fires during scene transitions.\nTapGestureTrigger\nA trigger that fires when the user taps."
  },
  {
    "title": "ProximityToCameraTrigger | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/proximitytocameratrigger",
    "html": "Overview\n\nThe runtime fires this trigger one time when the distance threshold it specifies is met. The runtime can fire the trigger again only if the user moves away from the threshold and then returns.\n\nDeclaration\nclass Preliminary_Trigger \"ProximityToCameraTrigger\"\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for the trigger.\naffectedObjects\nA list of prims for which the runtime checks proximity.\ndistance\nA threshold that measures the user’s proximity to one or more prims.\nSee Also\nTriggers\nPreliminary_Trigger\nA condition that, when met, performs an action.\nCollideTrigger\nA trigger that activates when specified objects collide.\nSceneTransitionTrigger\nA trigger that fires during scene transitions.\nTapGestureTrigger\nA trigger that fires when the user taps.\nNotificationTrigger\nA trigger that fires when an app posts a notification."
  },
  {
    "title": "ARPlaneAnchor.Alignment.horizontal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/alignment/horizontal",
    "html": "Discussion\n\nThe transform property for a horizontal plane anchor includes no rotation about the x- or z-axis. Thus, using this anchor's transform to place a 3D model asset in your scene results in the model appearing \"right side up\".\n\nSee Also\nAlignment Values\ncase vertical\nThe plane is parallel to gravity."
  },
  {
    "title": "ARPlaneAnchor.Alignment.vertical | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/alignment/vertical",
    "html": "Discussion\n\nThe transform property for a vertical plane anchor includes a rotation component. That is, the transform matrix represents the result of rotating a horizontal plane to match the orientation of the detected surface.\n\nSee Also\nAlignment Values\ncase horizontal\nThe plane is perpendicular to gravity."
  },
  {
    "title": "triangleIndices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanegeometry/2941051-triangleindices",
    "html": "Discussion\n\nEach 16-bit integer value in this buffer represents an index into the vertices and textureCoordinates buffers. Each set of three indices identifies the vertices that form a single triangle in the mesh. You can use buffer as an index buffer for a triangle mesh in GPU-based rendering or to create 3D model asset files.\n\nEach set of three indices forms a triangle, so the number of indices in the triangleIndices buffer is three times the triangleCount value.\n\nSee Also\nAccessing Mesh Data\nvertices\nA buffer of vertex positions for each point in the plane mesh.\nvertexCount\nThe number of elements in the vertices buffer.\ntextureCoordinates\nA buffer of texture coordinate values for each point in the plane mesh.\ntextureCoordinateCount\nThe number of elements in the textureCoordinates buffer.\ntriangleCount\nThe number of triangles described by the triangleIndices buffer."
  },
  {
    "title": "Preliminary_Trigger | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers/preliminary_trigger",
    "html": "Overview\n\nBecause it inherits Typed, this schema declares a Preliminary_Trigger as a type of prim. For more information about typed schemas, see USD Specification > Typed.\n\nThe runtime executes triggers based on:\n\nUser input, like a user’s tap gesture.\n\nScene state, including a prim’s proximity to the user’s device.\n\nProgrammatic conditions, like application state or a function result.\n\nDeclaration\nclass \"Preliminary_Trigger\" (\n    inherits = </Typed>\n)\n\n\nAdd a tap trigger to a cube\n\nThe following example shows how a prim named TapCube opts in to notification of user taps.\n\n#usda 1.0\n\n\ndef Cube \"Cube\" {}\n\n\ndef Preliminary_Trigger \"TapCube\" {\n    uniform token info:id = \"TapGesture\"\n    rel affectedObjects = [ </Cube> ]\n}\n\n\nTopics\nProperties\ninfo:id\nA unique identifier for a particular type of trigger.\nSee Also\nTriggers\nCollideTrigger\nA trigger that activates when specified objects collide.\nProximityToCameraTrigger\nA trigger that fires when the camera crosses the distance threshold of an object.\nSceneTransitionTrigger\nA trigger that fires during scene transitions.\nTapGestureTrigger\nA trigger that fires when the user taps.\nNotificationTrigger\nA trigger that fires when an app posts a notification."
  },
  {
    "title": "ARFrame.SegmentationClass.none | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/segmentationclass/none",
    "html": "See Also\nClassifying Pixels\ncase person\nA classification of a pixel in the segmentation buffer as part of a person."
  },
  {
    "title": "ARCoachingOverlayView.Goal.tracking | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayview/goal/tracking",
    "html": "Discussion\n\nWhen you use this goal, coaching overlay won't hide until the user has moved their device in a way that facilitates ARKit starting up a basic world tracking session.\n\nSee Also\nDefining a Goal\ncase anyPlane\nA goal that specifies your app requires a plane of any type.\ncase horizontalPlane\nA goal that specifies your app requires a horizontal plane.\ncase verticalPlane\nA goal that specifies your app requires a vertical plane.\ncase geoTracking\nA goal that specifies your app requires a precise geographic location."
  },
  {
    "title": "ARConfidenceLevel | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfidencelevel",
    "html": "Topics\nLevels\ncase low\nDepth-value accuracy in which the framework is less confident.\ncase medium\nDepth-value accuracy in which the framework is moderately confident.\ncase high\nDepth-value accuracy in which the framework is fairly confident.\nComparing a Level\nstatic func < (ARConfidenceLevel, ARConfidenceLevel) -> Bool\nCompares two confidence levels.\nRelationships\nConforms To\nComparable\nSendable\nSee Also\nDepth Information\nvar depthMap: CVPixelBuffer\nThe estimated distance from the device to its environment, in meters.\nvar confidenceMap: CVPixelBuffer?\nThe framework’s confidence in the accuracy of the depth-map data."
  },
  {
    "title": "ARCoachingOverlayView.Goal.anyPlane | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayview/goal/anyplane",
    "html": "Discussion\n\nWhen you use this goal, coaching overlay won't hide until the user has moved their device in a way that facilitates ARKit finding at least one surface. For the available surface types, see ARPlaneAnchor.Classification.\n\nSee Also\nDefining a Goal\ncase horizontalPlane\nA goal that specifies your app requires a horizontal plane.\ncase tracking\nA goal that specifies your app requires basic world tracking.\ncase verticalPlane\nA goal that specifies your app requires a vertical plane.\ncase geoTracking\nA goal that specifies your app requires a precise geographic location."
  },
  {
    "title": "confidenceMap | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ardepthdata/3566295-confidencemap",
    "html": "Discussion\n\nThe natural light of the physical environment affects the depthMap property such that ARKit is less confident about the accuracy of the LiDAR Scanner’s depth measurements for surfaces that are highly reflective, or that have high light absorption. This property measures the accuracy of the scene depth-data by containing an ARConfidenceLevel raw-value for every component in depthMap.\n\nCustom renderers that process confidence data on the GPU should choose a MTLPixelFormat according to the confidenceMap pixel format the app reads at runtime. Call CVPixelBufferGetPixelFormatType(_:) on the confidenceMap to get its format. If, for example, the confidenceMap format is kCVPixelFormatType_OneComponent8 (OSType `L008`), create a Metal texture with format MTLPixelFormat.r8Uint to send confidence data to the GPU.\n\nSee Also\nDepth Information\nvar depthMap: CVPixelBuffer\nThe estimated distance from the device to its environment, in meters.\nenum ARConfidenceLevel\nDegrees to which the framework is confident about depth-data accuracy."
  },
  {
    "title": "estimatedHorizontalPlane | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arhittestresult/resulttype/2887460-estimatedhorizontalplane",
    "html": "Discussion\n\nARKit provides two ways to locate real-world flat surfaces in a scene. Plane detection (enabled with planeDetection on your session configuration) is an ongoing process, continuously analyzing the scene to accurately map the position and extent of any planes in view. Because plane detection takes time, you can fall back to plane estimation to get an instant, but less accurate, indication of whether a 2D point in the camera image corresponds to a real-world flat surface.\n\nBecause plane detection results are more accurate than plane estimation results, ARKit prefers the former when searching for both. If your hit-test search includes both estimatedHorizontalPlane and one or more existingPlane types, and the search finds any already detected plane anchors, the search returns only the existing plane(s) and no estimated plane.\n\nAn estimated plane search returns at most one result—the best estimate for a horizontal plane intersecting the hit-test ray.\n\nSee Also\nResult Types\nstatic var featurePoint: ARHitTestResult.ResultType\nA point on a surface detected by ARKit, but not part of any detected planes.\nDeprecated\nstatic var estimatedVerticalPlane: ARHitTestResult.ResultType\nA point on a real-world planar surface detected during the search, whose orientation is parallel to gravity.\nDeprecated\nstatic var existingPlane: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), without considering the plane's size.\nDeprecated\nstatic var existingPlaneUsingExtent: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), respecting the plane's estimated size.\nDeprecated\nstatic var existingPlaneUsingGeometry: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), respecting the plane's estimated size and shape.\nDeprecated"
  },
  {
    "title": "ARTrackingStateNormal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/artrackingstate/artrackingstatenormal",
    "html": "See Also\nTracking States\nARTrackingStateNotAvailable\nCamera position tracking is not available.\nARTrackingStateLimited\nTracking is available, but the quality of results is questionable."
  },
  {
    "title": "anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arhittestresult/2867905-anchor",
    "html": "Discussion\n\nResults of the featurePoint type do not include an anchor.\n\nSee Also\nIdentifying Results\nvar type: ARHitTestResult.ResultType\nThe kind of detected feature the search result represents.\nDeprecated\nstruct ARHitTestResult.ResultType\nPossible types for specifying a hit-test search, or for the result of a hit-test search.\nDeprecated"
  },
  {
    "title": "type | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arhittestresult/2875720-type",
    "html": "Discussion\n\nYou specify one or more result types to search for when calling a hit-testing method. A result object has only one result type.\n\nSee Also\nIdentifying Results\nstruct ARHitTestResult.ResultType\nPossible types for specifying a hit-test search, or for the result of a hit-test search.\nDeprecated\nvar anchor: ARAnchor?\nThe anchor representing the detected surface, if any.\nDeprecated\nRelated Documentation\nfunc hitTest(CGPoint, types: ARHitTestResult.ResultType) -> [ARHitTestResult]\nSearches for real-world objects or AR anchors in the captured camera image corresponding to a point in the SceneKit view.\nfunc hitTest(CGPoint, types: ARHitTestResult.ResultType) -> [ARHitTestResult]\nSearches for real-world objects or AR anchors in the captured camera image corresponding to a point in the SpriteKit view.\nfunc hitTest(CGPoint, types: ARHitTestResult.ResultType) -> [ARHitTestResult]\nSearches for real-world objects or AR anchors in the captured camera image."
  },
  {
    "title": "anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastresult/3194593-anchor",
    "html": "Discussion\n\nIf you chose an existing plane target, ARKit provides its anchor. If you choose an estimated plane target, ARKit provides an anchor only if the ray intersects an existing plane.\n\nSee Also\nIdentifying Results\nvar worldTransform: simd_float4x4\nThe position, rotation, and scale, of the ray's intersection with the target.\nvar target: ARRaycastQuery.Target\nThe type of surface that the ray intersects.\nenum ARRaycastQuery.Target\nThe types of surface you allow a raycast to intersect with.\nvar targetAlignment: ARRaycastQuery.TargetAlignment\nThe alignment of the plane that the ray intersected.\nenum ARRaycastQuery.TargetAlignment\nA specification that indicates a target's alignment with respect to gravity."
  },
  {
    "title": "distance | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arhittestresult/2867903-distance",
    "html": "See Also\nExamining Result Geometry\nvar worldTransform: simd_float4x4\nThe position and orientation of the result relative to the world coordinate system.\nDeprecated\nvar localTransform: simd_float4x4\nThe position and orientation of the result relative to the nearest anchor or feature point.\nDeprecated"
  },
  {
    "title": "worldTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arhittestresult/2867907-worldtransform",
    "html": "Discussion\n\nThis transform matrix indicates the intersection point between the detected surface and the ray that created the hit-test result. A hit-test projects a 2D point in the image or view coordinate system along a ray into the 3D world space and reports results where that line intersects detected surfaces.\n\nThe session configuration's worldAlignment property defines the world coordinate system.\n\nSee Also\nExamining Result Geometry\nvar distance: CGFloat\nThe distance, in meters, from the camera to the detected surface.\nDeprecated\nvar localTransform: simd_float4x4\nThe position and orientation of the result relative to the nearest anchor or feature point.\nDeprecated"
  },
  {
    "title": "localTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arhittestresult/2867904-localtransform",
    "html": "Discussion\n\nThis transform matrix indicates the intersection point between the detected surface and the ray that created the hit-test result. A hit-test projects a 2D point in the image or view coordinate system along a ray into the 3D world space and reports results where that line intersects detected surfaces.\n\nSee Also\nExamining Result Geometry\nvar distance: CGFloat\nThe distance, in meters, from the camera to the detected surface.\nDeprecated\nvar worldTransform: simd_float4x4\nThe position and orientation of the result relative to the world coordinate system.\nDeprecated"
  },
  {
    "title": "ARWorldTrackingConfiguration.EnvironmentTexturing.manual | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/environmenttexturing/manual",
    "html": "Discussion\n\nWhen you use this environmentTexturing option, you must manually choose when and where to generate environment map textures:\n\nCreate an AREnvironmentProbeAnchor object with a transform indicating its position in the scene.\n\nAdd the probe anchor to the session with the add(anchor:) method.\n\nIf you display AR content using ARSCNView, SceneKit automatically retrieves texture maps from probe anchors and uses them to light the scene. Otherwise, use a delegate method such as session(_:didUpdate:) to find out when the probe anchor's texture has been updated and access the environmentTexture property.\n\nSee Also\nEnvironment Texture Options\ncase none\nThe framework doesn’t generate environment textures.\ncase automatic\nThe framework automatically determines when and where to generate environment textures."
  },
  {
    "title": "ARGeoTrackingStatus.State.notAvailable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/state/notavailable",
    "html": "Discussion\n\nThis state occurs when ARKit doesn’t have the landscape data necessary for visual localization at the user’s location. For more information, see ARGeoTrackingStatus.State.localizing.\n\nSee Also\nStates\ncase initializing\nThe session is initializing geo tracking.\ncase localized\nGeo tracking is localized.\ncase localizing\nGeo tracking is attempting to localize against a map."
  },
  {
    "title": "ARHitTestResult.ResultType | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arhittestresult/resulttype",
    "html": "Topics\nCreating a Result Type\ninit(rawValue: UInt)\nCreates a result type.\nResult Types\nstatic var featurePoint: ARHitTestResult.ResultType\nA point on a surface detected by ARKit, but not part of any detected planes.\nstatic var estimatedHorizontalPlane: ARHitTestResult.ResultType\nA point on a real-world planar surface detected during the search, whose orientation is perpendicular to gravity.\nstatic var estimatedVerticalPlane: ARHitTestResult.ResultType\nA point on a real-world planar surface detected during the search, whose orientation is parallel to gravity.\nstatic var existingPlane: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), without considering the plane's size.\nstatic var existingPlaneUsingExtent: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), respecting the plane's estimated size.\nstatic var existingPlaneUsingGeometry: ARHitTestResult.ResultType\nA point on a real-world plane (already detected with the planeDetection option), respecting the plane's estimated size and shape.\nRelationships\nConforms To\nOptionSet\nSendable\nSee Also\nIdentifying Results\nvar type: ARHitTestResult.ResultType\nThe kind of detected feature the search result represents.\nDeprecated\nvar anchor: ARAnchor?\nThe anchor representing the detected surface, if any.\nDeprecated"
  },
  {
    "title": "target | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastresult/3132060-target",
    "html": "See Also\nIdentifying Results\nvar worldTransform: simd_float4x4\nThe position, rotation, and scale, of the ray's intersection with the target.\nvar anchor: ARAnchor?\nThe anchor for the plane that the ray intersected.\nenum ARRaycastQuery.Target\nThe types of surface you allow a raycast to intersect with.\nvar targetAlignment: ARRaycastQuery.TargetAlignment\nThe alignment of the plane that the ray intersected.\nenum ARRaycastQuery.TargetAlignment\nA specification that indicates a target's alignment with respect to gravity."
  },
  {
    "title": "targetAlignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastresult/3132061-targetalignment",
    "html": "See Also\nIdentifying Results\nvar worldTransform: simd_float4x4\nThe position, rotation, and scale, of the ray's intersection with the target.\nvar anchor: ARAnchor?\nThe anchor for the plane that the ray intersected.\nvar target: ARRaycastQuery.Target\nThe type of surface that the ray intersects.\nenum ARRaycastQuery.Target\nThe types of surface you allow a raycast to intersect with.\nenum ARRaycastQuery.TargetAlignment\nA specification that indicates a target's alignment with respect to gravity."
  },
  {
    "title": "tongueOut | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2968190-tongueout",
    "html": "Discussion\n\nA value of 0.0 indicates that the tongue is fully inside the mouth; a value of 1.0 indicates that the tongue is as far out of the mouth as ARKit tracks."
  },
  {
    "title": "triangleIndices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacegeometry/2928199-triangleindices",
    "html": "Discussion\n\nEach 16-bit integer value in this buffer represents an index into the vertices and textureCoordinates buffers. Each set of three indices identifies the vertices that form a single triangle in the mesh. (That is, this buffer is appropriate for use as an index buffer for a triangle mesh in GPU-based rendering or creating 3D model asset files.)\n\nEach set of three indices forms a triangle, so the number of indices in the triangleIndices buffer is three times the triangleCount value.\n\nFace mesh topology is constant across ARFaceGeometry instances, so this buffer always describes the same arrangement of vertices. Only the vertices buffer changes between face meshes provided by an AR session, indicating the change in vertex positions as ARKit adapts the mesh to the shape and expression of the user's face.\n\nSee Also\nAccessing Mesh Data\nvertexCount\nThe number of elements in the vertices buffer.\nvertices\nA buffer of vertex positions for each point in the face mesh.\ntextureCoordinateCount\nThe number of elements in the textureCoordinates buffer.\ntextureCoordinates\nA buffer of texture coordinate values for each point in the face mesh.\ntriangleCount\nThe number of triangles described by the triangleIndices buffer."
  },
  {
    "title": "vertices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacegeometry/2928201-vertices",
    "html": "Discussion\n\nEach float3 value in this buffer represents the position of a vertex in the mesh, in face coordinates. (See Tracking Face Position and Orientation.)\n\nThe vertexCount property provides the number of elements in the buffer.\n\nSee Also\nAccessing Mesh Data\nvertexCount\nThe number of elements in the vertices buffer.\ntextureCoordinateCount\nThe number of elements in the textureCoordinates buffer.\ntextureCoordinates\nA buffer of texture coordinate values for each point in the face mesh.\ntriangleCount\nThe number of triangles described by the triangleIndices buffer.\ntriangleIndices\nA buffer of indices describing the triangle mesh formed by the face geometry's vertex data."
  },
  {
    "title": "textureCoordinates | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacegeometry/2928203-texturecoordinates",
    "html": "Discussion\n\nEach float2 value in this buffer represents the UV texture coordinates for the vertex at the corresponding index in the vertices buffer.\n\nThe textureCoordinateCount property provides the number of elements in the buffer.\n\nFace mesh topology is constant across ARFaceGeometry instances, so the data in this buffer always maps the same vertex indices to the same texture coordinates.\n\nSee Also\nAccessing Mesh Data\nvertexCount\nThe number of elements in the vertices buffer.\nvertices\nA buffer of vertex positions for each point in the face mesh.\ntextureCoordinateCount\nThe number of elements in the textureCoordinates buffer.\ntriangleCount\nThe number of triangles described by the triangleIndices buffer.\ntriangleIndices\nA buffer of indices describing the triangle mesh formed by the face geometry's vertex data."
  },
  {
    "title": "textureCoordinateCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacegeometry/2928197-texturecoordinatecount",
    "html": "Discussion\n\nFace mesh topology is constant across ARFaceGeometry instances, so the value of this property is the same for all instances.\n\nSee Also\nAccessing Mesh Data\nvertexCount\nThe number of elements in the vertices buffer.\nvertices\nA buffer of vertex positions for each point in the face mesh.\ntextureCoordinates\nA buffer of texture coordinate values for each point in the face mesh.\ntriangleCount\nThe number of triangles described by the triangleIndices buffer.\ntriangleIndices\nA buffer of indices describing the triangle mesh formed by the face geometry's vertex data."
  },
  {
    "title": "update(from:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnfacegeometry/2928196-update",
    "html": "Parameters\nfaceGeometry\n\nA coarse mesh representation of a face’s topology, dimensions, and expression.\n\nDiscussion\n\nTo update a SceneKit model of a face actively tracked in an AR session, call this method in your ARSCNViewDelegate object’s renderer(_:didUpdate:for:) callback, passing the geometry property from the ARFaceAnchor object that callback provides.\n\nAlternatively, you can create, configure, and visualize face models independent of an AR session by creating face geometry objects using the ARFaceGeometry init(blendShapes:) initializer and passing them to this method."
  },
  {
    "title": "vertexCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacegeometry/2928206-vertexcount",
    "html": "Discussion\n\nFace mesh topology is constant across ARFaceGeometry instances, so the value of this property is the same for all instances.\n\nSee Also\nAccessing Mesh Data\nvertices\nA buffer of vertex positions for each point in the face mesh.\ntextureCoordinateCount\nThe number of elements in the textureCoordinates buffer.\ntextureCoordinates\nA buffer of texture coordinate values for each point in the face mesh.\ntriangleCount\nThe number of triangles described by the triangleIndices buffer.\ntriangleIndices\nA buffer of indices describing the triangle mesh formed by the face geometry's vertex data."
  },
  {
    "title": "vertices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacegeometry/2931116-vertices",
    "html": "Discussion\n\nEach float3 value in this array represents the position of a vertex in the mesh, in face coordinates. (See Tracking Face Position and Orientation.)\n\nSee Also\nAccessing Mesh Data\nvar textureCoordinates: [vector_float2]\nAn array of texture coordinate values for each point in the face mesh.\nvar triangleCount: Int\nThe number of triangles described by the triangleIndices buffer.\nvar triangleIndices: [Int16]\nAn array of indices describing the triangle mesh formed by the face geometry's vertex data."
  },
  {
    "title": "textureCoordinates | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacegeometry/2931118-texturecoordinates",
    "html": "Discussion\n\nEach float2 value in this array represents the UV texture coordinates for the vertex at the corresponding index in the vertices buffer.Face mesh topology is constant across ARFaceGeometry instances, so the values in this array always maps the same vertex indices to the same texture coordinates.\n\nSee Also\nAccessing Mesh Data\nvar vertices: [simd_float3]\nAn array of vertex positions for each point in the face mesh.\nvar triangleCount: Int\nThe number of triangles described by the triangleIndices buffer.\nvar triangleIndices: [Int16]\nAn array of indices describing the triangle mesh formed by the face geometry's vertex data."
  },
  {
    "title": "triangleCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacegeometry/2928207-trianglecount",
    "html": "Discussion\n\nEach set of three indices forms a triangle, so the number of indices in the triangleIndices buffer is three times the triangleCount value.\n\nFace mesh topology is constant across ARFaceGeometry instances, so the value of this property is the same for all instances.\n\nSee Also\nAccessing Mesh Data\nvar vertices: [simd_float3]\nAn array of vertex positions for each point in the face mesh.\nvar textureCoordinates: [vector_float2]\nAn array of texture coordinate values for each point in the face mesh.\nvar triangleIndices: [Int16]\nAn array of indices describing the triangle mesh formed by the face geometry's vertex data."
  },
  {
    "title": "triangleIndices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacegeometry/2931117-triangleindices",
    "html": "Discussion\n\nEach 16-bit integer value in this array represents an index into the vertices and textureCoordinates buffers. Each set of three indices identifies the vertices that form a single triangle in the mesh. (That is, this array is appropriate for use as an index buffer for a triangle mesh in GPU-based rendering or creating 3D model asset files.)\n\nEach set of three indices forms a triangle, so the number of indices in the triangleIndices buffer is three times the triangleCount value.\n\nFace mesh topology is constant across ARFaceGeometry instances, so this array always describes the same arrangement of vertices. Only the vertices array changes between face meshes provided by an AR session, indicating the change in vertex positions as ARKit adapts the mesh to the shape and expression of the user's face.\n\nSee Also\nAccessing Mesh Data\nvar vertices: [simd_float3]\nAn array of vertex positions for each point in the face mesh.\nvar textureCoordinates: [vector_float2]\nAn array of texture coordinate values for each point in the face mesh.\nvar triangleCount: Int\nThe number of triangles described by the triangleIndices buffer."
  },
  {
    "title": "noseSneerLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928263-nosesneerleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nstatic let browDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nstatic let browDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nstatic let browInnerUp: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nstatic let browOuterUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nstatic let browOuterUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nstatic let cheekPuff: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of both cheeks.\nstatic let cheekSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the left eye.\nstatic let cheekSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the right eye.\nstatic let noseSneerRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "cheekSquintRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928238-cheeksquintright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nstatic let browDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nstatic let browDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nstatic let browInnerUp: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nstatic let browOuterUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nstatic let browOuterUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nstatic let cheekPuff: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of both cheeks.\nstatic let cheekSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the left eye.\nstatic let noseSneerLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the left side of the nose around the nostril.\nstatic let noseSneerRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "cheekPuff | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928232-cheekpuff",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nstatic let browDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nstatic let browDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nstatic let browInnerUp: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nstatic let browOuterUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nstatic let browOuterUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nstatic let cheekSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the left eye.\nstatic let cheekSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the right eye.\nstatic let noseSneerLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the left side of the nose around the nostril.\nstatic let noseSneerRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "browOuterUpLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928274-browouterupleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nstatic let browDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nstatic let browDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nstatic let browInnerUp: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nstatic let browOuterUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nstatic let cheekPuff: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of both cheeks.\nstatic let cheekSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the left eye.\nstatic let cheekSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the right eye.\nstatic let noseSneerLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the left side of the nose around the nostril.\nstatic let noseSneerRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "browOuterUpRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928255-browouterupright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nstatic let browDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nstatic let browDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nstatic let browInnerUp: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nstatic let browOuterUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nstatic let cheekPuff: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of both cheeks.\nstatic let cheekSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the left eye.\nstatic let cheekSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the right eye.\nstatic let noseSneerLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the left side of the nose around the nostril.\nstatic let noseSneerRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "mouthUpperUpRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928247-mouthupperupright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side."
  },
  {
    "title": "mouthLowerDownLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928242-mouthlowerdownleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "mouthStretchRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928268-mouthstretchright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "mouthRollLower | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928224-mouthrolllower",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "mouthShrugUpper | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928276-mouthshrugupper",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "mouthPressRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928245-mouthpressright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "mouthRollUpper | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928227-mouthrollupper",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "mouthShrugLower | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928230-mouthshruglower",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "browInnerUp | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928264-browinnerup",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nstatic let browDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nstatic let browDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nstatic let browOuterUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nstatic let browOuterUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nstatic let cheekPuff: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of both cheeks.\nstatic let cheekSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the left eye.\nstatic let cheekSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the right eye.\nstatic let noseSneerLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the left side of the nose around the nostril.\nstatic let noseSneerRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "mouthUpperUpLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928240-mouthupperupleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "mouthLowerDownRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928275-mouthlowerdownright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "browDownRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928254-browdownright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nEyebrows, Cheeks, and Nose\nstatic let browDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nstatic let browInnerUp: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nstatic let browOuterUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nstatic let browOuterUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nstatic let cheekPuff: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of both cheeks.\nstatic let cheekSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the left eye.\nstatic let cheekSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the right eye.\nstatic let noseSneerLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the left side of the nose around the nostril.\nstatic let noseSneerRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the right side of the nose around the nostril."
  },
  {
    "title": "mouthPressLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928241-mouthpressleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "jointLocalTransforms | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton3d/3229923-jointlocaltransforms",
    "html": "Discussion\n\nLocal space refers to a joint's position relative to its parent joint.\n\nSee Also\nGetting a Joint's Pose\njointModelTransforms\nThe model space transforms for each joint.\n- localTransformForJointName:\nReturns the local transform for a joint with a given name.\n- modelTransformForJointName:\nReturns the model transform for a joint with a given name."
  },
  {
    "title": "landmarkForJointNamed: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton2d/3229921-landmarkforjointnamed",
    "html": "Discussion\n\nJoint landmarks are normalized within the range [0..1] and are in the coordinate space of the current frame's camera image, where 0 is the upper left, and 1 is the bottom right.\n\nSee Also\nGetting Joint Landmarks\njointLandmarks\nThe joint landmarks in normalized coordinates."
  },
  {
    "title": "mouthFrownLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928277-mouthfrownleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "mouthDimpleLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928231-mouthdimpleleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "mouthDimpleRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928259-mouthdimpleright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "eyeLookInLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928260-eyelookinleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nLeft Eye\nstatic let eyeBlinkLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the eyelids over the left eye.\nstatic let eyeLookDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a downward gaze.\nstatic let eyeLookOutLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a leftward gaze.\nstatic let eyeLookUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with an upward gaze.\nstatic let eyeSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of the face around the left eye.\nstatic let eyeWideLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a widening of the eyelids around the left eye."
  },
  {
    "title": "eyeBlinkLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928261-eyeblinkleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nLeft Eye\nstatic let eyeLookDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a downward gaze.\nstatic let eyeLookInLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a rightward gaze.\nstatic let eyeLookOutLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a leftward gaze.\nstatic let eyeLookUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with an upward gaze.\nstatic let eyeSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of the face around the left eye.\nstatic let eyeWideLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a widening of the eyelids around the left eye."
  },
  {
    "title": "eyeLookDownLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928234-eyelookdownleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nLeft Eye\nstatic let eyeBlinkLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the eyelids over the left eye.\nstatic let eyeLookInLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a rightward gaze.\nstatic let eyeLookOutLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a leftward gaze.\nstatic let eyeLookUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with an upward gaze.\nstatic let eyeSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of the face around the left eye.\nstatic let eyeWideLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a widening of the eyelids around the left eye."
  },
  {
    "title": "jawRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928248-jawright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "HandAnchor.Chirality.left | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/chirality/left",
    "html": "See Also\nGetting hand chirality\ncase right\nA right hand."
  },
  {
    "title": "eyeLookOutLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928269-eyelookoutleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nLeft Eye\nstatic let eyeBlinkLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the eyelids over the left eye.\nstatic let eyeLookDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a downward gaze.\nstatic let eyeLookInLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a rightward gaze.\nstatic let eyeLookUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with an upward gaze.\nstatic let eyeSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of the face around the left eye.\nstatic let eyeWideLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a widening of the eyelids around the left eye."
  },
  {
    "title": "eyeLookUpLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928250-eyelookupleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nLeft Eye\nstatic let eyeBlinkLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the eyelids over the left eye.\nstatic let eyeLookDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a downward gaze.\nstatic let eyeLookInLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a rightward gaze.\nstatic let eyeLookOutLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a leftward gaze.\nstatic let eyeSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of the face around the left eye.\nstatic let eyeWideLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a widening of the eyelids around the left eye."
  },
  {
    "title": "eyeSquintLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928252-eyesquintleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nLeft Eye\nstatic let eyeBlinkLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the eyelids over the left eye.\nstatic let eyeLookDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a downward gaze.\nstatic let eyeLookInLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a rightward gaze.\nstatic let eyeLookOutLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a leftward gaze.\nstatic let eyeLookUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with an upward gaze.\nstatic let eyeWideLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a widening of the eyelids around the left eye."
  },
  {
    "title": "eyeWideLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928233-eyewideleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nLeft Eye\nstatic let eyeBlinkLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the eyelids over the left eye.\nstatic let eyeLookDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a downward gaze.\nstatic let eyeLookInLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a rightward gaze.\nstatic let eyeLookOutLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a leftward gaze.\nstatic let eyeLookUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with an upward gaze.\nstatic let eyeSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of the face around the left eye."
  },
  {
    "title": "eyeBlinkRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928262-eyeblinkright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nRight Eye\nstatic let eyeLookDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a downward gaze.\nstatic let eyeLookInRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a leftward gaze.\nstatic let eyeLookOutRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a rightward gaze.\nstatic let eyeLookUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with an upward gaze.\nstatic let eyeSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of the face around the right eye.\nstatic let eyeWideRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a widening of the eyelids around the right eye."
  },
  {
    "title": "eyeLookInRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928243-eyelookinright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nRight Eye\nstatic let eyeBlinkRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the eyelids over the right eye.\nstatic let eyeLookDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a downward gaze.\nstatic let eyeLookOutRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a rightward gaze.\nstatic let eyeLookUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with an upward gaze.\nstatic let eyeSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of the face around the right eye.\nstatic let eyeWideRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a widening of the eyelids around the right eye."
  },
  {
    "title": "eyeLookUpRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928258-eyelookupright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nRight Eye\nstatic let eyeBlinkRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the eyelids over the right eye.\nstatic let eyeLookDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a downward gaze.\nstatic let eyeLookInRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a leftward gaze.\nstatic let eyeLookOutRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a rightward gaze.\nstatic let eyeSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of the face around the right eye.\nstatic let eyeWideRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a widening of the eyelids around the right eye."
  },
  {
    "title": "mouthClose | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928266-mouthclose",
    "html": "Discussion\n\nThis coefficient describes a closing of the lips without relation to the position of the jaw (the jawOpen coefficient), so some values of the mouthClose coefficient can produce unrealistic facial expressions unless other coefficients are also set to realistic values.\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in three states:\n\nA neutral face (all ARFaceAnchor.BlendShapeLocation coefficient values at 0.0, including both jawOpen and mouthClose)\n\nSetting only the jawOpen coefficient to 1.0, while keeping all other coefficient values (including mouthClose) at 0.0\n\nSetting both the jawOpen and mouthClose coefficients to 1.0, while keeping all other coefficient values at 0.0\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "jawForward | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928229-jawforward",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "jawOpen | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928236-jawopen",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "eyeSquintRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928237-eyesquintright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nRight Eye\nstatic let eyeBlinkRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the eyelids over the right eye.\nstatic let eyeLookDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a downward gaze.\nstatic let eyeLookInRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a leftward gaze.\nstatic let eyeLookOutRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a rightward gaze.\nstatic let eyeLookUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with an upward gaze.\nstatic let eyeWideRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a widening of the eyelids around the right eye."
  },
  {
    "title": "eyeLookDownRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928272-eyelookdownright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nRight Eye\nstatic let eyeBlinkRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the eyelids over the right eye.\nstatic let eyeLookInRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a leftward gaze.\nstatic let eyeLookOutRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a rightward gaze.\nstatic let eyeLookUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with an upward gaze.\nstatic let eyeSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of the face around the right eye.\nstatic let eyeWideRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a widening of the eyelids around the right eye."
  },
  {
    "title": "mouthLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928228-mouthleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "eyeWideRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928267-eyewideright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nRight Eye\nstatic let eyeBlinkRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the eyelids over the right eye.\nstatic let eyeLookDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a downward gaze.\nstatic let eyeLookInRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a leftward gaze.\nstatic let eyeLookOutRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a rightward gaze.\nstatic let eyeLookUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with an upward gaze.\nstatic let eyeSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of the face around the right eye."
  },
  {
    "title": "eyeLookOutRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928265-eyelookoutright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nRight Eye\nstatic let eyeBlinkRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the eyelids over the right eye.\nstatic let eyeLookDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a downward gaze.\nstatic let eyeLookInRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a leftward gaze.\nstatic let eyeLookUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with an upward gaze.\nstatic let eyeSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of the face around the right eye.\nstatic let eyeWideRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a widening of the eyelids around the right eye."
  },
  {
    "title": "mouthRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928246-mouthright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "mouthPucker | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928257-mouthpucker",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "mouthFunnel | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928253-mouthfunnel",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "mouthSmileLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928249-mouthsmileleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARFrame.SegmentationClass.person | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/segmentationclass/person",
    "html": "See Also\nClassifying Pixels\ncase none\nA classification of a pixel in the segmentation buffer as unidentified."
  },
  {
    "title": "ARWorldTrackingConfiguration.EnvironmentTexturing.automatic | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/environmenttexturing/automatic",
    "html": "Discussion\n\nWhen you use this environmentTexturing option, ARKit automatically chooses positions in the scene to generate environment textures based on the camera imagery it has collected and the other anchors you've placed.\n\nIf you display AR content using ARSCNView, SceneKit automatically retrieves texture maps from probe anchors and uses them to light the scene. Otherwise, use a delegate method such as session(_:didUpdate:) to find out when the probe anchor's texture has been updated and access the environmentTexture property.\n\nSee Also\nEnvironment Texture Options\ncase none\nThe framework doesn’t generate environment textures.\ncase manual\nThe framework generates environment textures only for probe anchors you explicitly add to the session."
  },
  {
    "title": "ARWorldTrackingConfiguration.EnvironmentTexturing.none | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/environmenttexturing/none",
    "html": "See Also\nEnvironment Texture Options\ncase manual\nThe framework generates environment textures only for probe anchors you explicitly add to the session.\ncase automatic\nThe framework automatically determines when and where to generate environment textures."
  },
  {
    "title": "ARRaycastQuery.TargetAlignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery/targetalignment",
    "html": "Overview\n\nA raycast ignores potential targets with an alignment different than the one you specify in the raycast query.\n\nTopics\nChoosing a Target Alignment\ncase any\nThe case that indicates a target may be aligned in any way with respect to gravity.\ncase horizontal\nThe case that indicates a target is aligned horizontally with respect to gravity.\ncase vertical\nThe case that indicates a target is aligned vertically with respect to gravity.\nRelationships\nConforms To\nSendable\nSee Also\nSpecifying the Target\nvar target: ARRaycastQuery.Target\nA plane type that allows the raycast to terminate if it's encountered.\nenum ARRaycastQuery.Target\nThe types of surface you allow a raycast to intersect with.\nvar targetAlignment: ARRaycastQuery.TargetAlignment\nThe target's alignment with respect to gravity."
  },
  {
    "title": "ARRaycastQuery.Target | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery/target",
    "html": "Topics\nTargets\ncase estimatedPlane\nA raycast target that specifies nonplanar surfaces, or planes about which ARKit can only estimate.\ncase existingPlaneGeometry\nA raycast target that requires a plane to have a definitive size and shape.\ncase existingPlaneInfinite\nA raycast target that specifies a detected plane, regardless of its size and shape.\nRelationships\nConforms To\nSendable\nSee Also\nSpecifying the Target\nvar target: ARRaycastQuery.Target\nA plane type that allows the raycast to terminate if it's encountered.\nvar targetAlignment: ARRaycastQuery.TargetAlignment\nThe target's alignment with respect to gravity.\nenum ARRaycastQuery.TargetAlignment\nA specification that indicates a target's alignment with respect to gravity."
  },
  {
    "title": "environmentTexture | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arenvironmentprobeanchor/2977511-environmenttexture",
    "html": "Discussion\n\nThis texture is in the format MTLPixelFormat.bgra8Unorm_srgb."
  },
  {
    "title": "ARGeoTrackingStatus.State.initializing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/state/initializing",
    "html": "Discussion\n\nIn this state, the session is preparing tracking and an app has the opportunity to onboard users to the experience. The app watches for changes in stateReason and coaches the user accordingly to expedite initialization.\n\nSee Also\nStates\ncase localized\nGeo tracking is localized.\ncase localizing\nGeo tracking is attempting to localize against a map.\ncase notAvailable\nGeo tracking is not available."
  },
  {
    "title": "ambientColorTemperature | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arlightestimate/2921629-ambientcolortemperature",
    "html": "Discussion\n\nThis value is based on the internal white balance compensation of the camera device, and scaled to be appropriate for use in rendering architectures that use realistic lighting metrics. A value of 6500 represents neutral (pure white) lighting; lower values indicate a \"warmer\" yellow or orange tint, and higher values indicate a \"cooler\" blue tint.\n\nFor example, you can pass this value directly to the temperature property of a SceneKit ambient light for lighting results that roughly match those of the real-world scene captured by the device camera. (However, passing this value to SceneKit is generally not necessary; the ARSCNView class automatically sets SceneKit lighting based on this value.)\n\nSee Also\nExamining Light Parameters\nvar ambientIntensity: CGFloat\nThe estimated intensity, in lumens, of ambient light throughout the scene."
  },
  {
    "title": "ambientIntensity | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arlightestimate/2878308-ambientintensity",
    "html": "Discussion\n\nThis value is based on the internal exposure compensation of the camera device, and scaled to be appropriate for use in rendering architectures that use realistic lighting metrics. A value of 1000 represents \"neutral\" lighting.\n\nFor example, you can pass this value directly to the intensity property of a SceneKit ambient light for lighting results that roughly match those of the real-world scene captured by the device camera. (However, passing this value to SceneKit is generally not necessary; the ARSCNView class automatically sets SceneKit lighting based on this value.)\n\nSee Also\nExamining Light Parameters\nvar ambientColorTemperature: CGFloat\nThe estimated color temperature, in degrees Kelvin, of ambient light throughout the scene."
  },
  {
    "title": "horizontal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/planedetection/2867273-horizontal",
    "html": "See Also\nPlane Detection Options\nstatic var vertical: ARWorldTrackingConfiguration.PlaneDetection\nThe session detects surfaces that are parallel to gravity, regardless of other orientation."
  },
  {
    "title": "vertical | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/planedetection/2867271-vertical",
    "html": "See Also\nPlane Detection Options\nstatic var horizontal: ARWorldTrackingConfiguration.PlaneDetection\nThe session detects planar surfaces that are perpendicular to gravity."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/4108473",
    "html": "See Also\nComparing plane classifications\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (PlaneAnchor.Classification, PlaneAnchor.Classification) -> Bool"
  },
  {
    "title": "jointCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletondefinition/3340471-jointcount",
    "html": "See Also\nGetting Joint Information\nvar jointNames: [String]\nA collection of unique joint names.\nfunc index(for: ARSkeleton.JointName) -> Int\nReturns the index for a given joint identifier.\nvar parentIndices: [Int]\nThe parent index for each joint."
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/chirality/4108400-hashvalue",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nComparing hand chirality\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (HandAnchor.Chirality, HandAnchor.Chirality) -> Bool\nstatic func != (HandAnchor.Chirality, HandAnchor.Chirality) -> Bool"
  },
  {
    "title": "renderer(_:willUpdate:for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnviewdelegate/2865792-renderer",
    "html": "Parameters\nrenderer\n\nThe ARSCNView object rendering the scene.\n\nnode\n\nThe updated SceneKit node.\n\nanchor\n\nThe AR anchor corresponding to the node.\n\nDiscussion\n\nDepending on the session configuration, ARKit may automatically update anchors in a session. The view calls this method once for each updated anchor.\n\nSee Also\nHandling Content Updates\nfunc renderer(SCNSceneRenderer, nodeFor: ARAnchor) -> SCNNode?\nAsks the delegate to provide a SceneKit node corresponding to a newly added anchor.\nfunc renderer(SCNSceneRenderer, didAdd: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node corresponding to a new AR anchor has been added to the scene.\nfunc renderer(SCNSceneRenderer, didUpdate: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node's properties have been updated to match the current state of its corresponding anchor.\nfunc renderer(SCNSceneRenderer, didRemove: SCNNode, for: ARAnchor)\nTells the delegate that the SceneKit node corresponding to a removed AR anchor has been removed from the scene."
  },
  {
    "title": "mouthSmileRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928244-mouthsmileright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "renderer(_:nodeFor:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnviewdelegate/2865801-renderer",
    "html": "Parameters\nrenderer\n\nThe ARSCNView object rendering the scene.\n\nanchor\n\nThe anchor for which a node is requested.\n\nReturn Value\n\nA new SceneKit node, which ARKit will add to the scene and update to follow its corresponding AR anchor.\n\nDiscussion\n\nDepending on the session configuration, ARKit may automatically add anchors to a session. ARKit also calls this method to provide visual content for any ARAnchor objects you manually add using the session's add(anchor:) method.\n\nYou can implement this method to provide a new SCNNode object (or instance of an SCNNode subclass) containing any attachments you plan to use as a visual representation of the anchor. Note that ARKit controls the node's visibility and its transform property, so you may find it useful to add child nodes or adjust the node's pivot property to maintain any changes to position or orientation that you make.\n\nIf you return nil from this method, no node is added to the scene.\n\nAlternatively, if you do not implement this method, ARKit creates an empty node, and you can implement the renderer(_:didAdd:for:) method instead to provide visual content by attaching it to that node.\n\nSee Also\nHandling Content Updates\nfunc renderer(SCNSceneRenderer, didAdd: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node corresponding to a new AR anchor has been added to the scene.\nfunc renderer(SCNSceneRenderer, willUpdate: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node's properties will be updated to match the current state of its corresponding anchor.\nfunc renderer(SCNSceneRenderer, didUpdate: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node's properties have been updated to match the current state of its corresponding anchor.\nfunc renderer(SCNSceneRenderer, didRemove: SCNNode, for: ARAnchor)\nTells the delegate that the SceneKit node corresponding to a removed AR anchor has been removed from the scene."
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/chirality/4108399-hash",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nComparing hand chirality\nvar hashValue: Int\nvar description: String\nstatic func == (HandAnchor.Chirality, HandAnchor.Chirality) -> Bool\nstatic func != (HandAnchor.Chirality, HandAnchor.Chirality) -> Bool"
  },
  {
    "title": "renderer(_:didRemove:for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnviewdelegate/2865795-renderer",
    "html": "Parameters\nrenderer\n\nThe ARSCNView object rendering the scene.\n\nnode\n\nThe removed SceneKit node.\n\nanchor\n\nThe AR anchor corresponding to the node.\n\nDiscussion\n\nDepending on the session configuration, ARKit may automatically remove anchors from a session. The view calls this method once for each removed anchor.\n\nSee Also\nHandling Content Updates\nfunc renderer(SCNSceneRenderer, nodeFor: ARAnchor) -> SCNNode?\nAsks the delegate to provide a SceneKit node corresponding to a newly added anchor.\nfunc renderer(SCNSceneRenderer, didAdd: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node corresponding to a new AR anchor has been added to the scene.\nfunc renderer(SCNSceneRenderer, willUpdate: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node's properties will be updated to match the current state of its corresponding anchor.\nfunc renderer(SCNSceneRenderer, didUpdate: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node's properties have been updated to match the current state of its corresponding anchor."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/geometry/4139385-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting mesh geometry\nvar faces: GeometryElement\nThe faces of the mesh.\nvar vertices: GeometrySource\nThe vertices of the mesh.\nvar normals: GeometrySource\nThe normals of the mesh.\nvar classifications: GeometrySource?\nThe classification of each face in the mesh."
  },
  {
    "title": "mouthFrownRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928270-mouthfrownright",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "HandAnchor.Chirality.right | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/chirality/right",
    "html": "See Also\nGetting hand chirality\ncase left\nA left hand."
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/chirality/4108398",
    "html": "See Also\nComparing hand chirality\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func != (HandAnchor.Chirality, HandAnchor.Chirality) -> Bool"
  },
  {
    "title": "classifications | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/geometry/4131788-classifications",
    "html": "See Also\nInspecting mesh geometry\nvar faces: GeometryElement\nThe faces of the mesh.\nvar vertices: GeometrySource\nThe vertices of the mesh.\nvar normals: GeometrySource\nThe normals of the mesh.\nvar description: String\nA textual description of the mesh geometry."
  },
  {
    "title": "PlaneAnchor.Classification.table | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/table",
    "html": "See Also\nGetting known classifications\ncase ceiling\nA ceiling.\ncase door\nA door.\ncase floor\nA floor.\ncase seat\nA seat.\ncase wall\nA wall.\ncase window\nA window."
  },
  {
    "title": "normals | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/geometry/4108456-normals",
    "html": "See Also\nInspecting mesh geometry\nvar faces: GeometryElement\nThe faces of the mesh.\nvar vertices: GeometrySource\nThe vertices of the mesh.\nvar classifications: GeometrySource?\nThe classification of each face in the mesh.\nvar description: String\nA textual description of the mesh geometry."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/chirality/4108397",
    "html": "See Also\nComparing hand chirality\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (HandAnchor.Chirality, HandAnchor.Chirality) -> Bool"
  },
  {
    "title": "vertices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/geometry/4108457-vertices",
    "html": "See Also\nInspecting mesh geometry\nvar faces: GeometryElement\nThe faces of the mesh.\nvar normals: GeometrySource\nThe normals of the mesh.\nvar classifications: GeometrySource?\nThe classification of each face in the mesh.\nvar description: String\nA textual description of the mesh geometry."
  },
  {
    "title": "PlaneAnchor.Classification.wall | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/wall",
    "html": "See Also\nGetting known classifications\ncase ceiling\nA ceiling.\ncase door\nA door.\ncase floor\nA floor.\ncase seat\nA seat.\ncase table\nA table.\ncase window\nA window."
  },
  {
    "title": "MeshAnchor.MeshClassification.ceiling | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/ceiling",
    "html": "See Also\nGetting architecture classifications\ncase door\nA door.\ncase floor\nA floor.\ncase stairs\nA set of stairs.\ncase wall\nA wall.\ncase window\nA window."
  },
  {
    "title": "MeshAnchor.MeshClassification.wall | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/wall",
    "html": "See Also\nGetting architecture classifications\ncase ceiling\nA ceiling.\ncase door\nA door.\ncase floor\nA floor.\ncase stairs\nA set of stairs.\ncase window\nA window."
  },
  {
    "title": "MeshAnchor.MeshClassification.door | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/door",
    "html": "See Also\nGetting architecture classifications\ncase ceiling\nA ceiling.\ncase floor\nA floor.\ncase stairs\nA set of stairs.\ncase wall\nA wall.\ncase window\nA window."
  },
  {
    "title": "MeshAnchor.MeshClassification.stairs | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/stairs",
    "html": "See Also\nGetting architecture classifications\ncase ceiling\nA ceiling.\ncase door\nA door.\ncase floor\nA floor.\ncase wall\nA wall.\ncase window\nA window."
  },
  {
    "title": "MeshAnchor.MeshClassification.window | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/window",
    "html": "See Also\nGetting architecture classifications\ncase ceiling\nA ceiling.\ncase door\nA door.\ncase floor\nA floor.\ncase stairs\nA set of stairs.\ncase wall\nA wall."
  },
  {
    "title": "MeshAnchor.MeshClassification.floor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/floor",
    "html": "See Also\nGetting architecture classifications\ncase ceiling\nA ceiling.\ncase door\nA door.\ncase stairs\nA set of stairs.\ncase wall\nA wall.\ncase window\nA window."
  },
  {
    "title": "MeshAnchor.MeshClassification.homeAppliance | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/homeappliance",
    "html": "See Also\nGetting furniture classifications\ncase bed\nA bed.\ncase cabinet\nA cabinet.\ncase seat\nA seat.\ncase table\nA table."
  },
  {
    "title": "MeshAnchor.MeshClassification.seat | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/seat",
    "html": "See Also\nGetting furniture classifications\ncase bed\nA bed.\ncase cabinet\nA cabinet.\ncase homeAppliance\nA home appliance.\ncase table\nA table."
  },
  {
    "title": "MeshAnchor.MeshClassification.bed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/bed",
    "html": "See Also\nGetting furniture classifications\ncase cabinet\nA cabinet.\ncase homeAppliance\nA home appliance.\ncase seat\nA seat.\ncase table\nA table."
  },
  {
    "title": "MeshAnchor.MeshClassification.cabinet | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/cabinet",
    "html": "See Also\nGetting furniture classifications\ncase bed\nA bed.\ncase homeAppliance\nA home appliance.\ncase seat\nA seat.\ncase table\nA table."
  },
  {
    "title": "MeshAnchor.MeshClassification.table | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/table",
    "html": "See Also\nGetting furniture classifications\ncase bed\nA bed.\ncase cabinet\nA cabinet.\ncase homeAppliance\nA home appliance.\ncase seat\nA seat."
  },
  {
    "title": "rawValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/4180457-rawvalue",
    "html": "Relationships\nFrom Protocol\nRawRepresentable\nSee Also\nInspecting classifications\ninit?(rawValue: NSInteger)\nvar hashValue: Int\nfunc hash(into: inout Hasher)\ntypealias MeshAnchor.MeshClassification.RawValue\nstatic func != (MeshAnchor.MeshClassification, MeshAnchor.MeshClassification) -> Bool"
  },
  {
    "title": "referenceObjects(inGroupNamed:bundle:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/2968202-referenceobjects",
    "html": "Parameters\nname\n\nThe name of an AR Resource Group from your Xcode project's main asset catalog.\n\nbundle\n\nThe bundle from which to load asset catalog resources, or nil to use your app's main bundle.\n\nReturn Value\n\nA set of all unique reference objects in the specified group.\n\nDiscussion\n\nTo use the objects for detection in a world-tracking AR session, provide this set for your session configuration's detectionObjects property.\n\nSee Also\nLoading Reference Objects\ninit(archiveURL: URL)\nLoads a reference object from the specified file URL."
  },
  {
    "title": "MeshAnchor.MeshClassification.RawValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/rawvalue",
    "html": "See Also\nInspecting classifications\ninit?(rawValue: NSInteger)\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar rawValue: NSInteger\nstatic func != (MeshAnchor.MeshClassification, MeshAnchor.MeshClassification) -> Bool"
  },
  {
    "title": "init(archiveURL:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/2977513-init",
    "html": "Parameters\nurl\n\nThe local file URL containing the reference object to load.\n\nerror\n\nA pointer to an NSError object. If this method returns nil, check this pointer for an error describing the failure.\n\nReturn Value\n\nThe reference object contained in the file.\n\nDiscussion\n\nTo use the object for detection in a world-tracking AR session, add it to the detectionObjects set in your session configuration.\n\nSee Also\nLoading Reference Objects\nclass func referenceObjects(inGroupNamed: String, bundle: Bundle?) -> Set<ARReferenceObject>?\nLoads all reference objects in the specified AR Resource Group in your Xcode project's asset catalog."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/4180440",
    "html": "See Also\nInspecting classifications\ninit?(rawValue: NSInteger)\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar rawValue: NSInteger\ntypealias MeshAnchor.MeshClassification.RawValue"
  },
  {
    "title": "resourceGroupName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/3223435-resourcegroupname",
    "html": "Discussion\n\nIf ARKit loaded this object from an AR resource group in an asset catalog, ARKit sets the value of this property to the resource group's name. Otherwise, the value of this property is nil.\n\nSee Also\nExamining a Reference Object\nvar name: String?\nA descriptive name for the reference object.\nvar center: simd_float3\nThe center point of the reference object's space-mapping data.\nvar extent: simd_float3\nThe size of the reference object's space-mapping data.\nvar scale: simd_float3\nA scale factor for the local coordinate space the reference object defines."
  },
  {
    "title": "name | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/2968200-name",
    "html": "Discussion\n\nFor a reference object loaded from an Xcode asset catalog, this property is the name assigned in the asset catalog. You can also use this property to assign a name to an object you've recorded in an AR session using extractReferenceObject.\n\nNote\n\nThis string is not localized text intended for user display. However, in debugging you can use this property to indicate which reference object was detected.\n\nSee Also\nExamining a Reference Object\nvar resourceGroupName: String?\nvar center: simd_float3\nThe center point of the reference object's space-mapping data.\nvar extent: simd_float3\nThe size of the reference object's space-mapping data.\nvar scale: simd_float3\nA scale factor for the local coordinate space the reference object defines."
  },
  {
    "title": "scale | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/2999412-scale",
    "html": "Discussion\n\nMultiplying the extent by this scale results in the physical size of the object in meters.\n\nSee Also\nExamining a Reference Object\nvar name: String?\nA descriptive name for the reference object.\nvar resourceGroupName: String?\nvar center: simd_float3\nThe center point of the reference object's space-mapping data.\nvar extent: simd_float3\nThe size of the reference object's space-mapping data."
  },
  {
    "title": "export(to:previewImage:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/2968198-export",
    "html": "Parameters\nurl\n\nThe file URL at which to write the reference object data.\n\npreviewImage\n\nA thumbnail image to be embedded in the reference object's filesystem representation.\n\nARKit ignores preview images when loading reference objects. Instead, this image helps make reference object files visually identifiable in external tools like Xcode, Finder, and Quick Look.\n\nReturn Value\n\ntrue if the operation succeeded. If false, check the error parameter for failure details.\n\nDiscussion\n\nAfter exporting a reference object from your object-scanning app to a file, you can bundle that reference object into other apps you create by inserting it into an Xcode asset catalog.\n\nSee Also\nSaving Recorded Objects\nclass let archiveExtension: String\nThe standard filename extension for exported ARReferenceObject instances."
  },
  {
    "title": "archiveExtension | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/2968203-archiveextension",
    "html": "Discussion\n\nUse this filename extension when constructing a URL to save a reference object file with the export(to:previewImage:) method.\n\nSee Also\nSaving Recorded Objects\nfunc export(to: URL, previewImage: UIImage?)\nWrites a binary representation of the object to the specified file URL."
  },
  {
    "title": "rawFeaturePoints | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/2968201-rawfeaturepoints",
    "html": "Discussion\n\nThese points represent notable features detected in camera imagery during the session that recorded the reference object. ARKit extrapolates the locations of these features in 3D world coordinate space as part of the image and motion analysis that tracks the device's movement in a session. Taken together, these points loosely correlate to the contours of real-world objects that were in view of the camera during the session.\n\nARKit does not guarantee that the number and arrangement of raw feature points will remain stable between software releases. However, you can visualize the point cloud to debug your app's object recording or detection, or inspect its size to estimate the quality of a recorded object."
  },
  {
    "title": "ARGeoTrackingStatus.StateReason.worldTrackingUnstable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/statereason/worldtrackingunstable",
    "html": "Discussion\n\nThis reason indicates that ARKit’s local-space tracking is functioning at a limited capacity. To retrieve more information about the cause, an app needs to refer to the camera’s trackingState. For the possible causes of this state, see ARTrackingState and ARCamera.TrackingStateReason.\n\nSee Also\nStatus Reasons\ncase none\nNo issues reported.\ncase notAvailableAtLocation\nThe location doesn't provide geotracking.\ncase needLocationPermissions\nThe location requires user permission for geotracking.\ncase devicePointedTooLow\nThe position of the device is too low for geotracking.\ncase waitingForLocation\nA state in which the framework performs a check for the user's GPS position.\ncase waitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\ncase geoDataNotLoaded\nA state in which the framework downloads localization imagery.\ncase visualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "ARGeoTrackingStatus.StateReason.notAvailableAtLocation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/statereason/notavailableatlocation",
    "html": "Discussion\n\nThis reason indicates that ARKit does not have the necessary landscape data to support geo tracking at the user’s current location. See checkAvailability(completionHandler:) for more information.\n\nIf checkAvailability(completionHandler:) returns true and an app begins a geo-tracking session, ARKit provides this state reason when the user has moved to an unsupported area.\n\nSee Also\nStatus Reasons\ncase none\nNo issues reported.\ncase needLocationPermissions\nThe location requires user permission for geotracking.\ncase devicePointedTooLow\nThe position of the device is too low for geotracking.\ncase worldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\ncase waitingForLocation\nA state in which the framework performs a check for the user's GPS position.\ncase waitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\ncase geoDataNotLoaded\nA state in which the framework downloads localization imagery.\ncase visualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "mouthStretchLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928235-mouthstretchleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "ARGeoTrackingStatus.StateReason.geoDataNotLoaded | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/statereason/geodatanotloaded",
    "html": "Discussion\n\nARKit provides this reason in state ARGeoTrackingStatus.State.localizing when the session is actively attempting to download localization imagery (see Refine the User's Position with Imagery).\n\nIf this state persists for too long, it may indicate a network issue. If a reasonable amount of time elapses in this state reason, the app may consider requesting that the user check their internet connection.\n\nSee Also\nStatus Reasons\ncase none\nNo issues reported.\ncase notAvailableAtLocation\nThe location doesn't provide geotracking.\ncase needLocationPermissions\nThe location requires user permission for geotracking.\ncase devicePointedTooLow\nThe position of the device is too low for geotracking.\ncase worldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\ncase waitingForLocation\nA state in which the framework performs a check for the user's GPS position.\ncase waitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\ncase visualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera."
  },
  {
    "title": "ARGeoTrackingStatus.State | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/state",
    "html": "Overview\n\nFor any state in a frame’s geoTrackingStatus, ARKit provides a stateReason. A given geo-tracking status may intermix states and reasons, so the reasons are not tied to specific states.\n\nTopics\nStates\ncase initializing\nThe session is initializing geo tracking.\ncase localized\nGeo tracking is localized.\ncase localizing\nGeo tracking is attempting to localize against a map.\ncase notAvailable\nGeo tracking is not available.\nRelationships\nConforms To\nSendable\nSee Also\nChecking State\nvar state: ARGeoTrackingStatus.State\nA value that describes the session’s current geo-tracking state."
  },
  {
    "title": "ARGeoTrackingStatus.Accuracy | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/accuracy",
    "html": "Overview\n\nTo ensure the best possible user experience, an app must monitor and react to the geo-tracking accuracy. When accuracy is ARGeoTrackingStatus.Accuracy.low, the app needs to show content that’s more forgiving if ARKit is off by a small distance.\n\nFor example, if accuracy is ARGeoTrackingStatus.Accuracy.low, rendering a location anchor as a large ball several meters in the air is more appropriate than rendering an arrow that rests its point on a real-world surface. Because a larger ball isn’t meant to mark a precise location, any offset that results from low accuracy will be less noticeable to the user.\n\nApps that need higher-precision location anchors need to wait for ARGeoTrackingStatus.Accuracy.medium or ARGeoTrackingStatus.Accuracy.high accuracy before revealing rendered location-anchors, or dismissing user instructions.\n\nTopics\nAccuracies\ncase high\nGeo-tracking accuracy is high.\ncase undetermined\nGeo-tracking accuracy is undetermined.\ncase low\nGeo-tracking accuracy is low.\ncase medium\nGeo-tracking accuracy is average.\nRelationships\nConforms To\nSendable\nSee Also\nJudging Accuracy\nvar accuracy: ARGeoTrackingStatus.Accuracy\nThe accuracy of geo tracking at the time the session captured the frame."
  },
  {
    "title": "jointNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletondefinition/3194600-jointnames",
    "html": "Discussion\n\nRefer to this array to convert a joint index to a joint identifier.\n\nSee Also\nGetting Joint Information\nvar jointCount: Int\nThe skeleton's total number of joints.\nfunc index(for: ARSkeleton.JointName) -> Int\nReturns the index for a given joint identifier.\nvar parentIndices: [Int]\nThe parent index for each joint."
  },
  {
    "title": "ARCoachingOverlayView.Goal.geoTracking | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayview/goal/geotracking",
    "html": "Discussion\n\nWhen you set this goal, the coaching overlay provides the user instructions when they start a geotracking session, or when they go off course during a session.\n\nSee Also\nDefining a Goal\ncase anyPlane\nA goal that specifies your app requires a plane of any type.\ncase horizontalPlane\nA goal that specifies your app requires a horizontal plane.\ncase tracking\nA goal that specifies your app requires basic world tracking.\ncase verticalPlane\nA goal that specifies your app requires a vertical plane."
  },
  {
    "title": "indexForJointName: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletondefinition/3229929-indexforjointname",
    "html": "Discussion\n\nThis function returns NSNotFound if an invalid joint name is passed in.\n\nSee Also\nGetting Joint Information\njointNames\nA collection of unique joint names.\njointCount\nThe skeleton's total number of joints.\nparentIndices\nThe parent index for each joint."
  },
  {
    "title": "accuracy | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/3580875-accuracy",
    "html": "Discussion\n\nARKit populates this property after reaching ARGeoTrackingStatus.State.localized. To ensure the best possible user experience, an app must monitor and react to the geo-tracking accuracy. For more information, see ARGeoTrackingStatus.Accuracy.\n\nSee Also\nJudging Accuracy\nenum ARGeoTrackingStatus.Accuracy\nValues that are possible for the current accuracy of geo tracking."
  },
  {
    "title": "defaultBody3D | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletondefinition/3229928-defaultbody3d",
    "html": "Discussion\n\nThe default height of this skeleton is 1.66 meters.\n\nSee Also\nLocating in the Physical Environment\nvar neutralBodySkeleton3D: ARSkeleton3D?\nThe 3D skeleton in neutral pose."
  },
  {
    "title": "neutralBodySkeleton3D | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletondefinition/3229931-neutralbodyskeleton3d",
    "html": "Discussion\n\nThe neutral skeleton height is 1.66 meters and is posed in the shape of a T. This skeleton is provided to you for reference to all other possible skeleton poses.\n\nSee Also\nLocating in the Physical Environment\nclass var defaultBody3D: ARSkeletonDefinition\nThe default skeleton definition for bodies defined in 3D."
  },
  {
    "title": "jawLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation/2928273-jawleft",
    "html": "Discussion\n\nThe figure below shows a face geometry (see ARSCNFaceGeometry) in two states, demonstrating values of 0.0 and 1.0 for this coefficient. In both states, the values for all other ARFaceAnchor.BlendShapeLocation coefficients are set to 0.0.\n\nSee Also\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side."
  },
  {
    "title": "parentIndices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletondefinition/3194602-parentindices",
    "html": "Discussion\n\nThis property may be used to identify the hierarchical dependency between joints. If a line is drawn for every joint and its parent joint, the result is a visualization of the underlying skeleton. The joint with no parent is denoted as the root joint. The root joint's parent index is -1.\n\nSee Also\nGetting Joint Information\njointNames\nA collection of unique joint names.\njointCount\nThe skeleton's total number of joints.\n- indexForJointName:\nReturns the index for a given joint identifier."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/4284957-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting hand joints\nstatic var allCases: [HandSkeleton.JointName]\ntypealias HandSkeleton.JointName.AllCases\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (HandSkeleton.JointName, HandSkeleton.JointName) -> Bool\nstatic func != (HandSkeleton.JointName, HandSkeleton.JointName) -> Bool"
  },
  {
    "title": "HandSkeleton.JointName.littleFingerTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/littlefingertip",
    "html": "See Also\nLittle finger joints\ncase littleFingerIntermediateBase\ncase littleFingerIntermediateTip\ncase littleFingerKnuckle\ncase littleFingerMetacarpal"
  },
  {
    "title": "parentJoint | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/joint/4284950-parentjoint",
    "html": "See Also\nInspecting hand joints\nvar name: HandSkeleton.JointName\nA name that uniquely identifies this joint among others on the same skeleton.\nvar description: String\nA textual description of this joint."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/joint/4284946-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting hand joints\nvar name: HandSkeleton.JointName\nA name that uniquely identifies this joint among others on the same skeleton.\nvar parentJoint: HandSkeleton.Joint?\nThe joint that’s connected to this joint and more closely connected to the base of the skeleton."
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/alignment/4108467",
    "html": "See Also\nComparing plane alignment\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func != (PlaneAnchor.Alignment, PlaneAnchor.Alignment) -> Bool"
  },
  {
    "title": "PlaneAnchor.Classification.notAvailable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/notavailable",
    "html": "See Also\nGetting unknown classifications\ncase undetermined\nA plane classification hasn’t been determined yet.\ncase unknown\nA plane classification isn’t one of the known classes."
  },
  {
    "title": "PlaneAnchor.Classification.window | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/window",
    "html": "See Also\nGetting known classifications\ncase ceiling\nA ceiling.\ncase door\nA door.\ncase floor\nA floor.\ncase seat\nA seat.\ncase table\nA table.\ncase wall\nA wall."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/alignment/4169989-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nComparing plane alignment\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (PlaneAnchor.Alignment, PlaneAnchor.Alignment) -> Bool\nstatic func != (PlaneAnchor.Alignment, PlaneAnchor.Alignment) -> Bool"
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/4169990-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nComparing plane classifications\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (PlaneAnchor.Classification, PlaneAnchor.Classification) -> Bool\nstatic func != (PlaneAnchor.Classification, PlaneAnchor.Classification) -> Bool"
  },
  {
    "title": "PlaneAnchor.Classification.undetermined | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/undetermined",
    "html": "See Also\nGetting unknown classifications\ncase notAvailable\nA plane classification is currently unavailable.\ncase unknown\nA plane classification isn’t one of the known classes."
  },
  {
    "title": "PlaneAnchor.Classification.unknown | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/unknown",
    "html": "See Also\nGetting unknown classifications\ncase notAvailable\nA plane classification is currently unavailable.\ncase undetermined\nA plane classification hasn’t been determined yet."
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/4108478-hash",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nComparing plane classifications\nvar hashValue: Int\nvar description: String\nstatic func == (PlaneAnchor.Classification, PlaneAnchor.Classification) -> Bool\nstatic func != (PlaneAnchor.Classification, PlaneAnchor.Classification) -> Bool"
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/4108479-hashvalue",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nComparing plane classifications\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (PlaneAnchor.Classification, PlaneAnchor.Classification) -> Bool\nstatic func != (PlaneAnchor.Classification, PlaneAnchor.Classification) -> Bool"
  },
  {
    "title": "meshFaces | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/geometry/4131790-meshfaces",
    "html": "See Also\nInspecting plane geometry\nvar extent: PlaneAnchor.Geometry.Extent\nThe size of a plane.\nstruct PlaneAnchor.Geometry.Extent\nThe size of a plane.\nvar meshVertices: GeometrySource\nThe vertices in the mesh that describes a plane.\nvar description: String\nA textual description of the shape of a plane."
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/4108474",
    "html": "See Also\nComparing plane classifications\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func != (PlaneAnchor.Classification, PlaneAnchor.Classification) -> Bool"
  },
  {
    "title": "HandSkeleton.JointName.forearmWrist | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/forearmwrist",
    "html": "See Also\nForearm joints\ncase forearmArm\ncase wrist"
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/geometry/4139387-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting plane geometry\nvar extent: PlaneAnchor.Geometry.Extent\nThe size of a plane.\nstruct PlaneAnchor.Geometry.Extent\nThe size of a plane.\nvar meshVertices: GeometrySource\nThe vertices in the mesh that describes a plane.\nvar meshFaces: GeometryElement\nThe faces in the mesh that describes a plane."
  },
  {
    "title": "HandSkeleton.JointName.wrist | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/wrist",
    "html": "See Also\nForearm joints\ncase forearmArm\ncase forearmWrist"
  },
  {
    "title": "HandSkeleton.JointName.thumbTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/thumbtip",
    "html": "See Also\nThumb joints\ncase thumbIntermediateBase\ncase thumbIntermediateTip\ncase thumbKnuckle"
  },
  {
    "title": "extent | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/geometry/4218770-extent",
    "html": "See Also\nInspecting plane geometry\nstruct PlaneAnchor.Geometry.Extent\nThe size of a plane.\nvar meshVertices: GeometrySource\nThe vertices in the mesh that describes a plane.\nvar meshFaces: GeometryElement\nThe faces in the mesh that describes a plane.\nvar description: String\nA textual description of the shape of a plane."
  },
  {
    "title": "PlaneAnchor.Classification.floor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/floor",
    "html": "See Also\nGetting known classifications\ncase ceiling\nA ceiling.\ncase door\nA door.\ncase seat\nA seat.\ncase table\nA table.\ncase wall\nA wall.\ncase window\nA window."
  },
  {
    "title": "PlaneAnchor.Geometry.Extent | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/geometry/extent",
    "html": "Topics\nInspecting a plane extent\nvar width: Float\nThe width of a plane.\nvar height: Float\nThe height of a plane.\nvar anchorFromExtentTransform: simd_float4x4\nThe transform from the plane extent to the plane anchor’s coordinate system.\nvar description: String\nA textual description of the size of a plane.\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nInspecting plane geometry\nvar extent: PlaneAnchor.Geometry.Extent\nThe size of a plane.\nvar meshVertices: GeometrySource\nThe vertices in the mesh that describes a plane.\nvar meshFaces: GeometryElement\nThe faces in the mesh that describes a plane.\nvar description: String\nA textual description of the shape of a plane."
  },
  {
    "title": "meshVertices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/geometry/4108489-meshvertices",
    "html": "See Also\nInspecting plane geometry\nvar extent: PlaneAnchor.Geometry.Extent\nThe size of a plane.\nstruct PlaneAnchor.Geometry.Extent\nThe size of a plane.\nvar meshFaces: GeometryElement\nThe faces in the mesh that describes a plane.\nvar description: String\nA textual description of the shape of a plane."
  },
  {
    "title": "faces | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/geometry/4108455-faces",
    "html": "See Also\nInspecting mesh geometry\nvar vertices: GeometrySource\nThe vertices of the mesh.\nvar normals: GeometrySource\nThe normals of the mesh.\nvar classifications: GeometrySource?\nThe classification of each face in the mesh.\nvar description: String\nA textual description of the mesh geometry."
  },
  {
    "title": "MeshAnchor.MeshClassification.plant | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/plant",
    "html": "See Also\nGetting decoration classifications\ncase tv\nA television."
  },
  {
    "title": "MeshAnchor.MeshClassification.tv | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/tv",
    "html": "See Also\nGetting decoration classifications\ncase plant\nA plant."
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/4180452-hashvalue",
    "html": "See Also\nInspecting classifications\ninit?(rawValue: NSInteger)\nfunc hash(into: inout Hasher)\nvar rawValue: NSInteger\ntypealias MeshAnchor.MeshClassification.RawValue\nstatic func != (MeshAnchor.MeshClassification, MeshAnchor.MeshClassification) -> Bool"
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/4180451-hash",
    "html": "See Also\nInspecting classifications\ninit?(rawValue: NSInteger)\nvar hashValue: Int\nvar rawValue: NSInteger\ntypealias MeshAnchor.MeshClassification.RawValue\nstatic func != (MeshAnchor.MeshClassification, MeshAnchor.MeshClassification) -> Bool"
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification/4180454-init",
    "html": "Relationships\nFrom Protocol\nRawRepresentable\nSee Also\nInspecting classifications\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar rawValue: NSInteger\ntypealias MeshAnchor.MeshClassification.RawValue\nstatic func != (MeshAnchor.MeshClassification, MeshAnchor.MeshClassification) -> Bool"
  },
  {
    "title": "merging(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/3019548-merging",
    "html": "Parameters\nobject\n\nThe other reference object with which to combine this reference object.\n\nReturn Value\n\nA new ARReferenceObject that includes the spatial information from both objects. throws an ARError.\n\nDiscussion\n\nThe accuracy of 3D object detection depends on similarity of lighting and environmental conditions between when you scan a real object (producing an ARReferenceObject) and when a user of your app attempts to detect that object. If, for example, you scan an object in a bright environment, then a user attempts to detect it in a dark environment, ARKit may fail to recognize that the real object matches the reference object, or may not detect the object quickly.\n\nTo make a reference object that is more robust in a wide variety of detection conditions, scan the same real-world object multiple times: For each scan, vary the lighting conditions or the background environment to capture the variety of situations in which your app might attempt to detect the same real object. Then, use this method to combine those scan results into a single ARReferenceObject incorporating recognition information for all the conditions you scanned in.\n\nSee Also\nCreating Derivative Reference Objects\nfunc applyingTransform(simd_float4x4) -> ARReferenceObject\nReturns a new reference object created by applying the specified transform to this reference object's geometric data."
  },
  {
    "title": "ARGeoTrackingStatus.Accuracy.low | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/accuracy/low",
    "html": "Discussion\n\nThis value indicates that visual localization is complete and geo-tracking accuracy is low.\n\nOne technique an app can use to deal with low accuracy is to render location anchors with an asset that’s more forgiving, like a large ball. If an app renders the ball further in the air, any offset that results from low accuracy will be less noticeable, and less critical to the user.\n\nSee Also\nAccuracies\ncase high\nGeo-tracking accuracy is high.\ncase undetermined\nGeo-tracking accuracy is undetermined.\ncase medium\nGeo-tracking accuracy is average."
  },
  {
    "title": "ARGeoTrackingStatus.StateReason | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/statereason",
    "html": "Overview\n\nThese possible values of stateReason provide more information about a geotracking session's current state.\n\nTopics\nStatus Reasons\ncase none\nNo issues reported.\ncase notAvailableAtLocation\nThe location doesn't provide geotracking.\ncase needLocationPermissions\nThe location requires user permission for geotracking.\ncase devicePointedTooLow\nThe position of the device is too low for geotracking.\ncase worldTrackingUnstable\nThe position or motion of the device makes geotracking unstable.\ncase waitingForLocation\nA state in which the framework performs a check for the user's GPS position.\ncase waitingForAvailabilityCheck\nA state in which the framework performs a check for geotracking availability at the user's location.\ncase geoDataNotLoaded\nA state in which the framework downloads localization imagery.\ncase visualLocalizationFailed\nLocalization imagery failed to match the view from the device's camera.\nRelationships\nConforms To\nSendable\nSee Also\nDetermining the Reason\nvar stateReason: ARGeoTrackingStatus.StateReason\nThe reason for the frame’s geo-tracking state."
  },
  {
    "title": "state | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/3580876-state",
    "html": "Discussion\n\nFor any state in a frame’s geoTrackingStatus, ARKit provides a stateReason. A given geo-tracking status may intermix states and reasons, so the reasons are not tied to specific states.\n\nSee Also\nChecking State\nenum ARGeoTrackingStatus.State\nValues that are possible for the current state of geo-tracking."
  },
  {
    "title": "stateReason | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/3580877-statereason",
    "html": "See Also\nDetermining the Reason\nenum ARGeoTrackingStatus.StateReason\nThe reasons for the app's geotracking status."
  },
  {
    "title": "HandSkeleton.JointName.littleFingerIntermediateTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/littlefingerintermediatetip",
    "html": "See Also\nLittle finger joints\ncase littleFingerIntermediateBase\ncase littleFingerKnuckle\ncase littleFingerMetacarpal\ncase littleFingerTip"
  },
  {
    "title": "HandSkeleton.JointName.littleFingerIntermediateBase | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/littlefingerintermediatebase",
    "html": "See Also\nLittle finger joints\ncase littleFingerIntermediateTip\ncase littleFingerKnuckle\ncase littleFingerMetacarpal\ncase littleFingerTip"
  },
  {
    "title": "HandSkeleton.JointName.ringFingerTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/ringfingertip",
    "html": "See Also\nRing finger joints\ncase ringFingerIntermediateBase\ncase ringFingerIntermediateTip\ncase ringFingerKnuckle\ncase ringFingerMetacarpal"
  },
  {
    "title": "HandSkeleton.JointName.littleFingerKnuckle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/littlefingerknuckle",
    "html": "See Also\nLittle finger joints\ncase littleFingerIntermediateBase\ncase littleFingerIntermediateTip\ncase littleFingerMetacarpal\ncase littleFingerTip"
  },
  {
    "title": "name | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/joint/4284949-name",
    "html": "See Also\nInspecting hand joints\nvar parentJoint: HandSkeleton.Joint?\nThe joint that’s connected to this joint and more closely connected to the base of the skeleton.\nvar description: String\nA textual description of this joint."
  },
  {
    "title": "getCurrentWorldMapWithCompletionHandler: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2968206-getcurrentworldmapwithcompletion",
    "html": "Parameters\ncompletionHandler\n\nA block to be invoked asynchronously after ARKit finishes generating the world map. The block takes two parameters:\n\nworldMap\n\nThe generated ARWorldMap, or nil if a map could not be generated.\n\nerror\n\nIf the worldMap is nil, an NSError object describing the failure.\n\nDiscussion\n\nAn ARWorldMap encapsulates the state of a running ARSession. This state includes ARKit's awareness of the physical space the user moves the device in (which ARKit uses to determine the device's position and orientation), as well as any ARAnchor objects added to the session (which can represent detected real-world features or virtual content placed by your app). After you use this method to save a session's world map, you can assign it to a configuration's initialWorldMap property and use runWithConfiguration:options: to start another session with the same spatial awareness and anchors.\n\nBy saving world maps and using them to start new sessions, your app can add new AR capabilities:\n\nMultiuser AR experiences. Create a shared frame of reference by sending archived ARWorldMap objects to a nearby user's device. With two devices tracking the same world map, you can build a networked experience where both users can see and interact with the same virtual content.\n\nPersistent AR experiences. Save a world map when your app becomes inactive, then restore it the next time your app launches in the same physical environment. You can use anchors from the resumed world map to place the same virtual content at the same positions from the saved session.\n\nBefore saving a world map, monitor the worldMappingStatus property to verify that ARKit has an adequate understanding of the user's environment, ensuring that you can reliably make use of the saved map on a different device or at a later time.\n\nWorld map generation requires a world-tracking AR session. If you call this method on an ARSession not run with ARWorldTrackingConfiguration, it invokes your completion handler immediately, providing no world map and an error.\n\nImportant\n\nARKit calls your completionHandler on the session's delegateQueue (if set; on the main queue otherwise). If you need to perform expensive work from this handler (such as archiving and saving or sending the world map), do so on an appropriate dispatch queue to avoid disrupting performance.\n\nSee Also\nSaving or sharing state\nRecording and Replaying AR Session Data\nRecord an AR session in Reality Composer and replay it in your ARKit app."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdate/event/4111167",
    "html": "See Also\nComparing anchor update events\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (AnchorUpdate<AnchorType>.Event, AnchorUpdate<AnchorType>.Event) -> Bool"
  },
  {
    "title": "AnchorUpdate.Event.updated | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdate/event/updated",
    "html": "See Also\nInspecting anchor update events\ncase added\nAn event that occurs when ARKit starts tracking an anchor.\ncase removed\nAn event that occurs when ARKit stops tracking an anchor."
  },
  {
    "title": "HandSkeleton.JointName.littleFingerMetacarpal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/littlefingermetacarpal",
    "html": "See Also\nLittle finger joints\ncase littleFingerIntermediateBase\ncase littleFingerIntermediateTip\ncase littleFingerKnuckle\ncase littleFingerTip"
  },
  {
    "title": "parentFromJointTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/joint/4293518-parentfromjointtransform",
    "html": "Discussion\n\nThe root joint's parentFromJointTransform is an identity matrix.\n\nSee Also\nTracking the position of hand joints\nvar anchorFromJointTransform: simd_float4x4\nThe position and orientation of this joint relative to the base joint of the skeleton.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking this joint."
  },
  {
    "title": "anchorFromJointTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/joint/4293517-anchorfromjointtransform",
    "html": "See Also\nTracking the position of hand joints\nvar parentFromJointTransform: simd_float4x4\nThe transform from the joint to its parent joint’s coordinate system.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking this joint."
  },
  {
    "title": "isTracked | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/joint/4284947-istracked",
    "html": "See Also\nTracking the position of hand joints\nvar anchorFromJointTransform: simd_float4x4\nThe position and orientation of this joint relative to the base joint of the skeleton.\nvar parentFromJointTransform: simd_float4x4\nThe transform from the joint to its parent joint’s coordinate system."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/alignment/4108466",
    "html": "See Also\nComparing plane alignment\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (PlaneAnchor.Alignment, PlaneAnchor.Alignment) -> Bool"
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/alignment/4108468-hash",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nComparing plane alignment\nvar hashValue: Int\nvar description: String\nstatic func == (PlaneAnchor.Alignment, PlaneAnchor.Alignment) -> Bool\nstatic func != (PlaneAnchor.Alignment, PlaneAnchor.Alignment) -> Bool"
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/4284953",
    "html": "See Also\nInspecting hand joints\nvar description: String\nstatic var allCases: [HandSkeleton.JointName]\ntypealias HandSkeleton.JointName.AllCases\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (HandSkeleton.JointName, HandSkeleton.JointName) -> Bool"
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/4284960-hash",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nInspecting hand joints\nvar description: String\nstatic var allCases: [HandSkeleton.JointName]\ntypealias HandSkeleton.JointName.AllCases\nvar hashValue: Int\nstatic func == (HandSkeleton.JointName, HandSkeleton.JointName) -> Bool\nstatic func != (HandSkeleton.JointName, HandSkeleton.JointName) -> Bool"
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/4284954",
    "html": "See Also\nInspecting hand joints\nvar description: String\nstatic var allCases: [HandSkeleton.JointName]\ntypealias HandSkeleton.JointName.AllCases\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func != (HandSkeleton.JointName, HandSkeleton.JointName) -> Bool"
  },
  {
    "title": "HandSkeleton.JointName.forearmArm | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/forearmarm",
    "html": "See Also\nForearm joints\ncase forearmWrist\ncase wrist"
  },
  {
    "title": "view(_:willUpdate:for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskviewdelegate/2865591-view",
    "html": "Parameters\nview\n\nThe ARSKView object rendering the scene.\n\nnode\n\nThe updated SpriteKit node.\n\nanchor\n\nThe AR anchor corresponding to the node.\n\nDiscussion\n\nDepending on the session configuration, ARKit may automatically update anchors in a session. The view calls this method once for each updated anchor.\n\nSee Also\nHandling Content Updates\nfunc view(ARSKView, nodeFor: ARAnchor) -> SKNode?\nAsks the delegate to provide a SpriteKit node corresponding to a newly added anchor.\nfunc view(ARSKView, didAdd: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node corresponding to a new AR anchor has been added to the scene.\nfunc view(ARSKView, didUpdate: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node's properties have been updated to match the current state of its corresponding anchor.\nfunc view(ARSKView, didRemove: SKNode, for: ARAnchor)\nTells the delegate that the SpriteKit node corresponding to an AR anchor has been removed from the scene."
  },
  {
    "title": "HandSkeleton.JointName.thumbIntermediateBase | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/thumbintermediatebase",
    "html": "See Also\nThumb joints\ncase thumbIntermediateTip\ncase thumbKnuckle\ncase thumbTip"
  },
  {
    "title": "HandSkeleton.JointName.middleFingerIntermediateBase | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/middlefingerintermediatebase",
    "html": "See Also\nMiddle finger joints\ncase middleFingerIntermediateTip\ncase middleFingerKnuckle\ncase middleFingerMetacarpal\ncase middleFingerTip"
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate/reason/2947139",
    "html": "Parameters\nlhs\n\nA value to compare.\n\nrhs\n\nAnother value to compare.\n\nDiscussion\n\nInequality is the inverse of equality. For any values a and b, a != b implies that a == b is false.\n\nThis is the default implementation of the not-equal-to operator (!=) for any type that conforms to Equatable.\n\nSee Also\nHashes of Tracking Quality\nfunc hash(into: inout Hasher)\nHashes the reason by passing it to the given hash function.\nstatic func == (ARCamera.TrackingState.Reason, ARCamera.TrackingState.Reason) -> Bool\nIndicates whether two reasons are equal.\nvar hashValue: Int\nA value that identifies an object uniquely as compared to other instances of the same type."
  },
  {
    "title": "HandSkeleton.JointName.middleFingerIntermediateTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/middlefingerintermediatetip",
    "html": "See Also\nMiddle finger joints\ncase middleFingerIntermediateBase\ncase middleFingerKnuckle\ncase middleFingerMetacarpal\ncase middleFingerTip"
  },
  {
    "title": "HandSkeleton.JointName.thumbKnuckle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/thumbknuckle",
    "html": "See Also\nThumb joints\ncase thumbIntermediateBase\ncase thumbIntermediateTip\ncase thumbTip"
  },
  {
    "title": "HandSkeleton.JointName.thumbIntermediateTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/thumbintermediatetip",
    "html": "See Also\nThumb joints\ncase thumbIntermediateBase\ncase thumbKnuckle\ncase thumbTip"
  },
  {
    "title": "HandSkeleton.JointName.indexFingerIntermediateBase | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/indexfingerintermediatebase",
    "html": "See Also\nIndex finger joints\ncase indexFingerIntermediateTip\ncase indexFingerKnuckle\ncase indexFingerMetacarpal\ncase indexFingerTip"
  },
  {
    "title": "HandSkeleton.JointName.middleFingerTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/middlefingertip",
    "html": "See Also\nMiddle finger joints\ncase middleFingerIntermediateBase\ncase middleFingerIntermediateTip\ncase middleFingerKnuckle\ncase middleFingerMetacarpal"
  },
  {
    "title": "HandSkeleton.JointName.ringFingerIntermediateBase | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/ringfingerintermediatebase",
    "html": "See Also\nRing finger joints\ncase ringFingerIntermediateTip\ncase ringFingerKnuckle\ncase ringFingerMetacarpal\ncase ringFingerTip"
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate/reason/3238105",
    "html": "Parameters\na\n\nThe first reason to compare.\n\nb\n\nThe second reason to compare.\n\nReturn Value\n\ntrue if the two reasons are equal; otherwise, false.\n\nRelationships\nFrom Protocol\nEquatable\nSee Also\nHashes of Tracking Quality\nfunc hash(into: inout Hasher)\nHashes the reason by passing it to the given hash function.\nstatic func != (ARCamera.TrackingState.Reason, ARCamera.TrackingState.Reason) -> Bool\nReturns a Boolean value indicating whether two values are not equal.\nvar hashValue: Int\nA value that identifies an object uniquely as compared to other instances of the same type."
  },
  {
    "title": "parentIndices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletondefinition/3295996-parentindices",
    "html": "Discussion\n\nThis property may be used to identify the hierarchical dependency between joints. If a line is drawn for every joint and its parent joint, the result is a visualization of the underlying skeleton. The joint with no parent is denoted as the root joint. The root joint's parent index is -1.\n\nSee Also\nGetting Joint Information\nvar jointNames: [String]\nA collection of unique joint names.\nvar jointCount: Int\nThe skeleton's total number of joints.\nfunc index(for: ARSkeleton.JointName) -> Int\nReturns the index for a given joint identifier."
  },
  {
    "title": "index(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletondefinition/3295995-index",
    "html": "Discussion\n\nThis function returns NSNotFound if an invalid joint name is passed in.\n\nSee Also\nGetting Joint Information\nvar jointNames: [String]\nA collection of unique joint names.\nvar jointCount: Int\nThe skeleton's total number of joints.\nvar parentIndices: [Int]\nThe parent index for each joint."
  },
  {
    "title": "planeDetection | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arobjectscanningconfiguration/3001729-planedetection",
    "html": "Discussion\n\nBy default, plane detection is off. If you enable horizontal or vertical plane detection, the session adds ARPlaneAnchor objects and notifies your ARSessionDelegate, ARSCNViewDelegate, or ARSKViewDelegate object whenever its analysis of captured video images detects an area that appears to be a flat surface.\n\nIn an object-scanning session, you can use detected planes to help determine where the origin (anchor point) for a scanned object should be relative to its extent.\n\nSee Also\nEnabling Plane Detection\nstruct ARWorldTrackingConfiguration.PlaneDetection\nOptions for whether and how the framework detects flat surfaces in captured images."
  },
  {
    "title": "ARCamera.TrackingState.Reason.excessiveMotion | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate/reason/excessivemotion",
    "html": "See Also\nInhibitors of Tracking Quality\ncase initializing\nThe AR session has not gathered enough camera or motion data to provide tracking information.\ncase relocalizing\nThe AR session is attempting to resume after an interruption.\ncase insufficientFeatures\nThe scene visible to the camera doesn't contain enough distinguishable features for image-based position tracking."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider/mode/4108522",
    "html": "See Also\nComparing scene reconstruction modes\nstatic func == (SceneReconstructionProvider.Mode, SceneReconstructionProvider.Mode) -> Bool"
  },
  {
    "title": "view(_:nodeFor:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskviewdelegate/2865596-view",
    "html": "Parameters\nview\n\nThe ARSKView object rendering the scene.\n\nanchor\n\nThe anchor for which a node is requested.\n\nReturn Value\n\nA new SpriteKit node, which ARKit will add to the scene and update to follow its corresponding AR anchor.\n\nDiscussion\n\nDepending on the session configuration, ARKit may automatically add anchors to a session, such as the origin of the world coordinate system and detected planes. ARKit also calls this method to provide visual content for any ARAnchor objects you manually add using the session's add(anchor:) method.\n\nYou can implement this method to provide a new SKNode object (or instance of any system or custom SKNode subclass) you plan to use as a visual representation of the anchor.\n\nNote that ARKit controls the node's position, rotation, and scale to simulate a billboarded 3D effect even for 2D sprites. If you provide a SKTransformLayer node, ARKit applies a 3D transformation.\n\nAlternatively, if you do not implement this method, ARKit creates an empty node, and you can implement the view(_:didAdd:for:) method instead to provide visual content by adding children to that node.\n\nSee Also\nHandling Content Updates\nfunc view(ARSKView, didAdd: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node corresponding to a new AR anchor has been added to the scene.\nfunc view(ARSKView, willUpdate: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node's properties will be updated to match the current state of its corresponding anchor.\nfunc view(ARSKView, didUpdate: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node's properties have been updated to match the current state of its corresponding anchor.\nfunc view(ARSKView, didRemove: SKNode, for: ARAnchor)\nTells the delegate that the SpriteKit node corresponding to an AR anchor has been removed from the scene."
  },
  {
    "title": "view(_:didUpdate:for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskviewdelegate/2865590-view",
    "html": "Parameters\nview\n\nThe ARSKView object rendering the scene.\n\nnode\n\nThe updated SpriteKit node.\n\nanchor\n\nThe AR anchor corresponding to the node.\n\nDiscussion\n\nDepending on the session configuration, ARKit may automatically update anchors in a session. The view calls this method once for each updated anchor.\n\nSee Also\nHandling Content Updates\nfunc view(ARSKView, nodeFor: ARAnchor) -> SKNode?\nAsks the delegate to provide a SpriteKit node corresponding to a newly added anchor.\nfunc view(ARSKView, didAdd: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node corresponding to a new AR anchor has been added to the scene.\nfunc view(ARSKView, willUpdate: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node's properties will be updated to match the current state of its corresponding anchor.\nfunc view(ARSKView, didRemove: SKNode, for: ARAnchor)\nTells the delegate that the SpriteKit node corresponding to an AR anchor has been removed from the scene."
  },
  {
    "title": "view(_:didRemove:for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskviewdelegate/2865594-view",
    "html": "Parameters\nview\n\nThe ARSKView object rendering the scene.\n\nnode\n\nThe removed SpriteKit node.\n\nanchor\n\nThe AR anchor corresponding to the node.\n\nDiscussion\n\nDepending on the session configuration, ARKit may automatically remove anchors from a session. The view calls this method once for each removed anchor.\n\nSee Also\nHandling Content Updates\nfunc view(ARSKView, nodeFor: ARAnchor) -> SKNode?\nAsks the delegate to provide a SpriteKit node corresponding to a newly added anchor.\nfunc view(ARSKView, didAdd: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node corresponding to a new AR anchor has been added to the scene.\nfunc view(ARSKView, willUpdate: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node's properties will be updated to match the current state of its corresponding anchor.\nfunc view(ARSKView, didUpdate: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node's properties have been updated to match the current state of its corresponding anchor."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/chirality/4169988-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nComparing hand chirality\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (HandAnchor.Chirality, HandAnchor.Chirality) -> Bool\nstatic func != (HandAnchor.Chirality, HandAnchor.Chirality) -> Bool"
  },
  {
    "title": "altitudeSource | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/3551717-altitudesource",
    "html": "Discussion\n\nThe value of this property is ARGeoAnchor.AltitudeSource.userDefined if you set the altitude yourself (see getGeoLocation(forPoint:completionHandler:)).\n\nIf your app doesn’t set the altitude, ARKit populates this property to indicate the altitude’s expected accuracy (either ARGeoAnchor.AltitudeSource.precise, or ARGeoAnchor.AltitudeSource.coarse), depending on the level of confidence ARKit has with the altitude data that’s available at the time.\n\nSee Also\nDefining Altitude\nvar altitude: CLLocationDistance?\nVertical distance, in meters, between this anchor and sea level.\nenum ARGeoAnchor.AltitudeSource\nOptions for setting a location anchor’s altitude."
  },
  {
    "title": "initWithCoordinate: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/3551718-initwithcoordinate",
    "html": "Parameters\ncoordinate\n\nLattitude and longitude of the anchor’s geographic location.\n\nDiscussion\n\nBecause this initializer does not take an altitude argument, ARKit sets the anchor’s altitude to ground level.\n\nSee Also\nCreating a Geo Anchor\n- initWithCoordinate:altitude:\nInitializes a location anchor with the given coordinate and altitude.\n- initWithName:coordinate:\nInitializes a named location anchor with the given coordinates.\n- initWithName:coordinate:altitude:\nInitializes a named location anchor with the given coordinates and altitude."
  },
  {
    "title": "PlaneAnchor.Classification.door | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/door",
    "html": "See Also\nGetting known classifications\ncase ceiling\nA ceiling.\ncase floor\nA floor.\ncase seat\nA seat.\ncase table\nA table.\ncase wall\nA wall.\ncase window\nA window."
  },
  {
    "title": "PlaneAnchor.Classification.ceiling | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/ceiling",
    "html": "See Also\nGetting known classifications\ncase door\nA door.\ncase floor\nA floor.\ncase seat\nA seat.\ncase table\nA table.\ncase wall\nA wall.\ncase window\nA window."
  },
  {
    "title": "PlaneAnchor.Classification.seat | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification/seat",
    "html": "See Also\nGetting known classifications\ncase ceiling\nA ceiling.\ncase door\nA door.\ncase floor\nA floor.\ncase table\nA table.\ncase wall\nA wall.\ncase window\nA window."
  },
  {
    "title": "ARAppClipCodeAnchor.URLDecodingState | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arappclipcodeanchor/urldecodingstate",
    "html": "Overview\n\nThe possible states of decoding the url of an App Clip Code.\n\nTopics\nStates\ncase decoded\nA state that indicates the completed decoding of an App Clip Code URL.\ncase decoding\nA state that indicates the continuing process of decoding an App Clip Code's URL.\ncase failed\nA state that indicates the failure to decode an App Clip Code's URL.\nRelationships\nConforms To\nSendable\nSee Also\nDecoding the URL\nvar url: URL?\nThe URL encoded in an App Clip Code.\nvar urlDecodingState: ARAppClipCodeAnchor.URLDecodingState\nA state that indicates the process of decoding an App Clip Code URL."
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider/mode/4108525-hashvalue",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nInspecting scene reconstruction modes\nvar description: String\nA textual description of a scene reconstruction mode.\nfunc hash(into: inout Hasher)"
  },
  {
    "title": "recoverySuggestion | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/4241496-recoverysuggestion",
    "html": "Relationships\nFrom Protocol\nLocalizedError\nSee Also\nProviding recovery suggestions\nvar failureReason: String?\nA localized message that describes why the error occurred.\nvar helpAnchor: String?"
  },
  {
    "title": "SceneReconstructionProvider.Mode.classification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider/mode/classification",
    "html": "Discussion\n\nFor information about the classification types, see classifications."
  },
  {
    "title": "estimatedScaleFactor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodyanchor/3255162-estimatedscalefactor",
    "html": "Discussion\n\nThe default value is 1.0. If you set automaticSkeletonScaleEstimationEnabled to true on ARBodyTrackingConfiguration, ARKit sets this property to a value between 0.0 and 1.0.\n\nARKit must know the height of a person in the camera feed to estimate an accurate world position for the person's body anchor. ARKit uses the value of estimatedScaleFactor to correct the body anchor's position in the physical environment.\n\nThe default body is 1.8 meters tall."
  },
  {
    "title": "errorDescription | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/4241492-errordescription",
    "html": "Relationships\nFrom Protocol\nLocalizedError\nSee Also\nInspecting world-tracking errors\nlet anchor: WorldAnchor?\nThe anchor that caused a world-tracking error.\nvar code: WorldTrackingProvider.Error.Code\nThe error code for a world-tracking error.\nenum WorldTrackingProvider.Error.Code\nThe error codes for errors that world tracking providers throw.\nvar description: String\nA textual description of the error that occurred.\nvar localizedDescription: String\nA localized description of the error."
  },
  {
    "title": "HandSkeleton.JointName.ringFingerIntermediateTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/ringfingerintermediatetip",
    "html": "See Also\nRing finger joints\ncase ringFingerIntermediateBase\ncase ringFingerKnuckle\ncase ringFingerMetacarpal\ncase ringFingerTip"
  },
  {
    "title": "HandSkeleton.JointName.ringFingerKnuckle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/ringfingerknuckle",
    "html": "See Also\nRing finger joints\ncase ringFingerIntermediateBase\ncase ringFingerIntermediateTip\ncase ringFingerMetacarpal\ncase ringFingerTip"
  },
  {
    "title": "HandSkeleton.JointName.ringFingerMetacarpal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/ringfingermetacarpal",
    "html": "See Also\nRing finger joints\ncase ringFingerIntermediateBase\ncase ringFingerIntermediateTip\ncase ringFingerKnuckle\ncase ringFingerTip"
  },
  {
    "title": "init(_:orientation:physicalWidth:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceimage/2942252-init",
    "html": "Parameters\nimage\n\nA Core Graphics image object.\n\norientation\n\nThe intended display orientation for the image.\n\nphysicalWidth\n\nThe real-world width, in meters, of the image.\n\nDiscussion\n\nTo accurately recognize the position and orientation of an image in the AR environment, ARKit must know the image's physical size. When you call this initializer, ARKit uses the physicalWidth measurement and orientation you provide together with the aspect ratio of the image itself to calculate the physical height. Use the physicalSize property of the created ARReferenceImage object to retrieve these values.\n\nImportant\n\nARKit preprocesses reference images before using them for image detection. To provide reference images bundled with your app, create AR Reference Image assets in your Xcode asset catalog, and use the referenceImageSetNamed(_:in:) method to load them.\n\nSee Also\nCreating Reference Images\ninit(CVPixelBuffer, orientation: CGImagePropertyOrientation, physicalWidth: CGFloat)\nCreates a new reference image from a Core Video pixel buffer."
  },
  {
    "title": "ARRaycastQuery.TargetAlignment.any | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery/targetalignment/any",
    "html": "See Also\nChoosing a Target Alignment\ncase horizontal\nThe case that indicates a target is aligned horizontally with respect to gravity.\ncase vertical\nThe case that indicates a target is aligned vertically with respect to gravity."
  },
  {
    "title": "validate(completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceimage/3194594-validate",
    "html": "Discussion\n\nConcurrency Note\n\nYou can call this method from synchronous code using a completion handler, as shown on this page, or you can call it as an asynchronous method that has the following declaration:\n\nfunc validate() async throws\n\n\nFor information about concurrency and asynchronous code in Swift, see Calling Objective-C APIs Asynchronously.\n\nARKit considers certain images invalid for image tracking (for example, an image that's all white). Call this function on a reference image you create programmatically to make sure ARKit can track it, before passing it in to your session's detectionImages array.\n\nYou only need this function when you create a reference image programmatically, because Xcode performs this validation for you when you create a reference image in an asset catalog."
  },
  {
    "title": "resourceGroupName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceimage/3223434-resourcegroupname",
    "html": "Discussion\n\nIf ARKit loaded this image from an AR resource group in an asset catalog, ARKit sets the value of this property to the resource group's name. Otherwise, the value of this property is nil.\n\nSee Also\nExamining a Reference Image\nvar name: String?\nA descriptive name for the image.\nvar physicalSize: CGSize\nThe real-world dimensions, in meters, of the image."
  },
  {
    "title": "generateDilatedDepth(from:commandBuffer:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armattegenerator/3229913-generatedilateddepth",
    "html": "Return Value\n\nA dilated depth texture which consists of a single channel of type float32.\n\nDiscussion\n\nYou use the linear depth information this function provides when compositing a virtual object with the camera image."
  },
  {
    "title": "renderer(_:didUpdate:for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnviewdelegate/2865799-renderer",
    "html": "Parameters\nrenderer\n\nThe ARSCNView object rendering the scene.\n\nnode\n\nThe updated SceneKit node.\n\nanchor\n\nThe AR anchor corresponding to the node.\n\nDiscussion\n\nDepending on the session configuration, ARKit may automatically update anchors in a session. The view calls this method once for each updated anchor.\n\nSee Also\nHandling Content Updates\nfunc renderer(SCNSceneRenderer, nodeFor: ARAnchor) -> SCNNode?\nAsks the delegate to provide a SceneKit node corresponding to a newly added anchor.\nfunc renderer(SCNSceneRenderer, didAdd: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node corresponding to a new AR anchor has been added to the scene.\nfunc renderer(SCNSceneRenderer, willUpdate: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node's properties will be updated to match the current state of its corresponding anchor.\nfunc renderer(SCNSceneRenderer, didRemove: SCNNode, for: ARAnchor)\nTells the delegate that the SceneKit node corresponding to a removed AR anchor has been removed from the scene."
  },
  {
    "title": "createReferenceObjectWithTransform:center:extent:completionHandler: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/3001731-createreferenceobjectwithtransfo",
    "html": "Parameters\ntransform\n\nA transform matrix defining the origin and orientation of the local coordinate system for the region to extract.\n\ncenter\n\nA point, relative to the origin specified by transform, that defines the center of the bounding box for the region to extract.\n\nextent\n\nThe width, height, and depth of the region to extract, centered on the center point and oriented to the local coordinate system specified by transform.\n\ncompletionHandler\n\nA handler to be invoked asynchronously after ARKit finishes creating the reference object. The handler takes two parameters:\n\nreferenceObject\n\nThe generated ARReferenceObject, or nil if a reference object could not be created.\n\nerror\n\nIf the referenceObject is nil, an ARError describing the failure.\n\nReturn Value\n\nAn ARReferenceObject representing the specified region of the world map, or nil if an object could not be extracted.\n\nDiscussion\n\nImportant\n\nThis method is valid only when running a session with ARObjectScanningConfiguration, which enables the high-fidelity spatial data collection needed for scanning reference objects. Calling this method on a session with a different configuration immediately invokes your completionHandler with an error.\n\nTo use the extracted reference object for 3D object detection, assign it to the detectionObjects property of a world tracking configuration. You can bundle reference objects in an app by saving them to files and adding them to an Xcode asset catalog.\n\nWhen ARKit detects a reference image, the transform of the resulting ARObjectAnchor is based on the orgin of the reference object's coordinate system—the transform you specify when extracting the reference object. For example, if a reference object represents a physical item that sits on a horizontal surface, virtual content should appear to sit on whatever surface the physical object does. To adjust a reference object's origin after extracting it, use the referenceObjectByApplyingTransform: method."
  },
  {
    "title": "captureHighResolutionFrameWithCompletion: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/3975720-capturehighresolutionframewithco",
    "html": "Parameters\ncompletion\n\nCode you provide that the framework runs after attempting to generate the frame.\n\nDiscussion\n\nIf the function succeeds, the completion handler's frame contains a high quality, high resolution capturedImage.\n\nIn the event of failure, the completion block receives a non-nil error object. A call may fail if a previous request for a high resolution capture hasn't completed yet, or an underlying problem occurs in the system's capture pipeline. You can identify the failure reason in either case by checking for highResolutionFrameCaptureInProgress or highResolutionFrameCaptureFailed, respectively.\n\nARKit populates the frame's properties other than pixel data, including pose information, anchors, and frame semantics. The system provides the frame to your completion handler asynchronously.\n\nYou can call this function at any time during a session. The system delivers a high-resolution frame out-of-band, which means that it doesn't affect the other frames that the session receives at a regular interval, such as currentFrame or the frame argument to session:didUpdateFrame:.\n\nFor the highest resolution captured image, choose a non-binned videoFormat in your session's configuration. You can call recommendedVideoFormatForHighResolutionFrameCapturing to select the best option for you.\n\nFor the highest resolution still images, choose a videoFormat among your configuration's supportedVideoFormats that returns true for isRecommendedForHighResolutionFrameCapturing. If your app doesn't have specific resolution requirements, you can use the framework-recommended format that recommendedVideoFormatForHighResolutionFrameCapturing returns.\n\nSee Also\nAccessing the camera frame\ncurrentFrame\nThe most recent still frame captured by the active camera feed, including ARKit's interpretation of it.\nARFrame\nA video image captured as part of a session with position-tracking information."
  },
  {
    "title": "runWithConfiguration:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2875735-runwithconfiguration",
    "html": "Parameters\nconfiguration\n\nAn object that defines motion and scene tracking behaviors for the session.\n\noptions\n\nOptions affecting how existing session state (if any) transitions to the new configuration.\n\nIf the session is running for the first time, this parameter has no effect.\n\nDiscussion\n\nThe session tracks device motion, captures and processes scene imagery from the device camera, and coordinates with your delegate object or ARSCNView or ARSKView view only when running.\n\nCalling this method on a session that has already started transitions immediately to the new session configuration. The options parameter determines how existing session state transitions to the new configuration. By default, the session resumes device position tracking from the last known state and keeps any anchors already included in the session (those you've added manually with addAnchor:, as well as those added automatically by ARKit features such as plane detection or face tracking).\n\nThis method returns immediately when called, but the session continues to run.\n\nSee Also\nConfiguring and running a session\n- runWithConfiguration:\nStarts AR processing for the session with the specified configuration.\nidentifier\nA unique identifier of the running session.\nARSessionRunOptions\nOptions for transitioning an AR session's current state when you change its configuration.\nconfiguration\nAn object that defines motion and scene tracking behaviors for the session.\n- pause\nPauses processing in the session."
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/4284961-hashvalue",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nInspecting hand joints\nvar description: String\nstatic var allCases: [HandSkeleton.JointName]\ntypealias HandSkeleton.JointName.AllCases\nfunc hash(into: inout Hasher)\nstatic func == (HandSkeleton.JointName, HandSkeleton.JointName) -> Bool\nstatic func != (HandSkeleton.JointName, HandSkeleton.JointName) -> Bool"
  },
  {
    "title": "reduce(into:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218760-reduce",
    "html": "Parameters\ninitialResult\n\nThe value to use as the initial accumulating value. The nextPartialResult closure receives initialResult the first time the closure executes.\n\nnextPartialResult\n\nA closure that combines an accumulating value and an element of the asynchronous sequence into a new accumulating value, for use in the next call of the nextPartialResult closure or returned to the caller.\n\nReturn Value\n\nThe final accumulated value. If the sequence has no elements, the result is initialResult.\n\nDiscussion\n\nUse the reduce(into:_:) method to produce a single value from the elements of an entire sequence. For example, you can use this method on a sequence of numbers to find their sum or product.\n\nThe nextPartialResult closure executes sequentially with an accumulating value initialized to initialResult and each element of the sequence.\n\nPrefer this method over reduce(_:_:) for efficiency when the result is a copy-on-write type, for example an Array or Dictionary."
  },
  {
    "title": "HandSkeleton.JointName.indexFingerTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/indexfingertip",
    "html": "See Also\nIndex finger joints\ncase indexFingerIntermediateBase\ncase indexFingerIntermediateTip\ncase indexFingerKnuckle\ncase indexFingerMetacarpal"
  },
  {
    "title": "HandSkeleton.JointName.middleFingerKnuckle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/middlefingerknuckle",
    "html": "See Also\nMiddle finger joints\ncase middleFingerIntermediateBase\ncase middleFingerIntermediateTip\ncase middleFingerMetacarpal\ncase middleFingerTip"
  },
  {
    "title": "HandSkeleton.JointName.indexFingerMetacarpal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/indexfingermetacarpal",
    "html": "See Also\nIndex finger joints\ncase indexFingerIntermediateBase\ncase indexFingerIntermediateTip\ncase indexFingerKnuckle\ncase indexFingerTip"
  },
  {
    "title": "HandSkeleton.JointName.middleFingerMetacarpal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/middlefingermetacarpal",
    "html": "See Also\nMiddle finger joints\ncase middleFingerIntermediateBase\ncase middleFingerIntermediateTip\ncase middleFingerKnuckle\ncase middleFingerTip"
  },
  {
    "title": "HandSkeleton.JointName.indexFingerKnuckle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/indexfingerknuckle",
    "html": "See Also\nIndex finger joints\ncase indexFingerIntermediateBase\ncase indexFingerIntermediateTip\ncase indexFingerMetacarpal\ncase indexFingerTip"
  },
  {
    "title": "HandSkeleton.JointName.indexFingerIntermediateTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/indexfingerintermediatetip",
    "html": "See Also\nIndex finger joints\ncase indexFingerIntermediateBase\ncase indexFingerKnuckle\ncase indexFingerMetacarpal\ncase indexFingerTip"
  },
  {
    "title": "isAutoFocusEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arobjectscanningconfiguration/3001726-isautofocusenabled",
    "html": "Discussion\n\nAutofocus is enabled by default."
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate/reason/3238106-hash",
    "html": "Parameters\nhasher\n\nThe hash function to use to hash the reason.\n\nRelationships\nFrom Protocol\nHashable\nSee Also\nHashes of Tracking Quality\nstatic func == (ARCamera.TrackingState.Reason, ARCamera.TrackingState.Reason) -> Bool\nIndicates whether two reasons are equal.\nstatic func != (ARCamera.TrackingState.Reason, ARCamera.TrackingState.Reason) -> Bool\nReturns a Boolean value indicating whether two values are not equal.\nvar hashValue: Int\nA value that identifies an object uniquely as compared to other instances of the same type."
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider/mode/4108523",
    "html": "See Also\nComparing scene reconstruction modes\nstatic func != (SceneReconstructionProvider.Mode, SceneReconstructionProvider.Mode) -> Bool"
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider/mode/4108524-hash",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nInspecting scene reconstruction modes\nvar description: String\nA textual description of a scene reconstruction mode.\nvar hashValue: Int"
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider/mode/4169993-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting scene reconstruction modes\nvar hashValue: Int\nfunc hash(into: inout Hasher)"
  },
  {
    "title": "helpAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/4241494-helpanchor",
    "html": "See Also\nProviding recovery suggestions\nvar recoverySuggestion: String?\nA localized message that describes how someone might recover from the error.\nvar failureReason: String?\nA localized message that describes why the error occurred."
  },
  {
    "title": "failureReason | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/4241493-failurereason",
    "html": "Relationships\nFrom Protocol\nLocalizedError\nSee Also\nProviding recovery suggestions\nvar recoverySuggestion: String?\nA localized message that describes how someone might recover from the error.\nvar helpAnchor: String?"
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/4241491-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting world-tracking errors\nlet anchor: WorldAnchor?\nThe anchor that caused a world-tracking error.\nvar code: WorldTrackingProvider.Error.Code\nThe error code for a world-tracking error.\nenum WorldTrackingProvider.Error.Code\nThe error codes for errors that world tracking providers throw.\nvar localizedDescription: String\nA localized description of the error.\nvar errorDescription: String?\nA localized message that describes the error that occurred."
  },
  {
    "title": "code | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/4241490-code",
    "html": "See Also\nInspecting world-tracking errors\nlet anchor: WorldAnchor?\nThe anchor that caused a world-tracking error.\nenum WorldTrackingProvider.Error.Code\nThe error codes for errors that world tracking providers throw.\nvar description: String\nA textual description of the error that occurred.\nvar localizedDescription: String\nA localized description of the error.\nvar errorDescription: String?\nA localized message that describes the error that occurred."
  },
  {
    "title": "urlDecodingState | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arappclipcodeanchor/3697076-urldecodingstate",
    "html": "Discussion\n\nThe initial value of this property is ARAppClipCodeAnchor.URLDecodingState.decoding.\n\nSee Also\nDecoding the URL\nvar url: URL?\nThe URL encoded in an App Clip Code.\nenum ARAppClipCodeAnchor.URLDecodingState\nThe states in the process of decoding an App Clip code URL."
  },
  {
    "title": "initWithCoordinate:altitude: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/3551719-initwithcoordinate",
    "html": "Parameters\ncoordinate\n\nLattitude and longitude of the anchor’s geographic location.\n\naltitude\n\nVertical distance, in meters, between this anchor and sea level.\n\nSee Also\nCreating a Geo Anchor\n- initWithCoordinate:\nInitializes a new location anchor with the given coordinates.\n- initWithName:coordinate:\nInitializes a named location anchor with the given coordinates.\n- initWithName:coordinate:altitude:\nInitializes a named location anchor with the given coordinates and altitude."
  },
  {
    "title": "ARSession.CollaborationData.Priority.critical | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/collaborationdata/priority/critical",
    "html": "Discussion\n\nARKit sets the data priority to ARSession.CollaborationData.Priority.critical when it's needed to establish or continue a collaborative session.\n\nSee Also\nData Sensitivity\ncase optional\nA priority that indicates that collaboration can continue without this data."
  },
  {
    "title": "ARSession.CollaborationData.Priority.optional | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/collaborationdata/priority/optional",
    "html": "Discussion\n\nARKit sets the data priority to ARSession.CollaborationData.Priority.optional when the data is important and time-sensitive but the session can continue if it's not received.\n\nSee Also\nData Sensitivity\ncase critical\nA priority that indicates that collaboration depends on this data."
  },
  {
    "title": "ARGeoAnchor.AltitudeSource | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/altitudesource",
    "html": "Overview\n\nEach altitude source has unique performance and accuracy characteristics.\n\nTopics\nSources\ncase precise\nThe framework sets the altitude using a high-resolution digital-elevation model.\ncase coarse\nThe framework sets the altitude using a coarse digital-elevation model.\ncase userDefined\nThe app defines the altitude.\ncase unknown\nAltitude isn’t yet set.\nRelationships\nConforms To\nSendable\nSee Also\nDefining Altitude\nvar altitude: CLLocationDistance?\nVertical distance, in meters, between this anchor and sea level.\nvar altitudeSource: ARGeoAnchor.AltitudeSource\nA record of the source from which an altitude came."
  },
  {
    "title": "url | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arappclipcodeanchor/3697075-url",
    "html": "Discussion\n\nTo enable users to decode your App Clip code's URL from an App Clip or within an AR app, associate your App Clip with its App Clip Code(s) in App Store Connect; for more information, see set up an App Clip experience. Because each App Clip Code belongs to a specific App Clip experience, you can decode an App Clip Code URL in a full app’s AR experience only after first providing an accompanying App Clip.\n\nThis property is nil by default. The framework sets a value for this property when urlDecodingState is ARAppClipCodeAnchor.URLDecodingState.decoded. This property remains nil for App Clip Codes that belong to another App Clip experience or development team.\n\nDistribute Your App and App Clip for Testing\n\nAlthough you can launch an App Clip from the camera after creating an App Clip local experience, ARKit checks the App Clip experience registry in App Store Connect to determine if your App Clip associates to a particular App Clip Code URL. When ARKit confirms that an App Clip Code in the physical environment belongs to your App Clip experience, the framework sets the value of this property.\n\nAfter setting up an App Clip experience, distribute your app to testers; for more information, see Testing the launch experience of your App Clip.\n\nSee Also\nDecoding the URL\nvar urlDecodingState: ARAppClipCodeAnchor.URLDecodingState\nA state that indicates the process of decoding an App Clip Code URL.\nenum ARAppClipCodeAnchor.URLDecodingState\nThe states in the process of decoding an App Clip code URL."
  },
  {
    "title": "skeleton | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodyanchor/3229909-skeleton",
    "html": "Discussion\n\nUse this property to interpret motion information in 3D space."
  },
  {
    "title": "view(_:didAdd:for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskviewdelegate/2865588-view",
    "html": "Parameters\nview\n\nThe ARSKView object rendering the scene.\n\nnode\n\nThe newly added SpriteKit node.\n\nanchor\n\nThe AR anchor corresponding to the node.\n\nDiscussion\n\nDepending on the session configuration, ARKit may automatically add anchors to a session. The view calls this method once for each new anchor. ARKit also calls this method to provide visual content for any ARAnchor objects you manually add using the session's add(anchor:) method.\n\nYou can provide visual content for the anchor by adding child nodes.\n\nAlternatively, you can implement the view(_:nodeFor:) method to create your own node (or instance of an SKNode subclass) for an anchor.\n\nSee Also\nHandling Content Updates\nfunc view(ARSKView, nodeFor: ARAnchor) -> SKNode?\nAsks the delegate to provide a SpriteKit node corresponding to a newly added anchor.\nfunc view(ARSKView, willUpdate: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node's properties will be updated to match the current state of its corresponding anchor.\nfunc view(ARSKView, didUpdate: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node's properties have been updated to match the current state of its corresponding anchor.\nfunc view(ARSKView, didRemove: SKNode, for: ARAnchor)\nTells the delegate that the SpriteKit node corresponding to an AR anchor has been removed from the scene."
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/alignment/4108469-hashvalue",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nComparing plane alignment\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (PlaneAnchor.Alignment, PlaneAnchor.Alignment) -> Bool\nstatic func != (PlaneAnchor.Alignment, PlaneAnchor.Alignment) -> Bool"
  },
  {
    "title": "PlaneAnchor.Alignment.vertical | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/alignment/vertical",
    "html": "See Also\nGetting plane alignment\ncase horizontal\nThe plane is positioned horizontally."
  },
  {
    "title": "ARMeshClassification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshclassification",
    "html": "Overview\n\nWhen you enable sceneReconstruction on a world-tracking configuration, ARKit provides several mesh anchors (ARMeshAnchor) that collectively estimate the shape of the physical environment. Within that model of the real world, ARKit may identify specific objects, like seats, windows, tables, or walls. ARKit shares that information by exposing one or more ARMeshClassification instances in a mesh's geometry property.\n\nFor a sample app that demonstrates mesh classification, see Visualizing and Interacting with a Reconstructed Scene.\n\nTopics\nOptions\ncase ceiling\nThe face is a part of a real-world ceiling.\ncase door\nThe face is a part of a real-world door.\ncase floor\nThe face is a part of a real-world floor.\ncase none\nA face ARKit can't classify.\ncase seat\nThe face is a part of a real-world seat.\ncase table\nThe face is a part of a real-world table.\ncase wall\nThe face is a part of a real-world wall.\ncase window\nThe face is a part of a real-world window.\nRelationships\nConforms To\nSendable\nSee Also\nGetting Geometry Information\nvar classification: ARGeometrySource?\nClassification for each face in the mesh.\nvar faces: ARGeometryElement\nAn object that contains a buffer of vertex indices of the geometry's faces.\nclass ARGeometryElement\nA container for index data, such as vertex indices of a face.\nvar normals: ARGeometrySource\nRays that define which direction is outside for each face."
  },
  {
    "title": "ARRaycastQuery.Target.estimatedPlane | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery/target/estimatedplane",
    "html": "Discussion\n\nA raycast with this target intersects feature points around the ray that ARKit estimates may be a real-world surface.\n\nWhen combined with ARRaycastQuery.TargetAlignment.any, ARKit bases estimated plane alignment on the normal of the surface.\n\nWhen you set your world-tracking configuration's sceneReconstruction to one of the mesh options, ARKit allows a raycast with this target (and target-alignment ARRaycastQuery.TargetAlignment.any) to intersect the scene mesh. Then the raycast result can include points even on nonplanar surfaces or surfaces that have few or no features, such as a white wall. If you set sceneReconstruction to ARSceneReconstructionNone, raycasts ignore the scene mesh.\n\nSee Also\nTargets\ncase existingPlaneGeometry\nA raycast target that requires a plane to have a definitive size and shape.\ncase existingPlaneInfinite\nA raycast target that specifies a detected plane, regardless of its size and shape."
  },
  {
    "title": "generateMatte(from:commandBuffer:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armattegenerator/3223424-generatematte",
    "html": "Return Value\n\nAn alpha matte texture at the resolution you chose at initialization."
  },
  {
    "title": "init(device:matteResolution:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armattegenerator/3223425-init",
    "html": "Discussion\n\nTo create matte textures in real-time, create this object once and reuse it every frame."
  },
  {
    "title": "ARMatteGenerator.Resolution | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armattegenerator/resolution",
    "html": "Overview\n\nYou generate a matte texture every frame, specifying whether its resolution is the full size or half of the camera image.\n\nTopics\nChoosing a Resolution Option\ncase full\nAn option that specifies the full camera image resolution.\ncase half\nAn option that specifies half of the camera image resolution.\nRelationships\nConforms To\nSendable"
  },
  {
    "title": "AnchorUpdate.Event.added | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdate/event/added",
    "html": "See Also\nInspecting anchor update events\ncase updated\nAn event that occurs when an existing anchor updates data.\ncase removed\nAn event that occurs when ARKit stops tracking an anchor."
  },
  {
    "title": "worldTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastresult/3132062-worldtransform",
    "html": "See Also\nIdentifying Results\nvar anchor: ARAnchor?\nThe anchor for the plane that the ray intersected.\nvar target: ARRaycastQuery.Target\nThe type of surface that the ray intersects.\nenum ARRaycastQuery.Target\nThe types of surface you allow a raycast to intersect with.\nvar targetAlignment: ARRaycastQuery.TargetAlignment\nThe alignment of the plane that the ray intersected.\nenum ARRaycastQuery.TargetAlignment\nA specification that indicates a target's alignment with respect to gravity."
  },
  {
    "title": "ARMeshGeometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshgeometry",
    "html": "Overview\n\nThe information in this class holds the geometry data for a single anchor of the scene mesh. Each vertex in the anchor's mesh represents one connection point. Every three-vertex combination forms a unique triangle called a face. Each face includes an outside-directional normal and a classification. If ARKit cannot classify a particular face, the value is 0, –– the raw value for ARMeshClassification.none.\n\nTopics\nAccessing Geometry Data\nvar vertices: ARGeometrySource\nThe vertices of the mesh.\nclass ARGeometrySource\nMesh data in a buffer-based array.\nGetting Geometry Information\nvar classification: ARGeometrySource?\nClassification for each face in the mesh.\nenum ARMeshClassification\nEnumeration of different classes of real-world objects that ARKit can identify.\nvar faces: ARGeometryElement\nAn object that contains a buffer of vertex indices of the geometry's faces.\nclass ARGeometryElement\nA container for index data, such as vertex indices of a face.\nvar normals: ARGeometrySource\nRays that define which direction is outside for each face.\nRelationships\nInherits From\nNSObject\nConforms To\nNSSecureCoding\nSee Also\nAccessing the Mesh\nvar geometry: ARMeshGeometry\n3D information about the mesh such as its shape and classifications."
  },
  {
    "title": "getGeoLocationForPoint:completionHandler: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/3571352-getgeolocationforpoint",
    "html": "Parameters\nposition\n\nPosition in local coordinates to convert.\n\ncompletionHandler\n\nCode that control will execute when this function returns. The session runs this code on its delegate queue. The parameters are:\n\ncoordinate\n\nLocation coordinates (latitude, longitude).\n\naltitude\n\nThe altitude.\n\nerror\n\nThe reason, if conversion fails.\n\nReturn Value\n\nA latitude, longitude, and altitude for the argument position in the session’s world coordinate-space.\n\nDiscussion\n\nARKit refers to its local coordinate space as “world” coordinate space, but this is different from geographic coordinates. For more information on ARKit’s coordinate space, see setWorldOrigin:.\n\nTo succeed, this function requires an ARGeoTrackingConfiguration session with state equal to ARGeoTrackingStateLocalized."
  },
  {
    "title": "geometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshanchor/3516909-geometry",
    "html": "Discussion\n\nContains the anchor's portion of mesh data that, with any other mesh anchors in the AR session, collectively reconstruct the scene around the user. The mesh anchor records this data in it's own coordinate system.\n\nSee Also\nAccessing the Mesh\nclass ARMeshGeometry\nMesh information stored in an efficient, array-based format."
  },
  {
    "title": "removeAnchor: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2865607-removeanchor",
    "html": "Parameters\nanchor\n\nThe anchor to remove.\n\nDiscussion\n\nChanges to anchor tracking take effect when the next frame is captured.\n\nSee Also\nManaging anchors\n- addAnchor:\nAdds the specified anchor to be tracked by the session."
  },
  {
    "title": "name | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceimage/2941026-name",
    "html": "Discussion\n\nFor reference images loaded from an Xcode asset catalog, this property is the name assigned in the asset catalog. For programmatically created reference images, this value is nil.\n\nNote\n\nThis string is not localized text intended for user display. However, in debugging you can use this property to indicate which image was detected.\n\nSee Also\nExamining a Reference Image\nvar physicalSize: CGSize\nThe real-world dimensions, in meters, of the image.\nvar resourceGroupName: String?\nThe AR resource group name for this image."
  },
  {
    "title": "primaryLightDirection | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ardirectionallightestimate/2928221-primarylightdirection",
    "html": "Discussion\n\nWhen ARKit analyzes the directional lighting environment for a detected face, the resulting lighting estimate can represent the influence of multiple light sources with different directions and intensities. To access this level of detail for use in your custom rendering code, use the sphericalHarmonicsCoefficients property.\n\nIf your app displays AR content using a technology that doesn’t support environment-based lighting, this primaryLightDirection property represents the average of directional light sources in the scene. This vector is normalized and in world coordinate space.\n\nSee Also\nExamining Light Parameters\nvar sphericalHarmonicsCoefficients: Data\nData describing the estimated lighting environment in all directions.\nvar primaryLightIntensity: CGFloat\nThe estimated intensity, in lumens, of the strongest directional light source in the scene."
  },
  {
    "title": "referenceImages(inGroupNamed:bundle:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceimage/2948910-referenceimages",
    "html": "Parameters\nname\n\nThe name of an AR Resource Group from your Xcode project's main asset catalog.\n\nbundle\n\nThe bundle from which to load asset catalog resources, or nil to use your app's main bundle.\n\nReturn Value\n\nA set of all unique reference images in the specified group.\n\nDiscussion\n\nTo use the images for image detection in a world-tracking AR session, provide this set for your session configuration's detectionImages property."
  },
  {
    "title": "HandSkeleton.JointName.AllCases | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/allcases",
    "html": "See Also\nInspecting hand joints\nvar description: String\nstatic var allCases: [HandSkeleton.JointName]\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (HandSkeleton.JointName, HandSkeleton.JointName) -> Bool\nstatic func != (HandSkeleton.JointName, HandSkeleton.JointName) -> Bool"
  },
  {
    "title": "allCases | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname/4284956-allcases",
    "html": "Relationships\nFrom Protocol\nCaseIterable\nSee Also\nInspecting hand joints\nvar description: String\ntypealias HandSkeleton.JointName.AllCases\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (HandSkeleton.JointName, HandSkeleton.JointName) -> Bool\nstatic func != (HandSkeleton.JointName, HandSkeleton.JointName) -> Bool"
  },
  {
    "title": "supportsAppClipCodeTracking | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration/3697084-supportsappclipcodetracking",
    "html": "Discussion\n\nDevices require the Apple Neural Engine (ANE) to track App Clip Codes. The system sets this property to true if the device contains the ANE chip. The default value of this property is false.\n\nCall this function before setting appClipCodeTrackingEnabled.\n\nSee Also\nAccessing App Clip Codes\nInteracting with App Clip Codes in AR\nDisplay content and provide services in an AR experience with App Clip Codes.\nvar appClipCodeTrackingEnabled: Bool\nA Boolean value that indicates if the framework searches the physical environment for App Clip Codes.\nclass ARAppClipCodeAnchor\nAn anchor that tracks the position and orientation of an App Clip Code in the physical environment."
  },
  {
    "title": "checkAvailability(at:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration/3571350-checkavailability",
    "html": "Parameters\ncoordinate\n\nThe GPS location that the framework checks for availability.\n\ncompletionHandler\n\nCode you supply that runs after the function returns. The closure takes a Boolean argument that indicates whether geotracking is available.\n\nDiscussion\n\nThis function returns false under the following circumstances:\n\nARKit lacks localization imagery for the argument GPS coordinate.\n\nA network connection is unavailable to download localization imagery.\n\nThe device lacks cellular (GPS) capability.\n\nTo determine availability at the user’s GPS coordinate, use checkAvailability(completionHandler:) instead.\n\nFor a list of supported areas and cities, see ARGeoTrackingConfiguration.\n\nSee Also\nChecking Availability\nclass func checkAvailability(completionHandler: (Bool, Error?) -> Void)\nDetermines if geotracking supports the user’s current location."
  },
  {
    "title": "primaryLightIntensity | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ardirectionallightestimate/2928219-primarylightintensity",
    "html": "Discussion\n\nWhen ARKit analyzes the directional lighting environment for a detected face, the resulting lighting estimate can represent the influence of multiple light sources with different directions and intensities. To access this level of detail for use in your custom rendering code, use the sphericalHarmonicsCoefficients property.\n\nIf your app displays AR content using a technology that doesn’t support environment-based lighting, this primaryLightIntensity property represents the average of directional light sources in the scene. This value is scaled to be appropriate for use in rendering architectures that use realistic lighting metrics, with a value of 1000 representing neutral lighting.\n\nFor example, you can pass this value directly to the intensity property of a SceneKit directional light for lighting results that roughly match those of the real-world scene captured by the device camera. (However, passing this value to SceneKit is generally not necessary; the ARSCNView class automatically sets SceneKit lighting based on the sphericalHarmonicsCoefficients property.)\n\nSee Also\nExamining Light Parameters\nvar sphericalHarmonicsCoefficients: Data\nData describing the estimated lighting environment in all directions.\nvar primaryLightDirection: simd_float3\nA vector indicating the orientation of the strongest directional light source in the scene."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration/3566284-init",
    "html": "Discussion\n\nTo use the configuration in an AR experience, pass it as an argument to your app’s run(_:options:) function."
  },
  {
    "title": "ARKitSession.Events.Iterator | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/iterator",
    "html": "Topics\nType Aliases\ntypealias ARKitSession.Events.Iterator.Element\nInstance Methods\nfunc next() -> ARKitSession.Events.Element?\nRelationships\nConforms To\nAsyncIteratorProtocol"
  },
  {
    "title": "intrinsics | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/2875730-intrinsics",
    "html": "Discussion\n\nThe intrinsic matrix (commonly represented in equations as K) is based on physical characteristics of the device camera and a pinhole camera model. You can use the matrix to transform 3D coordinates to 2D coordinates on an image plane.\n\nThe values fx and fy are the pixel focal length, and are identical for square pixels. The values ox and oy are the offsets of the principal point from the top-left corner of the image frame. All values are expressed in pixels.\n\nSee Also\nExamining Imaging Parameters\nvar imageResolution: CGSize\nThe width and height, in pixels, of the captured camera image."
  },
  {
    "title": "ARFrame.WorldMappingStatus.extending | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/worldmappingstatus/extending",
    "html": "Discussion\n\nWhen the worldMappingStatus of the session's currentFrame is ARFrame.WorldMappingStatus.extending, the session has produced a high-fidelity internal map of the real-world spaces the device has recently passed thorugh, but is still collecting data to map the area around the device's current position and the scene visible to the camera.\n\nThis status provides moderate to high reliability for relocalizing to a saved world map, provided that:\n\nYou call getCurrentWorldMap(completionHandler:) to save the world map while the status of the currentFrame is ARFrame.WorldMappingStatus.extending.\n\nWhen you run a new session (later or on another device) from that ARWorldMap, the device running the new session passes through positions and orientations that were visited by the device that saved the session.\n\nSaving or sharing a world map at this time is likely to produce adequate results, but a higher quality world map may be possible if you wait until the user explores more of their surroundings and the status changes to ARFrame.WorldMappingStatus.mapped."
  },
  {
    "title": "flatMap(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218743-flatmap",
    "html": "Parameters\ntransform\n\nA mapping closure. transform accepts an element of this sequence as its parameter and returns an AsyncSequence.\n\nReturn Value\n\nA single, flattened asynchronous sequence that contains all elements in all the asynchronous sequences produced by transform.\n\nDiscussion\n\nUse this method to receive a single-level asynchronous sequence when your transformation produces an asynchronous sequence for each element.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 5. The transforming closure takes the received Int and returns a new Counter that counts that high. For example, when the transform receives 3 from the base sequence, it creates a new Counter that produces the values 1, 2, and 3. The flatMap(_:) method “flattens” the resulting sequence-of-sequences into a single AsyncSequence.\n\nlet stream = Counter(howHigh: 5)\n    .flatMap { Counter(howHigh: $0) }\nfor await number in stream {\n    print(number, terminator: \" \")\n}\n// Prints \"1 1 2 1 2 3 1 2 3 4 1 2 3 4 5 \"\n"
  },
  {
    "title": "flatMap(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218741-flatmap",
    "html": "Parameters\ntransform\n\nAn error-throwing mapping closure. transform accepts an element of this sequence as its parameter and returns an AsyncSequence. If transform throws an error, the sequence ends.\n\nReturn Value\n\nA single, flattened asynchronous sequence that contains all elements in all the asynchronous sequences produced by transform. The sequence ends either when the last sequence created from the last element from base sequence ends, or when transform throws an error.\n\nDiscussion\n\nUse this method to receive a single-level asynchronous sequence when your transformation produces an asynchronous sequence for each element.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 5. The transforming closure takes the received Int and returns a new Counter that counts that high. For example, when the transform receives 3 from the base sequence, it creates a new Counter that produces the values 1, 2, and 3. The flatMap(_:) method “flattens” the resulting sequence-of-sequences into a single AsyncSequence. However, when the closure receives 4, it throws an error, terminating the sequence.\n\ndo {\n    let stream = Counter(howHigh: 5)\n        .flatMap { (value) -> Counter in\n            if value == 4 {\n                throw MyError()\n            }\n            return Counter(howHigh: value)\n        }\n    for try await number in stream {\n        print(number, terminator: \" \")\n    }\n} catch {\n    print(error)\n}\n// Prints \"1 1 2 1 2 3 MyError() \"\n"
  },
  {
    "title": "filter(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218737-filter",
    "html": "Parameters\nisIncluded\n\nA closure that takes an element of the asynchronous sequence as its argument and returns a Boolean value that indicates whether to include the element in the filtered sequence.\n\nReturn Value\n\nAn asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\n\nDiscussion\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The filter(_:) method returns true for even values and false for odd values, thereby filtering out the odd values:\n\nlet stream = Counter(howHigh: 10)\n    .filter { $0 % 2 == 0 }\nfor await number in stream {\n    print(number, terminator: \" \")\n}\n// Prints \"2 4 6 8 10 \"\n"
  },
  {
    "title": "ARTrackingStateReasonRelocalizing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/artrackingstatereason/artrackingstatereasonrelocalizing",
    "html": "Discussion\n\nARKit cannot track device position or orientation when the session has been interrupted (for example, by dismissing the view hosting an AR session or switching to another app). When resuming the session after an interruption, you cannot be certain that the world coordinate system (used for placing anchors) matches the device's real-world environment.\n\nIf your session or view delegate implements the sessionShouldAttemptRelocalization: method and returns YES, ARKit attempts to reconcile pre- and post-interruption world tracking state. During this process, called relocalization, world tracking quality is ARTrackingStateLimited, with ARTrackingStateReasonRelocalizing as the reason for limited quality.\n\nIf successful, the relocalization process ends after a short time, tracking quality returns to the ARTrackingStateNormal state, and the world coordinate system and anchor positions generally reflect their state before the interruption.\n\nHowever, the speed and success rate of relocalization can vary depending on real-world conditions. You may wish to hide AR content or disable UI during relocalization, and reset tracking if relocalization doesn't succeed within a time frame appropriate for your app.\n\nSee Also\nReason Values\nARTrackingStateReasonNone\nThe current tracking state is not limited.\nARTrackingStateReasonInitializing\nThe AR session has not yet gathered enough camera or motion data to provide tracking information.\nARTrackingStateReasonExcessiveMotion\nThe device is moving too fast for accurate image-based position tracking.\nARTrackingStateReasonInsufficientFeatures\nThe scene visible to the camera does not contain enough distinguishable features for image-based position tracking."
  },
  {
    "title": "map(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218748-map",
    "html": "Parameters\ntransform\n\nA mapping closure. transform accepts an element of this sequence as its parameter and returns a transformed value of the same or of a different type. transform can also throw an error, which ends the transformed sequence.\n\nReturn Value\n\nAn asynchronous sequence that contains, in order, the elements produced by the transform closure.\n\nDiscussion\n\nUse the map(_:) method to transform every element received from a base asynchronous sequence. Typically, you use this to transform from one type of element to another.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 5. The closure provided to the map(_:) method takes each Int and looks up a corresponding String from a romanNumeralDict dictionary. This means the outer for await in loop iterates over String instances instead of the underlying Int values that Counter produces. Also, the dictionary doesn’t provide a key for 4, and the closure throws an error for any key it can’t look up, so receiving this value from Counter ends the modified sequence with an error.\n\nlet romanNumeralDict: [Int: String] =\n    [1: \"I\", 2: \"II\", 3: \"III\", 5: \"V\"]\n\n\ndo {\n    let stream = Counter(howHigh: 5)\n        .map { (value) throws -> String in\n            guard let roman = romanNumeralDict[value] else {\n                throw MyError()\n            }\n            return roman\n        }\n    for try await numeral in stream {\n        print(numeral, terminator: \" \")\n    }\n} catch {\n    print(\"Error: \\(error)\")\n}\n// Prints \"I II III Error: MyError() \"\n"
  },
  {
    "title": "prefix(while:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218756-prefix",
    "html": "Parameters\npredicate\n\nA closure that takes an element as a parameter and returns a Boolean value indicating whether the element should be included in the modified sequence.\n\nReturn Value\n\nAn asynchronous sequence of the initial, consecutive elements that satisfy predicate.\n\nDiscussion\n\nUse prefix(while:) to produce values while elements from the base sequence meet a condition you specify. The modified sequence ends when the predicate closure returns false.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The prefix(while:) method causes the modified sequence to pass along values so long as they aren’t divisible by 2 and 3. Upon reaching 6, the sequence ends:\n\nlet stream = Counter(howHigh: 10)\n    .prefix { $0 % 2 != 0 || $0 % 3 != 0 }\nfor try await number in stream {\n    print(number, terminator: \" \")\n}\n// Prints \"1 2 3 4 5 \"\n"
  },
  {
    "title": "allSatisfy(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218725-allsatisfy",
    "html": "Parameters\npredicate\n\nA closure that takes an element of the asynchronous sequence as its argument and returns a Boolean value that indicates whether the passed element satisfies a condition.\n\nReturn Value\n\ntrue if the sequence contains only elements that satisfy predicate; otherwise, false.\n\nDiscussion\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The allSatisfy(_:) method checks to see whether all elements produced by the sequence are less than 10.\n\nlet allLessThanTen = await Counter(howHigh: 10)\n    .allSatisfy { $0 < 10 }\nprint(allLessThanTen)\n// Prints \"false\"\n\n\nThe predicate executes each time the asynchronous sequence produces an element, until either the predicate returns false or the sequence ends.\n\nIf the asynchronous sequence is empty, this method returns true."
  },
  {
    "title": "WorldTrackingProvider.Error.Code | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/code",
    "html": "Topics\nDetermining causes for tracking failures\ncase addWorldAnchorFailed\nThe error code for when a world tracking provider can’t add a world anchor.\ncase removeWorldAnchorFailed\nThe error code for when a world tracking provider can’t remove a world anchor.\ncase worldAnchorLimitReached\nThe error code for when a world tracking provider reaches its world anchor limit.\nInspecting tracking failures\nvar description: String\nA textual description of the error code.\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (WorldTrackingProvider.Error.Code, WorldTrackingProvider.Error.Code) -> Bool\nstatic func != (WorldTrackingProvider.Error.Code, WorldTrackingProvider.Error.Code) -> Bool\nRelationships\nConforms To\nCustomStringConvertible\nSee Also\nInspecting world-tracking errors\nlet anchor: WorldAnchor?\nThe anchor that caused a world-tracking error.\nvar code: WorldTrackingProvider.Error.Code\nThe error code for a world-tracking error.\nvar description: String\nA textual description of the error that occurred.\nvar localizedDescription: String\nA localized description of the error.\nvar errorDescription: String?\nA localized message that describes the error that occurred."
  },
  {
    "title": "anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/4241489-anchor",
    "html": "See Also\nInspecting world-tracking errors\nvar code: WorldTrackingProvider.Error.Code\nThe error code for a world-tracking error.\nenum WorldTrackingProvider.Error.Code\nThe error codes for errors that world tracking providers throw.\nvar description: String\nA textual description of the error that occurred.\nvar localizedDescription: String\nA localized description of the error.\nvar errorDescription: String?\nA localized message that describes the error that occurred."
  },
  {
    "title": "localizedDescription | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error/4241495-localizeddescription",
    "html": "See Also\nInspecting world-tracking errors\nlet anchor: WorldAnchor?\nThe anchor that caused a world-tracking error.\nvar code: WorldTrackingProvider.Error.Code\nThe error code for a world-tracking error.\nenum WorldTrackingProvider.Error.Code\nThe error codes for errors that world tracking providers throw.\nvar description: String\nA textual description of the error that occurred.\nvar errorDescription: String?\nA localized message that describes the error that occurred."
  },
  {
    "title": "init(coordinate:altitude:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/3616116-init",
    "html": "Parameters\ncoordinate\n\nLattitude and longitude of the anchor’s geographic location.\n\naltitude\n\nVertical distance, in meters, between this anchor and sea level.\n\nSee Also\nCreating a Geo Anchor\ninit(name: String, coordinate: CLLocationCoordinate2D, altitude: CLLocationDistance)\nInitializes a named location anchor with the given coordinates and altitude.\ninit(name: String, coordinate: CLLocationCoordinate2D, altitude: CLLocationDistance?)\nInitializes a named location anchor with the given coordinates and altitude."
  },
  {
    "title": "altitude | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/3616115-altitude",
    "html": "Discussion\n\nNegative values indicate below sea level. This property is valid only when altitudeSource is a value other than ARGeoAnchor.AltitudeSource.unknown.\n\nSee Also\nDefining Altitude\nvar altitudeSource: ARGeoAnchor.AltitudeSource\nA record of the source from which an altitude came.\nenum ARGeoAnchor.AltitudeSource\nOptions for setting a location anchor’s altitude."
  },
  {
    "title": "init(name:coordinate:altitude:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/3616117-init",
    "html": "Parameters\nname\n\nName of the anchor.\n\ncoordinate\n\nLattitude and longitude of the anchor’s geographic location.\n\naltitude\n\nVertical distance, in meters, between this anchor and sea level.\n\nSee Also\nCreating a Geo Anchor\ninit(coordinate: CLLocationCoordinate2D, altitude: CLLocationDistance?)\nInitializes a location anchor with the given coordinate and altitude.\ninit(name: String, coordinate: CLLocationCoordinate2D, altitude: CLLocationDistance)\nInitializes a named location anchor with the given coordinates and altitude."
  },
  {
    "title": "init(name:coordinate:altitude:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/3551721-init",
    "html": "Parameters\nname\n\nName of the anchor.\n\ncoordinate\n\nLattitude and longitude of the anchor’s geographic location.\n\naltitude\n\nVertical distance, in meters, between this anchor and sea level.\n\nSee Also\nCreating a Geo Anchor\ninit(coordinate: CLLocationCoordinate2D, altitude: CLLocationDistance?)\nInitializes a location anchor with the given coordinate and altitude.\ninit(name: String, coordinate: CLLocationCoordinate2D, altitude: CLLocationDistance?)\nInitializes a named location anchor with the given coordinates and altitude."
  },
  {
    "title": "coordinate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor/3089133-coordinate",
    "html": "Discussion\n\nThis property is set by the app at anchor initialization."
  },
  {
    "title": "radius | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arappclipcodeanchor/3697074-radius",
    "html": "Discussion\n\nARKit estimates the value of this property at runtime. As the user views an App Clip Code from different angles, ARKit refines its estimate of the true radius of the App Clip Code."
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdate/event/4111168",
    "html": "See Also\nComparing anchor update events\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func != (AnchorUpdate<AnchorType>.Event, AnchorUpdate<AnchorType>.Event) -> Bool"
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdate/event/4111170-hash",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nComparing anchor update events\nvar hashValue: Int\nvar description: String\nstatic func == (AnchorUpdate<AnchorType>.Event, AnchorUpdate<AnchorType>.Event) -> Bool\nstatic func != (AnchorUpdate<AnchorType>.Event, AnchorUpdate<AnchorType>.Event) -> Bool"
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdate/event/4169985-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nComparing anchor update events\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (AnchorUpdate<AnchorType>.Event, AnchorUpdate<AnchorType>.Event) -> Bool\nstatic func != (AnchorUpdate<AnchorType>.Event, AnchorUpdate<AnchorType>.Event) -> Bool"
  },
  {
    "title": "AnchorUpdate.Event.removed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdate/event/removed",
    "html": "See Also\nInspecting anchor update events\ncase added\nAn event that occurs when ARKit starts tracking an anchor.\ncase updated\nAn event that occurs when an existing anchor updates data."
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdate/event/4111171-hashvalue",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nComparing anchor update events\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (AnchorUpdate<AnchorType>.Event, AnchorUpdate<AnchorType>.Event) -> Bool\nstatic func != (AnchorUpdate<AnchorType>.Event, AnchorUpdate<AnchorType>.Event) -> Bool"
  },
  {
    "title": "ARFrame.WorldMappingStatus.notAvailable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/worldmappingstatus/notavailable",
    "html": "Discussion\n\nWhen the worldMappingStatus of the session's currentFrame is ARFrame.WorldMappingStatus.notAvailable, the session has no internal map of the real-world space around the device, nor the scene visible to the camera. Calling getCurrentWorldMap(completionHandler:) at this time results in an error.\n\nThis status occurs shortly after starting a new session. To save or share a world map, wait for the user to explore their surroundings and the session's status to change to ARFrame.WorldMappingStatus.mapped or ARFrame.WorldMappingStatus.extending."
  },
  {
    "title": "ARFrame.WorldMappingStatus.limited | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/worldmappingstatus/limited",
    "html": "Discussion\n\nWhen the worldMappingStatus of the session's currentFrame is ARFrame.WorldMappingStatus.limited, the session has not yet fully mapped the real-world space around the device, nor the scene visible to the camera.\n\nAlthough it is possible at this time to save a world map by calling getCurrentWorldMap(completionHandler:), the resulting ARWorldMap is unlikely to be useful for relocalization in the real-world space near the device's current position.\n\nTo produce a higher quality world map, wait for the user to explore more of their surroundings and the session's status to change to ARFrame.WorldMappingStatus.mapped or ARFrame.WorldMappingStatus.extending."
  },
  {
    "title": "ARSessionRunOptions | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionrunoptions",
    "html": "Topics\nRun Options\nARSessionRunOptionResetTracking\nAn option to reset the device's position from the session's previous run.\nARSessionRunOptionRemoveExistingAnchors\nAn option to remove any anchor objects associated with the session's previous run.\nARSessionRunOptionStopTrackedRaycasts\nAn option to stop all active tracked raycasts.\nARSessionRunOptionResetSceneReconstruction\nAn option to reset the scene mesh.\nSee Also\nConfiguring and running a session\n- runWithConfiguration:options:\nStarts AR processing for the session with the specified configuration and options.\n- runWithConfiguration:\nStarts AR processing for the session with the specified configuration.\nidentifier\nA unique identifier of the running session.\nconfiguration\nAn object that defines motion and scene tracking behaviors for the session.\n- pause\nPauses processing in the session."
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationtype/4131670-hash",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nComparing authorizations\nvar hashValue: Int\nstatic func == (ARKitSession.AuthorizationType, ARKitSession.AuthorizationType) -> Bool\nstatic func != (ARKitSession.AuthorizationType, ARKitSession.AuthorizationType) -> Bool\nvar description: String"
  },
  {
    "title": "addAnchor: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2865612-addanchor",
    "html": "Parameters\nanchor\n\nThe anchor to add.\n\nDiscussion\n\nChanges to anchor tracking take effect when the next frame is captured.\n\nSee Also\nManaging anchors\n- removeAnchor:\nRemoves the specified anchor from tracking by the session."
  },
  {
    "title": "mesh | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/scenereconstruction/3521372-mesh",
    "html": "See Also\nModeling the Environment\ninit(rawValue: UInt)\nInitializes a scene-reconstruction object.\nstatic var meshWithClassification: ARConfiguration.SceneReconstruction\nAn approximate shape of the physical environment, including classification of the real-world objects within it."
  },
  {
    "title": "appClipCodeTrackingEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration/3697083-appclipcodetrackingenabled",
    "html": "Discussion\n\nWhen this property's value is true, the session delegate receives an ARAppClipCodeAnchor via session(_:didAdd:) for every App Clip Code that ARKit detects in the physical environment. The default value is false.\n\nBefore calling this function, check that the configuration supports App Clip Code tracking by calling supportsAppClipCodeTracking.\n\nTo avoid scanning a physical code that’s not connected to an App Clip, the system ensures that an app provides an App Clip before allowing the app to interact with App Clip Codes. Without providing an App Clip, the app can recognize codes in the environment by determining their physical location (transform), but code URLs (url) remain nil.\n\nSee Also\nAccessing App Clip Codes\nInteracting with App Clip Codes in AR\nDisplay content and provide services in an AR experience with App Clip Codes.\nclass var supportsAppClipCodeTracking: Bool\nA flag that indicates if the device tracks App Clip Codes.\nclass ARAppClipCodeAnchor\nAn anchor that tracks the position and orientation of an App Clip Code in the physical environment."
  },
  {
    "title": "meshWithClassification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/scenereconstruction/3521373-meshwithclassification",
    "html": "See Also\nModeling the Environment\ninit(rawValue: UInt)\nInitializes a scene-reconstruction object.\nstatic var mesh: ARConfiguration.SceneReconstruction\nA polygonal mesh approximation of the physical environment."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/scenereconstruction/3539111-init",
    "html": "See Also\nModeling the Environment\nstatic var mesh: ARConfiguration.SceneReconstruction\nA polygonal mesh approximation of the physical environment.\nstatic var meshWithClassification: ARConfiguration.SceneReconstruction\nAn approximate shape of the physical environment, including classification of the real-world objects within it."
  },
  {
    "title": "sphericalHarmonicsCoefficients | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ardirectionallightestimate/2928222-sphericalharmonicscoefficients",
    "html": "Discussion\n\nSpherical harmonics provide a compact mathematical model for the global lighting environment around a point in space, describing the distribution and colors of multiple directional light sources. When used in a renderer that supports environment-based lighting, spherical harmonics provide much less high-frequency detail than a cube map texture, but make much more efficient use of GPU resources.\n\nARKit provides second-level spherical harmonics in separate red, green, and blue data planes. Thus, this data buffer contains 3 sets of 9 coefficients, or a total of 27 values of 32-bit floating point type.\n\nSee Also\nExamining Light Parameters\nvar primaryLightDirection: simd_float3\nA vector indicating the orientation of the strongest directional light source in the scene.\nvar primaryLightIntensity: CGFloat\nThe estimated intensity, in lumens, of the strongest directional light source in the scene."
  },
  {
    "title": "reduce(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218758-reduce",
    "html": "Parameters\ninitialResult\n\nThe value to use as the initial accumulating value. The nextPartialResult closure receives initialResult the first time the closure runs.\n\nnextPartialResult\n\nA closure that combines an accumulating value and an element of the asynchronous sequence into a new accumulating value, for use in the next call of the nextPartialResult closure or returned to the caller.\n\nReturn Value\n\nThe final accumulated value. If the sequence has no elements, the result is initialResult.\n\nDiscussion\n\nUse the reduce(_:_:) method to produce a single value from the elements of an entire sequence. For example, you can use this method on an sequence of numbers to find their sum or product.\n\nThe nextPartialResult closure executes sequentially with an accumulating value initialized to initialResult and each element of the sequence.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 4. The reduce(_:_:) method sums the values received from the asynchronous sequence.\n\nlet sum = await Counter(howHigh: 4)\n    .reduce(0) {\n        $0 + $1\n    }\nprint(sum)\n// Prints \"10\"\n"
  },
  {
    "title": "min(by:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218752-min",
    "html": "Parameters\nareInIncreasingOrder\n\nA predicate that returns true if its first argument should be ordered before its second argument; otherwise, false.\n\nReturn Value\n\nThe sequence’s minimum element, according to areInIncreasingOrder. If the sequence has no elements, returns nil.\n\nDiscussion\n\nUse this method when the asynchronous sequence’s values don’t conform to Comparable, or when you want to apply a custom ordering to the sequence.\n\nThe predicate must be a strict weak ordering over the elements. That is, for any elements a, b, and c, the following conditions must hold:\n\nareInIncreasingOrder(a, a) is always false. (Irreflexivity)\n\nIf areInIncreasingOrder(a, b) and areInIncreasingOrder(b, c) are both true, then areInIncreasingOrder(a, c) is also true. (Transitive comparability)\n\nTwo elements are incomparable if neither is ordered before the other according to the predicate. If a and b are incomparable, and b and c are incomparable, then a and c are also incomparable. (Transitive incomparability)\n\nThe following example uses an enumeration of playing cards ranks, Rank, which ranges from ace (low) to king (high). An asynchronous sequence called RankCounter produces all elements of the array. The predicate provided to the min(by:) method sorts ranks based on their rawValue:\n\nenum Rank: Int {\n    case ace = 1, two, three, four, five, six, seven, eight, nine, ten, jack, queen, king\n}\n\n\nlet min = await RankCounter()\n    .min { $0.rawValue < $1.rawValue }\nprint(min ?? \"none\")\n// Prints \"ace\"\n"
  },
  {
    "title": "max(by:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218750-max",
    "html": "Parameters\nareInIncreasingOrder\n\nA predicate that returns true if its first argument should be ordered before its second argument; otherwise, false.\n\nReturn Value\n\nThe sequence’s minimum element, according to areInIncreasingOrder. If the sequence has no elements, returns nil.\n\nDiscussion\n\nUse this method when the asynchronous sequence’s values don’t conform to Comparable, or when you want to apply a custom ordering to the sequence.\n\nThe predicate must be a strict weak ordering over the elements. That is, for any elements a, b, and c, the following conditions must hold:\n\nareInIncreasingOrder(a, a) is always false. (Irreflexivity)\n\nIf areInIncreasingOrder(a, b) and areInIncreasingOrder(b, c) are both true, then areInIncreasingOrder(a, c) is also true. (Transitive comparability)\n\nTwo elements are incomparable if neither is ordered before the other according to the predicate. If a and b are incomparable, and b and c are incomparable, then a and c are also incomparable. (Transitive incomparability)\n\nThe following example uses an enumeration of playing cards ranks, Rank, which ranges from ace (low) to king (high). An asynchronous sequence called RankCounter produces all elements of the array. The predicate provided to the max(by:) method sorts ranks based on their rawValue:\n\nenum Rank: Int {\n    case ace = 1, two, three, four, five, six, seven, eight, nine, ten, jack, queen, king\n}\n\n\nlet max = await RankCounter()\n    .max { $0.rawValue < $1.rawValue }\nprint(max ?? \"none\")\n// Prints \"king\"\n"
  },
  {
    "title": "map(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218746-map",
    "html": "Parameters\ntransform\n\nA mapping closure. transform accepts an element of this sequence as its parameter and returns a transformed value of the same or of a different type.\n\nReturn Value\n\nAn asynchronous sequence that contains, in order, the elements produced by the transform closure.\n\nDiscussion\n\nUse the map(_:) method to transform every element received from a base asynchronous sequence. Typically, you use this to transform from one type of element to another.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 5. The closure provided to the map(_:) method takes each Int and looks up a corresponding String from a romanNumeralDict dictionary. This means the outer for await in loop iterates over String instances instead of the underlying Int values that Counter produces:\n\nlet romanNumeralDict: [Int: String] =\n    [1: \"I\", 2: \"II\", 3: \"III\", 5: \"V\"]\n\n\nlet stream = Counter(howHigh: 5)\n    .map { romanNumeralDict[$0] ?? \"(unknown)\" }\nfor await numeral in stream {\n    print(numeral, terminator: \" \")\n}\n// Prints \"I II III (unknown) V \"\n"
  },
  {
    "title": "drop(while:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218733-drop",
    "html": "Parameters\npredicate\n\nA closure that takes an element as a parameter and returns a Boolean value indicating whether to drop the element from the modified sequence.\n\nReturn Value\n\nAn asynchronous sequence that skips over values from the base sequence until the provided closure returns false.\n\nDiscussion\n\nUse drop(while:) to omit elements from an asynchronous sequence until the element received meets a condition you specify.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The drop(while:) method causes the modified sequence to ignore received values until it encounters one that is divisible by 3:\n\nlet stream = Counter(howHigh: 10)\n    .drop { $0 % 3 != 0 }\nfor await number in stream {\n    print(number, terminator: \" \")\n}\n// Prints \"3 4 5 6 7 8 9 10 \"\n\n\nAfter the predicate returns false, the sequence never executes it again, and from then on the sequence passes through elements from its underlying sequence as-is."
  },
  {
    "title": "contains(where:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218731-contains",
    "html": "Parameters\npredicate\n\nA closure that takes an element of the asynchronous sequence as its argument and returns a Boolean value that indicates whether the passed element represents a match.\n\nReturn Value\n\ntrue if the sequence contains an element that satisfies predicate; otherwise, false.\n\nDiscussion\n\nYou can use the predicate to check for an element of a type that doesn’t conform to the Equatable protocol, or to find an element that satisfies a general condition.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The contains(where:) method checks to see whether the sequence produces a value divisible by 3:\n\nlet containsDivisibleByThree = await Counter(howHigh: 10)\n    .contains { $0 % 3 == 0 }\nprint(containsDivisibleByThree)\n// Prints \"true\"\n\n\nThe predicate executes each time the asynchronous sequence produces an element, until either the predicate finds a match or the sequence ends."
  },
  {
    "title": "compactMap(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218729-compactmap",
    "html": "Parameters\ntransform\n\nAn error-throwing mapping closure. transform accepts an element of this sequence as its parameter and returns a transformed value of the same or of a different type. If transform throws an error, the sequence ends.\n\nReturn Value\n\nAn asynchronous sequence that contains, in order, the non-nil elements produced by the transform closure. The sequence ends either when the base sequence ends or when transform throws an error.\n\nDiscussion\n\nUse the compactMap(_:) method to transform every element received from a base asynchronous sequence, while also discarding any nil results from the closure. Typically, you use this to transform from one type of element to another.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 5. The closure provided to the compactMap(_:) method takes each Int and looks up a corresponding String from a romanNumeralDict dictionary. Since there is no key for 4, the closure returns nil in this case, which compactMap(_:) omits from the transformed asynchronous sequence. When the value is 5, the closure throws MyError, terminating the sequence.\n\nlet romanNumeralDict: [Int: String] =\n    [1: \"I\", 2: \"II\", 3: \"III\", 5: \"V\"]\n\n\ndo {\n    let stream = Counter(howHigh: 5)\n        .compactMap { (value) throws -> String? in\n            if value == 5 {\n                throw MyError()\n            }\n            return romanNumeralDict[value]\n        }\n    for try await numeral in stream {\n        print(numeral, terminator: \" \")\n    }\n} catch {\n    print(\"Error: \\(error)\")\n}\n// Prints \"I II III Error: MyError() \"\n"
  },
  {
    "title": "compactMap(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218727-compactmap",
    "html": "Parameters\ntransform\n\nA mapping closure. transform accepts an element of this sequence as its parameter and returns a transformed value of the same or of a different type.\n\nReturn Value\n\nAn asynchronous sequence that contains, in order, the non-nil elements produced by the transform closure.\n\nDiscussion\n\nUse the compactMap(_:) method to transform every element received from a base asynchronous sequence, while also discarding any nil results from the closure. Typically, you use this to transform from one type of element to another.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 5. The closure provided to the compactMap(_:) method takes each Int and looks up a corresponding String from a romanNumeralDict dictionary. Because there is no key for 4, the closure returns nil in this case, which compactMap(_:) omits from the transformed asynchronous sequence.\n\nlet romanNumeralDict: [Int: String] =\n    [1: \"I\", 2: \"II\", 3: \"III\", 5: \"V\"]\n    \nlet stream = Counter(howHigh: 5)\n    .compactMap { romanNumeralDict[$0] }\nfor await numeral in stream {\n    print(numeral, terminator: \" \")\n}\n// Prints \"I II III V \"\n"
  },
  {
    "title": "viewMatrix(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/2921672-viewmatrix",
    "html": "Parameters\norientation\n\nThe orientation in which the camera image is to be presented.\n\nReturn Value\n\nA view matrix appropriate for the camera with the specified orientation.\n\nDiscussion\n\nThis method has no effect on ARKit. Instead, this method uses the orientation parameter and the camera's state to construct a view matrix for your own rendering code.\n\nSee Also\nApplying Camera Geometry\nvar projectionMatrix: simd_float4x4\nA transform matrix appropriate for rendering 3D content to match the image captured by the camera.\nfunc projectionMatrix(for: UIInterfaceOrientation, viewportSize: CGSize, zNear: CGFloat, zFar: CGFloat) -> simd_float4x4\nReturns a transform matrix appropriate for rendering 3D content to match the image captured by the camera, using the specified parameters.\nfunc projectPoint(simd_float3, orientation: UIInterfaceOrientation, viewportSize: CGSize) -> CGPoint\nReturns the projection of a point from the 3D world space detected by ARKit into the 2D space of a view rendering the scene.\nfunc unprojectPoint(CGPoint, ontoPlane: simd_float4x4, orientation: UIInterfaceOrientation, viewportSize: CGSize) -> simd_float3?\nReturns the projection of a point from the 2D space of a view rendering the scene onto a plane in the 3D world space detected by ARKit."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arobjectscanningconfiguration/3001727-init",
    "html": "Discussion\n\nTo use the configuration in an AR session, pass it to the ARSession run(_:options:) method."
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate/reason/3238107-hashvalue",
    "html": "Discussion\n\nHash values are not guaranteed to be equal across different executions of your program. Do not save hash values to use during a future execution.\n\nImportant\n\nhashValue is deprecated as a Hashable requirement. To conform to Hashable, implement the hash(into:) requirement instead.\n\nRelationships\nFrom Protocol\nHashable\nSee Also\nHashes of Tracking Quality\nfunc hash(into: inout Hasher)\nHashes the reason by passing it to the given hash function.\nstatic func == (ARCamera.TrackingState.Reason, ARCamera.TrackingState.Reason) -> Bool\nIndicates whether two reasons are equal.\nstatic func != (ARCamera.TrackingState.Reason, ARCamera.TrackingState.Reason) -> Bool\nReturns a Boolean value indicating whether two values are not equal."
  },
  {
    "title": "classification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/3020726-classification",
    "html": "Discussion\n\nOn supported devices, ARKit automatically attempts to characterize each detected plane, describing it as a real-world surface such as as a wall, floor, or table. You can then use this information to enhance the realism of your AR experience; for example, by placing certain virtual content only on floors.\n\nPlane classification can take longer than plane detection, and ARKit reports classifications only for planes where it has a high confidence in the result. If ARKit currently has no classification result for a plane, this property's value is ARPlaneAnchor.Classification.none(_:), with an associated ARPlaneAnchor.Classification.Status value indicating why.\n\nPlane classification is available only on iPhone XS, iPhone XS Max, and iPhone XR. Before using classification results, check the isClassificationSupported class property to make sure you're on a supported device.\n\nSee Also\nClassifying a Plane\nclass var isClassificationSupported: Bool\nA Boolean value that indicates whether plane classification is available on the current device.\nenum ARPlaneAnchor.Classification\nPossible characterizations of real-world surfaces represented by plane anchors."
  },
  {
    "title": "ARPlaneAnchor.Classification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/classification",
    "html": "Overview\n\nYou get values of this type from a plane anchor's classification property, identifying both the likely type of real-world surface for a detected plane anchor and the state of ARKit's plane classification process.\n\nTopics\nPlane Classifications\ncase wall\nThe plane anchor represents a real-world wall or similar large vertical surface.\ncase floor\nThe plane anchor represents a real-world floor, ground plane, or similar large horizontal surface.\ncase ceiling\nThe plane anchor represents a real-world ceiling or similar overhead horizontal surface.\ncase table\nThe plane anchor represents a real-world table, desk, bar, or similar flat surface.\ncase seat\nThe plane anchor represents a real-world chair, stool, bench or similar flat surface.\ncase door\nThe plane anchor represents a real-world door or similar vertical surface.\ncase window\nThe plane anchor represents a real-world window or similar vertical surface.\nMissing Classification Status\ncase none(ARPlaneAnchor.Classification.Status)\nNo classification is available for the plane anchor.\nenum ARPlaneAnchor.Classification.Status\nReasons ARKit is unable to classify a plane.\nComparing Classifications\nstatic func == (ARPlaneAnchor.Classification, ARPlaneAnchor.Classification) -> Bool\nDetermines whether two plane classifications are equal.\nstatic func != (ARPlaneAnchor.Classification, ARPlaneAnchor.Classification) -> Bool\nReturns a Boolean value indicating whether two plane classifications aren't equal.\nSee Also\nClassifying a Plane\nclass var isClassificationSupported: Bool\nA Boolean value that indicates whether plane classification is available on the current device.\nvar classification: ARPlaneAnchor.Classification\nA general characterization of what kind of real-world surface the plane anchor represents."
  },
  {
    "title": "estimatedScaleFactor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arimageanchor/3075552-estimatedscalefactor",
    "html": "Discussion\n\nThe default value is 1.0, which means that a version of this image that ARKit recognizes in the physical environment exactly matches its reference image physicalSize.\n\nOtherwise, ARKit automatically corrects the image anchor's transform when estimatedScaleFactor is a value other than 1.0. This adjustment in turn, corrects ARKit's understanding of where the image anchor is located in the physical environment.\n\nSee automaticImageScaleEstimationEnabled."
  },
  {
    "title": "GeometryElement.Primitive | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement/primitive",
    "html": "Topics\nPrimitive shapes\ncase line\nTwo vertices that connect to form a line.\ncase triangle\nThree vertices that connect to form a triangle.\nInspecting geometry primitives\nvar description: String\nA textual description of a geometry primitive.\nvar hashValue: Int\nvar indexCount: Int\nfunc hash(into: inout Hasher)\nstatic func == (GeometryElement.Primitive, GeometryElement.Primitive) -> Bool\nstatic func != (GeometryElement.Primitive, GeometryElement.Primitive) -> Bool\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nRendering geometry elements\nvar buffer: MTLBuffer\nA Metal buffer that contains index data that defines the geometry of an object.\nvar primitive: GeometryElement.Primitive\nThe kind of primitive, lines or triangles, that a geometry element contains.\nvar count: Int\nThe number of primitives in the Metal buffer for a geometry element.\nvar bytesPerIndex: Int\nThe number of bytes that represent an index value."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement/4139375-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible"
  },
  {
    "title": "initialWorldMap | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arpositionaltrackingconfiguration/3255169-initialworldmap",
    "html": "Discussion\n\nAn ARWorldMap encapsulates the state of a running ARSession. This state includes ARKit's awareness of the physical space the user moves the device in (which ARKit uses to determine the device's position and orientation), as well as any ARAnchor objects added to the session (which can represent detected real-world features or virtual content placed by your app). After you use getCurrentWorldMap(completionHandler:) to save a session's world map, you can assign it to a configuration's initialWorldMap property and use run(_:options:) to start another session with the same spatial awareness and anchors.\n\nBy saving world maps and using them to start new sessions, your app can add new AR capabilities:\n\nMultiuser AR experiences. Create a shared frame of reference by sending archived ARWorldMap objects to a nearby user's device. With two devices tracking the same world map, you can build a networked experience where both users can see and interact with the same virtual content.\n\nPersistent AR experiences. Save a world map when your app becomes inactive, then restore it the next time your app launches in the same physical environment. You can use anchors from the resumed world map to place the same virtual content at the same positions from the saved session.\n\nWhen you run a session with an initial world map, the session starts in the ARCamera.TrackingState.limited(_:) (ARCamera.TrackingState.Reason.relocalizing) tracking state while ARKit attempts to reconcile the recorded world map with the current environment. If successful, the tracking state becomes ARCamera.TrackingState.normal after a short time, indicating that the current world coordinate system and anchors match those from the recorded world map.\n\nIf ARKit cannot reconcile the recorded world map with the current environment (for example, if the device is in an entirely different place from where the world map was recorded), the session remains in the ARCamera.TrackingState.Reason.relocalizing state indefinitely.\n\nSee Also\nCreating a Configuration\ninit()\nCreates a new positional tracking configuration."
  },
  {
    "title": "max(by:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180424-max",
    "html": "Parameters\nareInIncreasingOrder\n\nA predicate that returns true if its first argument should be ordered before its second argument; otherwise, false.\n\nReturn Value\n\nThe sequence’s minimum element, according to areInIncreasingOrder. If the sequence has no elements, returns nil.\n\nDiscussion\n\nUse this method when the asynchronous sequence’s values don’t conform to Comparable, or when you want to apply a custom ordering to the sequence.\n\nThe predicate must be a strict weak ordering over the elements. That is, for any elements a, b, and c, the following conditions must hold:\n\nareInIncreasingOrder(a, a) is always false. (Irreflexivity)\n\nIf areInIncreasingOrder(a, b) and areInIncreasingOrder(b, c) are both true, then areInIncreasingOrder(a, c) is also true. (Transitive comparability)\n\nTwo elements are incomparable if neither is ordered before the other according to the predicate. If a and b are incomparable, and b and c are incomparable, then a and c are also incomparable. (Transitive incomparability)\n\nThe following example uses an enumeration of playing cards ranks, Rank, which ranges from ace (low) to king (high). An asynchronous sequence called RankCounter produces all elements of the array. The predicate provided to the max(by:) method sorts ranks based on their rawValue:\n\nenum Rank: Int {\n    case ace = 1, two, three, four, five, six, seven, eight, nine, ten, jack, queen, king\n}\n\n\nlet max = await RankCounter()\n    .max { $0.rawValue < $1.rawValue }\nprint(max ?? \"none\")\n// Prints \"king\"\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "filter(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180411-filter",
    "html": "Parameters\nisIncluded\n\nA closure that takes an element of the asynchronous sequence as its argument and returns a Boolean value that indicates whether to include the element in the filtered sequence.\n\nReturn Value\n\nAn asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\n\nDiscussion\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The filter(_:) method returns true for even values and false for odd values, thereby filtering out the odd values:\n\nlet stream = Counter(howHigh: 10)\n    .filter { $0 % 2 == 0 }\nfor await number in stream {\n    print(number, terminator: \" \")\n}\n// Prints \"2 4 6 8 10 \"\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arpositionaltrackingconfiguration/3255168-init",
    "html": "See Also\nCreating a Configuration\nvar initialWorldMap: ARWorldMap?\nThe state from a previous AR session to attempt to resume with this session configuration."
  },
  {
    "title": "contains(where:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180405-contains",
    "html": "Parameters\npredicate\n\nA closure that takes an element of the asynchronous sequence as its argument and returns a Boolean value that indicates whether the passed element represents a match.\n\nReturn Value\n\ntrue if the sequence contains an element that satisfies predicate; otherwise, false.\n\nDiscussion\n\nYou can use the predicate to check for an element of a type that doesn’t conform to the Equatable protocol, or to find an element that satisfies a general condition.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The contains(where:) method checks to see whether the sequence produces a value divisible by 3:\n\nlet containsDivisibleByThree = await Counter(howHigh: 10)\n    .contains { $0 % 3 == 0 }\nprint(containsDivisibleByThree)\n// Prints \"true\"\n\n\nThe predicate executes each time the asynchronous sequence produces an element, until either the predicate finds a match or the sequence ends.\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "dropFirst(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180409-dropfirst",
    "html": "Parameters\ncount\n\nThe number of elements to drop from the beginning of the sequence. count must be greater than or equal to zero.\n\nReturn Value\n\nAn asynchronous sequence that drops the first count elements from the base sequence.\n\nDiscussion\n\nUse dropFirst(_:) when you want to drop the first n elements from the base sequence and pass through the remaining elements.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The dropFirst(_:) method causes the modified sequence to ignore the values 1 through 3, and instead emit 4 through 10:\n\nfor await number in Counter(howHigh: 10).dropFirst(3) {\n    print(number, terminator: \" \")\n}\n// Prints \"4 5 6 7 8 9 10 \"\n\n\nIf the number of elements to drop exceeds the number of elements in the sequence, the result is an empty sequence.\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationstatus/4131660",
    "html": "See Also\nComparing authorization states\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func != (ARKitSession.AuthorizationStatus, ARKitSession.AuthorizationStatus) -> Bool"
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationtype/4131668",
    "html": "See Also\nComparing authorizations\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func != (ARKitSession.AuthorizationType, ARKitSession.AuthorizationType) -> Bool\nvar description: String"
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationstatus/4131664-hashvalue",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nComparing authorization states\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (ARKitSession.AuthorizationStatus, ARKitSession.AuthorizationStatus) -> Bool\nstatic func != (ARKitSession.AuthorizationStatus, ARKitSession.AuthorizationStatus) -> Bool"
  },
  {
    "title": "ARKitSession.AuthorizationStatus.denied | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationstatus/denied",
    "html": "See Also\nGetting authorization states\ncase notDetermined\nThe user hasn’t yet granted or denied permission.\ncase allowed\nThe user granted your app permission to use the associated kind of ARKit data."
  },
  {
    "title": "ARKitSession.AuthorizationStatus.notDetermined | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationstatus/notdetermined",
    "html": "See Also\nGetting authorization states\ncase allowed\nThe user granted your app permission to use the associated kind of ARKit data.\ncase denied\nThe user denied your app permission to use the associated kind of ARKit data."
  },
  {
    "title": "ARKitSession.AuthorizationStatus.allowed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationstatus/allowed",
    "html": "See Also\nGetting authorization states\ncase notDetermined\nThe user hasn’t yet granted or denied permission.\ncase denied\nThe user denied your app permission to use the associated kind of ARKit data."
  },
  {
    "title": "ARKitSession.Event.authorizationChanged(type:status:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/event/authorizationchanged_type_status",
    "html": "Parameters\ntype\n\nThe type of authorization status that changed.\n\nstatus\n\nThe new state of authorization.\n\nSee Also\nObserving session events\ncase dataProviderStateChanged(dataProviders: [DataProvider], newState: DataProviderState, error: ARKitSession.Error?)\nAn event that represents a change in state of one of the data providers associated with a session.\nvar description: String\nA textual description of the authorization status."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/event/4169983-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nObserving session events\ncase authorizationChanged(type: ARKitSession.AuthorizationType, status: ARKitSession.AuthorizationStatus)\nAn event that represents a change in authorization status for a specific authorization type.\ncase dataProviderStateChanged(dataProviders: [DataProvider], newState: DataProviderState, error: ARKitSession.Error?)\nAn event that represents a change in state of one of the data providers associated with a session."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationtype/4131667",
    "html": "See Also\nComparing authorizations\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (ARKitSession.AuthorizationType, ARKitSession.AuthorizationType) -> Bool\nvar description: String"
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationtype/4131671-hashvalue",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nComparing authorizations\nfunc hash(into: inout Hasher)\nstatic func == (ARKitSession.AuthorizationType, ARKitSession.AuthorizationType) -> Bool\nstatic func != (ARKitSession.AuthorizationType, ARKitSession.AuthorizationType) -> Bool\nvar description: String"
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationtype/4169982-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nComparing authorizations\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (ARKitSession.AuthorizationType, ARKitSession.AuthorizationType) -> Bool\nstatic func != (ARKitSession.AuthorizationType, ARKitSession.AuthorizationType) -> Bool"
  },
  {
    "title": "code | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/4241453-code",
    "html": "See Also\nInspecting ARKit errors\nlet dataProvider: (DataProvider)?\nThe data provider that caused an error in a session.\nenum ARKitSession.Error.Code\nThe error codes for ARKit sessions.\nvar description: String\nA textual description of the error that occurred.\nvar localizedDescription: String\nA localized description of the error.\nvar errorDescription: String?\nA localized message that describes the error that occurred."
  },
  {
    "title": "helpAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/4241458-helpanchor",
    "html": "See Also\nProviding recovery suggestions\nvar recoverySuggestion: String?\nA localized message that describes how someone might recover from the error.\nvar failureReason: String?\nA localized message that describes why the error occurred."
  },
  {
    "title": "ARKitSession.AuthorizationType.handTracking | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationtype/handtracking",
    "html": "See Also\nRequesting authorization\ncase worldSensing\nThe authorization for access to plane detection, scene reconstruction, and image tracking."
  },
  {
    "title": "init(_:orientation:physicalWidth:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceimage/2942251-init",
    "html": "Parameters\nimage\n\nA Core Graphics image object.\n\norientation\n\nThe intended display orientation for the image.\n\nphysicalWidth\n\nThe real-world width, in meters, of the image.\n\nDiscussion\n\nTo accurately recognize the position and orientation of an image in the AR environment, ARKit must know the image's physical size. When you call this initializer, ARKit uses the physicalWidth measurement and orientation you provide together with the aspect ratio of the image itself to calculate the physical height. Use the physicalSize property of the created ARReferenceImage object to retrieve these values.\n\nImportant\n\nARKit preprocesses reference images before using them for image detection. To provide reference images bundled with your app, create AR reference image assets in your Xcode asset catalog, and use the referenceImages(inGroupNamed:bundle:) method to load them.\n\nSee Also\nCreating Reference Images\ninit(CGImage, orientation: CGImagePropertyOrientation, physicalWidth: CGFloat)\nCreates a new reference image from a Core Graphics image object."
  },
  {
    "title": "failureReason | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/4241457-failurereason",
    "html": "Relationships\nFrom Protocol\nLocalizedError\nSee Also\nProviding recovery suggestions\nvar recoverySuggestion: String?\nA localized message that describes how someone might recover from the error.\nvar helpAnchor: String?"
  },
  {
    "title": "recoverySuggestion | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/4241460-recoverysuggestion",
    "html": "Relationships\nFrom Protocol\nLocalizedError\nSee Also\nProviding recovery suggestions\nvar failureReason: String?\nA localized message that describes why the error occurred.\nvar helpAnchor: String?"
  },
  {
    "title": "errorDescription | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/4241456-errordescription",
    "html": "Relationships\nFrom Protocol\nLocalizedError\nSee Also\nInspecting ARKit errors\nlet dataProvider: (DataProvider)?\nThe data provider that caused an error in a session.\nvar code: ARKitSession.Error.Code\nThe error code for an ARKit session error.\nenum ARKitSession.Error.Code\nThe error codes for ARKit sessions.\nvar description: String\nA textual description of the error that occurred.\nvar localizedDescription: String\nA localized description of the error."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/4241455-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting ARKit errors\nlet dataProvider: (DataProvider)?\nThe data provider that caused an error in a session.\nvar code: ARKitSession.Error.Code\nThe error code for an ARKit session error.\nenum ARKitSession.Error.Code\nThe error codes for ARKit sessions.\nvar localizedDescription: String\nA localized description of the error.\nvar errorDescription: String?\nA localized message that describes the error that occurred."
  },
  {
    "title": "ARKitSession.Error.Code | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/code",
    "html": "Topics\nDetermining the cause of session errors\ncase dataProviderFailedToRun\nThe error code for when a data provider fails to run.\ncase dataProviderNotAuthorized\nThe error code for when a data provider is missing at least one authorization it needs to run.\nInspecting session errors\nvar description: String\nA textual description of the error code.\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (ARKitSession.Error.Code, ARKitSession.Error.Code) -> Bool\nstatic func != (ARKitSession.Error.Code, ARKitSession.Error.Code) -> Bool\nRelationships\nConforms To\nCustomStringConvertible\nSee Also\nInspecting ARKit errors\nlet dataProvider: (DataProvider)?\nThe data provider that caused an error in a session.\nvar code: ARKitSession.Error.Code\nThe error code for an ARKit session error.\nvar description: String\nA textual description of the error that occurred.\nvar localizedDescription: String\nA localized description of the error.\nvar errorDescription: String?\nA localized message that describes the error that occurred."
  },
  {
    "title": "localizedDescription | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/4241459-localizeddescription",
    "html": "See Also\nInspecting ARKit errors\nlet dataProvider: (DataProvider)?\nThe data provider that caused an error in a session.\nvar code: ARKitSession.Error.Code\nThe error code for an ARKit session error.\nenum ARKitSession.Error.Code\nThe error codes for ARKit sessions.\nvar description: String\nA textual description of the error that occurred.\nvar errorDescription: String?\nA localized message that describes the error that occurred."
  },
  {
    "title": "dataProvider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error/4241454-dataprovider",
    "html": "See Also\nInspecting ARKit errors\nvar code: ARKitSession.Error.Code\nThe error code for an ARKit session error.\nenum ARKitSession.Error.Code\nThe error codes for ARKit sessions.\nvar description: String\nA textual description of the error that occurred.\nvar localizedDescription: String\nA localized description of the error.\nvar errorDescription: String?\nA localized message that describes the error that occurred."
  },
  {
    "title": "maximumNumberOfTrackedImages | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arimagetrackingconfiguration/2968174-maximumnumberoftrackedimages",
    "html": "Discussion\n\nWhen you set a nonzero value for this property, the framework keeps that many image anchors up to date as the session progresses. The framework can track a maximum of four images simultaneously.\n\nThe word track in the property name refers to how the framework closely monitors the image’s physical position and orientation for any changes. If the image moves, the framework updates the associated ARImageAnchor transform with the new pose. ARKit checks for changes every frame.\n\nARKit tracks the first images it observes in the physical environment from the trackingImages set. When a session reaches the maximum number of tracked images, the framework attempts to track another member of the set only after one of the existing tracked images leaves the device’s view.\n\nThe default value is 1.\n\nSee Also\nChoosing Images to Track\nvar trackingImages: Set<ARReferenceImage>\nA set of images that ARKit searches for and tracks in the user's environment."
  },
  {
    "title": "trackingImages | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arimagetrackingconfiguration/2968176-trackingimages",
    "html": "Discussion\n\nAdd members to this set for each image that ARKit searches for in the user’s environment. When ARKit observes a matching image, the framework creates an ARImageAnchor object and adds it to the session. If you set maximumNumberOfTrackedImages to a value greater than 1, ARKit tracks multiple images as the session progresses, up to a maximum of four.\n\nTo define the reference images that this property contains, create an asset catalog in Xcode or create ARReferenceImage objects programmatically. For an example, see Tracking and altering images.\n\nSee Also\nChoosing Images to Track\nvar maximumNumberOfTrackedImages: Int\nThe number of image anchors to monitor closely for position and orientation updates."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arimagetrackingconfiguration/2968173-init",
    "html": "Discussion\n\nTo use the configuration in an AR session, pass it to the ARSession run(_:options:) method."
  },
  {
    "title": "environmentTexturing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration/3566283-environmenttexturing",
    "html": "Discussion\n\nEnvironment textures are cube-map textures that depict the view in all directions from a specific point in a scene. In 3D asset rendering, environment textures are the basis for image-based lighting algorithms where surfaces can realistically reflect light from their surroundings. ARKit generates environment textures during an AR session using camera imagery, allowing SceneKit or a custom-rendering engine to provide realistic image-based lighting for virtual objects in your AR experience.\n\nTo enable texture map generation for your configuration, change this property (from its default value of ARWorldTrackingConfiguration.EnvironmentTexturing.none):\n\nWith ARWorldTrackingConfiguration.EnvironmentTexturing.manual environment texturing, you identify points in the scene for which you want light probe texture maps by creating AREnvironmentProbeAnchor objects and adding them to the session.\n\nWith ARWorldTrackingConfiguration.EnvironmentTexturing.automatic environment texturing, ARKit automatically creates, positions, and adds AREnvironmentProbeAnchor objects to the session.\n\nIn both cases, ARKit automatically generates environment textures as the session collects camera imagery. Use a delegate method such as session(_:didUpdate:) to find out when a texture is available, and access it from the anchor's environmentTexture property.\n\nIf you display AR content using ARSCNView and the automaticallyUpdatesLighting option, SceneKit automatically retrieves AREnvironmentProbeAnchor texture maps and uses them to light the scene.\n\nSee Also\nCreating Realistic Reflections\nenum ARWorldTrackingConfiguration.EnvironmentTexturing\nOptions to generate environment textures in a world-tracking AR session.\nclass AREnvironmentProbeAnchor\nAn object that provides environmental lighting information for a specific area of space in a world-tracking AR session.\nvar wantsHDREnvironmentTextures: Bool\nA flag that instructs the framework to create environment textures in HDR format."
  },
  {
    "title": "wantsHDREnvironmentTextures | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration/3566288-wantshdrenvironmenttextures",
    "html": "Discussion\n\nIf you set environmentTexturing to .automatic in iOS 12 or later, ARKit gives you environment textures you cast on your app's virtual content to create realistic reflections. By default, the framework sets wantsHDREnvironmentTextures to true. When your renderer supports HDR environment textures in iOS 13, it enables your lighting engine to output more colors, with a more realistic result.\n\nBoth ARView and ARSCNView support HDR environment textures. For more information, see Adding Realistic Reflections to an AR Experience.\n\nFor a Metal app that doesn't yet support HDR environment textures, you can use the following code to receive LDR environment textures until you're ready to update your renderer for HDR.\n\nif #available(iOS 13, *) { \n    configuration.wantsHDREnvironmentTextures = false\n}\n\n\nSee Also\nCreating Realistic Reflections\nvar environmentTexturing: ARWorldTrackingConfiguration.EnvironmentTexturing\nAn option that determines how the framework generates environment textures.\nenum ARWorldTrackingConfiguration.EnvironmentTexturing\nOptions to generate environment textures in a world-tracking AR session.\nclass AREnvironmentProbeAnchor\nAn object that provides environmental lighting information for a specific area of space in a world-tracking AR session."
  },
  {
    "title": "detectionObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration/3566282-detectionobjects",
    "html": "Discussion\n\nUse this property to choose known 3D objects for ARKit to find in the user's environment and present as ARObjectAnchor for use in your AR experience.\n\nTo create reference objects for detection, scan them in a world-tracking session and use ARWorldMap to extract ARReferenceObject instances. You can then save reference objects as files and package them in any ARKit app you create using an Xcode asset catalog."
  },
  {
    "title": "ARGeoTrackingStatus.State.localizing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/state/localizing",
    "html": "Discussion\n\nIn ARGeoTrackingStatus.State.localizing, the session downloads localization imagery for the user’s geographic location and compares it with captures from the device’s camera. This process is referred to as visual localization. When ARKit succeeds in matching this imagery with captures from the camera, the state moves to ARGeoTrackingStatus.State.localized and the app is free to create location anchors. For more information about localization imagery, see Refine the User's Position with Imagery.\n\nAssisting the User with Visual Localization\n\nTo establish visual localization, the user must move the camera so it acquires the captures that ARKit needs. To elicit the right user movements in ARGeoTrackingStatus.State.localizing, the app needs to advise the user to:\n\nPoint the camera at buildings and other visual landmarks to help ARKit match the live camera data with its preexisting landscape-data.\n\nAvoid pointing the device at objects that are too general, like trees. It’s better to focus on distinct visuals, like structures, or signs.\n\nAvoid pointing the device at real-world objects that are transient, like parked cars, or a construction site.\n\nBecause lighting conditions can affect visual localization, avoid geo tracking at night.\n\nSee Also\nStates\ncase initializing\nThe session is initializing geo tracking.\ncase localized\nGeo tracking is localized.\ncase notAvailable\nGeo tracking is not available."
  },
  {
    "title": "automaticImageScaleEstimationEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration/3566280-automaticimagescaleestimationena",
    "html": "Discussion\n\nIf set to true, ARKit uses its knowledge of the world to set an image anchor's estimatedScaleFactor property, which corrects the image anchor's position in the physical environment.\n\nEnable this property when you want to detect different-sized versions of a reference image. ARKit must know the physical size of an image in the real world to accurately estimate its real-world position. Enable this property to tell ARKit to estimate a recognized image's physical size before it calculates the real-world position.\n\nSee Also\nDetecting or Tracking Images\nvar detectionImages: Set<ARReferenceImage>!\nA set of images that ARKit searches for in the user's environment.\nvar maximumNumberOfTrackedImages: Int\nThe number of image anchors to monitor closely for position and orientation updates."
  },
  {
    "title": "planeDetection | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration/3566287-planedetection",
    "html": "Discussion\n\nBy default, this configuration disables plane detection. If you enable horizontal or vertical plane detection, the session adds ARPlaneAnchor objects and notifies your ARSessionDelegate, ARSCNViewDelegate, or ARSKViewDelegate object when its analysis of captured video images detects an area that appears to be a flat surface.\n\nSee Also\nTracking Surfaces\nstruct ARWorldTrackingConfiguration.PlaneDetection\nOptions for whether and how the framework detects flat surfaces in captured images."
  },
  {
    "title": "detectionImages | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration/3566281-detectionimages",
    "html": "Discussion\n\nAdd members to this set for each image that ARKit searches for in the user’s environment. When ARKit detects a matching image, the framework creates an ARImageAnchor object and adds it to the session.\n\nTo define the reference images that this property contains, create an asset catalog in Xcode or create ARReferenceImage objects programmatically. For more information, see Detecting Images in an AR Experience.\n\nIf you set a nonzero value for maximumNumberOfTrackedImages, ARKit enables image tracking, which continuously updates the transform for up to four of the reference image anchors as the session progresses. For an example, see Tracking and altering images.\n\nLimit Reference Images for Performance\n\nImage detection accuracy and performance may decline as the number of images in this set increases. For best results, limit your detection image count to no more than around 100.\n\nTo detect more than 100 images, your app can allocate a certain amount of time for the first 100 images before moving on to the next 100, and so on. When you update the contents of this property, call run(_:options:) again with your app's configuration to effect the change.\n\nSee Also\nDetecting or Tracking Images\nvar maximumNumberOfTrackedImages: Int\nThe number of image anchors to monitor closely for position and orientation updates.\nvar automaticImageScaleEstimationEnabled: Bool\nA flag that instructs the framework to estimate and set the scale of a detected or tracked image on your behalf.\nRelated Documentation\nclass ARImageTrackingConfiguration\nA configuration that tracks known images using the rear-facing camera."
  },
  {
    "title": "maximumNumberOfTrackedImages | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration/3566285-maximumnumberoftrackedimages",
    "html": "Discussion\n\nWhen you set a nonzero value for this property, the framework keeps that many image anchors up to date as the session progresses. The framework can track a maximum of four images simultaneously.\n\nThe word track in the property name refers to how the framework closely monitors the image's physical position and orientation for any changes. If the image moves, the framework updates the associated ARImageAnchor transform with the new pose. ARKit checks for changes every frame.\n\nARKit tracks the first images it observes in the physical environment from the detectionImages set. When a session reaches the maximum number of tracked images, the framework attempts to track another member of the set only after one of the existing tracked images leaves the device’s view.\n\nThe default value is 0, which disables image tracking. If you add reference images to detectionImages with this property set to 0, ARKit creates image anchors for observed reference images but their positions only update infrequently, such as once every couple of seconds.\n\nSee Also\nDetecting or Tracking Images\nvar detectionImages: Set<ARReferenceImage>!\nA set of images that ARKit searches for in the user's environment.\nvar automaticImageScaleEstimationEnabled: Bool\nA flag that instructs the framework to estimate and set the scale of a detected or tracked image on your behalf."
  },
  {
    "title": "ARError.Code.locationUnauthorized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/locationunauthorized",
    "html": "Discussion\n\nTo resolve this issue, the app needs to ask the user to enable location access for this app in Settings.\n\nSee Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARError.Code.unsupportedConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/unsupportedconfiguration",
    "html": "Discussion\n\nCall isSupported on an ARConfiguration to ensure it's supported before attempting to create and run it on the session with run(with:).\n\nSee Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "first(where:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218739-first",
    "html": "Parameters\npredicate\n\nA closure that takes an element of the asynchronous sequence as its argument and returns a Boolean value that indicates whether the element is a match.\n\nReturn Value\n\nThe first element of the sequence that satisfies predicate, or nil if there is no element that satisfies predicate.\n\nDiscussion\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The first(where:) method returns the first member of the sequence that’s evenly divisible by both 2 and 3.\n\nlet divisibleBy2And3 = await Counter(howHigh: 10)\n    .first { $0 % 2 == 0 && $0 % 3 == 0 }\nprint(divisibleBy2And3 ?? \"none\")\n// Prints \"6\"\n\n\nThe predicate executes each time the asynchronous sequence produces an element, until either the predicate finds a match or the sequence ends."
  },
  {
    "title": "dropFirst(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events/4218735-dropfirst",
    "html": "Parameters\ncount\n\nThe number of elements to drop from the beginning of the sequence. count must be greater than or equal to zero.\n\nReturn Value\n\nAn asynchronous sequence that drops the first count elements from the base sequence.\n\nDiscussion\n\nUse dropFirst(_:) when you want to drop the first n elements from the base sequence and pass through the remaining elements.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The dropFirst(_:) method causes the modified sequence to ignore the values 1 through 3, and instead emit 4 through 10:\n\nfor await number in Counter(howHigh: 10).dropFirst(3) {\n    print(number, terminator: \" \")\n}\n// Prints \"4 5 6 7 8 9 10 \"\n\n\nIf the number of elements to drop exceeds the number of elements in the sequence, the result is an empty sequence."
  },
  {
    "title": "preferredIblVersion | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preferrediblversion",
    "html": "Overview\n\nThis metadata selects one of two predefined options for the scene's image-based lighting (IBL). AR Quick Look in iOS 16 and later observes this property to enhance the brightness, contrast, and visual definition of a scene’s virtual content.\n\nA value of 1 indicates the classic lighting environment, and a value of 2 indicates the new lighting environment.\n\nIf you omit the preferredIblVersion metadata or give it a value of 0, the system checks the asset’s creation timestamp. A timestamp of July 1, 2022, or later results in the new lighting environment; otherwise, the scene features classic lighting for backward compatibility. The system checks the timestamp of the .usd asset within the .usdz archive, not the archive's file creation date.\n\nDeclaration\nint preferredIblVersion = 0\n\n\nSelect the scene's image-based lighting\n\nThe following .usda definition chooses the new lighting environment:\n\n// asset.usda\n#usda 1.0\n(\n    customLayerData = {\n        dictionary Apple = {\n            int preferredIblVersion = 2\n        }\n    }\n)\n\n\n\n\nTip\n\nRealityKit doesn’t observe the preferredIblVersion metadata, but you configure the same lighting environment manually. See Specifying a lighting environment in AR Quick Look for more information about matching AR Quick Look’s lighting environment in RealityKit apps.\n\nSee Also\nScenes and lighting\nSpecifying a lighting environment in AR Quick Look\nAdd metadata to your USDZ file to specify its lighting characteristics.\nsceneLibrary\nMetadata that partitions an asset into scene-based units."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate/3930610",
    "html": "Parameters\nlhs\n\nThe left-argument camera tracking state.\n\nrhs\n\nThe right-argument camera tracking state.\n\nReturn Value\n\nReturns true if the camera tracking states aren't equal. Otherwise, returns false.\n\nDiscussion\n\nInequality is the inverse of equality. For any values a and b, a != b implies that a == b is false.\n\nThis is the default implementation of the not-equal-to operator (!=) for any type that conforms to Equatable.\n\nSee Also\nComparing camera tracking states\nstatic func == (ARCamera.TrackingState, ARCamera.TrackingState) -> Bool\nDetermines whether two camera tracking states are equal."
  },
  {
    "title": "Actions and triggers | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/actions_and_triggers",
    "html": "Overview\n\nCreate a Preliminary_Behavior to uniquely pair triggers with actions. You can configure some of the triggers and actions, like the settings for a proximity trigger, the velocity of an impulse action, and the audio file for background music or sound effects.\n\nThe runtime implements the concrete triggers and actions the schemas define. The exception is NotificationAction, which refers to custom effects that an app implements.\n\nTopics\nEssentials\nPreliminary_Behavior\nA typed schema that combines one or more triggers with associated actions.\nTriggers\nControl when the runtime executes an action.\nPreliminary_Trigger\nA condition that, when met, performs an action.\nCollideTrigger\nA trigger that activates when specified objects collide.\nProximityToCameraTrigger\nA trigger that fires when the camera crosses the distance threshold of an object.\nSceneTransitionTrigger\nA trigger that fires during scene transitions.\nTapGestureTrigger\nA trigger that fires when the user taps.\nNotificationTrigger\nA trigger that fires when an app posts a notification.\nActions\nExecute a unique response to a trigger.\nPreliminary_Action\nA specific task that a trigger performs.\nAudioAction\nAn action that plays audio.\nChangeSceneAction\nAn action that transitions from one scene to another.\nEmphasizeAction\nAn action that performs an animation to call attention to an object.\nGroupAction\nAn action that runs a list of other actions.\nImpulseAction\nAn action that adds velocity to an prim.\nLookAtCameraAction\nAn action that reorients an object to face the user’s camera.\nOrbitAction\nAn action that orbits a set of prims around another.\nSpinAction\nAn action that spins a prim.\nStartAnimationAction\nAn action that plays an asset’s animation.\nTransformAction\nAn action that animates from one transform to another.\nTransformAnimationAction\nAn action that plays a transform animation.\nVisibilityAction\nAn action that displays or hides objects over a period of time.\nWaitAction\nAn action that performs a delay.\nNotificationAction\nAn action that sends a custom notification to an app."
  },
  {
    "title": "imageResolution | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/2875717-imageresolution",
    "html": "Discussion\n\nThis size describes the image in the capturedImage buffer, which contains image data in the camera device's native sensor orientation. To convert image coordinates to match a specific display orientation of that image, use the viewMatrix(for:) or projectPoint(_:orientation:viewportSize:) method.\n\nSee Also\nExamining Imaging Parameters\nvar intrinsics: simd_float3x3\nA matrix that converts between the 2D camera plane and 3D world coordinate space."
  },
  {
    "title": "ARCamera.TrackingState | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate",
    "html": "Topics\nDetermining the camera tracking status\ncase notAvailable\nCamera position tracking is not available.\ncase limited(ARCamera.TrackingState.Reason)\nTracking is available, but the quality of results is questionable.\nenum ARCamera.TrackingState.Reason\nCauses of limited position-tracking quality.\ncase normal\nCamera position tracking is providing optimal results.\nComparing camera tracking states\nstatic func == (ARCamera.TrackingState, ARCamera.TrackingState) -> Bool\nDetermines whether two camera tracking states are equal.\nstatic func != (ARCamera.TrackingState, ARCamera.TrackingState) -> Bool\nReturns a Boolean value indicating whether two camera tracking states aren't equal.\nSee Also\nHandling Tracking Status\nvar trackingState: ARCamera.TrackingState\nThe general quality of position tracking available when the camera captured a frame."
  },
  {
    "title": "trackingState | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/2909008-trackingstate",
    "html": "Discussion\n\nWhen this value is ARCamera.TrackingState.limited(_:), see the associated ARCamera.TrackingState.Reason value for a possible cause of low tracking quality.\n\nSee Also\nHandling Tracking Status\nenum ARCamera.TrackingState\nValues for position tracking quality, with possible causes when tracking quality is limited."
  },
  {
    "title": "eulerAngles | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/2866109-eulerangles",
    "html": "See Also\nExamining Camera Geometry\nvar transform: simd_float4x4\nThe position and orientation of the camera in world coordinate space."
  },
  {
    "title": "geometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/2941025-geometry",
    "html": "Discussion\n\nThis mesh provides vertex, index, and texture coordinate buffers describing the estimated 2D footprint of the plane.\n\nYou can visualize the plane geometry by passing these buffers to your preferred rendering engine. To visualize a plane geometry using SceneKit, create an ARSCNPlaneGeometry instance and use its update(from:) method to update it to match the plane geometry.\n\nSee Also\nGeometry\nclass ARPlaneGeometry\nA 3D mesh describing the shape of a detected plane in world-tracking AR sessions.\nclass ARSCNPlaneGeometry\nA SceneKit representation of the 2D shape of a plane, for use with plane detection results in an AR session."
  },
  {
    "title": "ARPlaneAnchor.Alignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/alignment",
    "html": "Topics\nAlignment Values\ncase horizontal\nThe plane is perpendicular to gravity.\ncase vertical\nThe plane is parallel to gravity.\nRelationships\nConforms To\nSendable\nSee Also\nOrientation\nvar alignment: ARPlaneAnchor.Alignment\nThe general orientation of the detected plane with respect to gravity."
  },
  {
    "title": "alignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/2865599-alignment",
    "html": "See Also\nOrientation\nenum ARPlaneAnchor.Alignment\nValues describing possible general orientations of a detected plane with respect to gravity."
  },
  {
    "title": "ARPlaneGeometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplanegeometry",
    "html": "Overview\n\nThis class provides the estimated general shape of a detected plane, in the form of a detailed 3D mesh appropriate for use with various rendering technologies or for exporting 3D assets. (For a quick way to visualize a plane geometry using SceneKit, see the ARSCNPlaneGeometry class.)\n\nUnlike the ARPlaneAnchor center and extent properties, which estimate only a rectangular area for a detected plane, a plane anchor's geometry property provides a more detailed estimate of the 2D area covered by that plane. For example, if ARKit detects a circular tabletop, the resulting ARPlaneGeometry objects roughly match the general shape of the table. As the session continues to run, ARKit provides updated plane anchors whose associated geometry refines the estimated shape of the plane.\n\nYou can use this model to more precisely place 3D content that should appear only on a detected flat surface. For example, to ensure that virtual objects don't fall off the edge of a table. You can also use this model to create occlusion geometry, which hides other virtual content behind the detected surface in the camera image.\n\nThe shape of a plane geometry is always convex. That is, the boundary polygon for a plane geometry is a minimal convex hull enclosing all points that ARKit recognizes or estimates are part of the plane.\n\nTopics\nAccessing Mesh Data\nvar vertices: [simd_float3]\nAn array of vertex positions for each point in the plane mesh.\nvar textureCoordinates: [vector_float2]\nAn array of texture coordinate values for each point in the plane mesh.\nvar triangleCount: Int\nThe number of triangles described by the triangleIndices buffer.\nvar triangleIndices: [Int16]\nAn array of indices describing the triangle mesh formed by the plane geometry's vertex data.\nFinding Boundary Points\nvar boundaryVertices: [simd_float3]\nAn array of vertex positions for each point along the plane's boundary.\nRelationships\nInherits From\nNSObject\nConforms To\nNSSecureCoding\nSee Also\nGeometry\nvar geometry: ARPlaneGeometry\nA coarse triangle mesh representing the general shape of the detected plane.\nclass ARSCNPlaneGeometry\nA SceneKit representation of the 2D shape of a plane, for use with plane detection results in an AR session."
  },
  {
    "title": "ARSCNPlaneGeometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnplanegeometry",
    "html": "Overview\n\nARSCNPlaneGeometry is a subclass of SCNGeometry that wraps the mesh data provided by the ARPlaneGeometry class. You can use ARSCNPlaneGeometry to visualize the plane shape estimates provided by ARKit in a SceneKit view.\n\nImportant\n\nARSCNPlaneGeometry is available only in SceneKit views or renderers that use Metal. This class is not supported for OpenGL-based SceneKit rendering.\n\nAs your AR session continues to run, ARKit provides refined estimates of a detected plane's 2D shape. Use the update(from:) method to incorporate those refinements into the plane's SceneKit representation.\n\nTopics\nCreating a Geometry\ninit?(device: MTLDevice)\nCreates a SceneKit plane geometry for rendering with the specified Metal device object.\nUpdating the Geometry\nfunc update(from: ARPlaneGeometry)\nReshapes the SceneKit geometry to match the specified plane mesh.\nRelationships\nInherits From\nSCNGeometry\nSee Also\nGeometry\nvar geometry: ARPlaneGeometry\nA coarse triangle mesh representing the general shape of the detected plane.\nclass ARPlaneGeometry\nA 3D mesh describing the shape of a detected plane in world-tracking AR sessions."
  },
  {
    "title": "planeExtent | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/3950858-planeextent",
    "html": "Discussion\n\nWhen ARKit first detects a plane, the resulting ARPlaneAnchor object has a center value of (0,0,0), indicating that the translation portion of the anchor’s transform value locates the plane’s center point.\n\nAs the session runs, ARKit may determine that a previously detected plane anchor is part of a larger real-world surface, increasing the planeExtent width and height values. The plane’s new boundaries may not be symmetric around its initial position, so the center point changes relative to the anchor’s unchanged transform matrix.\n\nSimilarly, as the session runs, the framework may update the plane’s y-rotation to better fit its rectangular area in the environment. In iOS 15 and earlier, the framework rotates the plane anchor according to that angle. In iOS 16, the framework doesn't rotate the anchor automatically and its transform matrix remains unchanged. Instead, the framework exposes the angle in rotationOnYAxis that you apply to any plane extent geometry in your app.\n\nImportant\n\nApps that run on iOS 16 with a deployment target less than iOS 16 preserve the prior y-axis rotation behavior.\n\nSee Also\nDimensions\nvar center: simd_float3\nThe center point of the plane relative to its anchor position.\nclass ARPlaneExtent\nThe size and y-axis rotation of a detected plane.\nvar extent: simd_float3\nThe estimated width and length of the detected plane.\nDeprecated"
  },
  {
    "title": "center | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/2882056-center",
    "html": "Discussion\n\nWhen ARKit first detects a plane, the resulting ARPlaneAnchor object has a center value of (0,0,0), indicating that the translation portion of the anchor's transform value locates the plane's center point.\n\nAs scene analysis and plane detection continues, ARKit may determine that a previously detected plane anchor is part of a larger real-world surface, increasing the extent width and length values. The plane's new boundaries may not be symmetric around its initial position, so the center point changes relative to the anchor's (unchanged) transform matrix.\n\nAlthough the type of this property is vector_float3, a plane anchor is always two-dimensional, and is always positioned in only the x and z directions relative to its transform position. (That is, the y-component of this vector is always zero.)\n\nSee Also\nDimensions\nvar planeExtent: ARPlaneExtent\nThe estimated width, length, and y-axis rotation of the detected plane.\nclass ARPlaneExtent\nThe size and y-axis rotation of a detected plane.\nvar extent: simd_float3\nThe estimated width and length of the detected plane.\nDeprecated"
  },
  {
    "title": "physicalSize | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceimage/2941027-physicalsize",
    "html": "Discussion\n\nTo accurately recognize the position and orientation of an image in the AR environment, ARKit must know the image's physical size. You provide this information when creating an AR reference image in your Xcode project's asset catalog, or when programmatically creating an ARReferenceImage.\n\nWhen you want to recognize different-sized versions of a reference image, you set automaticImageScaleEstimationEnabled to true, and in this case, ARKit disregards physicalSize.\n\nSee Also\nExamining a Reference Image\nvar name: String?\nA descriptive name for the image.\nvar resourceGroupName: String?\nThe AR resource group name for this image."
  },
  {
    "title": "ARPlaneExtent | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneextent",
    "html": "Overview\n\nA plane anchor (ARPlaneAnchor) describes its size and orientation on the y-axis using the planeExtent property of this type.\n\nAt runtime, ARKit continually updates the anchor's width and height as the framework refines its knowledge of the plane's shape in the environment.\n\nSimilarly, as the session runs, the framework may update the plane’s y-rotation to better fit its rectangular area in the environment. In iOS 15 and earlier, the framework rotates the plane anchor according to that angle. In iOS 16, the framework doesn’t rotate the anchor automatically and its transform matrix remains unchanged. Instead, the framework exposes the angle in rotationOnYAxis that you apply to any plane extent geometry in your app.\n\nImportant\n\nApps that run on iOS 16 with a deployment target less than iOS 16 preserve the prior y-axis rotation behavior.\n\nSize and Rotate an Entity to a Plane's Extent\n\nThe following code defines a RealityKit entity sized to the plane extent's width and height. The helper function also applies the extent's suggested y-axis rotation by setting its transform yaw value to rotationOnYAxis.\n\nfunc createPlane(for planeAnchor: ARPlaneAnchor, material: Material) -> ModelEntity {\n\n\n    // Get the plane's extent.\n    let extent = planeAnchor.planeExtent\n\n\n    // Create a model entity sized to the plane's extent.\n    let planeEntity = ModelEntity(mesh: .generatePlane (width: extent.width, depth: extent.height),\n        materials: [material])\n\n\n    // Orient the entity according to the extent's y-axis rotation.\n    planeEntity.transform = Transform(pitch: 0, yaw: extent.rotationOnYAxis, roll: 0)\n\n\n    // Center the entity on the plane.\n    planeEntity.transform.translation = planeAnchor.center\n\n\n    return planeEntity\n}\n\n\nTopics\nInspecting Plane Size\nvar width: Float\nThe estimated width of the plane.\nvar height: Float\nThe estimated height of the plane.\nInspecting Plane Y-Rotation\nvar rotationOnYAxis: Float\nA radian value that indicates a plane's y-axis orientation.\nRelationships\nInherits From\nNSObject\nConforms To\nNSSecureCoding\nSee Also\nDimensions\nvar center: simd_float3\nThe center point of the plane relative to its anchor position.\nvar planeExtent: ARPlaneExtent\nThe estimated width, length, and y-axis rotation of the detected plane.\nvar extent: simd_float3\nThe estimated width and length of the detected plane.\nDeprecated"
  },
  {
    "title": "referenceImage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arimageanchor/2941037-referenceimage",
    "html": "Discussion\n\nThis object is always one of the ARReferenceImage objects you provided in the detectionImages array when configuring the session."
  },
  {
    "title": "isClassificationSupported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/2990938-isclassificationsupported",
    "html": "Discussion\n\nPlane classification is available on iOS devices with A12 or later GPU.\n\nOn devices without plane classification support, all plane anchors report a classification value of ARPlaneClassificationNone and a classificationStatus value of notAvailable.\n\nSee Also\nClassifying a Plane\nvar classification: ARPlaneAnchor.Classification\nA general characterization of what kind of real-world surface the plane anchor represents.\nenum ARPlaneAnchor.Classification\nPossible characterizations of real-world surfaces represented by plane anchors."
  },
  {
    "title": "buffer | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement/4108381-buffer",
    "html": "See Also\nRendering geometry elements\nvar primitive: GeometryElement.Primitive\nThe kind of primitive, lines or triangles, that a geometry element contains.\nenum GeometryElement.Primitive\nThe kinds of geometry primitives that a geometry element can contain.\nvar count: Int\nThe number of primitives in the Metal buffer for a geometry element.\nvar bytesPerIndex: Int\nThe number of bytes that represent an index value."
  },
  {
    "title": "extent | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor/2882055-extent",
    "html": "Discussion\n\nWarning\n\nIn iOS 16, use planeExtent instead.\n\nThe framework sets the x and z components to the width and length of the plane, respectively. The y-component is unused, with a constant value of 0.\n\nSee Also\nDimensions\nvar center: simd_float3\nThe center point of the plane relative to its anchor position.\nvar planeExtent: ARPlaneExtent\nThe estimated width, length, and y-axis rotation of the detected plane.\nclass ARPlaneExtent\nThe size and y-axis rotation of a detected plane."
  },
  {
    "title": "AnchorUpdateSequence.AsyncIterator | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/asynciterator",
    "html": "See Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "reduce(into:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180434-reduce",
    "html": "Parameters\ninitialResult\n\nThe value to use as the initial accumulating value. The nextPartialResult closure receives initialResult the first time the closure executes.\n\nnextPartialResult\n\nA closure that combines an accumulating value and an element of the asynchronous sequence into a new accumulating value, for use in the next call of the nextPartialResult closure or returned to the caller.\n\nReturn Value\n\nThe final accumulated value. If the sequence has no elements, the result is initialResult.\n\nDiscussion\n\nUse the reduce(into:_:) method to produce a single value from the elements of an entire sequence. For example, you can use this method on a sequence of numbers to find their sum or product.\n\nThe nextPartialResult closure executes sequentially with an accumulating value initialized to initialResult and each element of the sequence.\n\nPrefer this method over reduce(_:_:) for efficiency when the result is a copy-on-write type, for example an Array or Dictionary.\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "AnchorUpdateSequence.Element | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/element",
    "html": "See Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "primitive | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement/4131701-primitive",
    "html": "See Also\nRendering geometry elements\nvar buffer: MTLBuffer\nA Metal buffer that contains index data that defines the geometry of an object.\nenum GeometryElement.Primitive\nThe kinds of geometry primitives that a geometry element can contain.\nvar count: Int\nThe number of primitives in the Metal buffer for a geometry element.\nvar bytesPerIndex: Int\nThe number of bytes that represent an index value."
  },
  {
    "title": "count | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement/4108383-count",
    "html": "See Also\nRendering geometry elements\nvar buffer: MTLBuffer\nA Metal buffer that contains index data that defines the geometry of an object.\nvar primitive: GeometryElement.Primitive\nThe kind of primitive, lines or triangles, that a geometry element contains.\nenum GeometryElement.Primitive\nThe kinds of geometry primitives that a geometry element can contain.\nvar bytesPerIndex: Int\nThe number of bytes that represent an index value."
  },
  {
    "title": "AnchorUpdateSequence.Iterator | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/iterator",
    "html": "Topics\nPerforming sequence iterator operations\nfunc next() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>.Element?\ntypealias AnchorUpdateSequence.Iterator.Element\nRelationships\nConforms To\nAsyncIteratorProtocol\nSendable\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element"
  },
  {
    "title": "bytesPerIndex | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement/4108382-bytesperindex",
    "html": "See Also\nRendering geometry elements\nvar buffer: MTLBuffer\nA Metal buffer that contains index data that defines the geometry of an object.\nvar primitive: GeometryElement.Primitive\nThe kind of primitive, lines or triangles, that a geometry element contains.\nenum GeometryElement.Primitive\nThe kinds of geometry primitives that a geometry element can contain.\nvar count: Int\nThe number of primitives in the Metal buffer for a geometry element."
  },
  {
    "title": "reduce(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180432-reduce",
    "html": "Parameters\ninitialResult\n\nThe value to use as the initial accumulating value. The nextPartialResult closure receives initialResult the first time the closure runs.\n\nnextPartialResult\n\nA closure that combines an accumulating value and an element of the asynchronous sequence into a new accumulating value, for use in the next call of the nextPartialResult closure or returned to the caller.\n\nReturn Value\n\nThe final accumulated value. If the sequence has no elements, the result is initialResult.\n\nDiscussion\n\nUse the reduce(_:_:) method to produce a single value from the elements of an entire sequence. For example, you can use this method on an sequence of numbers to find their sum or product.\n\nThe nextPartialResult closure executes sequentially with an accumulating value initialized to initialResult and each element of the sequence.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 4. The reduce(_:_:) method sums the values received from the asynchronous sequence.\n\nlet sum = await Counter(howHigh: 4)\n    .reduce(0) {\n        $0 + $1\n    }\nprint(sum)\n// Prints \"10\"\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "prefix(while:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180430-prefix",
    "html": "Parameters\npredicate\n\nA closure that takes an element as a parameter and returns a Boolean value indicating whether the element should be included in the modified sequence.\n\nReturn Value\n\nAn asynchronous sequence of the initial, consecutive elements that satisfy predicate.\n\nDiscussion\n\nUse prefix(while:) to produce values while elements from the base sequence meet a condition you specify. The modified sequence ends when the predicate closure returns false.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The prefix(while:) method causes the modified sequence to pass along values so long as they aren’t divisible by 2 and 3. Upon reaching 6, the sequence ends:\n\nlet stream = Counter(howHigh: 10)\n    .prefix { $0 % 2 != 0 || $0 % 3 != 0 }\nfor try await number in stream {\n    print(number, terminator: \" \")\n}\n// Prints \"1 2 3 4 5 \"\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "prefix(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180428-prefix",
    "html": "Parameters\ncount\n\nThe maximum number of elements to return. The value of count must be greater than or equal to zero.\n\nReturn Value\n\nAn asynchronous sequence starting at the beginning of the base sequence with at most count elements.\n\nDiscussion\n\nUse prefix(_:) to reduce the number of elements produced by the asynchronous sequence.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The prefix(_:) method causes the modified sequence to pass through the first six values, then end.\n\nfor await number in Counter(howHigh: 10).prefix(6) {\n    print(number, terminator: \" \")\n}\n// Prints \"1 2 3 4 5 6 \"\n\n\nIf the count passed to prefix(_:) exceeds the number of elements in the base sequence, the result contains all of the elements in the sequence.\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "min(by:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180426-min",
    "html": "Parameters\nareInIncreasingOrder\n\nA predicate that returns true if its first argument should be ordered before its second argument; otherwise, false.\n\nReturn Value\n\nThe sequence’s minimum element, according to areInIncreasingOrder. If the sequence has no elements, returns nil.\n\nDiscussion\n\nUse this method when the asynchronous sequence’s values don’t conform to Comparable, or when you want to apply a custom ordering to the sequence.\n\nThe predicate must be a strict weak ordering over the elements. That is, for any elements a, b, and c, the following conditions must hold:\n\nareInIncreasingOrder(a, a) is always false. (Irreflexivity)\n\nIf areInIncreasingOrder(a, b) and areInIncreasingOrder(b, c) are both true, then areInIncreasingOrder(a, c) is also true. (Transitive comparability)\n\nTwo elements are incomparable if neither is ordered before the other according to the predicate. If a and b are incomparable, and b and c are incomparable, then a and c are also incomparable. (Transitive incomparability)\n\nThe following example uses an enumeration of playing cards ranks, Rank, which ranges from ace (low) to king (high). An asynchronous sequence called RankCounter produces all elements of the array. The predicate provided to the min(by:) method sorts ranks based on their rawValue:\n\nenum Rank: Int {\n    case ace = 1, two, three, four, five, six, seven, eight, nine, ten, jack, queen, king\n}\n\n\nlet min = await RankCounter()\n    .min { $0.rawValue < $1.rawValue }\nprint(min ?? \"none\")\n// Prints \"ace\"\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "map(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180422-map",
    "html": "Parameters\ntransform\n\nA mapping closure. transform accepts an element of this sequence as its parameter and returns a transformed value of the same or of a different type. transform can also throw an error, which ends the transformed sequence.\n\nReturn Value\n\nAn asynchronous sequence that contains, in order, the elements produced by the transform closure.\n\nDiscussion\n\nUse the map(_:) method to transform every element received from a base asynchronous sequence. Typically, you use this to transform from one type of element to another.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 5. The closure provided to the map(_:) method takes each Int and looks up a corresponding String from a romanNumeralDict dictionary. This means the outer for await in loop iterates over String instances instead of the underlying Int values that Counter produces. Also, the dictionary doesn’t provide a key for 4, and the closure throws an error for any key it can’t look up, so receiving this value from Counter ends the modified sequence with an error.\n\nlet romanNumeralDict: [Int: String] =\n    [1: \"I\", 2: \"II\", 3: \"III\", 5: \"V\"]\n\n\ndo {\n    let stream = Counter(howHigh: 5)\n        .map { (value) throws -> String in\n            guard let roman = romanNumeralDict[value] else {\n                throw MyError()\n            }\n            return roman\n        }\n    for try await numeral in stream {\n        print(numeral, terminator: \" \")\n    }\n} catch {\n    print(\"Error: \\(error)\")\n}\n// Prints \"I II III Error: MyError() \"\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "map(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180420-map",
    "html": "Parameters\ntransform\n\nA mapping closure. transform accepts an element of this sequence as its parameter and returns a transformed value of the same or of a different type.\n\nReturn Value\n\nAn asynchronous sequence that contains, in order, the elements produced by the transform closure.\n\nDiscussion\n\nUse the map(_:) method to transform every element received from a base asynchronous sequence. Typically, you use this to transform from one type of element to another.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 5. The closure provided to the map(_:) method takes each Int and looks up a corresponding String from a romanNumeralDict dictionary. This means the outer for await in loop iterates over String instances instead of the underlying Int values that Counter produces:\n\nlet romanNumeralDict: [Int: String] =\n    [1: \"I\", 2: \"II\", 3: \"III\", 5: \"V\"]\n\n\nlet stream = Counter(howHigh: 5)\n    .map { romanNumeralDict[$0] ?? \"(unknown)\" }\nfor await numeral in stream {\n    print(numeral, terminator: \" \")\n}\n// Prints \"I II III (unknown) V \"\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "flatMap(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180415-flatmap",
    "html": "Parameters\ntransform\n\nAn error-throwing mapping closure. transform accepts an element of this sequence as its parameter and returns an AsyncSequence. If transform throws an error, the sequence ends.\n\nReturn Value\n\nA single, flattened asynchronous sequence that contains all elements in all the asynchronous sequences produced by transform. The sequence ends either when the last sequence created from the last element from base sequence ends, or when transform throws an error.\n\nDiscussion\n\nUse this method to receive a single-level asynchronous sequence when your transformation produces an asynchronous sequence for each element.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 5. The transforming closure takes the received Int and returns a new Counter that counts that high. For example, when the transform receives 3 from the base sequence, it creates a new Counter that produces the values 1, 2, and 3. The flatMap(_:) method “flattens” the resulting sequence-of-sequences into a single AsyncSequence. However, when the closure receives 4, it throws an error, terminating the sequence.\n\ndo {\n    let stream = Counter(howHigh: 5)\n        .flatMap { (value) -> Counter in\n            if value == 4 {\n                throw MyError()\n            }\n            return Counter(howHigh: value)\n        }\n    for try await number in stream {\n        print(number, terminator: \" \")\n    }\n} catch {\n    print(error)\n}\n// Prints \"1 1 2 1 2 3 MyError() \"\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "makeAsyncIterator() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180419-makeasynciterator",
    "html": "Relationships\nFrom Protocol\nAsyncSequence\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "flatMap(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180417-flatmap",
    "html": "Parameters\ntransform\n\nA mapping closure. transform accepts an element of this sequence as its parameter and returns an AsyncSequence.\n\nReturn Value\n\nA single, flattened asynchronous sequence that contains all elements in all the asynchronous sequences produced by transform.\n\nDiscussion\n\nUse this method to receive a single-level asynchronous sequence when your transformation produces an asynchronous sequence for each element.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 5. The transforming closure takes the received Int and returns a new Counter that counts that high. For example, when the transform receives 3 from the base sequence, it creates a new Counter that produces the values 1, 2, and 3. The flatMap(_:) method “flattens” the resulting sequence-of-sequences into a single AsyncSequence.\n\nlet stream = Counter(howHigh: 5)\n    .flatMap { Counter(howHigh: $0) }\nfor await number in stream {\n    print(number, terminator: \" \")\n}\n// Prints \"1 1 2 1 2 3 1 2 3 4 1 2 3 4 5 \"\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "first(where:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180413-first",
    "html": "Parameters\npredicate\n\nA closure that takes an element of the asynchronous sequence as its argument and returns a Boolean value that indicates whether the element is a match.\n\nReturn Value\n\nThe first element of the sequence that satisfies predicate, or nil if there is no element that satisfies predicate.\n\nDiscussion\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The first(where:) method returns the first member of the sequence that’s evenly divisible by both 2 and 3.\n\nlet divisibleBy2And3 = await Counter(howHigh: 10)\n    .first { $0 % 2 == 0 && $0 % 3 == 0 }\nprint(divisibleBy2And3 ?? \"none\")\n// Prints \"6\"\n\n\nThe predicate executes each time the asynchronous sequence produces an element, until either the predicate finds a match or the sequence ends.\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "ARError.Code.cameraUnauthorized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/cameraunauthorized",
    "html": "Discussion\n\nTo use the device's camera:\n\nYour app's Info.plist file must provide a message for the NSCameraUsageDescription key. If this key is missing, any attempt to run an AR session fails with this error.\n\nWhen your app first attempts to run an AR session or otherwise use the camera, iOS automatically shows an alert with your camera usage description message, asking the user to grant camera permission to your app. If the user accepts this request, the session begins; otherwise the session fails with this error.\n\nIf the user has previously denied camera permission for your app, all attempts to run an AR session or otherwise use the camera fail with this error. To grant camera permission, the user must explicitly enable your app in the iOS Settings app, under Privacy > Camera.\n\nSee Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARError.Code.requestFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/requestfailed",
    "html": "See Also\nErrors\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "compactMap(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180401-compactmap",
    "html": "Parameters\ntransform\n\nA mapping closure. transform accepts an element of this sequence as its parameter and returns a transformed value of the same or of a different type.\n\nReturn Value\n\nAn asynchronous sequence that contains, in order, the non-nil elements produced by the transform closure.\n\nDiscussion\n\nUse the compactMap(_:) method to transform every element received from a base asynchronous sequence, while also discarding any nil results from the closure. Typically, you use this to transform from one type of element to another.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 5. The closure provided to the compactMap(_:) method takes each Int and looks up a corresponding String from a romanNumeralDict dictionary. Because there is no key for 4, the closure returns nil in this case, which compactMap(_:) omits from the transformed asynchronous sequence.\n\nlet romanNumeralDict: [Int: String] =\n    [1: \"I\", 2: \"II\", 3: \"III\", 5: \"V\"]\n    \nlet stream = Counter(howHigh: 5)\n    .compactMap { romanNumeralDict[$0] }\nfor await numeral in stream {\n    print(numeral, terminator: \" \")\n}\n// Prints \"I II III V \"\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "compactMap(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180403-compactmap",
    "html": "Parameters\ntransform\n\nAn error-throwing mapping closure. transform accepts an element of this sequence as its parameter and returns a transformed value of the same or of a different type. If transform throws an error, the sequence ends.\n\nReturn Value\n\nAn asynchronous sequence that contains, in order, the non-nil elements produced by the transform closure. The sequence ends either when the base sequence ends or when transform throws an error.\n\nDiscussion\n\nUse the compactMap(_:) method to transform every element received from a base asynchronous sequence, while also discarding any nil results from the closure. Typically, you use this to transform from one type of element to another.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 5. The closure provided to the compactMap(_:) method takes each Int and looks up a corresponding String from a romanNumeralDict dictionary. Since there is no key for 4, the closure returns nil in this case, which compactMap(_:) omits from the transformed asynchronous sequence. When the value is 5, the closure throws MyError, terminating the sequence.\n\nlet romanNumeralDict: [Int: String] =\n    [1: \"I\", 2: \"II\", 3: \"III\", 5: \"V\"]\n\n\ndo {\n    let stream = Counter(howHigh: 5)\n        .compactMap { (value) throws -> String? in\n            if value == 5 {\n                throw MyError()\n            }\n            return romanNumeralDict[value]\n        }\n    for try await numeral in stream {\n        print(numeral, terminator: \" \")\n    }\n} catch {\n    print(\"Error: \\(error)\")\n}\n// Prints \"I II III Error: MyError() \"\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "drop(while:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180407-drop",
    "html": "Parameters\npredicate\n\nA closure that takes an element as a parameter and returns a Boolean value indicating whether to drop the element from the modified sequence.\n\nReturn Value\n\nAn asynchronous sequence that skips over values from the base sequence until the provided closure returns false.\n\nDiscussion\n\nUse drop(while:) to omit elements from an asynchronous sequence until the element received meets a condition you specify.\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The drop(while:) method causes the modified sequence to ignore received values until it encounters one that is divisible by 3:\n\nlet stream = Counter(howHigh: 10)\n    .drop { $0 % 3 != 0 }\nfor await number in stream {\n    print(number, terminator: \" \")\n}\n// Prints \"3 4 5 6 7 8 9 10 \"\n\n\nAfter the predicate returns false, the sequence never executes it again, and from then on the sequence passes through elements from its underlying sequence as-is.\n\nSee Also\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "planeDetection | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arpositionaltrackingconfiguration/3255171-planedetection",
    "html": "Discussion\n\nBy default, this configuration disables plane detection. If you enable horizontal or vertical plane detection, the session adds ARPlaneAnchor objects and notifies your ARSessionDelegate, ARSCNViewDelegate, or ARSKViewDelegate object when its analysis of captured video images detects an area that appears to be a flat surface."
  },
  {
    "title": "allSatisfy(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence/4180399-allsatisfy",
    "html": "Parameters\npredicate\n\nA closure that takes an element of the asynchronous sequence as its argument and returns a Boolean value that indicates whether the passed element satisfies a condition.\n\nReturn Value\n\ntrue if the sequence contains only elements that satisfy predicate; otherwise, false.\n\nDiscussion\n\nIn this example, an asynchronous sequence called Counter produces Int values from 1 to 10. The allSatisfy(_:) method checks to see whether all elements produced by the sequence are less than 10.\n\nlet allLessThanTen = await Counter(howHigh: 10)\n    .allSatisfy { $0 < 10 }\nprint(allLessThanTen)\n// Prints \"false\"\n\n\nThe predicate executes each time the asynchronous sequence produces an element, until either the predicate returns false or the sequence ends.\n\nIf the asynchronous sequence is empty, this method returns true.\n\nSee Also\nPerforming sequence operations\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator"
  },
  {
    "title": "isAutoFocusEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arorientationtrackingconfiguration/2942263-isautofocusenabled",
    "html": "Discussion\n\nIn apps linked against the iOS 11.3 SDK or later, ARKit enables autofocus by default."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arorientationtrackingconfiguration/2923556-init",
    "html": "Discussion\n\nTo use the configuration in an AR session, pass it to the ARSession run(_:options:) method."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationstatus/4169981-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nComparing authorization states\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (ARKitSession.AuthorizationStatus, ARKitSession.AuthorizationStatus) -> Bool\nstatic func != (ARKitSession.AuthorizationStatus, ARKitSession.AuthorizationStatus) -> Bool"
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationstatus/4131659",
    "html": "See Also\nComparing authorization states\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (ARKitSession.AuthorizationStatus, ARKitSession.AuthorizationStatus) -> Bool"
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationstatus/4131663-hash",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nComparing authorization states\nvar hashValue: Int\nvar description: String\nstatic func == (ARKitSession.AuthorizationStatus, ARKitSession.AuthorizationStatus) -> Bool\nstatic func != (ARKitSession.AuthorizationStatus, ARKitSession.AuthorizationStatus) -> Bool"
  },
  {
    "title": "ARKitSession.Event.dataProviderStateChanged(dataProviders:newState:error:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/event/dataproviderstatechanged_dataproviders_newstate_error",
    "html": "Parameters\ndataProviders\n\nThe data providers associated with this session.\n\nnewState\n\nThe state that changed to cause the event.\n\nerror\n\nA session error assocated with the state change.\n\nSee Also\nObserving session events\ncase authorizationChanged(type: ARKitSession.AuthorizationType, status: ARKitSession.AuthorizationStatus)\nAn event that represents a change in authorization status for a specific authorization type.\nvar description: String\nA textual description of the authorization status."
  },
  {
    "title": "wantsHDREnvironmentTextures | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/3175416-wantshdrenvironmenttextures",
    "html": "Discussion\n\nIf you set environmentTexturing to .automatic in iOS 12 or later, ARKit gives you environment textures you cast on your app's virtual content to create realistic reflections. By default, the framework sets wantsHDREnvironmentTextures to true. When your renderer supports HDR environment textures in iOS 13, it enables your lighting engine to output more colors, with a more realistic result.\n\nBoth ARView and ARSCNView support HDR environment textures. For more information, see Adding Realistic Reflections to an AR Experience.\n\nFor a Metal app that doesn't yet support HDR environment textures, you can use the following code to receive LDR environment textures until you're ready to update your renderer for HDR.\n\nif #available(iOS 13, *) { \n    configuration.wantsHDREnvironmentTextures = false\n}\n\n\nSee Also\nCreating Realistic Reflections\nvar environmentTexturing: ARWorldTrackingConfiguration.EnvironmentTexturing\nAn option that determines how the framework generates environment textures.\nenum ARWorldTrackingConfiguration.EnvironmentTexturing\nOptions to generate environment textures in a world-tracking AR session.\nclass AREnvironmentProbeAnchor\nAn object that provides environmental lighting information for a specific area of space in a world-tracking AR session."
  },
  {
    "title": "ARSkeleton | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton",
    "html": "Overview\n\nAs a collection of joints, this protocol describes the state of a human body whose movements ARKit can track.\n\nThe ARSkeleton3D subclass provides you with the position of a tracked body's joints in 3D space, specifically with its jointLocalTransforms and jointModelTransforms properties.\n\nThe ARSkeleton2D subclass provides you with the position of a tracked body's joints in 2D space, by way of its jointLandmarks property.\n\nTopics\nGetting Joint Information\nvar definition: ARSkeletonDefinition\nThe particular configuration of joints that define a body's current state.\nfunc isJointTracked(Int) -> Bool\nTells you whether ARKit tracks a joint at a particular index.\nstruct ARSkeleton.JointName\nA name identifier for a joint.\nRelationships\nInherits From\nNSObject\nSee Also\nBody Data\nCapturing Body Motion in 3D\nTrack a person in the physical environment and visualize their motion by applying the same body movements to a virtual character.\nclass ARBody2D\nThe screen-space representation of a person ARKit recognizes in the camera feed.\nclass ARSkeleton3D\nThe skeleton of a human body that ARKit tracks in 3D space.\nclass ARSkeleton2D\nAn object that describes the locations of a body’s joints in the camera feed.\nclass ARSkeletonDefinition\nThe hierarchy of joints and their names."
  },
  {
    "title": "coachingOverlayViewWillActivate(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayviewdelegate/3152985-coachingoverlayviewwillactivate",
    "html": "Discussion\n\nOverride this function to hide your app's UI while the coaching overlay is active.\n\nSee Also\nEnabling Coaching\nfunc coachingOverlayViewDidDeactivate(ARCoachingOverlayView)\nTells you when the coaching experience is completely deactivated."
  },
  {
    "title": "ARError.Code.fileIOFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/fileiofailed",
    "html": "See Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARError.Code.insufficientFeatures | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/insufficientfeatures",
    "html": "Discussion\n\nFor more information about a session’s feature requirements, see Managing Session Life Cycle and Tracking Quality.\n\nSee Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARError.Code.invalidWorldMap | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/invalidworldmap",
    "html": "See Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARError.Code.sensorFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/sensorfailed",
    "html": "See Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARError.Code.worldTrackingFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/worldtrackingfailed",
    "html": "See Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARError.Code.sensorUnavailable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/sensorunavailable",
    "html": "See Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARError.Code.microphoneUnauthorized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/microphoneunauthorized",
    "html": "Discussion\n\nWhen this error occurs, the app needs to prompt the user to give permission to use the microphone in Settings.\n\nSee Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARError.Code.objectMergeFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/objectmergefailed",
    "html": "See Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "Preliminary_Text | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_text",
    "html": "Overview\n\nBecause Preliminary_Text prim is an Xformable prim, you define its position either by specifying an offset in world coordinates, or by specifying that the prim should inherit its parent transform.\n\nAlternatively, you can request that the runtime anchor a text prim in the real world by inheriting Preliminary_AnchoringAPI. Many AR experiences include anchored text to add information or context about real-world objects or virtual content.\n\nDeclaration\nclass Preliminary_Text \"Preliminary_Text\" (\n    inherits = </Gprim>\n)\n\n\nCreate single-line or flowing text\n\nThe following example demonstrates single-line text.\n\ndef Preliminary_Text \"heading\"\n{\n    string content = \"Units Sold\"\n    string[] font = [ \"Helvetica\", \"Arial\" ]\n    token wrapMode = \"singleLine\"\n    token horizontalAlignment = \"left\"\n    token verticalAlignment = \"baseline\"\n}\n\n\nThe following example shows text flowing in a rectangular region, with line breaks as needed.\n\ndef Preliminary_Text \"sign\"\n{\n    string content = \"Now is the time for all good people to come to the aid of their country.\"\n    string[] font = [ \"ZapfChancery\", \"Bradley Hand\", \"cursive\" ]\n    token wrapMode = \"flowing\"\n    float width = 120\n    float height = 100\n    token horizontalAlignment = \"center\"\n    token verticalAlignment = \"top\"\n}\n\n\nTopics\nProperties\ncontent\nThe characters that the text displays.\nfont\nAn array of font names.\npointSize\nThe size of the text’s font.\nwidth\nThe width of the text’s bounding box.\nheight\nThe height of the text’s bounding box.\ndepth\nA value that defines the depth, in scene units, of the text’s extrusion.\nwrapMode\nAn option that determines the flow of the text.\nhorizontalAlignment\nAn option that controls the text’s horizontal placement within its bounding box.\nverticalAlignment\nAn option that controls the text’s vertical placement within its bounding rectangle."
  },
  {
    "title": "locationUnauthorized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3601239-locationunauthorized",
    "html": "Discussion\n\nTo resolve this issue, the app needs to ask the user to enable location access for this app in Settings.\n\nSee Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "Preliminary_ReferenceImage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_referenceimage",
    "html": "Overview\n\nThis schema defines the criteria that the runtime uses to recognize an image in the physical environment, including pixel data and width.\n\nWhen RealityKit opens a USDZ file that defines a reference image, it instantiates an AnchorEntity with a component type AnchoringComponent.Target.image(group:name:).\n\nDeclaration\nclass Preliminary_ReferenceImage \"Preliminary_ReferenceImage\" (\n    inherits = </Typed>\n)\n\n\nDefine a reference image\n\nThe following example defines a prim named ImageReference that instructs the runtime to scan for an image described by image.png.\n\ndef Preliminary_ReferenceImage \"ImageReference\"\n{\n    uniform asset image = @image.png@\n    uniform double physicalWidth = 12\n}\n\n\nTopics\nProperties\nimage\nAn image file for which the runtime should search.\nphysicalWidth\nAn image’s width in centimeters.\nSee Also\nAnchoring\nPlacing a prim in the real world\nAnchor a prim to a real-world object that the runtime recognizes in the physical environment.\nPreliminary_AnchoringAPI\nA schema that defines the placement of a prim and its children at a real-world location."
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3727370",
    "html": "Parameters\nlhs\n\nThe left-argument error.\n\nrhs\n\nThe right-argument error.\n\nReturn Value\n\nReturns true if the errors are equal. Otherwise, returns false.\n\nSee Also\nComparing errors\nstatic func != (ARError, ARError) -> Bool\nReturns a Boolean value indicating whether two values aren't equal.\nfunc hash(into: inout Hasher)\nHashes the essential components of a value by feeding them into the given hasher.\nvar hashValue: Int\nThe hash value."
  },
  {
    "title": "geoTrackingNotAvailableAtLocation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3580947-geotrackingnotavailableatlocatio",
    "html": "Discussion\n\nThis error code indicates that ARKit does not have the necessary localization imagery to support geo tracking at the user’s current location. See checkAvailability(completionHandler:) for more information.\n\nIf checkAvailability(completionHandler:) returns true and an app begins geo-tracking session, ARKit provides this state reason when the user has moved to an unsupported area.\n\nSee Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "Preliminary_AnchoringAPI | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/preliminary_anchoringapi",
    "html": "Overview\n\nUse this schema to attach prims to real-world areas in an AR experience, such as surfaces, images, or faces. In non-AR viewers like studio mode in AR Quick Look, the runtime displays a prim by applying its transform –– that is, its position, rotation, and scale –– relative to the center of the view.\n\nNote\n\nAlthough ARKit features the ability to recognize predefined real-world objects (see ARReferenceObject), and location anchors (see ARGeoAnchor), the Preliminary_AnchoringAPI schema doesn’t support object or location anchors.\n\nDeclaration\nclass \"Preliminary_AnchoringAPI\"\n(\n    inherits = </APISchemaBase>\n)\n\n\nNest and layer anchorable prims\n\nWhen you assign this schema to a nested prim, the runtime positions the nested prim independently by not basing the child’s transform relative to its parent. As a result, the anchorable child is effectively out of the parent’s hierarchy. Similarly, if an asset defines one or more anchorable prims layered beneath another anchorable prim, the runtime positions each prim independently. However, any unanchorable children of an anchorable prim remain positioned relative to their parent.\n\nSince the runtime doesn’t observe anchorable prim hierarchies, you need to define all anchorable prims at the root of the document, thus, without a parent.\n\nTopics\nProperties\npreliminary:anchoring:type\nA option that specifies the type of anchor.\npreliminary:planeAnchoring:alignment\nAn option that specifies the orientation of a plane.\npreliminary:imageAnchoring:referenceImage\nThe characteristics of an image the runtime should scan for in order to attach a prim.\nSee Also\nAnchoring\nPlacing a prim in the real world\nAnchor a prim to a real-world object that the runtime recognizes in the physical environment.\nPreliminary_ReferenceImage\nA schema that defines the properties of an image in the physical environment."
  },
  {
    "title": "geoTrackingFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3580946-geotrackingfailed",
    "html": "Discussion\n\nARKit will raise an error with this error code when visual localization is taking too long. This situation indicates that the app has met all requirements for geo tracking except for visual localization. To try again, the app needs to ask the user pan the device around the physical environment to acquire different camera-feed imagery. For more information, see Assisting the User with Visual Localization.\n\nSee Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "worldTrackingFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2896955-worldtrackingfailed",
    "html": "See Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "init(_:userInfo:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3727374-init",
    "html": "Parameters\ncode\n\nThe error code.\n\nuserInfo\n\nAn object that provides more information about the error."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2947138",
    "html": "Parameters\nlhs\n\nA value to compare.\n\nrhs\n\nAnother value to compare.\n\nReturn Value\n\nReturns true if the errors aren't equal. Otherwise, returns false.\n\nDiscussion\n\nInequality is the inverse of equality. For any values a and b, a != b implies that a == b is false.\n\nThis is the default implementation of the not-equal-to operator (!=) for any type that conforms to Equatable.\n\nSee Also\nComparing errors\nstatic func == (ARError, ARError) -> Bool\nDetermines whether two errors are equal.\nfunc hash(into: inout Hasher)\nHashes the essential components of a value by feeding them into the given hasher.\nvar hashValue: Int\nThe hash value."
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3727372-hash",
    "html": "Parameters\nhasher\n\nThe hasher to use when combining the components of this instance.\n\nDiscussion\n\nImplement this function to conform to Hashable. The hashing components need to be the same as the ones passed into the conforming type’s == operator. Call hasher.combine(_:) with these components.\n\nImportant\n\nDon’t call finalize() on hasher because it can generate a compile-time error.\n\nSee Also\nComparing errors\nstatic func == (ARError, ARError) -> Bool\nDetermines whether two errors are equal.\nstatic func != (ARError, ARError) -> Bool\nReturns a Boolean value indicating whether two values aren't equal.\nvar hashValue: Int\nThe hash value."
  },
  {
    "title": "Placing a prim in the real world | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/placing_a_prim_in_the_real_world",
    "html": "Overview\n\nWhen the runtime detects real-world landmarks such as surfaces, images, and faces, it keeps tabs on them in case the app needs to refer to them later as data points for adding virtual content. To track a landmark, the runtime assigns an anchor and updates it to track the orientation and position of that landmark while it’s visible in the camera feed.\n\nUse this schema to attach prims to anchors that the runtime can recognize and track in the physical environment. When you assign the prim an anchor type, the runtime places the prim on the first anchor of that type that it sees in the AR experience.\n\nNote\n\nWhen a RealityKit app loads an anchored prim with loadAnchor(named:in:), RealityKit instantiates an AnchorEntity with the AnchoringComponent.Target set to the anchor type specified by the prim, such as a plane, image, or face.\n\nPlace a prim on a surface, image, or face\n\nThe following cube requests that the runtime place it on the first real-world horizontal surface detected in the physical environment.\n\ndef Cube \"PlaneAnchoredCube\" (\n    prepend apiSchemas = [ \"Preliminary_AnchoringAPI\" ]\n)\n{\n    uniform token preliminary:anchoring:type = \"plane\"\n    uniform token preliminary:planeAnchoring:alignment = \"horizontal\"\n    ...\n}\n\n\nThe cube defined in the following example sends a request to the runtime to place it at the real-world location of an image described by image.png.\n\ndef Cube \"ImageAnchoredCube\" (\n    prepend apiSchemas = [ \"Preliminary_AnchoringAPI\" ]\n)\n{\n    uniform token preliminary:anchoring:type = \"image\"\n    rel preliminary:imageAnchoring:referenceImage = <ImageReference>\n    ...\n\n\n    def Preliminary_ReferenceImage \"ImageReference\"\n    {\n      uniform asset image = @image.png@\n      uniform double physicalWidth = 0.12\n    }\n}\n\n\n\n\nThe following example instructs the runtime to place a cube at the real-word location of a detected face. In RealityKit, a face anchor centers its content at its origin, which is the detected face’s plane midpoint.\n\ndef Cube \"FaceAnchoredCube\" (\n    prepend apiSchemas = [ \"Preliminary_AnchoringAPI\" ]\n)\n{\n    uniform token preliminary:anchoring:type = \"face\"\n    ...\n}\n\n\n\n\nSee Also\nAnchoring\nPreliminary_AnchoringAPI\nA schema that defines the placement of a prim and its children at a real-world location.\nPreliminary_ReferenceImage\nA schema that defines the properties of an image in the physical environment."
  },
  {
    "title": "playbackMode | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/playbackmode",
    "html": "Overview\n\nSet the playbackMode metadata on the document’s stage, that is, the outermost container for scene description. For more information about setting stage metadata, see UsdStage > Stage Metadata.\n\nWhen an asset specifies playback metadata and a start animation, the animation plays automatically and loops until a trigger executes the StartAnimationAction. After the start animation completes, the asset's behaviors (Preliminary_Behavior) take control of the animation. If you don't define a start animation for the asset, this property indicates whether an animation should restart after it finishes playing.\n\nDeclaration\nuniform token playbackMode = \"loop\" (       \n    allowedTokens = [\"none\",\"loop\"]\n)\n\n\nPlayback Modes\nnone\n\nRuns a start animation only once until the system fires an animation trigger.\n\nloop\n\nPlays a start animation on a loop until the system fires an animation trigger.\n\nSpecify an initial idling animation\n\nThe following metadata demonstrates playbackMode for an asset that defines a StartAnimationAction. This example sets playbackMode to loop, idling the animation in a perpetual replay until a trigger executes the start action.\n\n#usda 1.0\n(\n    endTimeCode = 300\n    startTimeCode = 1\n    timeCodesPerSecond = 30\n    playbackMode = \"loop\"\n    upAxis = \"Y\"\n)\n\n\ndef Xform \"AnimatedModel\"\n{\n    def Xform \"body\"\n    {\n        double3 xformOp:translate.timeSamples = { ... }\n        uniform token[] xformOpOrder = [\"xformOp:translate\"]\n        ...\n    }\n    ...\n}\n\n\nSee Also\nAnimation\nautoPlay\nMetadata that specifies whether an animation plays automatically on load."
  },
  {
    "title": "autoPlay | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/autoplay",
    "html": "Overview\n\nSet the autoPlay metadata on the document’s stage, that is, the outermost container for scene description. For more information about setting stage metadata, see UsdStage > Stage Metadata.\n\nSet a value of true to play an animation automatically when the scene loads. If this property is false, the animation won't play on load. If an asset omits the autoPlay property, the animation plays on load for backward-compatibility.\n\nTo support assets that disable autoPlay, an app should provide a way to start the animation. For example, when an asset sets autoPlay to false, AR Quick Look displays a scrubber and a Play button the user can tap to start the animation.\n\nReality Composer sets this property to false when it exports a USDZ to enforce that only behaviors (Preliminary_Behavior) drive animations.\n\nDeclaration\nbool autoPlay\n\n\nDisable automatic animation playback\n\nThe following asset demonstrates autoPlay alongside existing animation properties, like startTimeCode and endTimeCode. Because this asset defines autoPlay as false, the runtime doesn’t automatically play the animation after it loads this asset.\n\n#usda 1.0\n(\n    endTimeCode = 300\n    startTimeCode = 1\n    timeCodesPerSecond = 30\n    autoPlay = false\n    upAxis = \"Y\"\n)\n\n\ndef Xform \"AnimatedModel\"\n{\n    def Xform \"body\"\n    {\n        double3 xformOp:translate.timeSamples = { ... }\n        uniform token[] xformOpOrder = [\"xformOp:translate\"]\n        ...\n    }\n    ...\n}\n\n\nSee Also\nAnimation\nplaybackMode\nMetadata that controls animation idling until a behavior takes over."
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3727373-hashvalue",
    "html": "Discussion\n\nBecause hash values aren't always equal across app executions, saving them for future execution isn't reliable.\n\nImportant\n\nhashValue is deprecated as a Hashable requirement. To conform to Hashable, implement the hash(into:) requirement instead.\n\nSee Also\nComparing errors\nstatic func == (ARError, ARError) -> Bool\nDetermines whether two errors are equal.\nstatic func != (ARError, ARError) -> Bool\nReturns a Boolean value indicating whether two values aren't equal.\nfunc hash(into: inout Hasher)\nHashes the essential components of a value by feeding them into the given hasher."
  },
  {
    "title": "errorUserInfo | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2896957-erroruserinfo",
    "html": "Discussion\n\nFor more information about error user info dictionaries, see userInfo.\n\nSee Also\nInspecting error information\nvar userInfo: [String : Any]\nThe user info dictionary.\nvar localizedDescription: String\nA string that contains a description of the error.\nstatic var errorDomain: String\nA string that indicates the error domain in Core Foundation."
  },
  {
    "title": "userInfo | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3727375-userinfo",
    "html": "Discussion\n\nFor more information about user info dictionaries, see userInfo.\n\nSee Also\nInspecting error information\nvar errorUserInfo: [String : Any]\nThe error user info dictionary.\nvar localizedDescription: String\nA string that contains a description of the error.\nstatic var errorDomain: String\nA string that indicates the error domain in Core Foundation."
  },
  {
    "title": "exposureOffset | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/3194569-exposureoffset",
    "html": "Discussion\n\nIf your app displays an AR experience using a custom Metal renderer, use this value to light your scene during its post-processed lighting stage.\n\nLighting a scene using exposureOffset is normally more performant than scaling each light source in your scene based on the value of the frame's lightEstimate."
  },
  {
    "title": "exposureDuration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/3182986-exposureduration",
    "html": "Discussion\n\nIf you display an AR experience using a custom Metal renderer, use this value to determine how much motion blur to apply to your virtual content.\n\nIf ARSCNView is your renderer, SceneKit automatically applies motion blur to your virtual content. For more information, see rendersMotionBlur."
  },
  {
    "title": "projectionMatrix(for:viewportSize:zNear:zFar:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/2923539-projectionmatrix",
    "html": "Parameters\norientation\n\nThe orientation in which the camera image is to be presented.\n\nviewportSize\n\nThe size, in points, of the view in which the camera image is to be presented.\n\nzNear\n\nThe distance from the camera to the near clipping plane.\n\nzFar\n\nThe distance from the camera to the far clipping plane.\n\nReturn Value\n\nA projection matrix that provides an aspect fill and rotation for the provided viewport size and orientation.\n\nDiscussion\n\nThis method has no effect on ARKit, and the zNear and zFar parameters have no relationships to ARKit camera state. Instead, this method uses those parameters as well as the camera's state to construct a projection matrix for use in your own rendering code.\n\nSee Also\nApplying Camera Geometry\nvar projectionMatrix: simd_float4x4\nA transform matrix appropriate for rendering 3D content to match the image captured by the camera.\nfunc viewMatrix(for: UIInterfaceOrientation) -> simd_float4x4\nReturns a transform matrix for converting from world space to camera space.\nfunc projectPoint(simd_float3, orientation: UIInterfaceOrientation, viewportSize: CGSize) -> CGPoint\nReturns the projection of a point from the 3D world space detected by ARKit into the 2D space of a view rendering the scene.\nfunc unprojectPoint(CGPoint, ontoPlane: simd_float4x4, orientation: UIInterfaceOrientation, viewportSize: CGSize) -> simd_float3?\nReturns the projection of a point from the 2D space of a view rendering the scene onto a plane in the 3D world space detected by ARKit."
  },
  {
    "title": "unprojectPoint(_:ontoPlane:orientation:viewportSize:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/3003663-unprojectpoint",
    "html": "Parameters\npoint\n\nThe point in 2D view space to project onto a plane.\n\nThe coordinate space for this point has its origin is in the upper left corner and a size matching the viewportSize parameter.\n\nplaneTransform\n\nA transform matrix specifying the position and orientation of a plane (with infinite extent) in 3D world space. The plane is the xz-plane of the local coordinate space this transform defines.\n\norientation\n\nThe orientation in which the camera image is to be presented.\n\nviewportSize\n\nThe size, in points, of the view in which the camera image is to be presented.\n\nReturn Value\n\nThe 3D point in world space where a ray projected from the specified 2D point intersects the specified plane, or nil if the ray does not intersect the plane.\n\nDiscussion\n\nIf you display AR content with SceneKit, the ARSCNView class provides an otherwise equivalent unprojectPoint(_:ontoPlane:) method that requires fewer parameters (because the view can infer its orientation and size).\n\nSee Also\nApplying Camera Geometry\nvar projectionMatrix: simd_float4x4\nA transform matrix appropriate for rendering 3D content to match the image captured by the camera.\nfunc projectionMatrix(for: UIInterfaceOrientation, viewportSize: CGSize, zNear: CGFloat, zFar: CGFloat) -> simd_float4x4\nReturns a transform matrix appropriate for rendering 3D content to match the image captured by the camera, using the specified parameters.\nfunc viewMatrix(for: UIInterfaceOrientation) -> simd_float4x4\nReturns a transform matrix for converting from world space to camera space.\nfunc projectPoint(simd_float3, orientation: UIInterfaceOrientation, viewportSize: CGSize) -> CGPoint\nReturns the projection of a point from the 3D world space detected by ARKit into the 2D space of a view rendering the scene."
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate/3930611",
    "html": "Parameters\nlhs\n\nThe left-argument camera tracking state.\n\nrhs\n\nThe right-argument camera tracking state.\n\nReturn Value\n\nReturns true if the camera tracking states are equal. Otherwise, returns false.\n\nRelationships\nFrom Protocol\nEquatable\nSee Also\nComparing camera tracking states\nstatic func != (ARCamera.TrackingState, ARCamera.TrackingState) -> Bool\nReturns a Boolean value indicating whether two camera tracking states aren't equal."
  },
  {
    "title": "Saving and Loading World Data | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/data_management/saving_and_loading_world_data",
    "html": "Overview\n\nThis sample app demonstrates a simple AR experience for iOS 12 devices. Before exploring the code, try building and running the app to familiarize yourself with the user experience it demonstrates:\n\nRun the app. You can look around and tap to place a virtual 3D object on real-world surfaces. (Tap again to relocate the object.)\n\nAfter you’ve explored the environment, the Save Experience button becomes available. Tap it to save ARKit’s world-mapping data to local storage.\n\nTap the Load Experience button. (You can do this immediately, or after quitting and relaunching the app, even if the app has been terminated in the background.)\n\nWhile ARKit attempts to resume an AR session from the saved world-mapping data, the app displays a snapshot of the camera view from the time that data was saved. For best results, move the device so that the camera view matches the screenshot.\n\nFollow the steps below to see how this app uses the ARWorldMap class to save and restore ARKit’s spatial mapping state.\n\nGetting Started\n\nRequires Xcode 10.0, iOS 12.0 and an iOS device with A9 or later processor.\n\nRun the AR Session and Place AR Content\n\nThis app extends the basic workflow for building an ARKit app. (For details, see Tracking and Visualizing Planes.) It defines an ARWorldTrackingConfiguration with plane detection enabled, then runs that configuration in the ARSession attached to the ARSCNView that displays the AR experience.\n\nWhen UITapGestureRecognizer detects a tap on the screen, the handleSceneTap method uses ARKit hit-testing to find a 3D point on a real-world surface, then places an ARAnchor marking that position. When ARKit calls the delegate method renderer(_:didAdd:for:), the app loads a 3D model for ARSCNView to display at the anchor’s position.\n\nCapture and Save the AR World Map\n\nAn ARWorldMap object contains a snapshot of all the spatial mapping information that ARKit uses to locate the user’s device in real-world space. To save a map that can reliably be used for restoring your AR session later, you’ll first need to find a good time to capture the map.\n\nARKit provides a worldMappingStatus value that indicates whether it’s currently a good time to capture a world map (or if it’s better to wait until ARKit has mapped more of the local environment). This app uses that value to provide visual feedback and choose when to make the Save Experience button available:\n\n// Enable Save button only when the mapping status is good and an object has been placed\nswitch frame.worldMappingStatus {\ncase .extending, .mapped:\n    saveExperienceButton.isEnabled =\n        virtualObjectAnchor != nil && frame.anchors.contains(virtualObjectAnchor!)\ndefault:\n    saveExperienceButton.isEnabled = false\n}\nstatusLabel.text = \"\"\"\nMapping: \\(frame.worldMappingStatus.description)\nTracking: \\(frame.camera.trackingState.description)\n\"\"\"\n\n\nWhen the user taps the Save Experience button, the app calls getCurrentWorldMap(completionHandler:) to capture the map from the running ARSession, then serializes it to a Data object with NSKeyedArchiver and writes it to local storage:\n\nsceneView.session.getCurrentWorldMap { worldMap, error in\n    guard let map = worldMap\n        else { self.showAlert(title: \"Can't get current world map\", message: error!.localizedDescription); return }\n    \n    // Add a snapshot image indicating where the map was captured.\n    guard let snapshotAnchor = SnapshotAnchor(capturing: self.sceneView)\n        else { fatalError(\"Can't take snapshot\") }\n    map.anchors.append(snapshotAnchor)\n    \n    do {\n        let data = try NSKeyedArchiver.archivedData(withRootObject: map, requiringSecureCoding: true)\n        try data.write(to: self.mapSaveURL, options: [.atomic])\n        DispatchQueue.main.async {\n            self.loadExperienceButton.isHidden = false\n            self.loadExperienceButton.isEnabled = true\n        }\n    } catch {\n        fatalError(\"Can't save map: \\(error.localizedDescription)\")\n    }\n}\n\n\nTo help a user resume the AR experience from this map later, the app also captures a snapshot of the camera view with the example SnapshotAnchor class and stores it in the world map.\n\nLoad and Relocalize to a Saved Map\n\nWhen the app launches, it checks local storage for a world map file it may have saved in an earlier session:\n\ndo {\n    guard let worldMap = try NSKeyedUnarchiver.unarchivedObject(ofClass: ARWorldMap.self, from: data)\n        else { fatalError(\"No ARWorldMap in archive.\") }\n    return worldMap\n} catch {\n    fatalError(\"Can't unarchive ARWorldMap from file data: \\(error)\")\n}\n\n\nIf that file exists and can be deserialized as an ARWorldMap object, the app makes its Load Experience button available. When you tap the button, the app tells ARKit to attempt resuming the session captured in that world map, by creating and running an ARWorldTrackingConfiguration using that map as the initialWorldMap:\n\nlet configuration = self.defaultConfiguration // this app's standard world tracking settings\nconfiguration.initialWorldMap = worldMap\nsceneView.session.run(configuration, options: [.resetTracking, .removeExistingAnchors])\n\n\nARKit then attempts to relocalize to the new world map—that is, to reconcile the received spatial-mapping information with what it senses of the local environment. This process is more likely to succeed if the user moves to areas of the local environment that they visited during the previous session. To help the user successfully resume the saved experience, this app uses the example SnapshotAnchor class to save a camera image in the world map, then displays that image while ARKit is relocalizing.\n\nRestore AR Content After Relocalization\n\nSaving a world map also archives all anchors currently associated with the AR session. After you successfully run a session from a saved world map, the session contains all anchors previously saved in the map. You can use saved anchors to restore virtual content from a previous session.\n\nIn this app, after relocalizing to a previously saved world map, the virtual object placed in the previous session automatically appears at its saved position. The same ARSCNView delegate method renderer(_:didAdd:for:) fires both when you directly add an anchor to the session and when the session restores anchors from a world map. To determine which saved anchor represents the virtual object, this app uses the ARAnchor name property.\n\nfunc renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor) {\n    guard anchor.name == virtualObjectAnchorName\n        else { return }\n    \n    // save the reference to the virtual object anchor when the anchor is added from relocalizing\n    if virtualObjectAnchor == nil {\n        virtualObjectAnchor = anchor\n    }\n    node.addChildNode(virtualObject)\n}\n\n\nIn your own AR experience, you can choose among various techniques for restoring virtual content associated with saved anchors. For example:\n\nAn app for visualizing furniture from a fixed catalog might save an identifier for each placed object in the corresponding anchor’s name, then use that identifier to determine which 3D model to display when resuming a session from a saved map.\n\nA game that places virtual characters to play in the user’s environment might create various custom ARAnchor subclasses to store gameplay data specific to each character, so that resuming a session from a saved map also restores the state of the game. (See ARAnchor Subclassing Notes and ARAnchorCopying.)\n\nSee Also\nWorld Data\nclass ARWorldMap\nThe state in a world-tracking AR session during which a device maps the user's position in physical space and proximity to anchor objects."
  },
  {
    "title": "projectPoint(_:orientation:viewportSize:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/2923538-projectpoint",
    "html": "Parameters\npoint\n\nThe 3D world-space point to project into 2D view space.\n\norientation\n\nThe orientation in which the camera image is to be presented.\n\nviewportSize\n\nThe size, in points, of the view in which the camera image is to be presented.\n\nReturn Value\n\nThe projection of the specified point into a 2D pixel coordinate space whose origin is in the upper left corner and whose size matches that of the viewportSize parameter.\n\nDiscussion\n\nIf you display AR content with SceneKit, the ARSCNView class provides an otherwise equivalent projectPoint(_:) method that requires fewer parameters (because the view can infer its orientation and size).\n\nSee Also\nApplying Camera Geometry\nvar projectionMatrix: simd_float4x4\nA transform matrix appropriate for rendering 3D content to match the image captured by the camera.\nfunc projectionMatrix(for: UIInterfaceOrientation, viewportSize: CGSize, zNear: CGFloat, zFar: CGFloat) -> simd_float4x4\nReturns a transform matrix appropriate for rendering 3D content to match the image captured by the camera, using the specified parameters.\nfunc viewMatrix(for: UIInterfaceOrientation) -> simd_float4x4\nReturns a transform matrix for converting from world space to camera space.\nfunc unprojectPoint(CGPoint, ontoPlane: simd_float4x4, orientation: UIInterfaceOrientation, viewportSize: CGSize) -> simd_float3?\nReturns the projection of a point from the 2D space of a view rendering the scene onto a plane in the 3D world space detected by ARKit."
  },
  {
    "title": "projectionMatrix | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/2887458-projectionmatrix",
    "html": "Discussion\n\nReading this property's value is equivalent to calling the projectionMatrix(withViewportSize:orientation:zNear:zFar:) method, using the camera's imageResolution and intrinsics properties to derive size and orientation, and passing default values of 0.001 and 1000.0 for the near and far clipping planes.\n\nSee Also\nApplying Camera Geometry\nfunc projectionMatrix(for: UIInterfaceOrientation, viewportSize: CGSize, zNear: CGFloat, zFar: CGFloat) -> simd_float4x4\nReturns a transform matrix appropriate for rendering 3D content to match the image captured by the camera, using the specified parameters.\nfunc viewMatrix(for: UIInterfaceOrientation) -> simd_float4x4\nReturns a transform matrix for converting from world space to camera space.\nfunc projectPoint(simd_float3, orientation: UIInterfaceOrientation, viewportSize: CGSize) -> CGPoint\nReturns the projection of a point from the 3D world space detected by ARKit into the 2D space of a view rendering the scene.\nfunc unprojectPoint(CGPoint, ontoPlane: simd_float4x4, orientation: UIInterfaceOrientation, viewportSize: CGSize) -> simd_float3?\nReturns the projection of a point from the 2D space of a view rendering the scene onto a plane in the 3D world space detected by ARKit."
  },
  {
    "title": "isActive | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayview/3152977-isactive",
    "html": "Discussion\n\nIf activatesAutomatically is enabled, this flag tells you whether coaching is in progress. Assign a delegate to coordinate your actions with the coaching overlay and allow coachingOverlayViewWillActivate(_:) to notify you when the coaching overlay is active.\n\nWhen the coaching overlay is deactivating, isActive is false. If the animated property of setActive(_:animated:) is true, isActive and isHidden are false while the coaching overlay is fading out. When the coaching overlay is deactivated without animation, or when the animation finishes, ARKit notifies you by calling coachingOverlayViewDidDeactivate(_:).\n\nSee Also\nActivating the View\nvar activatesAutomatically: Bool\nA flag that indicates whether the coaching view activates automatically, depending on the current session state.\nfunc setActive(Bool, animated: Bool)\nControls whether coaching is in progress."
  },
  {
    "title": "Creating a Fog Effect Using Scene Depth | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/environmental_analysis/creating_a_fog_effect_using_scene_depth",
    "html": "Overview\n\nDevices such as the second-generation iPad Pro 11-inch and fourth-generation iPad Pro 12.9-inch can use the LiDAR Scanner to calculate the distance of real-world objects from the user. In world-tracking experiences on iOS 14, ARKit provides a buffer that describes the objects’ distance from the device in meters.\n\nThis sample app uses the depth buffer to create a virtual fog effect in real time. To draw its graphics, the sample app uses a small Metal renderer. ARKit provides precise depth values for objects in the camera feed, so the sample app applies a Gaussian blur using Metal Performance Shaders (MPS) to soften the fog effect. While drawing the camera image to the screen, the renderer checks the depth texture at every pixel, and overlays a fog color based on that pixel’s distance from the device. For more information on sampling textures and drawing with Metal, see Creating and Sampling Textures.\n\nEnable Scene Depth and Run a Session\n\nIn order to avoid running an unsupported configuration, the sample app first checks whether the device supports scene depth.\n\nif !ARWorldTrackingConfiguration.supportsFrameSemantics(.sceneDepth) ||\n    !ARWorldTrackingConfiguration.supportsFrameSemantics(.smoothedSceneDepth) {\n    // Ensure that the device supports scene depth and present\n    //  an error-message view controller, if not.\n    let storyboard = UIStoryboard(name: \"Main\", bundle: nil)\n    window?.rootViewController = storyboard.instantiateViewController(withIdentifier: \"unsupportedDeviceMessage\")\n}\n\n\nIf the device running the app doesn’t support scene depth, the sample project will stop. Optionally, the app could present the user with an error message and continue the experience without scene depth.\n\nIf the device supports scene depth, the sample app creates a world-tracking configuration and enables the smoothedSceneDepth option on the ARConfiguration.FrameSemantics property.\n\nconfiguration.frameSemantics = .smoothedSceneDepth\n\n\nThen, the sample project begins the AR experience by running the session.\n\nsession.run(configuration)\n\nAccess the Scene’s Depth\n\nARKit provides the depth buffer (depthMap) as a CVPixelBuffer on the current frame’s sceneDepth or smoothedSceneDepth property, depending on the enabled frame semantics. This sample app visualizes smoothedSceneDepth by default. The raw depth values in sceneDepth can create the impression of a flicker effect, but the process of averaging the depth differences across frames smooths the visual into a more realistic fog effect. For debug purposes, the sample allows switching between smoothedSceneDepth and sceneDepth with an onscreen toggle.\n\nguard let sceneDepth = frame.smoothedSceneDepth ?? frame.sceneDepth else {\n    print(\"Failed to acquire scene depth.\")\n    return\n}\nvar pixelBuffer: CVPixelBuffer!\npixelBuffer = sceneDepth.depthMap\n\n\nEvery pixel in the depth buffer maps to a region of the visible scene, which defines that region’s distance from the device in meters. Because the sample project draws to the screen using Metal, it converts the pixel buffer to a Metal texture to transfer the depth data to the GPU for rendering.\n\nvar texturePixelFormat: MTLPixelFormat!\nsetMTLPixelFormat(&texturePixelFormat, basedOn: pixelBuffer)\ndepthTexture = createTexture(fromPixelBuffer: pixelBuffer, pixelFormat: texturePixelFormat, planeIndex: 0)\n\n\nTo set the depth texture’s Metal pixel format, the sample project calls CVPixelBufferGetPixelFormatType(_:) with the depthMap and chooses an appropriate mapping based on the result.\n\nfileprivate func setMTLPixelFormat(_ texturePixelFormat: inout MTLPixelFormat?, basedOn pixelBuffer: CVPixelBuffer!) {\n    if CVPixelBufferGetPixelFormatType(pixelBuffer) == kCVPixelFormatType_DepthFloat32 {\n        texturePixelFormat = .r32Float\n    } else if CVPixelBufferGetPixelFormatType(pixelBuffer) == kCVPixelFormatType_OneComponent8 {\n        texturePixelFormat = .r8Uint\n    } else {\n        fatalError(\"Unsupported ARDepthData pixel-buffer format.\")\n    }\n}\n\nApply a Blur to the Depth Buffer\n\nAs a benefit of rendering its graphics with Metal, this app has at its disposal the display conveniences of MPS. The sample project uses the MPS Gaussian Blur filter to make realistic fog. When instantiating the filter, the sample project passes a sigma of 5 to specify a 5-pixel radius blur.\n\nblurFilter = MPSImageGaussianBlur(device: device, sigma: 5)\n\n\nNote\n\nTo gain performance at the cost of precision, the app can add allowReducedPrecision to the blur filter’s options, which reduces computation time by using half instead of float.\n\nMPS requires input and output images that define the source and destination pixel data for the filter operation.\n\nlet inputImage = MPSImage(texture: depthTexture, featureChannels: 1)\nlet outputImage = MPSImage(texture: filteredDepthTexture, featureChannels: 1)\n\n\nThe sample app passes the input and output images to the blur’s encode function, which schedules the blur to happen on the GPU.\n\nblur.encode(commandBuffer: commandBuffer, sourceImage: inputImage, destinationImage: outputImage)\n\n\nNote\n\nIn-place MPS operations can save time, memory, and power. Since in-place MPS requires fallback code for devices that don’t support it, this sample project doesn’t use it. For more information on in-place operations, see Image Filters.\n\nVisualize the Blurred Depth to Create Fog\n\nMetal renders by providing to the GPU a fragment shader that draws the app’s graphics. Since the sample project renders a camera image, it packages up the camera image for the fragment shader by calling setFragmentTexture.\n\nrenderEncoder.setFragmentTexture(CVMetalTextureGetTexture(cameraImageY), index: 0)\nrenderEncoder.setFragmentTexture(CVMetalTextureGetTexture(cameraImageCbCr), index: 1)\n\n\nNext, the sample app packages up the filtered depth texture.\n\nrenderEncoder.setFragmentTexture(filteredDepthTexture, index: 2)\n\n\nThe sample project’s GPU-side code fields the texture arguments in the order of the index argument. For example, the fragment shader fields the texture with index 0 above as the argument containing the suffix texture(0), as shown in the example below.\n\nfragment half4 fogFragmentShader(FogColorInOut in [[ stage_in ]],\ntexture2d<float, access::sample> cameraImageTextureY [[ texture(0) ]],\ntexture2d<float, access::sample> cameraImageTextureCbCr [[ texture(1) ]],\ndepth2d<float, access::sample> arDepthTexture [[ texture(2) ]],\n\n\nTo output a rendering, Metal calls the fragment shader once for every pixel it draws to the destination. The sample project’s fragment shader begins by reading the RGB value of the current pixel in the camera image. The object “s” is a sampler, which enables the shader to inspect a texture at a specific location. The value in.texCoordCamera refers to this pixel’s relative location within the camera image.\n\nconstexpr sampler s(address::clamp_to_edge, filter::linear);\n\n\n// Sample this pixel's camera image color.\nfloat4 rgb = ycbcrToRGBTransform(\n    cameraImageTextureY.sample(s, in.texCoordCamera),\n    cameraImageTextureCbCr.sample(s, in.texCoordCamera)\n);\nhalf4 cameraColor = half4(rgb);\n\n\nBy sampling the depth texture at in.texCoordCamera, the shader queries for depth at the same relative location that it did for the camera image, and obtains the current pixel’s distance in meters from the device.\n\nfloat depth = arDepthTexture.sample(s, in.texCoordCamera);\n\n\nTo determine the amount of fog that covers this pixel, the sample app calculates a fraction using the current pixel’s distance divided by the distance at which the fog effect fully saturates the scene.\n\n// Determine this fragment's percentage of fog.\nfloat fogPercentage = depth / fogMax;\n\n\nThe mix function mixes two colors based on a percentage. The sample project passes in the RGB values, fog color, and fog percentage to create the right amount of fog for the current pixel.\n\nhalf4 foggedColor = mix(cameraColor, fogColor, fogPercentage);\n\n\nAfter Metal calls the fragment shader for every pixel, the view presents the final, fogged image of the physical environment to the screen.\n\nVisualize Confidence Data\n\nARKit provides the confidenceMap property within ARDepthData to measure the accuracy of the corresponding depth data (depthMap). Although this sample project doesn’t factor depth confidence into its fog effect, confidence data could filter out lower-accuracy depth values if the app’s algorithm required it.\n\nTo provide a sense for depth confidence, this sample app visualizes confidence data at runtime using the confidenceDebugVisualizationEnabled in the Shaders.metal file.\n\n// Set to `true` to visualize confidence.\nbool confidenceDebugVisualizationEnabled = false;\n\n\nWhen the renderer accesses the current frame’s scene depth, the sample project creates a Metal texture of the confidenceMap to draw it on the GPU.\n\npixelBuffer = sceneDepth.confidenceMap\nsetMTLPixelFormat(&texturePixelFormat, basedOn: pixelBuffer)\nconfidenceTexture = createTexture(fromPixelBuffer: pixelBuffer, pixelFormat: texturePixelFormat, planeIndex: 0)\n\n\nWhile the renderer schedules its drawing, the sample project packages up the confidence texture for the GPU by calling setFragmentTexture.\n\nrenderEncoder.setFragmentTexture(CVMetalTextureGetTexture(confidenceTexture), index: 3)\n\n\nThe GPU-side code fields confidence data as the fragment shader’s third texture argument.\n\ntexture2d<uint> arDepthConfidence [[ texture(3) ]])\n\n\nTo access the confidence value of the current pixel’s depth, the fragment shader samples the confidence texture at in.texCoordCamera. Each confidence value in this texture is a uint equivalent of its corresponding case in the ARConfidenceLevel enum.\n\nuint confidence = arDepthConfidence.sample(s, in.texCoordCamera).x;\n\n\nBased on the confidence value at the current pixel, the fragment shader creates a normalized percentage of the confidence color to overlay.\n\nfloat confidencePercentage = (float)confidence / (float)maxConfidence;\n\n\nThe sample project calls the mix function to blend the confidence color into the processed pixel based on the confidence percentage.\n\nreturn mix(confidenceColor, foggedColor, confidencePercentage);\n\n\nAfter Metal calls the fragment shader for every pixel, the view presents the camera image augmented with the confidence visualization.\n\nThis sample uses the color red to identify parts of the scene in which depth confidence is less than ARConfidenceLevel.high. At low confidence depth values with a normalized percentage of 0, the visualization renders solid red (confidenceColor). For high confidence depth values with a value of one, the mix call returns the unfiltered, fogged camera-image color (foggedColor). At medium-confidence areas of the scene, the mix call returns a blend of both colors that applies a reddish tint to the fogged camera-image.\n\nSee Also\nVideo Frame Analysis\nDisplaying a Point Cloud Using Scene Depth\nPresent a visualization of the physical environment by placing points based a scene’s depth data.\nclass ARFrame\nA video image captured as part of a session with position-tracking information.\nclass ARPointCloud\nA collection of points in the world coordinate space of the AR session.\nclass ARDepthData\nAn object that describes the distance to regions of the real world from the plane of the camera."
  },
  {
    "title": "errorCode | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2896953-errorcode",
    "html": "Discussion\n\nThis property stores a raw value for a particular ARError.Code.\n\nSee Also\nIdentifying an error cause\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "localizedDescription | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2896956-localizeddescription",
    "html": "Discussion\n\nFor more information, see localizedDescription.\n\nSee Also\nInspecting error information\nvar errorUserInfo: [String : Any]\nThe error user info dictionary.\nvar userInfo: [String : Any]\nThe user info dictionary.\nstatic var errorDomain: String\nA string that indicates the error domain in Core Foundation."
  },
  {
    "title": "code | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3727371-code",
    "html": "Discussion\n\nSee ARError.Code for this property’s possible values.\n\nSee Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "errorDomain | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2896954-errordomain",
    "html": "Discussion\n\nFor more information about Core Foundation error domains, see Error domains.\n\nSee Also\nInspecting error information\nvar errorUserInfo: [String : Any]\nThe error user info dictionary.\nvar userInfo: [String : Any]\nThe user info dictionary.\nvar localizedDescription: String\nA string that contains a description of the error."
  },
  {
    "title": "requestFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3566757-requestfailed",
    "html": "See Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARTrackedRaycast | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/artrackedraycast",
    "html": "Overview\n\nTracked raycasting improves hit-testing techniques by repeating the query for a 3D position in succession. ARKit provides you with an updated position as it refines its understanding of world over time.\n\nTo start a tracked raycast, you call trackedRaycast(_:updateHandler:) on your app's current ARSession.\n\nTopics\nStopping Tracking\nfunc stopTracking()\nStops repeating the raycast query.\nRelationships\nInherits From\nNSObject\nSee Also\nRaycasting\nPlacing Objects and Handling 3D Interaction\nPlace virtual content at tracked, real-world locations, and enable the user to interact with virtual content by using gestures.\nclass ARRaycastQuery\nA mathematical ray you use to find 3D positions on real-world surfaces.\nclass ARRaycastResult\nInformation about a real-world surface found by examining a point on the screen."
  },
  {
    "title": "ARRaycastQuery | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastquery",
    "html": "Overview\n\nYou create a raycast query by providing a 3D vector and starting place.\n\nTo create a raycast query using a 2D screen location and default vector that casts outward in the z-direction from the user, use the convenience functions, makeRaycastQuery(from:allowing:alignment:) on ARView, or raycastQuery(from:allowing:alignment:) on ARSCNView.\n\nRaycasts can intersect with planes (flat surfaces) or meshes (uneven surfaces). To intersect with planes, see ARRaycastQuery.Target. To intersect with meshes, see ARRaycastQuery.Target.estimatedPlane.\n\nTopics\nCreating a Raycast Query\ninit(origin: simd_float3, direction: simd_float3, allowing: ARRaycastQuery.Target, alignment: ARRaycastQuery.TargetAlignment)\nCreates a new raycast query.\nSpecifying the Target\nvar target: ARRaycastQuery.Target\nA plane type that allows the raycast to terminate if it's encountered.\nenum ARRaycastQuery.Target\nThe types of surface you allow a raycast to intersect with.\nvar targetAlignment: ARRaycastQuery.TargetAlignment\nThe target's alignment with respect to gravity.\nenum ARRaycastQuery.TargetAlignment\nA specification that indicates a target's alignment with respect to gravity.\nInterpreting the Ray\nvar direction: simd_float3\nA vector that describes the ray's trajectory in 3D space.\nvar origin: simd_float3\nA 3D coordinate that defines the ray's starting place.\nRelationships\nInherits From\nNSObject\nSee Also\nRaycasting\nPlacing Objects and Handling 3D Interaction\nPlace virtual content at tracked, real-world locations, and enable the user to interact with virtual content by using gestures.\nclass ARTrackedRaycast\nA raycast query that ARKit repeats in succession to give you refined results over time.\nclass ARRaycastResult\nInformation about a real-world surface found by examining a point on the screen."
  },
  {
    "title": "invalidReferenceImage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2953984-invalidreferenceimage",
    "html": "Discussion\n\nThis error occurs when a reference image in the configuration's detectionImages lacks identifying features, such as when the image contains all white pixels.\n\nSee Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "invalidReferenceObject | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2968493-invalidreferenceobject",
    "html": "See Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "invalidWorldMap | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2968494-invalidworldmap",
    "html": "See Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "microphoneUnauthorized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2968495-microphoneunauthorized",
    "html": "Discussion\n\nWhen this error occurs, the app needs to prompt the user to give permission to use the microphone in Settings.\n\nSee Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "sensorUnavailable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2896951-sensorunavailable",
    "html": "See Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "unsupportedConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2896949-unsupportedconfiguration",
    "html": "Discussion\n\nCall isSupported on an ARConfiguration to ensure it's supported before attempting to create and run it on the session with run(with:).\n\nSee Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "objectMergeFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3019589-objectmergefailed",
    "html": "See Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "sensorFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2896952-sensorfailed",
    "html": "See Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "renderer(_:didAdd:for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnviewdelegate/2865794-renderer",
    "html": "Parameters\nrenderer\n\nThe ARSCNView object rendering the scene.\n\nnode\n\nThe newly added SceneKit node.\n\nanchor\n\nThe AR anchor corresponding to the node.\n\nDiscussion\n\nDepending on the session configuration, ARKit may automatically add anchors to a session. The view calls this method once for each new anchor. ARKit also calls this method to provide visual content for any ARAnchor objects you manually add using the session's add(anchor:) method.\n\nYou can provide visual content for the anchor by attaching geometry (or other SceneKit features) to this node or by adding child nodes.\n\nAlternatively, you can implement the renderer(_:nodeFor:) method to create your own node (or instance of an SCNNode subclass) for an anchor.\n\nSee Also\nHandling Content Updates\nfunc renderer(SCNSceneRenderer, nodeFor: ARAnchor) -> SCNNode?\nAsks the delegate to provide a SceneKit node corresponding to a newly added anchor.\nfunc renderer(SCNSceneRenderer, willUpdate: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node's properties will be updated to match the current state of its corresponding anchor.\nfunc renderer(SCNSceneRenderer, didUpdate: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node's properties have been updated to match the current state of its corresponding anchor.\nfunc renderer(SCNSceneRenderer, didRemove: SCNNode, for: ARAnchor)\nTells the delegate that the SceneKit node corresponding to a removed AR anchor has been removed from the scene."
  },
  {
    "title": "Providing 2D Virtual Content with SpriteKit | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskview/providing_2d_virtual_content_with_spritekit",
    "html": "Overview\n\nSpriteKit is inherently for 2D visual content, but augmented reality involves real-world 3D spaces. Use the ARSKView class to create AR experiences by providing 2D sprites (SKNode objects) that correspond to real-world 3D positions (ARAnchor objects). When the user moves the device, the view automatically rotates and scales the SpriteKit nodes corresponding to anchors so that they appear to track the real world seen by the camera.\n\nFor example, you can place 2D images that appear to float in 3D space:\n\n// Create a transform with a translation of 0.2 meters in front of the camera.\nvar translation = matrix_identity_float4x4\ntranslation.columns.3.z = -0.2\nlet transform = simd_mul(view.session.currentFrame.camera.transform, translation)\n\n\n// Add a new anchor to the session.\nlet anchor = ARAnchor(transform: transform)\nview.session.add(anchor: anchor)\n\n\n// (Elsewhere...) Provide a label node to represent the anchor.\nfunc view(_ view: ARSKView, nodeFor anchor: ARAnchor) -> SKNode? {\n    return SKLabelNode(text: \"👾\")\n}\n\n\nThe view(_:nodeFor:) method above returns an SKLabelNode object, which displays a text label. Like most SpriteKit nodes, this class creates a 2D visual representation, so the ARSKView class presents the node in a billboard style: The sprite scales and rotates (around its z-axis) so that it appears to follow the 3D position of its anchor, but always faces toward the camera.\n\nSee Also\nFirst Steps\nvar session: ARSession\nThe AR session that manages motion tracking and camera image processing for the view's contents."
  },
  {
    "title": "session | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskview/2865589-session",
    "html": "Discussion\n\nA view creates its own session object; use this property to access and configure the view's session.\n\nSee Also\nFirst Steps\nProviding 2D Virtual Content with SpriteKit\nUse SpriteKit to place two-dimensional images in 3D space in your AR experience."
  },
  {
    "title": "delegate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskview/2865597-delegate",
    "html": "See Also\nResponding to AR Updates\nprotocol ARSKViewDelegate\nMethods you can implement to mediate the automatic synchronization of SpriteKit content with an AR session."
  },
  {
    "title": "hitTest(_:types:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskview/2875733-hittest",
    "html": "Parameters\npoint\n\nA point in the 2D coordinate system of the view.\n\ntypes\n\nThe types of hit-test result to search for.\n\nReturn Value\n\nA list of results, sorted from nearest to farthest (in distance from the camera).\n\nDiscussion\n\nHit testing searches for real-world objects or surfaces detected through the AR session's processing of the camera image. A 2D point in the view's coordinate system can refer to any point along a 3D line that starts at the device camera and extends in a direction determined by the device orientation and camera projection. This method searches along that line, returning all objects that intersect it in order of distance from the camera.\n\nNote\n\nThis method searches for AR anchors and real-world objects detected by the AR session, not SpriteKit content displayed in the view. To search for SpriteKit objects, use the nodes(at:) method of the view's SpriteKit scene.\n\nThe behavior of a hit test depends on which types you specify and the order you specify them in. For details, see ARHitTestResult and the various ARHitTestResult.ResultType options."
  },
  {
    "title": "allowsContentScaling | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arquicklookpreviewitem/3152993-allowscontentscaling",
    "html": "Discussion\n\nBy default, the user can scale your virtual content in AR Quick Look. Use this property to disable scaling in situations where it's not appropriate."
  },
  {
    "title": "node(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskview/2873513-node",
    "html": "Parameters\nanchor\n\nAn anchor in the view's AR session.\n\nReturn Value\n\nThe node whose position in the AR scene the anchor tracks, or nil if the anchor has no associated node or is not in the view's AR session.\n\nSee Also\nMapping Content to Real-World Positions\nfunc anchor(for: SKNode) -> ARAnchor?\nReturns the AR anchor associated with the specified SpriteKit node, if any."
  },
  {
    "title": "anchor(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskview/2875729-anchor",
    "html": "Parameters\nnode\n\nA SpriteKit node in the view's scene.\n\nReturn Value\n\nThe ARAnchor object tracking the node, or nil if the node is not associated with an anchor or not in the view's scene.\n\nSee Also\nMapping Content to Real-World Positions\nfunc node(for: ARAnchor) -> SKNode?\nReturns the SpriteKit node associated with the specified AR anchor, if any."
  },
  {
    "title": "canonicalWebPageURL | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arquicklookpreviewitem/3182990-canonicalwebpageurl",
    "html": "Discussion\n\nThe default value is nil, which indicates that Quick Look shares the USDZ file when the user invokes the iOS share sheet. If you set this property to a web page URL, then Quick Look shares that web resource when the user invokes the share sheet."
  },
  {
    "title": "ARSkeleton3D | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton3d",
    "html": "Overview\n\nAn ARBodyAnchor contains one instance of this ARSkeleton subclass to provide its joint positions in 3D space. The jointLocalTransforms property describes a joint's 3D offset from its parent joint. The jointModelTransforms property describes a joint's 3D offset from the body anchor's transform.\n\nTopics\nGetting a Joint's Pose\nvar jointLocalTransforms: [simd_float4x4]\nThe local space transforms for each joint.\nvar jointModelTransforms: [simd_float4x4]\nThe model space transforms for each joint.\nfunc localTransform(for: ARSkeleton.JointName) -> simd_float4x4?\nReturns the local transform for a joint with a given name.\nfunc modelTransform(for: ARSkeleton.JointName) -> simd_float4x4?\nReturns the model transform for a joint with a given name.\nRelationships\nInherits From\nARSkeleton\nSee Also\nBody Data\nCapturing Body Motion in 3D\nTrack a person in the physical environment and visualize their motion by applying the same body movements to a virtual character.\nclass ARBody2D\nThe screen-space representation of a person ARKit recognizes in the camera feed.\nclass ARSkeleton2D\nAn object that describes the locations of a body’s joints in the camera feed.\nclass ARSkeleton\nThe interface for the skeleton of a tracked body.\nclass ARSkeletonDefinition\nThe hierarchy of joints and their names."
  },
  {
    "title": "ARSkeleton2D | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeleton2d",
    "html": "Overview\n\nARSkeleton2D provides you with a 2D body's joints in a flat hierarchy so you can access them efficiently. The joint locations are normalized within the range [0..1] in the coordinate space of the current frame's camera image, where 0 is the upper left, and 1 is the bottom right.\n\nTo access a skeleton's joints by name, you use landmark(forJointNamed:). To access a named joint by index (for example, for performance reasons), you query the definition for the named joint index using index(for:), then access jointLandmarks using the resulting index.\n\nTopics\nGetting Joint Landmarks\nvar jointLandmarks: [simd_float2]\nThe joint landmarks in normalized coordinates.\nfunc landmark(for: ARSkeleton.JointName) -> simd_float2?\nReturns the location of a joint with a given name.\nRelationships\nInherits From\nARSkeleton\nSee Also\nBody Data\nCapturing Body Motion in 3D\nTrack a person in the physical environment and visualize their motion by applying the same body movements to a virtual character.\nclass ARBody2D\nThe screen-space representation of a person ARKit recognizes in the camera feed.\nclass ARSkeleton3D\nThe skeleton of a human body that ARKit tracks in 3D space.\nclass ARSkeleton\nThe interface for the skeleton of a tracked body.\nclass ARSkeletonDefinition\nThe hierarchy of joints and their names."
  },
  {
    "title": "Adding Realistic Reflections to an AR Experience | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/camera_lighting_and_effects/adding_realistic_reflections_to_an_ar_experience",
    "html": "Overview\n\nThis app provides a simple AR experience demonstrating the environment texturing features in ARKit 2 and SceneKit. After you build and run the app, explore your surroundings in the camera view. Then, tap a nearby horizontal surface to place a virtual object: a mirror-finish sphere. After you place the object, you can drag it around or tap to move it to another location. You can also pinch to make the object bigger or smaller.\n\nNotice the surface of the virtual sphere shows a generally realistic (if not perfectly accurate) reflection of its real-world surroundings. To create reflective virtual surfaces, a renderer (such as SceneKit) needs an environment texture—an image that captures the view in all directions from a certain point in the scene (called an environment probe). Realistically rendering reflections for multiple objects, or moving objects, may require multiple environment textures, each capturing the scene from a different point of view.\n\nARKit generates environment textures by collecting camera imagery during the AR session. Because ARKit cannot see the scene in all directions, it uses machine learning to extrapolate a realistic environment from available imagery.\n\nGetting Started\n\nBefore you can run the sample code project, you’ll need:\n\nXcode 10 or later.\n\niOS 12 or later.\n\nAn iOS device with an A9 processor or later.\n\nSet Up Environment Texturing\n\nAs with any AR experience, you run a session with a world tracking configuration and whatever other options you want to enable. (For example, this app allows you to place virtual objects on flat surfaces, so it enables horizontal plane detection.) To generate environment textures, also set the configuration’s environmentTexturing property:\n\nlet configuration = ARWorldTrackingConfiguration()\nconfiguration.planeDetection = .horizontal\nconfiguration.environmentTexturing = .automatic\nsceneView.session.run(configuration)\n\n\nWith ARWorldTrackingConfiguration.EnvironmentTexturing.automatic environment texturing (the default for this app) ARKit automatically chooses when and where to generate textures.\n\nRender Virtual Objects with Reflection\n\nBecause this app also uses ARSCNView to display AR content, SceneKit automatically uses the appropriate environment texture to render each virtual object in the scene. In SceneKit, any asset using physicallyBased materials automatically uses environmental lighting. With environmental lighting, the shading for each point on a surface depends on nearby light probe textures or the global lighting environment in the direction that point faces.\n\nThe visual effect of environment texturing depends on how you configure the properties of a physically based material. (Typically, ) For example, materials with a high roughness pick up some diffuse color from the texture, and materials with low roughness and high metalness reflect their surroundings with a mirror-like finish.\n\nNote\n\nIf your AR experience uses a rendering technology other than SceneKit, you’ll need to retrieve the generated textures yourself and determine how to use them appropriately in your shading engine. First, implement the session(_:didUpdate:) delegate method to be notified when ARKit generates environment probe textures. In that method, use the environmentTexture property of each AREnvironmentProbeAnchor object to get the texture.\n\nPlace Environment Probes Manually for Enhanced Results\n\nAutomatic environment texturing is all you need for basic environmental lighting or reflection effects. To render reflections more realistically, however, each reflective object needs an environment probe texture that accurately captures the area close to that object. For example, in the images above, the virtual sphere reflects the real cup when the cup is close to the sphere’s real-world position.\n\nTo more precisely define environment probes, choose ARWorldTrackingConfiguration.EnvironmentTexturing.manual environment texturing when you configure your AR session, then create your own AREnvironmentProbeAnchor instance for each virtual object you want to use environmental lighting with. Initialize each probe’s extent and position (using transform) based on the size of the corresponding virtual object:\n\n// Make sure the probe encompasses the object and provides some surrounding area to appear in reflections.\nvar extent = object.extents * object.simdScale\nextent.x *= 3 // Reflect an area 3x the width of the object.\nextent.z *= 3 // Reflect an area 3x the depth of the object.\n\n\n// Also include some vertical area around the object, but keep the bottom of the probe at the\n// bottom of the object so that it captures the real-world surface underneath.\nlet verticalOffset = float3(0, extent.y, 0)\nlet transform = float4x4(translation: object.simdPosition + verticalOffset)\nextent.y *= 2\n\n\n// Create the new environment probe anchor and add it to the session.\nlet probeAnchor = AREnvironmentProbeAnchor(transform: transform, extent: extent)\nsceneView.session.add(anchor: probeAnchor)\n\n\nThis code applies the rules below to optimally capture the area around each virtual object:\n\nThe probe’s position should be at the top center of the virtual object, and the y component of its extent should be twice the height of the object. This ensures that the bottom of the probe extent aligns with the bottom of the virtual object, accurately capturing the real surface the object sits on.\n\nThe x and z components of the probe’s extent should be three times the width and depth of the object, ensuring that the probe captures the area beneath and around the object.\n\nUse Environment Texturing Wisely\n\nFollow these tips to keep your app’s use of environment texturing realistic and efficient:\n\nAvoid virtual content that requires accurate reflections, such as mirror-finish surfaces.\n\nIn general, an AR experience doesn’t have all the information needed to produce a perfect imitation of reality. Good AR experiences carefully design content to hide limitations in realism, preserving the illusion that virtual objects inhabit the user’s real-world surroundings.\n\nARKit environment textures don’t image the environment in all directions around the user, and don’t update in real time, so some kinds of content aren’t well suited for use in AR. For example, a user encountering a virtual mirror may expect to see their own reflection. Design virtual content to use fully-reflective surfaces only in small or highly-detailed parts, and use less reflectivity in large flat surfaces.\n\nHandle moving objects.\n\nRendering a virtual object with realistic reflections require an environment probe that captures a small area around that object. If the object changes position, the corresponding environment probe needs to change to reflect the object’s new surroundings. When manually placing probes, consider one or more of these strategies for handling objects that move:\n\nIf the path of an object’s movement is known ahead of time, create multiple environment probe anchors and place them along that path.\n\nCreate a global environment probe with an extra-large extent to fall back to when rendering objects that have moved outside the extent of nearby probes.\n\nAfter an object moves, create a new probe to capture the area around its new position, and remove environment probes associated with earlier positions.\n\nWhen you display AR content with ARSCNView, SceneKit automatically interpolates between environment textures for any objects that overlap the extents of multiple environment probes.\n\nDon’t generate environment textures too often.\n\nARKit requires some time to collect camera imagery, and combining and extrapolating that imagery to produce environment textures requires computational resources. Frequently adding new AREnvironmentProbeAnchor instances to your AR session may not produce noticeable changes in the displayed scene, but does cost battery power and reduce the performance overhead available for other aspects of your AR experience.\n\nThis app creates new environment probes whenever the user moves or resizes the virtual object, but limits such updates to occur no more often than once per second. (See the sample updateEnvironmentProbe(atTime:) function.)\n\nAvoid abrupt transitions between different environment textures.\n\nWith ARSCNView, if the environment probe texture(s) affecting an object change (either because the object moves or because a new texture becomes available for its current position), SceneKit automatically uses a short fade-in animation to transition to the new result. Depending on what environment textures are in use before and after the transition, that change may be jarring to the user.\n\nTo avoid unrealistic transitions, this sample app waits until the first environment texture becomes available before allowing the user to place virtual content. In automatic mode, as soon as the session begins, ARKit automatically begins generating a fallback environment texture covering a large area. In manual mode, the app creates its own fallback environment probe. (See the sample updateSceneEnvironmentProbe(for:) function.) Waiting until this environment texture is available ensures that virtual objects always reflect an environment appropriate to the session.\n\nAlternatively, your app may include a static environment-map texture for use as a fallback when environment texturing is not available (for example, to support earlier iOS versions). In this case, try to design or select a texture that appears realistic in a wide variety of situations.\n\nSee Also\nLighting Effects\nclass AREnvironmentProbeAnchor\nAn object that provides environmental lighting information for a specific area of space in a world-tracking AR session.\nclass ARLightEstimate\nEstimated scene lighting information associated with a captured video frame in an AR session.\nclass ARDirectionalLightEstimate\nEstimated environmental lighting information associated with a captured video frame in a face-tracking AR session."
  },
  {
    "title": "AREnvironmentProbeAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arenvironmentprobeanchor",
    "html": "Overview\n\nEnvironment textures depict the view in all directions from a specific point in a scene. In 3D asset rendering, environment textures are the basis for image-based lighting algorithms where surfaces can realistically reflect light from their surroundings. ARKit can generate environment textures during an AR session using camera imagery, allowing SceneKit or a custom-rendering engine to provide realistic image-based lighting for virtual objects in your AR experience.\n\nTo enable texture map generation for an AR session, set the environmentTexturing property:\n\nWith ARWorldTrackingConfiguration.EnvironmentTexturing.manual environment texturing, you identify points in the scene for which you want light probe texture maps by creating AREnvironmentProbeAnchor objects and adding them to the session.\n\nWith ARWorldTrackingConfiguration.EnvironmentTexturing.automatic environment texturing, ARKit automatically creates, positions, and adds AREnvironmentProbeAnchor objects to the session.\n\nIn both cases, ARKit automatically generates environment textures as the session collects camera imagery. Use a delegate method such as session(_:didUpdate:) to find out when a texture is available, and access it from the anchor's environmentTexture property.\n\nIf you display AR content using ARSCNView and the automaticallyUpdatesLighting option, SceneKit automatically retrieves AREnvironmentProbeAnchor texture maps and uses them to light the scene.\n\nTopics\nCreating Probe Anchors\ninit(transform: simd_float4x4, extent: simd_float3)\nCreates a new environment probe anchor.\ninit(name: String, transform: simd_float4x4, extent: simd_float3)\nCreates a new anchor object with a descriptive name.\nAccessing Texture Maps\nvar environmentTexture: MTLTexture?\nA cube-map texture that represents the view in all directions from the probe anchor's position.\nExamining a Probe Anchor\nvar extent: simd_float3\nThe area around the anchor's position that contains the texture.\nRelationships\nInherits From\nARAnchor\nSee Also\nLighting Effects\nAdding Realistic Reflections to an AR Experience\nUse ARKit to generate environment probe textures from camera imagery and render reflective virtual objects.\nclass ARLightEstimate\nEstimated scene lighting information associated with a captured video frame in an AR session.\nclass ARDirectionalLightEstimate\nEstimated environmental lighting information associated with a captured video frame in a face-tracking AR session."
  },
  {
    "title": "environmentTexturing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/2977509-environmenttexturing",
    "html": "Discussion\n\nEnvironment textures are cube-map textures that depict the view in all directions from a specific point in a scene. In 3D asset rendering, environment textures are the basis for image-based lighting algorithms where surfaces can realistically reflect light from their surroundings. ARKit generates environment textures during an AR session using camera imagery, allowing SceneKit or a custom-rendering engine to provide realistic image-based lighting for virtual objects in your AR experience.\n\nTo enable texture map generation for your configuration, change this property (from its default value of ARWorldTrackingConfiguration.EnvironmentTexturing.none):\n\nWith ARWorldTrackingConfiguration.EnvironmentTexturing.manual environment texturing, you identify points in the scene for which you want light probe texture maps by creating AREnvironmentProbeAnchor objects and adding them to the session.\n\nWith ARWorldTrackingConfiguration.EnvironmentTexturing.automatic environment texturing, ARKit automatically creates, positions, and adds AREnvironmentProbeAnchor objects to the session.\n\nIn both cases, ARKit automatically generates environment textures as the session collects camera imagery. Use a delegate method such as session(_:didUpdate:) to find out when a texture is available, and access it from the anchor's environmentTexture property.\n\nIf you display AR content using ARSCNView and the automaticallyUpdatesLighting option, SceneKit automatically retrieves AREnvironmentProbeAnchor texture maps and uses them to light the scene.\n\nSee Also\nCreating Realistic Reflections\nenum ARWorldTrackingConfiguration.EnvironmentTexturing\nOptions to generate environment textures in a world-tracking AR session.\nclass AREnvironmentProbeAnchor\nAn object that provides environmental lighting information for a specific area of space in a world-tracking AR session.\nvar wantsHDREnvironmentTextures: Bool\nA flag that instructs the framework to create environment textures in HDR format."
  },
  {
    "title": "ARWorldTrackingConfiguration.EnvironmentTexturing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/environmenttexturing",
    "html": "Topics\nEnvironment Texture Options\ncase none\nThe framework doesn’t generate environment textures.\ncase manual\nThe framework generates environment textures only for probe anchors you explicitly add to the session.\ncase automatic\nThe framework automatically determines when and where to generate environment textures.\nRelationships\nConforms To\nSendable\nSee Also\nCreating Realistic Reflections\nvar environmentTexturing: ARWorldTrackingConfiguration.EnvironmentTexturing\nAn option that determines how the framework generates environment textures.\nclass AREnvironmentProbeAnchor\nAn object that provides environmental lighting information for a specific area of space in a world-tracking AR session.\nvar wantsHDREnvironmentTextures: Bool\nA flag that instructs the framework to create environment textures in HDR format."
  },
  {
    "title": "userFaceTrackingEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/3223422-userfacetrackingenabled",
    "html": "Discussion\n\nWhen enabled, ARKit provides you with an ARFaceAnchor that represents the user's face during your world tracking session. For example, you can share avatar expressions with multiple users in a multiplayer game, or enable the player to control a virtual object in the physical environment using facial expressions.\n\nCheck whether the device supports tracking of the user's face by using supportsUserFaceTracking before enabling this property.\n\nSee Also\nTracking the User's Face\nclass var supportsUserFaceTracking: Bool\nA Boolean value that tells you whether the iOS device supports tracking the user's face during a world-tracking session."
  },
  {
    "title": "supportsUserFaceTracking | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/3223421-supportsuserfacetracking",
    "html": "Discussion\n\nCheck the value of this property first, before you enable face tracking using userFaceTrackingEnabled.\n\nSee Also\nTracking the User's Face\nvar userFaceTrackingEnabled: Bool\nA flag that determines whether ARKit tracks the user's face in a world-tracking session."
  },
  {
    "title": "detectionImages | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/2941063-detectionimages",
    "html": "Discussion\n\nAdd members to this set for each image that ARKit searches for in the user’s environment. When ARKit detects a matching image, the framework creates an ARImageAnchor object and adds it to the session.\n\nTo define the reference images that this property contains, create an asset catalog in Xcode or create ARReferenceImage objects programmatically. For more information, see Detecting Images in an AR Experience.\n\nIf you set a nonzero value for maximumNumberOfTrackedImages, ARKit enables image tracking, which continuously updates the transform for up to four of the reference image anchors as the session progresses. For an example, see Tracking and altering images.\n\nLimit Reference Images for Performance\n\nImage detection accuracy and performance may decline as the number of images in this set increases. For best results, limit your detection image count to no more than around 100.\n\nTo detect more than 100 images, your app can allocate a certain amount of time for the first 100 images before moving on to the next 100, and so on. When you update the contents of this property, call run(_:options:) again with your app's configuration to effect the change.\n\nSee Also\nDetecting or Tracking Images\nvar maximumNumberOfTrackedImages: Int\nThe number of image anchors to monitor closely for position and orientation updates.\nvar automaticImageScaleEstimationEnabled: Bool\nA flag that instructs the framework to estimate and set the scale of a detected or tracked image on your behalf.\nRelated Documentation\nclass ARImageTrackingConfiguration\nA configuration that tracks known images using the rear-facing camera."
  },
  {
    "title": "maximumNumberOfTrackedImages | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/2968182-maximumnumberoftrackedimages",
    "html": "Discussion\n\nWhen you set a nonzero value for this property, the framework keeps that many image anchors up to date as the session progresses. The framework can track a maximum of four images simultaneously.\n\nThe word track in the property name refers to how the framework closely monitors the image’s physical position and orientation for any changes. If the image moves, the framework updates the associated ARImageAnchor transform with the new pose. ARKit checks for changes every frame.\n\nARKit tracks the first images it observes in the physical environment from the detectionImages set. When a session reaches the maximum number of tracked images, the framework attempts to track another member of the set only after one of the existing tracked images leaves the device’s view.\n\nThe default value is 0, which disables image tracking. If you add reference images to detectionImages with this property set to 0, ARKit creates image anchors for observed reference images but their positions only update infrequently, such as once every couple of seconds.\n\nSee Also\nDetecting or Tracking Images\nvar detectionImages: Set<ARReferenceImage>!\nA set of images that ARKit searches for in the user's environment.\nvar automaticImageScaleEstimationEnabled: Bool\nA flag that instructs the framework to estimate and set the scale of a detected or tracked image on your behalf."
  },
  {
    "title": "automaticImageScaleEstimationEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/3075551-automaticimagescaleestimationena",
    "html": "Discussion\n\nIf set to true, ARKit uses its knowledge of the world to set an image anchor's estimatedScaleFactor property, which corrects the image anchor's position in the physical environment.\n\nEnable this property when you want to detect different-sized versions of a reference image. ARKit must know the physical size of an image in the real world to accurately estimate its real-world position. Enable this property to tell ARKit to estimate a recognized image's physical size before it calculates the real-world position.\n\nSee Also\nDetecting or Tracking Images\nvar detectionImages: Set<ARReferenceImage>!\nA set of images that ARKit searches for in the user's environment.\nvar maximumNumberOfTrackedImages: Int\nThe number of image anchors to monitor closely for position and orientation updates."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/2923549-init",
    "html": "Discussion\n\nTo use the configuration in an AR experience, pass it as an argument to your app’s run(_:options:) function.\n\nSee Also\nCreating a Configuration\nvar initialWorldMap: ARWorldMap?\nThe state from a previous AR session to attempt to resume with this session configuration."
  },
  {
    "title": "sceneReconstruction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/3521376-scenereconstruction",
    "html": "Discussion\n\nWhen you enable scene reconstruction, ARKit provides a polygonal mesh that estimates the shape of the physical environment. Before setting this property, call supportsSceneReconstruction(_:) to ensure device support. For a sample app that demonstrates scene reconstruction, see Visualizing and Interacting with a Reconstructed Scene.\n\nIf you enable plane detection, ARKit applies that information to the mesh. Where the LiDAR scanner may produce a slightly uneven mesh on a real-world surface, ARKit smooths out the mesh where it detects a plane on that surface.\n\nIf you enable people occlusion, ARKit adjusts the mesh according to any people it detects in the camera feed. ARKit removes any part of the scene mesh that overlaps with people, as defined by the personSegmentation or personSegmentationWithDepth frame semantics.\n\nSee Also\nTracking Surfaces\nvar planeDetection: ARWorldTrackingConfiguration.PlaneDetection\nA value that specifies whether and how the session automatically attempts to detect flat surfaces in the camera-captured image.\nstruct ARWorldTrackingConfiguration.PlaneDetection\nOptions for whether and how the framework detects flat surfaces in captured images.\nclass func supportsSceneReconstruction(ARConfiguration.SceneReconstruction) -> Bool\nChecks if the device supports scene reconstruction."
  },
  {
    "title": "supportsSceneReconstruction(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/3521377-supportsscenereconstruction",
    "html": "Discussion\n\nScene reconstruction requires a device with a LiDAR Scanner, such as the fourth-generation iPad Pro.\n\nSee Also\nTracking Surfaces\nvar planeDetection: ARWorldTrackingConfiguration.PlaneDetection\nA value that specifies whether and how the session automatically attempts to detect flat surfaces in the camera-captured image.\nstruct ARWorldTrackingConfiguration.PlaneDetection\nOptions for whether and how the framework detects flat surfaces in captured images.\nvar sceneReconstruction: ARConfiguration.SceneReconstruction\nA flag that enables scene reconstruction."
  },
  {
    "title": "planeDetection | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/2923548-planedetection",
    "html": "Discussion\n\nBy default, this configuration disables plane detection. If you enable horizontal or vertical plane detection, the session adds ARPlaneAnchor objects and notifies your ARSessionDelegate, ARSCNViewDelegate, or ARSKViewDelegate object when its analysis of captured video images detects an area that appears to be a flat surface.\n\nSee Also\nTracking Surfaces\nstruct ARWorldTrackingConfiguration.PlaneDetection\nOptions for whether and how the framework detects flat surfaces in captured images.\nvar sceneReconstruction: ARConfiguration.SceneReconstruction\nA flag that enables scene reconstruction.\nclass func supportsSceneReconstruction(ARConfiguration.SceneReconstruction) -> Bool\nChecks if the device supports scene reconstruction."
  },
  {
    "title": "coachingOverlayViewDidRequestSessionReset(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayviewdelegate/3192182-coachingoverlayviewdidrequestses",
    "html": "Discussion\n\nImplement this function to do the actions your app requires to restart the AR experience. For example, you might hide custom relocalization UI, deallocate resources, or restore virtual content to a starting location.\n\nIf you don't implement this function, the coaching overlay resets the session for you––equivalent to calling run(_:options:) with the resetTracking option––when the user taps Start Over."
  },
  {
    "title": "delegate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayview/3152978-delegate",
    "html": "Discussion\n\nNormally, you set the value of this property to your app's view controller.\n\nSee Also\nDelegating Events\nprotocol ARCoachingOverlayViewDelegate\nA set of callbacks you implement to be notified of coaching events."
  },
  {
    "title": "ARError.Code.invalidReferenceImage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/invalidreferenceimage",
    "html": "Discussion\n\nThis error occurs when you supply a reference image to the configuration's detectionImages but ARKit determined it's unusable. This can happen when the image data doesn't contain enough features to identify a unique picture, for example, it's all white.\n\nSee Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARCoachingOverlayViewDelegate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayviewdelegate",
    "html": "Overview\n\nImplement a delegate to coordinate your app's actions with coaching overlay. For example, when the coaching overlay determines the user needs guidance, you hide your app's UI to allow the user to focus on the coaching experience. When the coaching overlay determines the goal is met, you show your app's UI and begin your app's AR experience.\n\nTopics\nEnabling Coaching\nfunc coachingOverlayViewWillActivate(ARCoachingOverlayView)\nTells you when the coaching overlay view activates.\nfunc coachingOverlayViewDidDeactivate(ARCoachingOverlayView)\nTells you when the coaching experience is completely deactivated.\nRestarting the Session\nfunc coachingOverlayViewDidRequestSessionReset(ARCoachingOverlayView)\nTells you when the user taps the coaching overlay view's Start Over button while the session is relocalizing.\nRelationships\nInherits From\nNSObjectProtocol\nSee Also\nDelegating Events\nvar delegate: ARCoachingOverlayViewDelegate?\nAn object you supply that implements coaching event callbacks."
  },
  {
    "title": "ARError.Code.invalidCollaborationData | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/invalidcollaborationdata",
    "html": "Discussion\n\nARKit produces this error when an app passes invalid data into the update(with:) function.\n\nSee Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARError.Code.invalidReferenceObject | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/invalidreferenceobject",
    "html": "See Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARError.Code.invalidConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/invalidconfiguration",
    "html": "See Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARError.Code.geoTrackingNotAvailableAtLocation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/geotrackingnotavailableatlocation",
    "html": "Discussion\n\nThis error code indicates that ARKit does not have the necessary localization imagery to support geo tracking at the user’s current location. See checkAvailability(completionHandler:) for more information.\n\nIf checkAvailability(completionHandler:) returns true and an app begins geo-tracking session, ARKit provides this state reason when the user has moved to an unsupported area.\n\nSee Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "sceneLibrary | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/scenelibrary",
    "html": "Overview\n\nSet a prim’s kind metadata to sceneLibrary to create a scene. Partitioning an asset’s contents into multiple scenes offers the following benefits:\n\nPrims segment content into distinct parts that have differing utilities and lifetimes.\n\nUnlike USD Variant Sets, you can mark each scene active or inactive independently.\n\nActions can operate on scenes.\n\nThe runtime doesn’t support multiple active scenes in a single file. When a USDZ file loads, the runtime displays the first active scene. The runtime ignores nested scenes. If an asset doesn’t define a scene, the runtime loads the full file contents as a single scene.\n\nDefine a scene using the def keyword to specify the asset’s active scene –– that is, the one that the runtime makes visible on load. Use the over keyword to specify inactive scenes –– those that are invisible on load. The runtime doesn’t display inactive scenes. The advantage of inactive scenes is that you can activate one in an editor during development, and an app can transition to an inactive scene during the AR experience.\n\nThis specification is backward compatible because of the way traditional USD runtimes skip over definitions. A runtime that doesn’t support the sceneLibrary metadata displays only the first active scene.\n\nDeclaration\nkind = \"sceneLibrary\"\n\n\nPartition content into scenes\n\nThe following example demonstrates a scene template on which you can base your asset.\n\ndef Xform \"Root\" (\n    # Because you can apply `kind` to any type of prim, that allows any prim to be a scene.\n    kind = \"sceneLibrary\"\n)\n{\n    # `PrimaryScene` is immediately active due to the `def` keyword.\n    def Xform \"PrimaryScene\" (\n        # `sceneName` can be any human-readable string.\n        sceneName = \"Primary Scene\"\n    )\n    {\n    }\n\n\n    # `SecondaryScene` is inactive due to the over keyword.\n    over Xform \"SecondaryScene\" (\n        sceneName = \"Secondary Scene\"\n    )\n    {\n    }\n}\n\n\n\n\nUse the sceneName property to refer to a scene to load it or transition to it from another scene.\n\nSee Also\nScenes and lighting\nSpecifying a lighting environment in AR Quick Look\nAdd metadata to your USDZ file to specify its lighting characteristics.\npreferredIblVersion\nMetadata that determines the lighting environment of virtual content."
  },
  {
    "title": "Schema definitions for third-party DCCs | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar/schema_definitions_for_third-party_dccs",
    "html": "Overview\n\nThese schema definition files contain a codified version of the specification addendum defined by USDZ schemas for AR. As a developer of third-party DCC software, you enable your users to configure interactive and AR features in their 3D assets by implementing the specification and providing additional UI.\n\nIntegrate interactive and AR schemas\n\nTo recognize and validate syntax, and to participate in USD features such as transform hierarchies, incorporate the new interactive and AR schemas into your DCC by copying the schema.usda files into your USD library and rebuilding. For more information, see Generating New Schema Classes."
  },
  {
    "title": "ARError.Code.highResolutionFrameCaptureInProgress | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/highresolutionframecaptureinprogress",
    "html": "Discussion\n\nThe system provides this error to the completion handler of captureHighResolutionFrame(completion:) for failed operations.\n\nSee Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline."
  },
  {
    "title": "ARError.Code.highResolutionFrameCaptureFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/highresolutionframecapturefailed",
    "html": "Discussion\n\nThe system provides this error to the completion handler of captureHighResolutionFrame(completion:) for failed operations.\n\nSee Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "ARError.Code.geoTrackingFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code/geotrackingfailed",
    "html": "Discussion\n\nARKit will raise an error with this error code when visual localization is taking too long. This situation indicates that the app has met all requirements for geo tracking except for visual localization. To try again, the app needs to ask the user pan the device around the physical environment to acquire different camera-feed imagery. For more information, see Assisting the User with Visual Localization.\n\nSee Also\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "detectionImages | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration/3229911-detectionimages",
    "html": "Discussion\n\nAdd members to this set for each image that ARKit searches for in the user’s environment. When ARKit detects a matching image, the framework creates an ARImageAnchor object and adds it to the session.\n\nTo define the reference images that this property contains, create an asset catalog in Xcode or create ARReferenceImage objects programmatically. For more information, see Detecting Images in an AR Experience.\n\nIf you set a nonzero value for maximumNumberOfTrackedImages, ARKit enables image tracking, which continuously updates the transform for up to four of the reference image anchors as the session progresses. For an example, see Tracking and altering images.\n\nLimit Reference Images for Performance\n\nImage detection accuracy and performance may decline as the number of images in this set increases. For best results, limit your detection image count to no more than around 100.\n\nTo detect more than 100 images, your app can allocate a certain amount of time for the first 100 images before moving on to the next 100, and so on. When you update the contents of this property, call run(_:options:) again with your app's configuration to effect the change.\n\nSee Also\nEnabling Image Tracking\nvar automaticImageScaleEstimationEnabled: Bool\nA flag that instructs ARKit to estimate and set the scale of a tracked image on your behalf.\nvar maximumNumberOfTrackedImages: Int\nThe number of image anchors to monitor closely for position and orientation updates."
  },
  {
    "title": "supportsAppClipCodeTracking | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration/3697082-supportsappclipcodetracking",
    "html": "Discussion\n\nDevices require the Apple Neural Engine (ANE) to track App Clip Codes. The system sets this property to true if the device contains the ANE chip. The default value of this property is false.\n\nCall this function before setting appClipCodeTrackingEnabled.\n\nSee Also\nAccessing App Clip Codes\nInteracting with App Clip Codes in AR\nDisplay content and provide services in an AR experience with App Clip Codes.\nvar appClipCodeTrackingEnabled: Bool\nA Boolean value that indicates if the framework searches the physical environment for App Clip Codes.\nclass ARAppClipCodeAnchor\nAn anchor that tracks the position and orientation of an App Clip Code in the physical environment."
  },
  {
    "title": "ARWorldTrackingConfiguration.PlaneDetection | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/planedetection",
    "html": "Overview\n\nUse an empty set literal [] to specify no plane detection.\n\nTopics\nPlane Detection Option Creation\ninit(rawValue: UInt)\nCreates plane detection options.\nPlane Detection Options\nstatic var horizontal: ARWorldTrackingConfiguration.PlaneDetection\nThe session detects planar surfaces that are perpendicular to gravity.\nstatic var vertical: ARWorldTrackingConfiguration.PlaneDetection\nThe session detects surfaces that are parallel to gravity, regardless of other orientation.\nRelationships\nConforms To\nOptionSet\nSendable\nSee Also\nTracking Surfaces\nvar planeDetection: ARWorldTrackingConfiguration.PlaneDetection\nA value that specifies whether and how the session automatically attempts to detect flat surfaces in the camera-captured image.\nvar sceneReconstruction: ARConfiguration.SceneReconstruction\nA flag that enables scene reconstruction.\nclass func supportsSceneReconstruction(ARConfiguration.SceneReconstruction) -> Bool\nChecks if the device supports scene reconstruction."
  },
  {
    "title": "exifData | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/3930051-exifdata",
    "html": "Discussion\n\nThe system's image capture pipeline produces pixel data and auxiliary information for each exposure. The AR frame exposes this pixel data through capturedImage and the auxilliary info through this property (exifData). Example EXIF data includes camera manufacturer, orientation, compression, resolution, exposure, and the date and time that the exposure occurred.\n\nSee Also\nAccessing camera data\nvar camera: ARCamera\nInformation about the camera position, orientation, and imaging parameters used to capture the frame.\nvar capturedImage: CVPixelBuffer\nA pixel buffer containing the image captured by the camera.\nvar timestamp: TimeInterval\nThe time at which the frame was captured.\nvar cameraGrainIntensity: Float\nA value that specifies the amount of grain present in the camera grain texture.\nvar cameraGrainTexture: MTLTexture?\nA tileable Metal texture created by ARKit to match the visual characteristics of the current video stream."
  },
  {
    "title": "ARSCNFaceGeometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnfacegeometry",
    "html": "Overview\n\nThis class is a subclass of SCNGeometry that wraps the mesh data provided by the ARFaceGeometry class. You can use ARSCNFaceGeometry to quickly and easily visualize face topology and facial expressions provided by ARKit in a SceneKit view.\n\nImportant\n\nARSCNFaceGeometry is available only in SceneKit views or renderers that use Metal. This class is not supported for OpenGL-based SceneKit rendering.\n\nFace mesh topology is constant for the lifetime of an ARSCNFaceGeometry object. That is, the geometry's single SCNGeometryElement object always describes the same arrangement of vertices, and the texcoord geometry source always maps the same vertices to the same texture coordinates.\n\nWhen you modify the geometry with the update(from:) method, only the contents of the vertex geometry source change, indicating the difference in vertex positions as ARKit adapts the mesh to the shape and expression of the user's face.\n\nTopics\nCreating a Geometry\ninit?(device: MTLDevice)\nCreates a SceneKit face geometry for rendering with the specified Metal device object.\ninit?(device: MTLDevice, fillMesh: Bool)\nCreates a SceneKit face geometry, optionally filling in gaps in the mesh for the eyes and mouth.\nUpdating the Geometry\nfunc update(from: ARFaceGeometry)\nDeforms the SceneKit geometry to match the specified face mesh.\nRelationships\nInherits From\nSCNGeometry\nSee Also\nFace Data\nTracking and Visualizing Faces\nDetect faces in a front-camera AR experience, overlay virtual content, and animate facial expressions in real-time.\nCombining User Face-Tracking and World Tracking\nTrack the user’s face in an app that displays an AR experience with the rear camera.\nclass ARFaceGeometry\nA 3D mesh describing face topology for use in face-tracking AR sessions."
  },
  {
    "title": "ARFrame.SegmentationClass | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/segmentationclass",
    "html": "Overview\n\nARKit applies the categories defined in this class based on its interpretation of the camera feed's pixel data. Only people are identified in a camera feed, and therefore the available pixel classifications are either ARFrame.SegmentationClass.person or ARFrame.SegmentationClass.none.\n\nTopics\nClassifying Pixels\ncase person\nA classification of a pixel in the segmentation buffer as part of a person.\ncase none\nA classification of a pixel in the segmentation buffer as unidentified.\nRelationships\nConforms To\nSendable\nSee Also\nChecking for people\nvar detectedBody: ARBody2D?\nThe screen position information of a body that ARKit recognizes in the camera image.\nclass ARBody2D\nThe screen-space representation of a person ARKit recognizes in the camera feed.\nvar segmentationBuffer: CVPixelBuffer?\nA buffer that contains pixel information identifying the shape of objects from the camera feed that you use to occlude virtual content.\nvar estimatedDepthData: CVPixelBuffer?\nA buffer that represents the estimated depth values from the camera feed that you use to occlude virtual content."
  },
  {
    "title": "session(_:didOutputCollaborationData:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionobserver/3152999-session",
    "html": "Parameters\nsession\n\nThe app's collaborative session.\n\ndata\n\nThe information to share with participants.\n\nDiscussion\n\nThe data parameter contains information about your perspective in the physical environment. When ARKit invokes this function, send the data object to nearby users. The users update their session with your data to gain your app's model of the physical environment in addition to their own. For more information, see isCollaborationEnabled.\n\nFor an example app that implements this callback, see Creating a Collaborative Session."
  },
  {
    "title": "sessionInterruptionEnded(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionobserver/2891619-sessioninterruptionended",
    "html": "Parameters\nsession\n\nThe session providing information.\n\nDiscussion\n\nAfter a session has been interrupted (see the sessionWasInterrupted(_:) delegate method), it automatically resumes running whenever the conditions that caused the interruption improve.\n\nWhen a session resumes, it continues tracking from its last known state. However, if the device has moved since the interruption began, the ARKit world coordinate system and anchor positions no longer match their original real-world frame of reference. To attempt recovery of world tracking from before the interruption, implement the sessionShouldAttemptRelocalization(_:) delegate method.\n\nSee Also\nHandling Interruptions\nfunc sessionWasInterrupted(ARSession)\nTells the delegate that the session has temporarily stopped processing frames and tracking device position.\nfunc sessionShouldAttemptRelocalization(ARSession) -> Bool\nAsks the delegate whether to attempt recovery of world-tracking state after an interruption."
  },
  {
    "title": "session(_:didUpdate:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessiondelegate/2865624-session",
    "html": "Parameters\nsession\n\nThe session providing information.\n\nanchors\n\nThe anchors whose properties have changed.\n\nDiscussion\n\nDepending on the session configuration, ARKit may automatically update the properties of anchors in a session.\n\nIf you display an AR experience using SceneKit or SpriteKit, you can implement one of the following methods instead to track not only the anchors in the session but also any corresponding SceneKit or SpriteKit content:\n\nARSCNView: renderer(_:willUpdate:for:) or renderer(_:didUpdate:for:)\n\nARSKView: view(_:willUpdate:for:) or view(_:didUpdate:for:)\n\nSee Also\nHandling Content Updates\nfunc session(ARSession, didAdd: [ARAnchor])\nTells the delegate that one or more anchors have been added to the session.\nfunc session(ARSession, didRemove: [ARAnchor])\nTells the delegate that one or more anchors have been removed from the session."
  },
  {
    "title": "goal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayview/3192180-goal",
    "html": "Discussion\n\nThe goal you choose determines the particular instructions the coaching overlay presents to the user.\n\nSee Also\nDefining a Goal\nenum ARCoachingOverlayView.Goal\nThe options that specify your app's tracking requirements."
  },
  {
    "title": "ARDepthData | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ardepthdata",
    "html": "Overview\n\nThis object contains the following depth information that the LiDAR scanner captures at runtime:\n\nEvery pixel in the depthMap maps to a region of the visible scene (capturedImage), where the pixel value defines that region’s distance from the plane of the camera in meters.\n\nThe confidenceMap property measures the accuracy of the corresponding depth data in depthMap, and is useful in filtering out lower-accuracy depth values if an app’s algorithm required it.\n\nARWorldTrackingConfiguration exposes this depth information in the sceneDepth property which it updates every frame. To enable scene depth, add the sceneDepth frame semantic to a world-tracking configuration’s frameSemantics and frames vended by the session contain ARDepthData captured by the LiDAR scanner.\n\nTopics\nDepth Information\nvar depthMap: CVPixelBuffer\nThe estimated distance from the device to its environment, in meters.\nvar confidenceMap: CVPixelBuffer?\nThe framework’s confidence in the accuracy of the depth-map data.\nenum ARConfidenceLevel\nDegrees to which the framework is confident about depth-data accuracy.\nRelationships\nInherits From\nNSObject\nSee Also\nVideo Frame Analysis\nDisplaying a Point Cloud Using Scene Depth\nPresent a visualization of the physical environment by placing points based a scene’s depth data.\nCreating a Fog Effect Using Scene Depth\nApply virtual fog to the physical environment.\nclass ARFrame\nA video image captured as part of a session with position-tracking information.\nclass ARPointCloud\nA collection of points in the world coordinate space of the AR session."
  },
  {
    "title": "activatesAutomatically | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayview/3152976-activatesautomatically",
    "html": "Discussion\n\nThe default value is true. If enabled, the coaching overlay sets isActive automatically, depending on whether it needs user intervention to meet the current goal. The coaching overlay activates when the session is initializing or when tracking conditions have degraded past a certain threshold.\n\nSee Also\nActivating the View\nvar isActive: Bool\nA flag that indicates whether coaching is in progress.\nfunc setActive(Bool, animated: Bool)\nControls whether coaching is in progress."
  },
  {
    "title": "ARCoachingOverlayView.Goal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayview/goal",
    "html": "Overview\n\nThis property contains the available options when you set a coaching overlay's goal. The coaching overlay adjusts its messaging to the user based on the value.\n\nTopics\nDefining a Goal\ncase anyPlane\nA goal that specifies your app requires a plane of any type.\ncase horizontalPlane\nA goal that specifies your app requires a horizontal plane.\ncase tracking\nA goal that specifies your app requires basic world tracking.\ncase verticalPlane\nA goal that specifies your app requires a vertical plane.\ncase geoTracking\nA goal that specifies your app requires a precise geographic location.\nRelationships\nConforms To\nSendable\nSee Also\nDefining a Goal\nvar goal: ARCoachingOverlayView.Goal\nA field that indicates your app's tracking requirements."
  },
  {
    "title": "setActive(_:animated:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayview/3152981-setactive",
    "html": "Parameters\nactive\n\nA flag you set to indicate whether the coaching overlay should activate or deactivate.\n\nanimated\n\nA flag that when true, fades the coaching overlay in or out. When you pass a value of false, the coaching overlay shows or hides instantly.\n\nDiscussion\n\nIf the animated property of setActive(_:animated:) is true, isActive and isHidden are false while the coaching overlay is fading out. When the coaching overlay is deactivated without animation, or when the animation finishes, ARKit notifies you by calling coachingOverlayViewDidDeactivate(_:).\n\nSee Also\nActivating the View\nvar activatesAutomatically: Bool\nA flag that indicates whether the coaching view activates automatically, depending on the current session state.\nvar isActive: Bool\nA flag that indicates whether coaching is in progress."
  },
  {
    "title": "session | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayview/3152980-session",
    "html": "Discussion\n\nThe coaching overlay monitors your app's ARSession and reacts according to its tracking status. You don't need to set this property if you set sessionProvider instead.\n\nSee Also\nProviding the Session\nvar sessionProvider: ARSessionProviding?\nAn object you designate that provides the current session."
  },
  {
    "title": "Displaying a Point Cloud Using Scene Depth | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/environmental_analysis/displaying_a_point_cloud_using_scene_depth",
    "html": "Overview\n\nDepth Cloud is an app that uses Metal to display a camera feed by placing a collection of points in the physical environment, according to depth information from the device’s LiDAR Scanner. For every distance sample in the session’s periodic depth reading (depthMap), the app places a virtual dot at that location in the physical environment, with the final result resembling a point cloud. Depth Cloud colors the cloud according to ARKit’s camera image (capturedImage).\n\nFor every entry in the depth map — and therefore, for every dot in the cloud — the sample app checks the corresponding pixel in the camera image and assigns the pixel’s color to the dot. When the user views the point cloud straight on, the app’s display appears nearly identical to a camera feed. To demonstrate the cloud’s 3D shape, the sample app continuously rotates the cloud to change its viewing angle with respect to the user.\n\nThe following figure illustrates a point cloud from a single frame of data, rotated on the y-axis to reveal a dark area behind the apple where the current sensor readings lack color and depth information.\n\nThe app cycles through depth confidence values (see confidenceMap), enlarges the depth buffer, and toggles ARKit’s smooth depth option (smoothedSceneDepth). By applying the user’s selections to the point cloud live, the user can see the difference that the settings make throughout the experience.\n\nNote\n\nWWDC20 session 10611: Explore ARKit 4 references a prior version of this sample app that accumulates points in a cloud. For the original version of the app as shown in the session, clone the initial commit from the Git repository in the download’s root folder.\n\nFor a practical application of ARKit’s depth data, see Creating a Fog Effect Using Scene Depth.\n\nSet Up a Camera Feed\n\nTo display a camera feed, the sample project defines a SwiftUI Scene whose body contains a single window. To abstract view code from window code, the sample project wraps all of its display in a single View called MetalDepthView.\n\n@main\nstruct PointCloudDepthSample: App {\n    var body: some Scene {\n        WindowGroup {\n            MetalDepthView()\n\n\nBecause Depth Cloud draws graphics using Metal, the sample project displays the camera feed by defining custom GPU code. The sample project accesses ARKit’s camera feed in its ARReceiver.swift file and wraps it in a custom ARData object for eventual transfer to the GPU.\n\narData.colorImage = frame.capturedImage\n\nEnsure Device Support and Start a Session\n\nDevices require the LiDAR Scanner to access the scene’s depth. In the depth-visualization view’s body definition, the app prevents running an unsupported configuration by checking if the device supports scene depth.\n\nif !ARWorldTrackingConfiguration.supportsFrameSemantics([.sceneDepth, .smoothedSceneDepth]) {\n    Text(\"Unsupported Device: This app requires the LiDAR Scanner to access the scene's depth.\")\n\n\nTo separate data acquisition from its display, the sample app wraps ARKit calls in its ARProvider class.\n\nvar arProvider: ARProvider = ARProvider()\n\n\nThe AR provider runs a world-tracking configuration and requests information about the scene’s depth by configuring the scene-depth frame semantics (see sceneDepth and smoothedSceneDepth).\n\nlet config = ARWorldTrackingConfiguration()\nconfig.frameSemantics = [.sceneDepth, .smoothedSceneDepth]\narSession.run(config)\n\nAccess the Scene’s Depth\n\nIn response to the configuration’s scene-depth frame semantics, the framework defines the frame’s depthMap properties of sceneDepth and smoothedSceneDepth on the session’s currentFrame.\n\nfunc session(_ session: ARSession, didUpdate frame: ARFrame) {\n    if(frame.sceneDepth != nil) && (frame.smoothedSceneDepth != nil) {\n        arData.depthImage = frame.sceneDepth?.depthMap\n        arData.depthSmoothImage = frame.smoothedSceneDepth?.depthMap\n\n\nBecause the sample project draws its graphics using Metal, the app’s CPU code bundles up data that its GPU code needs to display the experience. To model the physical environment with a point cloud, the app needs camera capture data to color each point and depth data to position them.\n\nThe sample project positions each point in GPU code, so the CPU side packages the depth data in a Metal texture for use on the GPU.\n\ndepthContent.texture = lastArData?.depthImage?.texture(withFormat: .r32Float, planeIndex: 0, addToCache: textureCache!)!\n\n\nThe sample project colors each point in GPU code, so the CPU side packages the camera data for use on the GPU.\n\ncolorYContent.texture = lastArData?.colorImage?.texture(withFormat: .r8Unorm, planeIndex: 0, addToCache: textureCache!)!\ncolorCbCrContent.texture = lastArData?.colorImage?.texture(withFormat: .rg8Unorm, planeIndex: 1, addToCache: textureCache!)!\n\nConvert Camera Data\n\nIn the pointCloudVertexShader function (see the sample project’s shaders.metal file), the sample project creates a point for every value in the depth texture and determines the point’s color by sampling that depth-texture value’s position in the camera image. Each vertex calculates its x and y location in the camera image by converting its position in the one-dimensional vertex array, to a 2D position in the depth texture.\n\nuint2 pos;\n// Count the rows that are depth-texture-width wide to determine the y-value.\npos.y = vertexID / depthTexture.get_width();\n\n\n// The x-position is the remainder of the y-value division.\npos.x = vertexID % depthTexture.get_width();\n\n\nThe system’s camera-capture pipeline represents data in YUV format, which the sample project models using a luminance map (colorYtexture) and a blue versus red chromaticity map (colorCbCrtexture). The GPU color format is RGBA, which requires the sample project to convert the camera data to display it. The shader samples the luminance and chromaticity textures at the vertex’s x, y position and applies a static conversion factor.\n\nconstexpr sampler textureSampler (mag_filter::linear,\n                                  min_filter::linear);\nout.coor = { pos.x / (depthTexture.get_width() - 1.0f), pos.y / (depthTexture.get_height() - 1.0f) };\nhalf y = colorYtexture.sample(textureSampler, out.coor).r;\nhalf2 uv = colorCbCrtexture.sample(textureSampler, out.coor).rg - half2(0.5h, 0.5h);\n// Convert YUV to RGB inline.\nhalf4 rgbaResult = half4(y + 1.402h * uv.y, y - 0.7141h * uv.y - 0.3441h * uv.x, y + 1.772h * uv.x, 1.0h);\n\n\nNote\n\nFor brevity, the sample project demonstrates YUV to RGB conversion inline. To see an example that extracts static conversion factors to a 4 x 4 matrix, see Displaying an AR Experience with Metal.\n\nSet Up the Point Cloud View\n\nTo display a camera feed by using a point cloud, the project defines a UIViewRepresentable object, MetalPointCloud, which contains an MTKView that displays Metal content.\n\nstruct MetalPointCloud: UIViewRepresentable {\n\n\nThe project inserts the point cloud view into the view hierarchy by embedding it within the MetalDepthView layout.\n\nHStack() {\n    Spacer()\n    MetalPointCloud(arData: arProvider,\n                    confSelection: $selectedConfidence,\n                    scaleMovement: $scaleMovement).zoomOnTapModifier(\n                        height: geometry.size.width / 2 / sizeW * sizeH,\n                        width: geometry.size.width / 2, title: \"\")\n\n\nAs representable of UIView, the Metal texture view defines a coordinator, CoordinatorPointCloud.\n\nfunc makeCoordinator() -> CoordinatorPointCloud {\n    return CoordinatorPointCloud(arData: arData, confSelection: $confSelection, scaleMovement: $scaleMovement)\n}\n\n\nThe point cloud coordinator extends MTKCoordinator, which the sample shares across its other views that display Metal content.\n\nfinal class CoordinatorPointCloud: MTKCoordinator {\n\n\nAs an MTKViewDelegate, MTKCoordinator handles relevant events that occur throughout the Metal view life cycle.\n\nclass MTKCoordinator: NSObject, MTKViewDelegate {\n\n\nIn the UIView representable’s makeUIView implementation, the sample project assigns the coordinator as the view’s delegate.\n\nfunc makeUIView(context: UIViewRepresentableContext<MetalPointCloud>) -> MTKView {\n    let mtkView = MTKView()\n    mtkView.delegate = context.coordinator\n\n\nAt runtime, the display link then calls the Metal coordinator’s draw(in:) implementation to issue CPU-side rendering commands.\n\noverride func draw(in view: MTKView) {\n    content = arData.depthContent\n    let confidence = (arData.isToUpsampleDepth) ? arData.upscaledConfidence:arData.confidenceContent\n    guard arData.lastArData != nil else {\n\nDisplay the Point Cloud with GPU Code\n\nThe sample project draws the point cloud on the GPU. The point cloud view packages up several textures that its corresponding GPU code requires as input.\n\nencoder.setVertexTexture(content.texture, index: 0)\nencoder.setVertexTexture(confidence.texture, index: 1)\nencoder.setVertexTexture(arData.colorYContent.texture, index: 2)\nencoder.setVertexTexture(arData.colorCbCrContent.texture, index: 3)\n\n\nSimilarly, the point cloud view packages up several calculated properties that its corresponding GPU code requires as input.\n\nencoder.setVertexBytes(&pmv, length: MemoryLayout<matrix_float4x4>.stride, index: 0)\nencoder.setVertexBytes(&cameraIntrinsics, length: MemoryLayout<matrix_float3x3>.stride, index: 1)\nencoder.setVertexBytes(&confSelection, length: MemoryLayout<Int>.stride, index: 2)\n\n\nTo call into the GPU functions that draw the point cloud, the sample defines a pipeline state that queues up its pointCloudVertexShader and pointCloudFragmentShader Metal functions (see the project’s shaders.metal file).\n\npipelineDescriptor.vertexFunction = library.makeFunction(name: \"pointCloudVertexShader\")\npipelineDescriptor.fragmentFunction = library.makeFunction(name: \"pointCloudFragmentShader\")\n\n\nOn the GPU, the point cloud vertex shader determines each point’s color and position on the screen. In the function signature, the vertex shader receives the input textures and properties sent by the CPU code.\n\nvertex ParticleVertexInOut pointCloudVertexShader(\n    uint vertexID [[ vertex_id ]],\n    texture2d<float, access::read> depthTexture [[ texture(0) ]],\n    texture2d<float, access::read> confTexture [[ texture(1) ]],\n    constant float4x4& viewMatrix [[ buffer(0) ]],\n    constant float3x3& cameraIntrinsics [[ buffer(1) ]],\n    constant int &confFilterMode [[ buffer(2) ]],\n    texture2d<half> colorYtexture [[ texture(2) ]],\n    texture2d<half> colorCbCrtexture [[ texture(3) ]]\n    )\n{ // ...\n\n\nThe code bases the point’s world position on its location and depth in the camera feed.\n\nfloat xrw = ((int)pos.x - cameraIntrinsics[2][0]) * depth / cameraIntrinsics[0][0];\nfloat yrw = ((int)pos.y - cameraIntrinsics[2][1]) * depth / cameraIntrinsics[1][1];\nfloat4 xyzw = { xrw, yrw, depth, 1.f };\n\n\nThe point’s screen position is a product of its world position and the argument projection matrix.\n\nfloat4 vecout = viewMatrix * xyzw;\n\n\nThe vertex function outputs the point’s screen position, along with the point’s color as a converted RGB result.\n\nout.color = rgbaResult;\nout.clipSpacePosition = vecout;\n\n\nThe fragment shader receives the vertex function output in its function signature.\n\nfragment half4 pointCloudFragmentShader(\n    ParticleVertexInOut in [[stage_in]])\n\n\nAfter filtering any points that are too close to the device’s camera, the fragment shader queues the remaining points for display by returning the color of each vertex.\n\nif (in.depth < 1.0f)\n    discard_fragment();\nelse\n{\n    return in.color;\n\nChange the Cloud’s Orientation to Convey Depth\n\nAs the user views the point cloud straight on, it appears visually equivalent to the 2D camera image. But, when the sample app rotates the point cloud slightly, the 3D shape of the point cloud becomes apparent to the user.\n\nThe point cloud’s screen position is a factor of its projection matrix. In the sample project’s calcCurrentPMVMatrix function (see MetalPointCloud.swift), the function sets up a basic matrix.\n\nfunc calcCurrentPMVMatrix(viewSize: CGSize) -> matrix_float4x4 {\n    let projection: matrix_float4x4 = makePerspectiveMatrixProjection(fovyRadians: Float.pi / 2.0,\n                                                                      aspect: Float(viewSize.width) / Float(viewSize.height),\n                                                                      nearZ: 10.0, farZ: 8000.0)\n\n\nTo adjust the point cloud’s orientation with respect to the user, the sample app conversely sets up translation and rotation offsets for the camera’s pose.\n\n// Randomize the camera scale.\ntranslationCamera.columns.3 = [150 * sinf, -150 * cossqr, -150 * scaleMovement * sinsqr, 1]\n// Randomize the camera movement.\ncameraRotation = simd_quatf(angle: staticAngle, axis: SIMD3(x: -sinsqr / 3, y: -cossqr / 3, z: 0))\n\n\nThe sample project applies the camera pose offset to the original projection matrix before returning the adjusted result.\n\nlet rotationMatrix: matrix_float4x4 = matrix_float4x4(cameraRotation)\nlet pmv = projection * rotationMatrix * translationCamera * translationOrig * orientationOrig\nreturn pmv\n\nEnlarge the Depth Buffer\n\nARKit’s depth map contains precise, low-resolution depth values for objects in the camera feed. To create the illusion of a high-resolution depth map, the sample app offers UI to enlarge the depth map using Metal Performance Shaders (MPS). By filling in gaps in the framework’s depth information, the enlarged depth buffer creates the illusion of more depth information in the scene.\n\nThe sample project uses MPS to enlarge the depth buffer; see the ARDataProvider.swift file. The ARProvider class initializer creates a guided filter to enlarge the depth buffer.\n\nguidedFilter = MPSImageGuidedFilter(device: metalDevice, kernelDiameter: guidedFilterKernelDiameter)\n\n\nTo align the sizes of the related visuals — the camera image and confidence texture — the AR provider uses an MPS bilinear scale filter.\n\nmpsScaleFilter = MPSImageBilinearScale(device: metalDevice)\n\n\nIn the processLastARData routine, the AR provider creates an additional Metal command buffer for a compute pass that enlarges the depth buffer.\n\nif isToUpsampleDepth {\n\n\nThe AR provider converts the input depth data to RGB format, as required by the guided filter.\n\nlet convertYUV2RGBFunc = lib.makeFunction(name: \"convertYCbCrToRGBA\")\npipelineStateCompute = try metalDevice.makeComputePipelineState(function: convertYUV2RGBFunc!)\n\n\nAfter encoding the bilinear scale and guided filters, the AR provider sets the enlarged depth buffer.\n\ndepthContent.texture = destDepthTexture\n\nDisplay Depth and Confidence\n\nIn addition to the point cloud visualization, the sample app adds simultaneous depth distance and confidence visualizations. The user refers to either visualization at any time during the experience to better grasp the accuracy of the LiDAR Scanner’s reading of the physical environment.\n\nTo display the depth and confidence visualizations, Depth Cloud defines a UIViewRepresentable object, MetalTextureView, which contains an MTKView that displays Metal content (see the project’s MetalTextureView.swift file). This setup is similar to MetalDepthView, except that the sample app stores the view’s displayable content in a single texture.\n\nencoder.setFragmentTexture(content.texture, index: 0)\n\n\nThe project inserts the depth visualization view into the view hierarchy by embedding it within the MetalDepthView layout in the project’s MetalViewSample.swift file.\n\nScrollView(.horizontal) {\n    HStack() {\n        MetalTextureViewDepth(content: arProvider.depthContent, confSelection: $selectedConfidence)\n            .zoomOnTapModifier(height: sizeH, width: sizeW, title: isToUpsampleDepth ? \"Upscaled Depth\" : \"Depth\")\n\n\nThe depth visualization view’s contents consist of a texture that contains depth data from the AR session’s current frame.\n\ndepthContent.texture = lastArData?.depthImage?.texture(withFormat: .r32Float, planeIndex: 0, addToCache: textureCache!)!\n\n\nThe depth-texture view’s coordinator, CoordinatorDepth, assigns a shader that fills the texture.\n\npipelineDescriptor.fragmentFunction = library.makeFunction(name: \"planeFragmentShaderDepth\")\n\n\nThe planeFragmentShaderDepth shader (see shaders.metal) converts the depth values into RGB, as required to display them.\n\nfragment half4 planeFragmentShaderDepth(ColorInOut in [[stage_in]], texture2d<float, access::sample> textureDepth [[ texture(0) ]])\n{\n    constexpr sampler colorSampler(address::clamp_to_edge, filter::nearest);\n    float4 s = textureDepth.sample(colorSampler, in.texCoord);\n    \n    // Size the color gradient to a maximum distance of 2.5 meters.\n    // The LiDAR Scanner supports a value no larger than 5.0; the\n    // sample app uses a value of 2.5 to better distinguish depth\n    // in smaller environments.\n    half val = s.r / 2.5h;\n    half4 res = getJetColorsFromNormalizedVal(val);\n    return res;\n\n\nSimilarly, the project inserts the confidence visualization view into the view hierarchy by embedding it within the MetalDepthView layout in the project’s MetalViewSample.swift file.\n\nMetalTextureViewConfidence(content: arProvider.confidenceContent)\n    .zoomOnTapModifier(height: sizeH, width: sizeW, title: \"Confidence\")\n\n\nThe confidence visualization view’s contents consist of a texture that contains confidence data from the AR session’s current frame.\n\nconfidenceContent.texture = lastArData?.confidenceImage?.texture(withFormat: .r8Unorm, planeIndex: 0, addToCache: textureCache!)!\n\n\nThe confidence-texture view’s coordinator, CoordinatorConfidence, assigns a shader that fills the texture.\n\npipelineDescriptor.fragmentFunction = library.makeFunction(name: \"planeFragmentShaderConfidence\")\n\n\nThe planeFragmentShaderConfidence shader (see shaders.metal) converts the depth values into RGB, as required to display them.\n\nfragment half4 planeFragmentShaderConfidence(ColorInOut in [[stage_in]], texture2d<float, access::sample> textureIn [[ texture(0) ]])\n{\n    constexpr sampler colorSampler(address::clamp_to_edge, filter::nearest);\n    float4 s = textureIn.sample(colorSampler, in.texCoord);\n    float res = round( 255.0f*(s.r) ) ;\n    int resI = int(res);\n    half4 color = half4(0.0h, 0.0h, 0.0h, 0.0h);\n    if (resI == 0)\n        color = half4(1.0h, 0.0h, 0.0h, 1.0h);\n    else if (resI == 1)\n        color = half4(0.0h, 1.0h, 0.0h, 1.0h);\n    else if (resI == 2)\n        color = half4(0.0h, 0.0h, 1.0h, 1.0h);\n    return color;\n\nSee Also\nVideo Frame Analysis\nCreating a Fog Effect Using Scene Depth\nApply virtual fog to the physical environment.\nclass ARFrame\nA video image captured as part of a session with position-tracking information.\nclass ARPointCloud\nA collection of points in the world coordinate space of the AR session.\nclass ARDepthData\nAn object that describes the distance to regions of the real world from the plane of the camera."
  },
  {
    "title": "ARPointCloud | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arpointcloud",
    "html": "Overview\n\nUse the ARFrame rawFeaturePoints property to obtain a point cloud representing intermediate results of the scene analysis ARKit uses to perform world tracking.\n\nTopics\nIdentifying Feature Points\nvar points: [simd_float3]\nThe list of detected points.\nvar identifiers: [UInt64]\nA list of unique identifiers corresponding to detected feature points.\nRelationships\nInherits From\nNSObject\nConforms To\nNSSecureCoding\nSee Also\nVideo Frame Analysis\nDisplaying a Point Cloud Using Scene Depth\nPresent a visualization of the physical environment by placing points based a scene’s depth data.\nCreating a Fog Effect Using Scene Depth\nApply virtual fog to the physical environment.\nclass ARFrame\nA video image captured as part of a session with position-tracking information.\nclass ARDepthData\nAn object that describes the distance to regions of the real world from the plane of the camera."
  },
  {
    "title": "highResolutionFrameCaptureFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3930612-highresolutionframecapturefailed",
    "html": "Discussion\n\nThe system provides this error to the completion handler of captureHighResolutionFrame(completion:) for failed operations.\n\nSee Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "Placing Objects and Handling 3D Interaction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/environmental_analysis/placing_objects_and_handling_3d_interaction",
    "html": "Overview\n\nThe key facet of an AR experience is the ability to intermix virtual and real-world objects. A flat surface is the optimum location for setting a virtual object. To assist ARKit with finding surfaces, you tell the user to move their device in ways that help ARKit prepare the experience. ARKit provides a view that tailors its instructions to the user, guiding them to the surface that your app needs.\n\nTo enable the user to put a virtual item on the real-world surface when they tap the screen, ARKit incorporates ray casting, which provides a 3D location in physical space that corresponds to the screen’s touch location. When the user rotates or otherwise moves the virtual items they place, you respond to the respective touch gestures and correlate that input to the virtual content’s look in the physical environment.\n\nNote\n\nARKit requires a device with an A9 or later processor. ARKit is not available in iOS Simulator.\n\nSet a Goal to Coach the User’s Movement\n\nTo enable your app to detect real-world surfaces, you use a world tracking configuration. For ARKit to establish tracking, the user must physically move their device to allow ARKit to get a sense of perspective. To communicate this need to the user, you use a view provided by ARKit that presents the user with instructional diagrams and verbal guidance, called ARCoachingOverlayView. For example, when you start the app, the first thing the user sees is a message and animation from the coaching overlay telling them to move their device left and right, repeatedly, in order to get started.\n\nTo enable the user to place virtual content on a horizontal surface, you set the coaching overlay goal accordingly.\n\nfunc setGoal() {\n    coachingOverlay.goal = .horizontalPlane\n}\n\n\nThe coaching overlay then tailors its instructions according to the goal you choose. After ARKit gets a sense of perspective, the coaching overlay instructs the user to find a surface.\n\nRespond to Coaching Events\n\nTo make sure the coaching overlay provides guidance to the user whenever ARKit determines it’s necessary, you set activatesAutomatically to true.\n\nfunc setActivatesAutomatically() {\n    coachingOverlay.activatesAutomatically = true\n}\n\n\nThe coaching overlay activates automatically when the app starts, or when tracking degrades past a certain threshold. In those situations, ARKit notifies your delegate by calling coachingOverlayViewWillActivate(_:). In response to this event, hide your app’s UI to enable the user to focus on the instructions that the coaching overlay provides.\n\nfunc coachingOverlayViewWillActivate(_ coachingOverlayView: ARCoachingOverlayView) {\n    upperControlsView.isHidden = true\n}\n\n\nWhen the coaching overlay determines that the goal has been met, it disappears from the user’s view. ARKit notifies your delegate that the coaching process has ended, which is when you show your app’s main user interface.\n\nfunc coachingOverlayViewDidDeactivate(_ coachingOverlayView: ARCoachingOverlayView) {\n    upperControlsView.isHidden = false\n}\n\n\nPlace Virtual Content\n\nTo give the user an idea of where they can place virtual content, annotate the environment to give them a preview. The sample app draws a square that gives the user visual confirmation of the shape and alignment of the surfaces that ARKit is aware of.\n\nTo figure out where to put the square in the real world, you use an ARRaycastQuery to ask ARKit where any surfaces exist in the real world. First, you create a ray-cast query that defines the 2D point on the screen you’re interested in. Because the focus square is aligned with the center of the screen, you create a query for the screen center.\n\nfunc getRaycastQuery(for alignment: ARRaycastQuery.TargetAlignment = .any) -> ARRaycastQuery? {\n    return raycastQuery(from: screenCenter, allowing: .estimatedPlane, alignment: alignment)\n}\n\n\nThen, you execute the ray-cast query by asking the session to cast it.\n\nfunc castRay(for query: ARRaycastQuery) -> [ARRaycastResult] {\n    return session.raycast(query)\n}\n\n\nARKit returns a position in the results parameter that includes the depth of where that point lies on a surface in the real world. To give the user a preview of where on the real-world surface a user can place their virtual content, update the focus square’s position using the ray-cast result’s worldTransform:\n\nfunc setPosition(with raycastResult: ARRaycastResult, _ camera: ARCamera?) {\n    let position = raycastResult.worldTransform.translation\n    recentFocusSquarePositions.append(position)\n    updateTransform(for: raycastResult, camera: camera)\n}\n\n\nThe ray-cast result also indicates how the surface is angled with respect to gravity. To preview the angle at which the user’s virtual content can be placed on the surface, update the focus square’s simdWorldTransform with the result’s orientation.\n\nfunc updateOrientation(basedOn raycastResult: ARRaycastResult) {\n    self.simdOrientation = raycastResult.worldTransform.orientation\n}\n\n\nIf your app offers different types of virtual content, give the user an interface to choose from. The sample app exposes a selection menu when the user taps the plus button. When the user chooses an item from the list, you instantiate the corresponding 3D model and anchor it in the world at the focus square’s current position.\n\nfunc placeVirtualObject(_ virtualObject: VirtualObject) {\n    guard focusSquare.state != .initializing, let query = virtualObject.raycastQuery else {\n        self.statusViewController.showMessage(\"CANNOT PLACE OBJECT\\nTry moving left or right.\")\n        if let controller = self.objectsViewController {\n            self.virtualObjectSelectionViewController(controller, didDeselectObject: virtualObject)\n        }\n        return\n    }\n   \n    let trackedRaycast = createTrackedRaycastAndSet3DPosition(of: virtualObject, from: query,\n                                                              withInitialResult: virtualObject.mostRecentInitialPlacementResult)\n    \n    virtualObject.raycast = trackedRaycast\n    virtualObjectInteraction.selectedObject = virtualObject\n    virtualObject.isHidden = false\n}\n\n\nRefine the Position of Virtual Content Over Time\n\nAs the session runs, ARKit analyzes each camera image and learns more about the layout of the physical environment. When ARKit updates its estimated size and position of real-world surfaces, you may need to update the position of your app’s virtual content to match. To help make it easy, ARKit notifies you when it corrects its understanding of the scene by way of an ARTrackedRaycast.\n\nfunc createTrackedRaycastAndSet3DPosition(of virtualObject: VirtualObject, from query: ARRaycastQuery,\n                                          withInitialResult initialResult: ARRaycastResult? = nil) -> ARTrackedRaycast? {\n    if let initialResult = initialResult {\n        self.setTransform(of: virtualObject, with: initialResult)\n    }\n    \n    return session.trackedRaycast(query) { (results) in\n        self.setVirtualObject3DPosition(results, with: virtualObject)\n    }\n}\n\n\nARKit successively repeats the query you provide to a tracked ray cast, and it calls the closure you provide only when the results differ from prior results. The code you provide in the closure is your response to ARKit’s updated scene understanding. In this case, you check your ray-cast intersections against the updated planes and apply those positions to your app’s virtual content.\n\nprivate func setVirtualObject3DPosition(_ results: [ARRaycastResult], with virtualObject: VirtualObject) {\n    \n    guard let result = results.first else {\n        fatalError(\"Unexpected case: the update handler is always supposed to return at least one result.\")\n    }\n    \n    self.setTransform(of: virtualObject, with: result)\n    \n    // If the virtual object is not yet in the scene, add it.\n    if virtualObject.parent == nil {\n        self.sceneView.scene.rootNode.addChildNode(virtualObject)\n        virtualObject.shouldUpdateAnchor = true\n    }\n    \n    if virtualObject.shouldUpdateAnchor {\n        virtualObject.shouldUpdateAnchor = false\n        self.updateQueue.async {\n            self.sceneView.addOrUpdateAnchor(for: virtualObject)\n        }\n    }\n}\n\n\nManage Tracked Ray Casts\n\nBecause ARKit continues to call them, tracked ray casts can increasingly consume resources as the user places more virtual content. Stop the tracked ray cast when you no longer need refined positions over time, such as when a virtual balloon takes flight, or when you remove a virtual object from your scene.\n\nfunc removeVirtualObject(at index: Int) {\n    guard loadedObjects.indices.contains(index) else { return }\n    \n    // Stop the object's tracked ray cast.\n    loadedObjects[index].stopTrackedRaycast()\n    \n    // Remove the visual node from the scene graph.\n    loadedObjects[index].removeFromParentNode()\n    // Recoup resources allocated by the object.\n    loadedObjects[index].unload()\n    loadedObjects.remove(at: index)\n}\n\n\nTo stop a tracked ray cast, you call its stopTracking() function:\n\nfunc stopTrackedRaycast() {\n    raycast?.stopTracking()\n    raycast = nil\n}\n\n\nEnable User Interaction with Virtual Content\n\nTo allow users to move virtual content in the world after they’ve placed it, implement a pan gesture recognizer.\n\nfunc createPanGestureRecognizer(_ sceneView: VirtualObjectARView) {\n    let panGesture = ThresholdPanGesture(target: self, action: #selector(didPan(_:)))\n    panGesture.delegate = self\n    sceneView.addGestureRecognizer(panGesture)\n}\n\n\nWhen the user pans an object, you request its position along the object’s path across the plane. Because the object’s position is transitory, use a raycast(_:) instead of using a tracked ray cast. In this case, a one-time hit test is appropriate because you don’t need refined position results over time for these requests.\n\nfunc translate(_ object: VirtualObject, basedOn screenPos: CGPoint) {\n    object.stopTrackedRaycast()\n    \n    // Update the object by using a one-time position request.\n    if let query = sceneView.raycastQuery(from: screenPos, allowing: .estimatedPlane, alignment: object.allowedAlignment) {\n        viewController.createRaycastAndUpdate3DPosition(of: object, from: query)\n    }\n}\n\n\nRay casting gives you orientation information about the surface at a given screen point. While dragging, you avoid quick changes in orientation by subtracting the gesture’s rotation from the current object rotation.\n\n@objc\nfunc didRotate(_ gesture: UIRotationGestureRecognizer) {\n    guard gesture.state == .changed else { return }\n    \n    trackedObject?.objectRotation -= Float(gesture.rotation)\n    \n    gesture.rotation = 0\n}\n\n\nHandle Interruption in Tracking\n\nIn cases where tracking conditions are poor, ARKit invokes your delegate’s sessionWasInterrupted(_:). In these circumstances, the positions of your app’s virtual content may be inaccurate with respect to the camera feed, so hide your virtual content.\n\nfunc hideVirtualContent() {\n    virtualObjectLoader.loadedObjects.forEach { $0.isHidden = true }\n}\n\n\nRestore your app’s virtual content when tracking conditions improve. To notify you of improved conditions, ARKit calls your delegate’s session(_:cameraDidChangeTrackingState:) function, passing in a camera trackingState equal to ARTrackingStateNormal.\n\nfunc session(_ session: ARSession, cameraDidChangeTrackingState camera: ARCamera) {\n    statusViewController.showTrackingQualityInfo(for: camera.trackingState, autoHide: true)\n    switch camera.trackingState {\n    case .notAvailable, .limited:\n        statusViewController.escalateFeedback(for: camera.trackingState, inSeconds: 3.0)\n    case .normal:\n        statusViewController.cancelScheduledMessage(for: .trackingStateEscalation)\n        showVirtualContent()\n    }\n}\n\n\nRestore an Interrupted AR Experience\n\nWhen a session is interrupted, ARKit asks if you want to try to restore the AR experience. You do that by opting in to relocalization, by overriding sessionShouldAttemptRelocalization(_:) and returning true.\n\nfunc sessionShouldAttemptRelocalization(_ session: ARSession) -> Bool {\n    return true\n}\n\n\nDuring relocalization, the coaching overlay displays tailored instructions to the user. To allow the user to focus on the coaching process, hide your app’s UI when coaching is enabled.\n\nfunc coachingOverlayViewWillActivate(_ coachingOverlayView: ARCoachingOverlayView) {\n    upperControlsView.isHidden = true\n}\n\n\nWhen ARKit succeeds in restoring the experience, show your app’s UI again so everything appears the way it was before the interruption. When the coaching overlay disappears from the user’s view, ARKit invokes your coachingOverlayViewDidDeactivate(_:) callback, which is where you restore your app’s UI.\n\nfunc coachingOverlayViewDidDeactivate(_ coachingOverlayView: ARCoachingOverlayView) {\n    upperControlsView.isHidden = false\n}\n\n\nEnable the User to Start Over Rather Than Restore\n\nIf the user decides to give up on restoring the session, you restart the experience in your delegate’s coachingOverlayViewDidRequestSessionReset(_:) function. ARKit invokes this callback when the user taps the coaching overlay’s Start Over button.\n\nfunc coachingOverlayViewDidRequestSessionReset(_ coachingOverlayView: ARCoachingOverlayView) {\n    restartExperience()\n}\n\n\nSee Also\nRaycasting\nclass ARRaycastQuery\nA mathematical ray you use to find 3D positions on real-world surfaces.\nclass ARTrackedRaycast\nA raycast query that ARKit repeats in succession to give you refined results over time.\nclass ARRaycastResult\nInformation about a real-world surface found by examining a point on the screen."
  },
  {
    "title": "ARHitTestResult | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arhittestresult",
    "html": "Overview\n\nIf you use SceneKit or SpriteKit as your renderer, you can search for real-world surfaces at a screen point using:\n\nARSCNView hitTest(_:types:)\n\nARSKView hitTest(_:types:)\n\nOtherwise, you can search the camera image for real-world content using the ARFrame hitTest(_:types:) method. Because a frame is independent of a view, for this method you pass a point specified in normalized image coordinates (where (0,0) is the top left corner of the image and (1,1) is the lower right).\n\nAll these methods return an array of ARHitTestResult objects describing the content found. The number and order of results in the array depends on the search types you specify and the order you specify them in. For example, consider the code below:\n\nlet results = view.hitTest(point, [.existingPlaneUsingGeometry, .estimatedHorizontalPlane])\n\n\nThis hitTest(_:types:) call searches first for plane anchors already present in the session (according to the session configuration's planeDetection settings); returning any such results (in order of distance from the camera) as the first elements in the array. This call also (due to the estimatedHorizontalPlane request) attempts to determine whether the hit test ray intersects any horizontal surface not already found by plane detection, and returns that result (if any) as the last element in the array.\n\nTopics\nIdentifying Results\nvar type: ARHitTestResult.ResultType\nThe kind of detected feature the search result represents.\nstruct ARHitTestResult.ResultType\nPossible types for specifying a hit-test search, or for the result of a hit-test search.\nvar anchor: ARAnchor?\nThe anchor representing the detected surface, if any.\nExamining Result Geometry\nvar distance: CGFloat\nThe distance, in meters, from the camera to the detected surface.\nvar worldTransform: simd_float4x4\nThe position and orientation of the result relative to the world coordinate system.\nvar localTransform: simd_float4x4\nThe position and orientation of the result relative to the nearest anchor or feature point.\nRelationships\nInherits From\nNSObject"
  },
  {
    "title": "highResolutionFrameCaptureInProgress | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3930613-highresolutionframecaptureinprog",
    "html": "Discussion\n\nThe system provides this error to the completion handler of captureHighResolutionFrame(completion:) for failed operations.\n\nSee Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline."
  },
  {
    "title": "ARRaycastResult | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arraycastresult",
    "html": "Overview\n\nIf you use ARView or ARSCNView as your renderer, you can search for real-world surfaces at a screen point using the raycast(from:allowing:alignment:), and raycastQuery(from:allowing:alignment:) functions, respectively.\n\nIf you use a custom renderer, you can find real-world positions using screen points with:\n\nThe raycastQuery(from:allowing:alignment:) function of ARFrame.\n\nThe raycast(_:) function of ARSession.\n\nFor tracked raycasting, you call trackedRaycast(_:updateHandler:) on your app's current ARSession.\n\nTopics\nIdentifying Results\nvar worldTransform: simd_float4x4\nThe position, rotation, and scale, of the ray's intersection with the target.\nvar anchor: ARAnchor?\nThe anchor for the plane that the ray intersected.\nvar target: ARRaycastQuery.Target\nThe type of surface that the ray intersects.\nenum ARRaycastQuery.Target\nThe types of surface you allow a raycast to intersect with.\nvar targetAlignment: ARRaycastQuery.TargetAlignment\nThe alignment of the plane that the ray intersected.\nenum ARRaycastQuery.TargetAlignment\nA specification that indicates a target's alignment with respect to gravity.\nRelationships\nInherits From\nNSObject\nSee Also\nRaycasting\nPlacing Objects and Handling 3D Interaction\nPlace virtual content at tracked, real-world locations, and enable the user to interact with virtual content by using gestures.\nclass ARRaycastQuery\nA mathematical ray you use to find 3D positions on real-world surfaces.\nclass ARTrackedRaycast\nA raycast query that ARKit repeats in succession to give you refined results over time."
  },
  {
    "title": "stopTracking() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/artrackedraycast/3132069-stoptracking",
    "html": "Discussion\n\nA tracked raycast updates continuously until you stop it explicitly by calling stopTracking(). A raycast will automatically stop when:\n\nARKit calls sessionWasInterrupted(_:).\n\nYou change the session's configuration.\n\nYou deallocate the ARTrackedRaycast."
  },
  {
    "title": "ARGeoTrackingStatus.State.localized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus/state/localized",
    "html": "Discussion\n\nIn ARGeoTrackingStatus.State.localized, ARKit has completed visual localization and the app is free to place location anchors (ARGeoAnchor).\n\nSee Also\nStates\ncase initializing\nThe session is initializing geo tracking.\ncase localizing\nGeo tracking is attempting to localize against a map.\ncase notAvailable\nGeo tracking is not available."
  },
  {
    "title": "ARSCNDebugOptions | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscndebugoptions",
    "html": "Discussion\n\nTo use these debugging overlays, add them to the option set in the view's debugOptions property (inherited from the SCNView class).\n\nTopics\nAR Debugging Overlays\nstatic let showWorldOrigin: SCNDebugOptions\nDisplay a coordinate axis visualization indicating the position and orientation of the AR world coordinate system.\nstatic let showFeaturePoints: SCNDebugOptions\nDisplay a point cloud showing intermediate results of the scene analysis that ARKit uses to track device position."
  },
  {
    "title": "automaticallyUpdatesLighting | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/2887446-automaticallyupdateslighting",
    "html": "Discussion\n\nIf this value is true (the default), the view automatically creates one or more SCNLight objects, adds them to the scene, and updates their properties to reflect estimated lighting information from the camera scene. Set this value to false if you want to directly control all lighting in the SceneKit scene.\n\nSee Also\nRelated Documentation\nvar isLightEstimationEnabled: Bool\nA Boolean value specifying whether ARKit analyzes scene lighting in captured camera images."
  },
  {
    "title": "unprojectPoint(_:ontoPlane:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/3003664-unprojectpoint",
    "html": "Parameters\npoint\n\nA point in the 2D coordinate system of the view to project onto a plane.\n\nplaneTransform\n\nA transform matrix specifying the position and orientation of a plane (with infinite extent) in 3D world space. The plane is the xz-plane of the local coordinate space this transform defines.\n\nReturn Value\n\nThe 3D point in world space where a ray projected from the specified 2D point intersects the specified plane. If the ray does not intersect the plane, this method returns nil.\n\nSee Also\nMapping Content to Real-World Positions\nfunc anchor(for: SCNNode) -> ARAnchor?\nReturns the AR anchor associated with the specified SceneKit node, if any.\nfunc node(for: ARAnchor) -> SCNNode?\nReturns the SceneKit node associated with the specified AR anchor, if any."
  },
  {
    "title": "anchor(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/2875546-anchor",
    "html": "Parameters\nnode\n\nA SceneKit node in the view's scene.\n\nReturn Value\n\nThe ARAnchor object tracking the node, or nil if the node is not associated with an anchor or not in the view's scene.\n\nSee Also\nMapping Content to Real-World Positions\nfunc node(for: ARAnchor) -> SCNNode?\nReturns the SceneKit node associated with the specified AR anchor, if any.\nfunc unprojectPoint(CGPoint, ontoPlane: simd_float4x4) -> simd_float3?\nReturns the projection of a point from 2D view onto a plane in the 3D world space detected by ARKit."
  },
  {
    "title": "node(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/2874122-node",
    "html": "Parameters\nanchor\n\nAn anchor in the view's AR session.\n\nReturn Value\n\nThe node whose position in the AR scene the anchor tracks, or nil if the anchor has no associated node or is not in the view's AR session.\n\nSee Also\nMapping Content to Real-World Positions\nfunc anchor(for: SCNNode) -> ARAnchor?\nReturns the AR anchor associated with the specified SceneKit node, if any.\nfunc unprojectPoint(CGPoint, ontoPlane: simd_float4x4) -> simd_float3?\nReturns the projection of a point from 2D view onto a plane in the 3D world space detected by ARKit."
  },
  {
    "title": "hitTest(_:types:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/2875544-hittest",
    "html": "Parameters\npoint\n\nA point in the 2D coordinate system of the view.\n\ntypes\n\nThe types of hit-test result to search for.\n\nReturn Value\n\nA list of results, sorted from nearest to farthest (in distance from the camera).\n\nDiscussion\n\nHit testing searches for real-world objects or surfaces detected through the AR session's processing of the camera image. A 2D point in the view's coordinate system can refer to any point along a 3D line that starts at the device camera and extends in a direction determined by the device orientation and camera projection. This method searches along that line, returning all objects that intersect it in order of distance from the camera.\n\nNote\n\nThis method searches for AR anchors and real-world objects detected by the AR session, not SceneKit content displayed in the view. To search for SceneKit objects, use the view's hitTest(_:options:) method instead.\n\nThe behavior of a hit test depends on which types you specify and the order you specify them in. For details, see ARHitTestResult and the various ARHitTestResult.ResultType options.\n\nSee Also\nFinding Real-World Surfaces\nfunc raycastQuery(from: CGPoint, allowing: ARRaycastQuery.Target, alignment: ARRaycastQuery.TargetAlignment) -> ARRaycastQuery?\nCreates a raycast query that originates from a point on the view, aligned with the center of the camera's field of view."
  },
  {
    "title": "raycastQuery(from:allowing:alignment:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/3194595-raycastquery",
    "html": "Discussion\n\nWhen you call this function, ARKit creates a ray that extends in the positive z-direction from the argument screen space point, to determine if any of the argument targets exist in the physical environment anywhere along the ray. If so, ARKit returns a 3D position where the ray intersects the target.\n\nSee Also\nFinding Real-World Surfaces\nfunc hitTest(CGPoint, types: ARHitTestResult.ResultType) -> [ARHitTestResult]\nSearches for real-world objects or AR anchors in the captured camera image corresponding to a point in the SceneKit view.\nDeprecated"
  },
  {
    "title": "invalidConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3001736-invalidconfiguration",
    "html": "See Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "session | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/2865796-session",
    "html": "Discussion\n\nA view creates its own session object; use this property to access and configure the view's session.\n\nSee Also\nEssentials\nProviding 3D Virtual Content with SceneKit\nUse SceneKit to add realistic three-dimensional objects to your AR experience.\nvar scene: SCNScene\nThe SceneKit scene to be displayed in the view."
  },
  {
    "title": "insufficientFeatures | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3001735-insufficientfeatures",
    "html": "Discussion\n\nFor more information about a session's feature requirements, see Managing Session Life Cycle and Tracking Quality.\n\nSee Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "delegate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/2865797-delegate",
    "html": "See Also\nResponding to AR Updates\nprotocol ARSCNViewDelegate\nMethods you can implement to mediate the automatic synchronization of SceneKit content with an AR session."
  },
  {
    "title": "fileIOFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2968491-fileiofailed",
    "html": "See Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "cameraUnauthorized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/2919981-cameraunauthorized",
    "html": "Discussion\n\nTo use the device’s camera:\n\nYour app’s Info.plist file must provide a message for the NSCameraUsageDescription key. If this key is missing, any attempt to run an AR session fails with this error.\n\nWhen your app first attempts to run an AR session or otherwise use the camera, iOS automatically shows an alert with your camera usage description message, asking the user to grant camera permission to your app. If the user accepts this request, the session begins; otherwise the session fails with this error.\n\nIf the user has previously denied camera permission for your app, all attempts to run an AR session or otherwise use the camera fail with this error. To grant camera permission, the user must explicitly enable your app in the iOS Settings app, under Privacy > Camera.\n\nSee Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "invalidCollaborationData | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/3393312-invalidcollaborationdata",
    "html": "Discussion\n\nARKit produces this error when an app passes invalid data into the update(with:) function.\n\nSee Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "scene | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/2875547-scene",
    "html": "Discussion\n\nNote\n\nUnlike the parent SCNView class, an ARSCNView object requires a non-nil scene to display.\n\nSee Also\nEssentials\nProviding 3D Virtual Content with SceneKit\nUse SceneKit to add realistic three-dimensional objects to your AR experience.\nvar session: ARSession\nThe AR session that manages motion tracking and camera image processing for the view's contents."
  },
  {
    "title": "supportsAppClipCodeTracking | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/3697086-supportsappclipcodetracking",
    "html": "Discussion\n\nDevices require the Apple Neural Engine (ANE) to track App Clip Codes. The system sets this property to true if the device contains the ANE chip. The default value of this property is false.\n\nCall this function before setting appClipCodeTrackingEnabled.\n\nSee Also\nAccessing App Clip Codes\nInteracting with App Clip Codes in AR\nDisplay content and provide services in an AR experience with App Clip Codes.\nvar appClipCodeTrackingEnabled: Bool\nA Boolean value that indicates if the framework searches the physical environment for App Clip Codes.\nclass ARAppClipCodeAnchor\nAn anchor that tracks the position and orientation of an App Clip Code in the physical environment."
  },
  {
    "title": "appClipCodeTrackingEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/3697085-appclipcodetrackingenabled",
    "html": "Discussion\n\nWhen this property's value is true, the session delegate recieves an ARAppClipCodeAnchor via session(_:didAdd:) for every App Clip Code that ARKit detects in the physical environment. The default value is false.\n\nBefore calling this function, check that the configuration supports App Clip Code tracking by calling supportsAppClipCodeTracking.\n\nTo avoid scanning a physical code that’s not connected to an App Clip, the system ensures that an app provides an App Clip before allowing the app to interact with App Clip Codes. Without providing an App Clip, the app can recognize codes in the environment by determining their physical location (transform), but code URLs (url) remain nil.\n\nSee Also\nAccessing App Clip Codes\nInteracting with App Clip Codes in AR\nDisplay content and provide services in an AR experience with App Clip Codes.\nclass var supportsAppClipCodeTracking: Bool\nA flag that indicates if the device tracks App Clip Codes.\nclass ARAppClipCodeAnchor\nAn anchor that tracks the position and orientation of an App Clip Code in the physical environment."
  },
  {
    "title": "isAutoFocusEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/2942262-isautofocusenabled",
    "html": "Discussion\n\nFor apps deployed to iOS 11.3 or later, ARKit enables autofocus by default."
  },
  {
    "title": "ARLightEstimate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arlightestimate",
    "html": "Overview\n\nIf you enable the isLightEstimationEnabled setting, ARKit provides light estimates in the lightEstimate property of each ARFrame it delivers.\n\nIf you render your own overlay graphics for the AR scene, you can use this information in shading algorithms to help make those graphics match the real-world lighting conditions of the scene captured by the camera. The ARSCNView class automatically uses this information to configure SceneKit lighting.\n\nTopics\nExamining Light Parameters\nvar ambientIntensity: CGFloat\nThe estimated intensity, in lumens, of ambient light throughout the scene.\nvar ambientColorTemperature: CGFloat\nThe estimated color temperature, in degrees Kelvin, of ambient light throughout the scene.\nRelationships\nInherits From\nNSObject\nSee Also\nLighting Effects\nAdding Realistic Reflections to an AR Experience\nUse ARKit to generate environment probe textures from camera imagery and render reflective virtual objects.\nclass AREnvironmentProbeAnchor\nAn object that provides environmental lighting information for a specific area of space in a world-tracking AR session.\nclass ARDirectionalLightEstimate\nEstimated environmental lighting information associated with a captured video frame in a face-tracking AR session."
  },
  {
    "title": "Tracking and Visualizing Faces | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/tracking_and_visualizing_faces",
    "html": "Overview\n\nThis sample app presents a simple interface allowing you to choose between five augmented reality (AR) visualizations on devices with a TrueDepth front-facing camera.\n\nAn overlay of x/y/z axes indicating the ARKit coordinate system tracking the face (and in iOS 12, the position and orientation of each eye).\n\nThe face mesh provided by ARKit, showing automatic estimation of the real-world directional lighting environment, as well as a texture you can use to map 2D imagery onto the face.\n\nVirtual 3D content that appears to attach to (and interact with) the user’s real face.\n\nLive camera video texture-mapped onto the ARKit face mesh, with which you can create effects that appear to distort the user’s real face in 3D.\n\nA simple robot character whose facial expression animates to match that of the user, showing how to use ARKit’s animation blend shape values to create experiences like the system Animoji app.\n\nUse the tab bar to switch between these modes.\n\nImportant\n\nFace tracking supports devices with Apple Neural Engine in iOS 14 and iPadOS 14 and requires a device with a TrueDepth camera on iOS 13 and iPadOS 13 and earlier. To run the sample app, set the run destination to an actual device; the Simulator doesn’t support augmented reality.\n\nStart a Face-Tracking Session in a SceneKit View\n\nLike other uses of ARKit, face tracking requires configuring and running a session (an ARSession object) and rendering the camera image together with virtual content in a view. This sample uses ARSCNView to display 3D content with SceneKit, but you can also use SpriteKit or build your own renderer using Metal (see ARSKView and Displaying an AR Experience with Metal).\n\nFace tracking differs from other uses of ARKit in the class you use to configure the session. To enable face tracking, create an instance of ARFaceTrackingConfiguration, configure its properties, and pass it to the run(_:options:) method of the AR session associated with your view, as shown here:\n\nguard ARFaceTrackingConfiguration.isSupported else { return }\nlet configuration = ARFaceTrackingConfiguration()\nif #available(iOS 13.0, *) {\n    configuration.maximumNumberOfTrackedFaces = ARFaceTrackingConfiguration.supportedNumberOfTrackedFaces\n}\nconfiguration.isLightEstimationEnabled = true\nsceneView.session.run(configuration, options: [.resetTracking, .removeExistingAnchors])\n\n\nTrack the Position and Orientation of a Face\n\nWhen face tracking is active, ARKit automatically adds ARFaceAnchor objects to the running AR session, containing information about the user’s face, including its position and orientation. (ARKit detects and provides information about only face at a time. If multiple faces are present in the camera image, ARKit chooses the largest or most clearly recognizable face.)\n\nIn a SceneKit-based AR experience, you can add 3D content corresponding to a face anchor in the renderer(_:nodeFor:) or renderer(_:didAdd:for:) delegate method. ARKit manages a SceneKit node for the anchor, and updates that node’s position and orientation on each frame, so any SceneKit content you add to that node automatically follows the position and orientation of the user’s face.\n\nfunc renderer(_ renderer: SCNSceneRenderer, nodeFor anchor: ARAnchor) -> SCNNode? {\n    // This class adds AR content only for face anchors.\n    guard anchor is ARFaceAnchor else { return nil }\n    \n    // Load an asset from the app bundle to provide visual content for the anchor.\n    contentNode = SCNReferenceNode(named: \"coordinateOrigin\")\n    \n    // Add content for eye tracking in iOS 12.\n    self.addEyeTransformNodes()\n    \n    // Provide the node to ARKit for keeping in sync with the face anchor.\n    return contentNode\n}\n\n\nThis example uses a convenience extension on SCNReferenceNode to load content from an .scn file in the app bundle. The ARSCNView method provides that node to ARSCNView, allowing ARKit to automatically adjust the node’s position and orientation to match the tracked face.\n\nUse Face Geometry to Model the User’s Face\n\nARKit provides a coarse 3D mesh geometry matching the size, shape, topology, and current facial expression of the user’s face. ARKit also provides the ARSCNFaceGeometry class, offering an easy way to visualize this mesh in SceneKit.\n\nYour AR experience can use this mesh to place or draw content that appears to attach to the face. For example, by applying a semitransparent texture to this geometry you could paint virtual tattoos or makeup onto the user’s skin.\n\nTo create a SceneKit face geometry, initialize an ARSCNFaceGeometry object with the Metal device your SceneKit view uses for rendering, and assign that geometry to the SceneKit node tracking the face anchor.\n\nfunc renderer(_ renderer: SCNSceneRenderer, nodeFor anchor: ARAnchor) -> SCNNode? {\n    guard let sceneView = renderer as? ARSCNView,\n        anchor is ARFaceAnchor else { return nil }\n    \n    #if targetEnvironment(simulator)\n    #error(\"ARKit is not supported in iOS Simulator. Connect a physical iOS device and select it as your Xcode run destination, or select Generic iOS Device as a build-only destination.\")\n    #else\n    let faceGeometry = ARSCNFaceGeometry(device: sceneView.device!)!\n    let material = faceGeometry.firstMaterial!\n    \n    material.diffuse.contents = #imageLiteral(resourceName: \"wireframeTexture\") // Example texture map image.\n    material.lightingModel = .physicallyBased\n    \n    contentNode = SCNNode(geometry: faceGeometry)\n    #endif\n    return contentNode\n}\n\n\nNote\n\nThis example uses a texture with transparency to create the illusion of colorful grid lines painted onto a real face. You can use the wireframeTexture.png image included with this sample code project as a starting point to design your own face textures.\n\nARKit updates its face mesh conform to the shape of the user’s face, even as the user blinks, talks, and makes various expressions. To make the displayed face model follow the user’s expressions, retrieve an updated face meshes in the renderer(_:didUpdate:for:) delegate callback, then update the ARSCNFaceGeometry object in your scene to match by passing the new face mesh to its update(from:) method:\n\nfunc renderer(_ renderer: SCNSceneRenderer, didUpdate node: SCNNode, for anchor: ARAnchor) {\n    guard let faceGeometry = node.geometry as? ARSCNFaceGeometry,\n        let faceAnchor = anchor as? ARFaceAnchor\n        else { return }\n    \n    faceGeometry.update(from: faceAnchor.geometry)\n}\n\n\nPlace 3D Content on the User’s Face\n\nAnother use of the face mesh that ARKit provides is to create occlusion geometry in your scene. An occlusion geometry is a 3D model that doesn’t render any visible content (allowing the camera image to show through), but obstructs the camera’s view of other virtual content in the scene.\n\nThis technique creates the illusion that the real face interacts with virtual objects, even though the face is a 2D camera image and the virtual content is a rendered 3D object. For example, if you place an occlusion geometry and virtual glasses on the user’s face, the face can obscure the frame of the glasses.\n\nTo create an occlusion geometry for the face, start by creating an ARSCNFaceGeometry object as in the previous example. However, instead of configuring that object’s SceneKit material with a visible appearance, set the material to render depth but not color during rendering:\n\nlet faceGeometry = ARSCNFaceGeometry(device: sceneView.device!)!\nfaceGeometry.firstMaterial!.colorBufferWriteMask = []\nocclusionNode = SCNNode(geometry: faceGeometry)\nocclusionNode.renderingOrder = -1\n\n\nBecause the material renders depth, other objects rendered by SceneKit correctly appear in front of it or behind it. But because the material doesn’t render color, the camera image appears in its place.\n\nThe sample app combines this technique with a SceneKit object positioned in front of the user’s eyes, creating an effect where the user’s nose realistically obscures the object. This object uses physically-based materials, so it automatically benefits from the real-time directional lighting information that ARFaceTrackingConfiguration provides.\n\nNote\n\nThe ARFaceGeometry.obj file included in this sample project represents ARKit’s face geometry in a neutral pose. You can use this as a template to design your own 3D art assets for placement on a real face.\n\nMap Camera Video onto 3D Face Geometry\n\nFor additional creative uses of face tracking, you can texture-map the live 2D video feed from the camera onto the 3D geometry that ARKit provides. After mapping pixels in the camera video onto the corresponding points on ARKit’s face mesh, you can modify that mesh, creating illusions such as resizing or distorting the user’s face in 3D.\n\nFirst, create an ARSCNFaceGeometry for the face and assign the camera image to its main material. ARSCNView automatically sets the scene’s background material to use the live video feed from the camera, so you can set the geometry to use the same material.\n\n// Show video texture as the diffuse material and disable lighting.\nlet faceGeometry = ARSCNFaceGeometry(device: sceneView.device!, fillMesh: true)!\nlet material = faceGeometry.firstMaterial!\nmaterial.diffuse.contents = sceneView.scene.background.contents\nmaterial.lightingModel = .constant\n\n\nTo correctly align the camera image to the face, you’ll also need to modify the texture coordinates that SceneKit uses for rendering the image on the geometry. One easy way to perform this mapping is with a SceneKit shader modifier (see the SCNShadable protocol). The shader code here applies the coordinate system transformations needed to convert each vertex position in the mesh from 3D scene space to the 2D image space used by the video texture:\n\n// Transform the vertex to the camera coordinate system.\nfloat4 vertexCamera = scn_node.modelViewTransform * _geometry.position;\n\n\n// Camera projection and perspective divide to get normalized viewport coordinates (clip space).\nfloat4 vertexClipSpace = scn_frame.projectionTransform * vertexCamera;\nvertexClipSpace /= vertexClipSpace.w;\n\n\n// XY in clip space is [-1,1]x[-1,1], so adjust to UV texture coordinates: [0,1]x[0,1].\n// Image coordinates are Y-flipped (upper-left origin).\nfloat4 vertexImageSpace = float4(vertexClipSpace.xy * 0.5 + 0.5, 0.0, 1.0);\nvertexImageSpace.y = 1.0 - vertexImageSpace.y;\n\n\n// Apply ARKit's display transform (device orientation * front-facing camera flip).\nfloat4 transformedVertex = displayTransform * vertexImageSpace;\n\n\n// Output as texture coordinates for use in later rendering stages.\n_geometry.texcoords[0] = transformedVertex.xy;\n\n\nWhen you assign a shader code string to the geometry entry point, SceneKit configures its renderer to automatically run that code on the GPU for each vertex in the mesh. This shader code also needs to know the intended orientation for the camera image, so the sample gets that from the ARKit displayTransform(for:viewportSize:) method and passes it to the shader’s displayTransform argument:\n\n// Pass view-appropriate image transform to the shader modifier so\n// that the mapped video lines up correctly with the background video.\nlet affineTransform = frame.displayTransform(for: .portrait, viewportSize: sceneView.bounds.size)\nlet transform = SCNMatrix4(affineTransform)\nfaceGeometry.setValue(SCNMatrix4Invert(transform), forKey: \"displayTransform\")\n\n\nNote\n\nThis example’s shader modifier also applies a constant scale factor to all vertices, causing the user’s face to appear larger than life. Try other transformations to distort the face in other ways.\n\nAnimate a Character with Blend Shapes\n\nIn addition to the face mesh shown in the earlier examples, ARKit also provides a more abstract representation of the user’s facial expressions. You can use this representation (called blend shapes) to control animation parameters for your own 2D or 3D assets, creating a character that follows the user’s real facial movements and expressions.\n\nAs a basic demonstration of blend shape animation, this sample includes a simple model of a robot character’s head, created using SceneKit primitive shapes. (See the robotHead.scn file in the source code.)\n\nTo get the user’s current facial expression, read the blendShapes dictionary from the face anchor in the renderer(_:didUpdate:for:) delegate callback. Then, examine the key-value pairs in that dictionary to calculate animation parameters for your 3D content and update that content accordingly.\n\nlet blendShapes = faceAnchor.blendShapes\nguard let eyeBlinkLeft = blendShapes[.eyeBlinkLeft] as? Float,\n    let eyeBlinkRight = blendShapes[.eyeBlinkRight] as? Float,\n    let jawOpen = blendShapes[.jawOpen] as? Float\n    else { return }\neyeLeftNode.scale.z = 1 - eyeBlinkLeft\neyeRightNode.scale.z = 1 - eyeBlinkRight\njawNode.position.y = originalJawY - jawHeight * jawOpen\n\n\nThere are more than 50 unique ARFaceAnchor.BlendShapeLocation coefficients, of which your app can use as few or as many as necessary to create the artistic effect you want. In this sample, the BlendShapeCharacter class performs this calculation, mapping the eyeBlinkLeft and eyeBlinkRight parameters to one axis of the scale factor of the robot’s eyes, and the jawOpen parameter to offset the position of the robot’s jaw.\n\nSee Also\nFace Tracking\nCombining User Face-Tracking and World Tracking\nTrack the user’s face in an app that displays an AR experience with the rear camera.\nclass ARFaceAnchor\nAn anchor for a unique face that is visible in the front-facing camera."
  },
  {
    "title": "Scanning and detecting 3D objects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/scanning_and_detecting_3d_objects",
    "html": "Overview\n\nOne way to build compelling AR experiences is to recognize features of the user’s environment and use them to trigger the appearance of virtual content. For example, a museum app might add interactive 3D visualizations when the user points their device at a displayed sculpture or artifact.\n\nIn iOS 12, you can create such AR experiences by enabling object detection in ARKit: Your app provides reference objects, which encode three-dimensional spatial features of known real-world objects, and ARKit tells your app when and where it detects the corresponding real-world objects during an AR session.\n\nThis sample code project provides multiple ways to make use of object detection:\n\nRun the app to scan a real-world object and export a reference object file, which you can use in your own apps to detect that object.\n\nUse the ARObjectScanningConfiguration and ARReferenceObject classes as demonstrated in this sample app to record reference objects as part of your own asset production pipeline.\n\nUse detectionObjects in a world-tracking AR session to recognize a reference object and create AR interactions.\n\nNote\n\nARKit requires an iOS device with A9 processor or later. ARKit is not supported in iOS Simulator.\n\nConfigure your physical environment to enhance object scanning\n\nSet up your physical environment according to the following guidelines. Use these recommendations as a target configuration even if it’s unreachable in the specific circumstances of your scanning environment. You can scan objects outside of these specifications if necessary, but they provide ARKit with the conditions most conducive to object scanning.\n\nLight the object with an illuminance of 250 to 400 lux, and ensure that it’s well-lit from all sides.\n\nProvide a light temperature of around ~6500 Kelvin (D65)––similar with daylight. Avoid warm or any other tinted light sources.\n\nSet the object in front of a matte, middle gray background.\n\nScan real-world objects with an iOS app\n\nThe programming steps to scan and define a reference object that ARKit can use for detection are simple. (See “Create a Reference Object in an AR Session” below.) However, the fidelity of the reference object you create, and thus your success at detecting that reference object in your own apps, depends on your physical interactions with the object when scanning. Build and run this app on your iOS device to walk through a series of steps for getting high-quality scan data, resulting in reference object files that you can use for detection in your own apps.\n\nChoose an iOS Device. For easy object scanning, use a recent, high-performance iOS device. Scanned objects can be detected on any ARKit-supported device, but the process of creating a high-quality scan is faster and smoother on a high-performance device.\n\nPosition the object. When first run, the app displays a box that roughly estimates the size of whatever real-world objects appear centered in the camera view. Position the object you want to scan on a surface free of other objects (like an empty tabletop). Then move your device so that the object appears centered in the box, and tap the Next button.\n\nDefine bounding box. Before scanning, you need to tell the app what region of the world contains the object you want to scan. Drag to move the box around in 3D, or press and hold on a side of the box and then drag to resize it. (Or, if you leave the box untouched, you can move around the object and the app will attempt to automatically fit a box around it.) Make sure the bounding box contains only features of the object you want to scan (not those from the environment it’s in), then tap the Scan button.\n\nScan the object. Move around to look at the object from different angles. For best results, move slowly and avoid abrupt motions. The app highlights parts of the bounding box to indicate when you’ve scanned enough to recognize the object from the corresponding direction. Be sure to scan on all sides from which you want users of your app to be able to recognize the object. The app automatically proceeds to the next step when a scan is complete, or you can tap the Stop button to proceed manually.\n\nAdjust origin. The app displays x, y, and z coordinate axis lines showing the object’s anchor point, or origin. Drag the circles to move the origin relative to the object. In this step you can also use the Add (+) button to load a 3D model in USDZ format. The app displays the model as it would appear in AR upon detecting the real-world object, and uses the model’s size to adjust the scale of the reference object. Tap the Test button when done.\n\nTest and export. The app has now created an ARReferenceObject and has reconfigured its session to detect it. Look at the real-world object from different angles, in various environments and lighting conditions, to verify that ARKit reliably recognizes its position and orientation. Tap the Export button to open a share sheet for saving the finished .arobject file. For example, you can easily send it to your development Mac using AirDrop, or send it to the Files app to save it to iCloud Drive.\n\nNote\n\nAn ARReferenceObject contains only the spatial feature information needed for ARKit to recognize the real-world object, and is not a displayable 3D reconstruction of that object.\n\nDetect reference objects in an AR experience\n\nYou can use an Xcode asset catalog to bundle reference objects in an app for use in detection:\n\nOpen your project’s asset catalog, then use the Add button (+) to add a new AR resource group.\n\nDrag .arobject files from the Finder into the newly created resource group.\n\nOptionally, for each reference object, use the inspector to provide a descriptive name for your own use.\n\nNote\n\nPut all objects you want to look for in the same session into a resource group, and use separate resource groups to hold sets of objects for use in separate sessions. For example, a museum app might use separate sessions (and thus separate resource groups) for recognizing displays in different wings of the museum.\n\nTo enable object detection in an AR session, load the reference objects you want to detect as ARReferenceObject instances, provide those objects for the detectionObjects property of an ARWorldTrackingConfiguration, and run an ARSession with that configuration:\n\nlet configuration = ARWorldTrackingConfiguration()\nguard let referenceObjects = ARReferenceObject.referenceObjects(inGroupNamed: \"gallery\", bundle: nil) else {\n    fatalError(\"Missing expected asset catalog resources.\")\n}\nconfiguration.detectionObjects = referenceObjects\nsceneView.session.run(configuration)\n\n\nWhen ARKit detects one of your reference objects, the session automatically adds a corresponding ARObjectAnchor to its list of anchors. To respond to an object being recognized, implement an appropriate ARSessionDelegate, ARSCNViewDelegate, or ARSKViewDelegate method that reports the new anchor being added to the session. For example, in a SceneKit-based app you can implement renderer(_:didAdd:for:) to add a 3D asset to the scene, automatically matching the position and orientation of the anchor:\n\nfunc renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor) {\n    if let objectAnchor = anchor as? ARObjectAnchor {\n        node.addChildNode(self.model)\n    }\n}\n\n\nFor best results with object scanning and detection, follow these tips:\n\nARKit looks for areas of clear, stable visual detail when scanning and detecting objects. Detailed, textured objects work better for detection than plain or reflective objects.\n\nObject scanning and detection is optimized for objects small enough to fit on a tabletop.\n\nAn object to be detected must have the same shape as the scanned reference object. Rigid objects work better for detection than soft bodies or items that bend, twist, fold, or otherwise change shape.\n\nDetection works best when the lighting conditions for the real-world object to be detected are similar to those in which the original object was scanned. Consistent indoor lighting works best.\n\nCreate a reference object in an AR session\n\nThis sample app provides one way to create reference objects. You can also scan reference objects in your own app—for example, to build asset management tools for defining AR content that goes into other apps you create.\n\nA reference object encodes a slice of the internal spatial-mapping data that ARKit uses to track a device’s position and orientation. To enable the high-quality data collection required for object scanning, run a session with ARObjectScanningConfiguration:\n\nlet configuration = ARObjectScanningConfiguration()\nconfiguration.planeDetection = .horizontal\nsceneView.session.run(configuration, options: .resetTracking)\n\n\nDuring your object-scanning AR session, scan the object from various angles to make sure you collect enough spatial data to recognize it. (If you’re building your own object-scanning tools, help users walk through the same steps this sample app provides.)\n\nAfter scanning, call createReferenceObject(transform:center:extent:completionHandler:) to produce an ARReferenceObject from a region of the user environment mapped by the session:\n\n// Extract the reference object based on the position & orientation of the bounding box.\nsceneView.session.createReferenceObject(\n    transform: boundingBox.simdWorldTransform,\n    center: SIMD3<Float>(), extent: boundingBox.extent,\n    completionHandler: { object, error in\n        if let referenceObject = object {\n            // Adjust the object's origin with the user-provided transform.\n            self.scannedReferenceObject = referenceObject.applyingTransform(origin.simdTransform)\n            self.scannedReferenceObject!.name = self.scannedObject.scanName\n            \n            if let referenceObjectToMerge = ViewController.instance?.referenceObjectToMerge {\n                ViewController.instance?.referenceObjectToMerge = nil\n                \n                // Show activity indicator during the merge.\n                ViewController.instance?.showAlert(title: \"\", message: \"Merging previous scan into this scan...\", buttonTitle: nil)\n                \n                // Try to merge the object which was just scanned with the existing one.\n                self.scannedReferenceObject?.mergeInBackground(with: referenceObjectToMerge, completion: { (mergedObject, error) in\n\n\n                    if let mergedObject = mergedObject {\n                        self.scannedReferenceObject = mergedObject\n                        ViewController.instance?.showAlert(title: \"Merge successful\",\n                                                           message: \"The previous scan has been merged into this scan.\", buttonTitle: \"OK\")\n                        creationFinished(self.scannedReferenceObject)\n\n\n                    } else {\n                        print(\"Error: Failed to merge scans. \\(error?.localizedDescription ?? \"\")\")\n                        let message = \"\"\"\n                                Merging the previous scan into this scan failed. Please make sure that\n                                there is sufficient overlap between both scans and that the lighting\n                                environment hasn't changed drastically.\n                                Which scan do you want to use for testing?\n                                \"\"\"\n                        let thisScan = UIAlertAction(title: \"Use This Scan\", style: .default) { _ in\n                            creationFinished(self.scannedReferenceObject)\n                        }\n                        let previousScan = UIAlertAction(title: \"Use Previous Scan\", style: .default) { _ in\n                            self.scannedReferenceObject = referenceObjectToMerge\n                            creationFinished(self.scannedReferenceObject)\n                        }\n                        ViewController.instance?.showAlert(title: \"Merge failed\", message: message, actions: [thisScan, previousScan])\n                    }\n                })\n            } else {\n                creationFinished(self.scannedReferenceObject)\n            }\n        } else {\n            print(\"Error: Failed to create reference object. \\(error!.localizedDescription)\")\n            creationFinished(nil)\n        }\n    })\n\n\nWhen detecting a reference object, ARKit reports its position based on the origin the reference object defines. If you want to place virtual content that appears to sit on the same surface as the real-world object, make sure the reference object’s origin is placed at the point where the real-world object sits. To adjust the origin after capturing an ARReferenceObject, use the applyingTransform(_:) method.\n\nAfter you obtain an ARReferenceObject, you can either use it immediately for detection (see “Detect Reference Objects in an AR Experience” above) or save it as an .arobject file for use in later sessions or other ARKit-based apps. To save an object to a file, use the export(to:previewImage:) method. In that method, you can provide a picture of the real-world object for Xcode to use as a preview image.\n\nSee Also\nPhysical Objects\nVisualizing and Interacting with a Reconstructed Scene\nEstimate the shape of the physical environment using a polygonal mesh.\nclass ARObjectAnchor\nAn anchor for a real-world 3D object that ARKit detects in the physical environment.\nclass ARReferenceObject\nThe description of a 3D object that you want ARKit to detect in the physical environment."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/referenceimage/4139409-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting a reference image\nvar physicalSize: CGSize\nThe size, in meters, of a reference image in the real world.\nvar name: String?\nThe name of a reference image."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration/3194572-init",
    "html": "See Also\nCreating a Configuration\nvar initialWorldMap: ARWorldMap?\nThe state from a previous AR session to attempt to resume with this session configuration."
  },
  {
    "title": "stopTrackedRaycasts | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/runoptions/3340470-stoptrackedraycasts",
    "html": "Discussion\n\nBy default, when you call the run(_:options:) method on a session that is running or has run before, the session keeps tracking any ARTrackedRaycast objects that you previously added by calling trackedRaycast(_:updateHandler:).\n\nUse stopTrackedRaycasts if you want to stop all active tracked raycasts. Alternatively, you can stop individual raycasts by calling stopTracking() on individual raycasts.\n\nSee Also\nRun Options\nstatic var resetTracking: ARSession.RunOptions\nAn option to reset the device's position from the session's previous run.\nstatic var removeExistingAnchors: ARSession.RunOptions\nAn option to remove any anchor objects associated with the session's previous run.\nstatic var resetSceneReconstruction: ARSession.RunOptions\nAn option to reset the scene mesh."
  },
  {
    "title": "initialWorldMap | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration/3255165-initialworldmap",
    "html": "Discussion\n\nAn ARWorldMap encapsulates the state of a running ARSession. This state includes ARKit's awareness of the physical space the user moves the device in (which ARKit uses to determine the device's position and orientation), as well as any ARAnchor objects added to the session (which can represent detected real-world features or virtual content placed by your app). After you use getCurrentWorldMap(completionHandler:) to save a session's world map, you can assign it to a configuration's initialWorldMap property and use run(_:options:) to start another session with the same spatial awareness and anchors.\n\nBy saving world maps and using them to start new sessions, your app can add new AR capabilities:\n\nMultiuser AR experiences. Create a shared frame of reference by sending archived ARWorldMap objects to a nearby user's device. With two devices tracking the same world map, you can build a networked experience where both users can see and interact with the same virtual content.\n\nPersistent AR experiences. Save a world map when your app becomes inactive, then restore it the next time your app launches in the same physical environment. You can use anchors from the resumed world map to place the same virtual content at the same positions from the saved session.\n\nWhen you run a session with an initial world map, the session starts in the ARCamera.TrackingState.limited(_:) (ARCamera.TrackingState.Reason.relocalizing) tracking state while ARKit attempts to reconcile the recorded world map with the current environment. If successful, the tracking state becomes ARCamera.TrackingState.normal after a short time, indicating that the current world coordinate system and anchors match those from the recorded world map.\n\nIf ARKit cannot reconcile the recorded world map with the current environment (for example, if the device is in an entirely different place from where the world map was recorded), the session remains in the ARCamera.TrackingState.Reason.relocalizing state indefinitely.\n\nSee Also\nCreating a Configuration\ninit()\nCreates a new body tracking configuration."
  },
  {
    "title": "Providing 3D Virtual Content with SceneKit | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/providing_3d_virtual_content_with_scenekit",
    "html": "Overview\n\nBecause ARKit automatically matches SceneKit space to the real world, placing a virtual object so that it appears to maintain a real-world position requires that you set the object's SceneKit position appropriately. For example, in a default configuration, the following code places a 10-centimeter cube 20 centimeters in front of the camera's initial position:\n\nlet cubeNode = SCNNode(geometry: SCNBox(width: 0.1, height: 0.1, length: 0.1, chamferRadius: 0))\ncubeNode.position = SCNVector3(0, 0, -0.2) // SceneKit/AR coordinates are in meters\nsceneView.scene.rootNode.addChildNode(cubeNode)\n\n\nThe code above places an object directly in the view’s SceneKit scene. The object automatically appears to track a real-world position because ARKit matches SceneKit space to real-world space.\n\nAlternatively, you can use the ARAnchor class to track real-world positions, either by creating anchors yourself and adding them to the session or by observing anchors that ARKit automatically creates. For example, when plane detection is enabled, ARKit adds and updates anchors for each detected plane. To add visual content for these anchors, implement ARSCNViewDelegate methods such as the following:\n\nfunc renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor) {\n    // This visualization covers only detected planes.\n    guard let planeAnchor = anchor as? ARPlaneAnchor else { return }\n\n\n    // Create a SceneKit plane to visualize the node using its position and extent.\n    let plane = SCNPlane(width: CGFloat(planeAnchor.extent.x), height: CGFloat(planeAnchor.extent.z))\n    let planeNode = SCNNode(geometry: plane)\n    planeNode.position = SCNVector3Make(planeAnchor.center.x, 0, planeAnchor.center.z)\n\n\n    // SCNPlanes are vertically oriented in their local coordinate space.\n    // Rotate it to match the horizontal orientation of the ARPlaneAnchor.\n    planeNode.transform = SCNMatrix4MakeRotation(-Float.pi / 2, 1, 0, 0)\n\n\n    // ARKit owns the node corresponding to the anchor, so make the plane a child node.\n    node.addChildNode(planeNode)\n}\n\n\nFollow Best Practices for Designing 3D Assets\n\nUse the SceneKit physically based lighting model for materials for a more realistic appearance. (See the SCNMaterial class and the Badger: Advanced Rendering in SceneKit sample code project.)\n\nBake ambient occlusion shading so that objects appear properly lit in a wide variety of scene lighting conditions.\n\nIf you create a virtual object that you intend to place on a real-world flat surface in AR, include a transparent plane with a soft shadow texture below the object in your 3D asset.\n\nSee Also\nEssentials\nvar session: ARSession\nThe AR session that manages motion tracking and camera image processing for the view's contents.\nvar scene: SCNScene\nThe SceneKit scene to be displayed in the view."
  },
  {
    "title": "isAutoFocusEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration/3194571-isautofocusenabled",
    "html": "Discussion\n\nFor apps deployed to iOS 11.3 or later, ARKit enables autofocus by default."
  },
  {
    "title": "automaticSkeletonScaleEstimationEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration/3255163-automaticskeletonscaleestimation",
    "html": "Discussion\n\nWhen you assign this property a value of true, you instruct ARKit to compute a value for estimatedScaleFactor for a person's body anchor. ARKit must know the physical height of a person in the real world to accurately estimate the person's real-world position. You enable this property to tell ARKit to estimate a recognized person's physical height before it assigns the body anchor's position."
  },
  {
    "title": "planeDetection | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration/3194574-planedetection",
    "html": "Discussion\n\nBy default, plane detection is off. If you enable horizontal or vertical plane detection, the session adds ARPlaneAnchor objects and notifies your ARSessionDelegate, ARSCNViewDelegate, or ARSKViewDelegate object whenever its analysis of captured video images detects an area that appears to be a flat surface.\n\nSee Also\nEnabling Plane Detection\nstruct ARWorldTrackingConfiguration.PlaneDetection\nOptions for whether and how the framework detects flat surfaces in captured images."
  },
  {
    "title": "automaticImageScaleEstimationEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration/3229910-automaticimagescaleestimationena",
    "html": "Discussion\n\nIf set to true, ARKit uses its knowledge of the world to set an image anchor's estimatedScaleFactor property, which corrects the image anchor's position in the physical environment.\n\nEnable this property when you want to detect different sized versions of a reference image. ARKit must know the physical size of an image in the real world to accurately estimate its real-world position. You enable this property to tell ARKit to estimate a recognized image's physical size before it calculates the real-world position.\n\nSee Also\nEnabling Image Tracking\nvar detectionImages: Set<ARReferenceImage>\nA set of images that ARKit searches for in the user's environment.\nvar maximumNumberOfTrackedImages: Int\nThe number of image anchors to monitor closely for position and orientation updates."
  },
  {
    "title": "environmentTexturing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration/3255164-environmenttexturing",
    "html": "Discussion\n\nEnvironment textures are cube-map textures that depict the view in all directions from a specific point in a scene. In 3D asset rendering, environment textures are the basis for image-based lighting algorithms where surfaces can realistically reflect light from their surroundings. ARKit can generate environment textures during an AR session using camera imagery, allowing SceneKit or a custom rendering engine to provide realistic image-based lighting for virtual objects in your AR experience.\n\nTo enable texture map generation for your configuration, change this property (from its default value of ARWorldTrackingConfiguration.EnvironmentTexturing.none):\n\nWith ARWorldTrackingConfiguration.EnvironmentTexturing.manual environment texturing, you identify points in the scene for which you want light probe texture maps by creating AREnvironmentProbeAnchor objects and adding them to the session.\n\nWith ARWorldTrackingConfiguration.EnvironmentTexturing.automatic environment texturing, ARKit automatically creates, positions, and adds AREnvironmentProbeAnchor objects to the session.\n\nIn both cases, ARKit automatically generates environment textures as the session collects camera imagery. Use a delegate method such as session(_:didUpdate:) to find out when a texture is available, and access it from the anchor's environmentTexture property.\n\nIf you display AR content using ARSCNView and the automaticallyUpdatesLighting option, SceneKit automatically retrieves AREnvironmentProbeAnchor texture maps and uses them to light the scene.\n\nSee Also\nAdding Realistic Reflections\nvar wantsHDREnvironmentTextures: Bool\nA flag that instructs ARKit to create environment textures in HDR format."
  },
  {
    "title": "wantsHDREnvironmentTextures | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration/3255166-wantshdrenvironmenttextures",
    "html": "Discussion\n\nThe default value is true. If your renderer supports HDR environment textures, this feature effects more realistic reflections.\n\nRealityKit and SceneKit both support HDR environment textures. For more information, see Adding Realistic Reflections to an AR Experience.\n\nSee Also\nAdding Realistic Reflections\nvar environmentTexturing: ARWorldTrackingConfiguration.EnvironmentTexturing\nThe behavior ARKit uses for generating environment textures."
  },
  {
    "title": "maximumNumberOfTrackedImages | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration/3229912-maximumnumberoftrackedimages",
    "html": "Discussion\n\nWhen you set a nonzero value for this property, the framework keeps that many image anchors up to date as the session progresses. The framework can track a maximum of four images simultaneously.\n\nThe word track in the property name refers to how the framework closely monitors the image’s physical position and orientation for any changes. If the image moves, the framework updates the associated ARImageAnchor transform with the new pose. ARKit checks for changes every frame.\n\nARKit tracks the first images it observes in the physical environment from the detectionImages set. When a session reaches the maximum number of tracked images, the framework attempts to track another member of the set only after one of the existing tracked images leaves the device’s view.\n\nThe default value is 0, which disables image tracking. If you add reference images to detectionImages with this property set to 0, ARKit creates image anchors for observed reference images but their positions only update infrequently, such as once every couple of seconds.\n\nSee Also\nEnabling Image Tracking\nvar automaticImageScaleEstimationEnabled: Bool\nA flag that instructs ARKit to estimate and set the scale of a tracked image on your behalf.\nvar detectionImages: Set<ARReferenceImage>\nA set of images that ARKit searches for in the user's environment."
  },
  {
    "title": "appClipCodeTrackingEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration/3697081-appclipcodetrackingenabled",
    "html": "Discussion\n\nWhen this property's value is true, the session delegate recieves an ARAppClipCodeAnchor via session(_:didAdd:) for every App Clip Code that ARKit detects in the physical environment. The default value is false.\n\nBefore calling this function, check that the configuration supports App Clip Code tracking by calling supportsAppClipCodeTracking.\n\nTo avoid scanning a physical code that’s not connected to an App Clip, the system ensures that an app provides an App Clip before allowing the app to interact with App Clip Codes. Without providing an App Clip, the app can recognize codes in the environment by determining their physical location (transform), but code URLs (url) remain nil.\n\nSee Also\nAccessing App Clip Codes\nInteracting with App Clip Codes in AR\nDisplay content and provide services in an AR experience with App Clip Codes.\nclass var supportsAppClipCodeTracking: Bool\nA flag that indicates if the device tracks App Clip Codes.\nclass ARAppClipCodeAnchor\nAn anchor that tracks the position and orientation of an App Clip Code in the physical environment."
  },
  {
    "title": "ARErrorDomain | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerrordomain",
    "html": "See Also\nHandling Errors\nfunc session(ARSession, didFailWithError: Error)\nTells the delegate that the session has stopped running due to an error."
  },
  {
    "title": "session(_:didOutputAudioSampleBuffer:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionobserver/2923544-session",
    "html": "Parameters\nsession\n\nThe session providing information.\n\naudioSampleBuffer\n\nThe sample buffer that was output.\n\nDiscussion\n\nARKit calls this method on your delegate object only if you're running an AR session with a configuration whose providesAudioData setting is true."
  },
  {
    "title": "session(_:didFailWithError:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionobserver/2887453-session",
    "html": "Parameters\nsession\n\nThe session providing information.\n\nerror\n\nAn object describing the failure.\n\nSee Also\nHandling Errors\nlet ARErrorDomain: String\nThe error domain for NSError objects produced by an AR session."
  },
  {
    "title": "session(_:didChange:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionobserver/3580878-session",
    "html": "Parameters\nsession\n\nThe geo-tracking session.\n\ngeoTrackingStatus\n\nThe new status.\n\nDiscussion\n\nTo create and maintain an effective geo-tracking session, an app must react promptly when ARKit changes the geo-tracking status. For more information, see ARGeoTrackingStatus.\n\nARKit invokes this callback only for ARGeoTrackingConfiguration sessions.\n\nSee Also\nResponding to Tracking Quality Changes\nfunc session(ARSession, cameraDidChangeTrackingState: ARCamera)\nInforms the delegate of changes to the quality of ARKit's device position tracking."
  },
  {
    "title": "session(_:didRemove:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessiondelegate/2865622-session",
    "html": "Parameters\nsession\n\nThe session providing information.\n\nanchors\n\nThe anchors newly removed from the session.\n\nDiscussion\n\nDepending on the session configuration, ARKit may automatically remove anchors from a session.\n\nIf you display an AR experience using SceneKit or SpriteKit, you can instead implement one of the following methods instead to track not only the anchors in the session but also any corresponding SceneKit or SpriteKit content:\n\nARSCNView: renderer(_:didRemove:for:)\n\nARSKView: view(_:didRemove:for:)\n\nSee Also\nHandling Content Updates\nfunc session(ARSession, didAdd: [ARAnchor])\nTells the delegate that one or more anchors have been added to the session.\nfunc session(ARSession, didUpdate: [ARAnchor])\nTells the delegate that the session has adjusted the properties of one or more anchors."
  },
  {
    "title": "session(_:cameraDidChangeTrackingState:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionobserver/2887450-session",
    "html": "Parameters\nsession\n\nThe session providing information.\n\ncamera\n\nThe camera whose tracking parameters have changed. (Equivalent to currentFrame.camera.)\n\nDiscussion\n\nARKit tracks the position and orientation of the device relative to a virtual scene through a combination of motion sensing and image processing. As such, factors that affect signal quality from the device's motion sensing hardware or that interfere with scene detection in the camera image can result in poor estimates of the device position relative to the virtual scene.\n\nSee Also\nResponding to Tracking Quality Changes\nfunc session(ARSession, didChange: ARGeoTrackingStatus)\nListen and react to geo-tracking state changes."
  },
  {
    "title": "timestamp | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/2867973-timestamp",
    "html": "See Also\nAccessing camera data\nvar camera: ARCamera\nInformation about the camera position, orientation, and imaging parameters used to capture the frame.\nvar capturedImage: CVPixelBuffer\nA pixel buffer containing the image captured by the camera.\nvar cameraGrainIntensity: Float\nA value that specifies the amount of grain present in the camera grain texture.\nvar cameraGrainTexture: MTLTexture?\nA tileable Metal texture created by ARKit to match the visual characteristics of the current video stream.\nvar exifData: [String : Any]\nAuxiliary data for the captured image."
  },
  {
    "title": "cameraGrainIntensity | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/3255172-cameragrainintensity",
    "html": "Discussion\n\nThis property is normalized within the range [0..1], where zero specifies no grain, and one specifies the maximum amount of grain.\n\nWhen you apply this value to the depth component of the cameraGrainTexture, you select among variations of visual image noise data stored in the Metal texture that conceptually match this level of intensity.\n\nSee Also\nAccessing camera data\nvar camera: ARCamera\nInformation about the camera position, orientation, and imaging parameters used to capture the frame.\nvar capturedImage: CVPixelBuffer\nA pixel buffer containing the image captured by the camera.\nvar timestamp: TimeInterval\nThe time at which the frame was captured.\nvar cameraGrainTexture: MTLTexture?\nA tileable Metal texture created by ARKit to match the visual characteristics of the current video stream.\nvar exifData: [String : Any]\nAuxiliary data for the captured image."
  },
  {
    "title": "cameraGrainTexture | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/3255173-cameragraintexture",
    "html": "Discussion\n\nCamera grain enhances the visual cohesion of the real and augmented aspects of your user experience by enabling your app's virtual content to take on similar image noise characteristics that naturally occur in the camera feed.\n\nIf ARSCNView is your renderer, SceneKit applies camera grain to your app's virtual content by default. For more information, see rendersCameraGrain.\n\nEnable Image Noise on a Custom Renderer\n\nFor apps that display an AR experience using a custom Metal renderer, ARKit provides you with an cameraGrainTexture that matches the noise it detects in the current video stream. Set the noise texture onto your renderer to apply its characteristics to your virtual content.\n\nThe depth dimension of the image noise texture contains variations that you select at runtime based on the cameraGrainIntensity of the current frame.\n\nSee Also\nAccessing camera data\nvar camera: ARCamera\nInformation about the camera position, orientation, and imaging parameters used to capture the frame.\nvar capturedImage: CVPixelBuffer\nA pixel buffer containing the image captured by the camera.\nvar timestamp: TimeInterval\nThe time at which the frame was captured.\nvar cameraGrainIntensity: Float\nA value that specifies the amount of grain present in the camera grain texture.\nvar exifData: [String : Any]\nAuxiliary data for the captured image."
  },
  {
    "title": "displayTransform(for:viewportSize:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/2923543-displaytransform",
    "html": "Parameters\norientation\n\nThe orientation intended for presenting the view.\n\nviewportSize\n\nThe size, in points, of the view intended for rendering the camera image.\n\nReturn Value\n\nA transform matrix that converts from normalized image coordinates in the captured image to normalized image coordinates that account for the specified parameters.\n\nDiscussion\n\nNormalized image coordinates range from (0,0) in the upper left corner of the image to (1,1) in the lower right corner.\n\nThis method creates an affine transform representing the rotation and aspect-fit crop operations necessary to adapt the camera image to the specified orientation and to the aspect ratio of the specified viewport. The affine transform does not scale to the viewport's pixel size.\n\nThe capturedImage pixel buffer is the original image captured by the device camera, and thus not adjusted for device orientation or view aspect ratio.\n\nSee Also\nAccessing scene data\nvar lightEstimate: ARLightEstimate?\nAn estimate of lighting conditions based on the camera image.\nvar rawFeaturePoints: ARPointCloud?\nThe current intermediate results of the scene analysis ARKit uses to perform world tracking.\nvar capturedDepthData: AVDepthData?\nDepth data captured in front-camera experiences.\nvar capturedDepthDataTimestamp: TimeInterval\nThe time at which depth data for the frame (if any) was captured.\nvar sceneDepth: ARDepthData?\nData on the distance between a device's rear camera and real-world objects in an AR experience.\nvar smoothedSceneDepth: ARDepthData?\nAn average of distance measurements between a device's rear camera and real-world objects that creates smoother visuals in an AR experience."
  },
  {
    "title": "capturedDepthDataTimestamp | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/2928209-captureddepthdatatimestamp",
    "html": "Discussion\n\nFace-based AR (see ARFaceTrackingConfiguration) uses the front-facing, depth-sensing camera on compatible devices. When running such a configuration, frames vended by the session contain a depth map captured by the depth camera in addition to the color pixel buffer (see capturedImage) captured by the color camera. This property’s value is always zero when running other AR configurations.\n\nThe depth-sensing camera provides data at a different frame rate than the color camera, so this property’s value may not exactly match the timestamp property for the image captured by the color camera, and can also be zero if no depth data was captured at the same time as the current color image.\n\nSee Also\nAccessing scene data\nvar lightEstimate: ARLightEstimate?\nAn estimate of lighting conditions based on the camera image.\nfunc displayTransform(for: UIInterfaceOrientation, viewportSize: CGSize) -> CGAffineTransform\nReturns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.\nvar rawFeaturePoints: ARPointCloud?\nThe current intermediate results of the scene analysis ARKit uses to perform world tracking.\nvar capturedDepthData: AVDepthData?\nDepth data captured in front-camera experiences.\nvar sceneDepth: ARDepthData?\nData on the distance between a device's rear camera and real-world objects in an AR experience.\nvar smoothedSceneDepth: ARDepthData?\nAn average of distance measurements between a device's rear camera and real-world objects that creates smoother visuals in an AR experience."
  },
  {
    "title": "smoothedSceneDepth | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/3674209-smoothedscenedepth",
    "html": "Discussion\n\nThis property describes the distance between a device's camera and objects or areas in the real world, including ARKit’s confidence in the estimated distance. This is similar to sceneDepth except that the framework smoothes the depth data over time to lessen its frame-to-frame delta.\n\nThis property is nil by default. Add the smoothedSceneDepth frame semantic to your configuration’s frameSemantics to instruct the framework to populate this value with ARDepthData captured by the LiDAR scanner.\n\nCall supportsFrameSemantics(_:) on your app’s configuration to support smoothed scene depth on select devices and configurations.\n\nSee Also\nAccessing scene data\nvar lightEstimate: ARLightEstimate?\nAn estimate of lighting conditions based on the camera image.\nfunc displayTransform(for: UIInterfaceOrientation, viewportSize: CGSize) -> CGAffineTransform\nReturns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.\nvar rawFeaturePoints: ARPointCloud?\nThe current intermediate results of the scene analysis ARKit uses to perform world tracking.\nvar capturedDepthData: AVDepthData?\nDepth data captured in front-camera experiences.\nvar capturedDepthDataTimestamp: TimeInterval\nThe time at which depth data for the frame (if any) was captured.\nvar sceneDepth: ARDepthData?\nData on the distance between a device's rear camera and real-world objects in an AR experience."
  },
  {
    "title": "rawFeaturePoints | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/2887449-rawfeaturepoints",
    "html": "Discussion\n\nThese points represent notable features detected in the camera image. Their positions in 3D world coordinate space are extrapolated as part of the image analysis that ARKit performs in order to accurately track the device's position, orientation, and movement. Taken together, these points loosely correlate to the contours of real-world objects in view of the camera.\n\nARKit does not guarantee that the number and arrangement of raw feature points will remain stable between software releases, or even between subsequent frames in the same session. Regardless, the point cloud can sometimes prove useful when debugging your app's placement of virtual objects into the real-world scene.\n\nIf you display AR content with SceneKit using the ARSCNView class, you can display this point cloud with the showFeaturePoints debug option.\n\nFeature point detection requires a ARWorldTrackingConfiguration session.\n\nSee Also\nAccessing scene data\nvar lightEstimate: ARLightEstimate?\nAn estimate of lighting conditions based on the camera image.\nfunc displayTransform(for: UIInterfaceOrientation, viewportSize: CGSize) -> CGAffineTransform\nReturns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.\nvar capturedDepthData: AVDepthData?\nDepth data captured in front-camera experiences.\nvar capturedDepthDataTimestamp: TimeInterval\nThe time at which depth data for the frame (if any) was captured.\nvar sceneDepth: ARDepthData?\nData on the distance between a device's rear camera and real-world objects in an AR experience.\nvar smoothedSceneDepth: ARDepthData?\nAn average of distance measurements between a device's rear camera and real-world objects that creates smoother visuals in an AR experience."
  },
  {
    "title": "capturedDepthData | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/2928208-captureddepthdata",
    "html": "Discussion\n\nFrames vended by the session contain a depth map captured by the depth sensor in addition to the color pixel buffer (see capturedImage) captured by the color camera. The depth-sensing camera provides data at a different frame rate than the color camera, so this property’s value can be nil if no depth data was captured at the same time as the current color image.\n\nThis depth data is available only in face-based experiences (see ARFaceTrackingConfiguration) using the device's front TrueDepth camera. This property’s value is nil when running other AR configurations.\n\nSee Also\nAccessing scene data\nvar lightEstimate: ARLightEstimate?\nAn estimate of lighting conditions based on the camera image.\nfunc displayTransform(for: UIInterfaceOrientation, viewportSize: CGSize) -> CGAffineTransform\nReturns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.\nvar rawFeaturePoints: ARPointCloud?\nThe current intermediate results of the scene analysis ARKit uses to perform world tracking.\nvar capturedDepthDataTimestamp: TimeInterval\nThe time at which depth data for the frame (if any) was captured.\nvar sceneDepth: ARDepthData?\nData on the distance between a device's rear camera and real-world objects in an AR experience.\nvar smoothedSceneDepth: ARDepthData?\nAn average of distance measurements between a device's rear camera and real-world objects that creates smoother visuals in an AR experience."
  },
  {
    "title": "sceneDepth | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/3566299-scenedepth",
    "html": "Discussion\n\nThis property describes the distance between a device's camera and objects or areas in the real world, including ARKit’s confidence in the estimated distance.\n\nThis property is nil by default. Add the sceneDepth frame semantic to your configuration’s frameSemantics to instruct the framework to populate this value with ARDepthData captured by the LiDAR scanner.\n\nCall supportsFrameSemantics(_:) on your app’s configuration to support scene depth on select devices and configurations.\n\nSee Also\nAccessing scene data\nvar lightEstimate: ARLightEstimate?\nAn estimate of lighting conditions based on the camera image.\nfunc displayTransform(for: UIInterfaceOrientation, viewportSize: CGSize) -> CGAffineTransform\nReturns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.\nvar rawFeaturePoints: ARPointCloud?\nThe current intermediate results of the scene analysis ARKit uses to perform world tracking.\nvar capturedDepthData: AVDepthData?\nDepth data captured in front-camera experiences.\nvar capturedDepthDataTimestamp: TimeInterval\nThe time at which depth data for the frame (if any) was captured.\nvar smoothedSceneDepth: ARDepthData?\nAn average of distance measurements between a device's rear camera and real-world objects that creates smoother visuals in an AR experience."
  },
  {
    "title": "raycastQuery(from:allowing:alignment:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/3194578-raycastquery",
    "html": "Parameters\npoint\n\nA normalized coordinate in the UI system, where 0 is top-left, and 1 is bottom-right.\n\ntarget\n\nThe types of plane you allow this ray cast to intersect with.\n\ntargetAlignment\n\nAn alignment with respect to gravity a plane must have to interset this ray.\n\nDiscussion\n\nTo cast the ray, you pass the resulting query to your current session via raycast(_:) or trackedRaycast(_:updateHandler:).\n\nSee Also\nTracking and interacting with the real world\nvar anchors: [ARAnchor]\nThe list of anchors representing positions tracked or objects detected in the scene.\nfunc hitTest(CGPoint, types: ARHitTestResult.ResultType) -> [ARHitTestResult]\nSearches for real-world objects or AR anchors in the captured camera image.\nDeprecated"
  },
  {
    "title": "worldMappingStatus | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/2990930-worldmappingstatus",
    "html": "Discussion\n\nEvery world-tracking session builds an internal world map, which ARKit uses to determine the device's position in the user's environment. You can save a snapshot of that internal state as an ARWorldMap object, which you can use later to resume the saved session or share between devices to create a multiuser AR experience.\n\nWhile you can call getCurrentWorldMap(completionHandler:) at any time during a session, some times are better than others. Use this property to determine whether the session currently has enough data to generate a useful ARWorldMap or if it's better to wait for the session to map more of the user's local environment first.\n\nSee Also\nChecking world-mapping status\nenum ARFrame.WorldMappingStatus\nPossible values describing how thoroughly ARKit has mapped the the area visible in a given frame."
  },
  {
    "title": "hitTest(_:types:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/2875718-hittest",
    "html": "Parameters\npoint\n\nA point in normalized image coordinate space. (The point (0,0) represents the top left corner of the image, and the point (1,1) represents the bottom right corner.)\n\ntypes\n\nThe types of hit-test result to search for.\n\nReturn Value\n\nA list of results, sorted from nearest to farthest (in distance from the camera).\n\nDiscussion\n\nHit testing searches for real-world objects or surfaces detected through the AR session's processing of the camera image. A 2D point in the image coordinates can refer to any point along a 3D line that starts at the device camera and extends in a direction determined by the device orientation and camera projection. This method searches along that line, returning all objects that intersect it in order of distance from the camera.\n\nNote\n\nIf you use ARKit with a SceneKit or SpriteKit view, the ARSCNView hitTest(_:types:) or ARSKView hitTest(_:types:) method lets you specify a search point in view coordinates.\n\nThe behavior of a hit test depends on which types you specify and the order you specify them in. For details, see ARHitTestResult and the various ARHitTestResult.ResultType options.\n\nSee Also\nTracking and interacting with the real world\nvar anchors: [ARAnchor]\nThe list of anchors representing positions tracked or objects detected in the scene.\nfunc raycastQuery(from: CGPoint, allowing: ARRaycastQuery.Target, alignment: ARRaycastQuery.TargetAlignment) -> ARRaycastQuery\nGet a ray-cast query for a screen point."
  },
  {
    "title": "ARFrame.WorldMappingStatus | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/worldmappingstatus",
    "html": "Topics\nEnumeration Cases\ncase notAvailable\nNo world map is available.\ncase limited\nWorld tracking has not yet sufficiently mapped the area around the current device position.\ncase extending\nWorld tracking has mapped recently visited areas, but is still mapping around the current device position.\ncase mapped\nWorld tracking has adequately mapped the visible area.\nRelationships\nConforms To\nSendable\nSee Also\nChecking world-mapping status\nvar worldMappingStatus: ARFrame.WorldMappingStatus\nThe feasibility of generating or relocalizing a world map for this frame."
  },
  {
    "title": "geoTrackingStatus | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/3580861-geotrackingstatus",
    "html": "Discussion\n\nFor the current geo-tracking status, check the value of this property on the session’s currentFrame. ARKit populates this property only for ARGeoTrackingConfiguration sessions.\n\nSee Also\nAssessing geo-tracking condition\nclass ARGeoTrackingStatus\nThe state, accuracy, and reason that are possible for geo-tracking’s current condition."
  },
  {
    "title": "ARBody2D | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbody2d",
    "html": "Overview\n\nWhen ARKit recognizes a person in the camera feed, it estimates the screen-space location of the body's joints and provides the location to you through current frame's detectedBody.\n\nTopics\nGetting Joint Information\nvar skeleton: ARSkeleton2D\nAn object that contains the screen position of a body's joints.\nRelationships\nInherits From\nNSObject\nSee Also\nChecking for people\nvar detectedBody: ARBody2D?\nThe screen position information of a body that ARKit recognizes in the camera image.\nvar segmentationBuffer: CVPixelBuffer?\nA buffer that contains pixel information identifying the shape of objects from the camera feed that you use to occlude virtual content.\nvar estimatedDepthData: CVPixelBuffer?\nA buffer that represents the estimated depth values from the camera feed that you use to occlude virtual content.\nenum ARFrame.SegmentationClass\nA categorization of a pixel that defines a type of content you use to occlude your app's virtual content."
  },
  {
    "title": "detectedBody | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/3255174-detectedbody",
    "html": "Discussion\n\nTo enable 2D body detection, you add the bodyDetection frame semantic to your configuration's frameSemantics property, or run your session with an ARBodyTrackingConfiguration, in which body detection is enabled by default.\n\nSee Also\nChecking for people\nclass ARBody2D\nThe screen-space representation of a person ARKit recognizes in the camera feed.\nvar segmentationBuffer: CVPixelBuffer?\nA buffer that contains pixel information identifying the shape of objects from the camera feed that you use to occlude virtual content.\nvar estimatedDepthData: CVPixelBuffer?\nA buffer that represents the estimated depth values from the camera feed that you use to occlude virtual content.\nenum ARFrame.SegmentationClass\nA categorization of a pixel that defines a type of content you use to occlude your app's virtual content."
  },
  {
    "title": "anchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldmap/2968208-anchors",
    "html": "Discussion\n\nThis array is a snapshot of ARAnchor objects (including subclasses of ARAnchor) in the session as of when the world map was captured, including the transform for each. When you create a world map, ARKit automatically includes all anchors whose positions are assumed to be static with respect to the environment—that is, anchor classes that don't conform to ARTrackable. After you create a world map, you can choose to add more anchors to, or remove anchors from, this list before saving or sharing the map.\n\nWhen you run a session from a world map (and after ARKit successfully reconciles the map to the user's current environment), the session automatically includes all anchors from that world map. You can then use those anchors to recreate the placement of virtual content so that it matches the session the world map was captured in.\n\nSee Also\nExamining a World Map\nvar center: simd_float3\nThe center point of the world map's space-mapping data, relative to the world coordinate origin of the session the map was recorded in.\nvar extent: simd_float3\nThe size of the world map's space-mapping data, relative to the world coordinate origin of the session the map was recorded in."
  },
  {
    "title": "geometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/2928271-geometry",
    "html": "Discussion\n\nThis mesh provides vertex, index, and texture coordinate buffers describing the 3D shape of the face, conforming a generic face model to match the dimensions, shape, and current expression of the detected face.\n\nYou can visualize the face geometry by passing these buffers to your preferred rendering engine. To visualize a face geometry using SceneKit, create an ARSCNFaceGeometry instance and use its update(from:) method to update it to match the face geometry."
  },
  {
    "title": "ARFaceGeometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacegeometry",
    "html": "Overview\n\nThis class provides a general model for the detailed topology of a face, in the form of a 3D mesh appropriate for use with various rendering technologies or for exporting 3D assets. (For a quick way to visualize a face geometry using SceneKit, see the ARSCNFaceGeometry class.)\n\nWhen you obtain a face geometry from an ARFaceAnchor object in a face-tracking AR session, the model conforms to match the dimensions, shape, and current expression of the detected face. You can also create a face mesh using a dictionary of named blend shape coefficients, which provides a detailed, but more efficient, description of the face’s current expression.\n\nIn an AR session, you can use this model as the basis for overlaying content that follows the shape of the user’s face—for example, to apply virtual makeup or tattoos. You can also use this model to create occlusion geometry, which hides other virtual content behind the 3D shape of the detected face in the camera image.\n\nNote\n\nFace mesh topology is constant across ARFaceGeometry instances. That is, the values of the vertexCount, textureCoordinateCount, and triangleCount properties never change, the triangleIndices buffer always describes the same arrangement of vertices, and the textureCoordinates buffer always maps the same vertex indices to the same texture coordinates.\n\nOnly the vertices buffer changes between face meshes provided by an AR session, indicating the change in vertex positions as ARKit adapts the mesh to the shape and expression of the user's face.\n\nTopics\nAccessing Mesh Data\nvar vertices: [simd_float3]\nAn array of vertex positions for each point in the face mesh.\nvar textureCoordinates: [vector_float2]\nAn array of texture coordinate values for each point in the face mesh.\nvar triangleCount: Int\nThe number of triangles described by the triangleIndices buffer.\nvar triangleIndices: [Int16]\nAn array of indices describing the triangle mesh formed by the face geometry's vertex data.\nCreating a Mesh from Blend Shapes\ninit?(blendShapes: [ARFaceAnchor.BlendShapeLocation : NSNumber])\nCreates a face geometry matching the facial expression described in the specified dictionary.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nSee Also\nFace Data\nTracking and Visualizing Faces\nDetect faces in a front-camera AR experience, overlay virtual content, and animate facial expressions in real-time.\nCombining User Face-Tracking and World Tracking\nTrack the user’s face in an app that displays an AR experience with the rear camera.\nclass ARSCNFaceGeometry\nA SceneKit representation of face topology for use with face information that an AR session provides."
  },
  {
    "title": "center | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldmap/2968209-center",
    "html": "Discussion\n\nThe extent and center properties together define a bounding box for the data recorded in the world map.\n\nSee Also\nExamining a World Map\nvar anchors: [ARAnchor]\nThe set of anchors recorded in the world map.\nvar extent: simd_float3\nThe size of the world map's space-mapping data, relative to the world coordinate origin of the session the map was recorded in."
  },
  {
    "title": "extent | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldmap/2968211-extent",
    "html": "Discussion\n\nThe extent and center properties together define a bounding box for the data recorded in the world map.\n\nSee Also\nExamining a World Map\nvar anchors: [ARAnchor]\nThe set of anchors recorded in the world map.\nvar center: simd_float3\nThe center point of the world map's space-mapping data, relative to the world coordinate origin of the session the map was recorded in."
  },
  {
    "title": "anchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/2867988-anchors",
    "html": "Discussion\n\nYou can manually add or remove anchors to track locations in the scene using the ARSession class. Depending on session configuration, ARKit may also add anchors, such as the origin of the world coordinate system or automatically detected planes.\n\nSee Also\nTracking and interacting with the real world\nfunc raycastQuery(from: CGPoint, allowing: ARRaycastQuery.Target, alignment: ARRaycastQuery.TargetAlignment) -> ARRaycastQuery\nGet a ray-cast query for a screen point.\nfunc hitTest(CGPoint, types: ARHitTestResult.ResultType) -> [ARHitTestResult]\nSearches for real-world objects or AR anchors in the captured camera image.\nDeprecated"
  },
  {
    "title": "rawFeaturePoints | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldmap/2968213-rawfeaturepoints",
    "html": "Discussion\n\nThese points represent notable features detected in camera imagery during the session that recorded the world map. ARKit extrapolates the locations of these features in 3D world coordinate space as part of the image and motion analysis that tracks the device's movement in a session. Taken together, these points loosely correlate to the contours of real-world objects that were in view of the camera during the session.\n\nARKit does not guarantee that the number and arrangement of raw feature points will remain stable between software releases. However, you can visualize the point cloud to debug your world map recording, or inspect its size to estimate the quality of a recorded world map."
  },
  {
    "title": "blendShapes | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/2928251-blendshapes",
    "html": "Discussion\n\nEach key in this dictionary (an ARFaceAnchor.BlendShapeLocation constant) represents one of many specific facial features recognized by ARKit. The corresponding value for each key is a floating point number indicating the current position of that feature relative to its neutral configuration, ranging from 0.0 (neutral) to 1.0 (maximum movement).\n\nYou can use blend shape coefficients to animate a 2D or 3D character in ways that follow the user’s facial expressions. ARKit provides many blend shape coefficients, resulting in a detailed model of a facial expression; however, you can use as many or as few of the coefficients as you desire to create a visual effect. For example, you might animate a simple cartoon character using only the jawOpen, eyeBlinkLeft, and eyeBlinkRight coefficients. A professional 3D artist could create a detailed character model rigged for realistic animation using a larger set, or the entire set, of coefficients.\n\nYou can also use blend shape coefficients to record a specific facial expression and reuse it later. The ARFaceGeometry init(blendShapes:) initializer creates a detailed 3D mesh from a dictionary equivalent to this property’s value; the serialized form of a blend shapes dictionary is more portable than that of the face mesh those coefficients describe.\n\nSee Also\nUsing Blend Shapes\nstruct ARFaceAnchor.BlendShapeLocation\nIdentifiers for specific facial features, for use with coefficients describing the relative movements of those features."
  },
  {
    "title": "OS_ar_authorization_result | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_authorization_result",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "applyingTransform(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/2977514-applyingtransform",
    "html": "Parameters\ntransform\n\nA transform matrix in the local coordinate space of the reference object.\n\nReturn Value\n\nThe transformed reference object.\n\nDiscussion\n\nYou define the local coordinate space of a reference object when you extract it from an ARWorldMap. If an existing reference object has a local coordinate origin that doesn't fit well with the object's intended use, call this method to change the reference object's origin with respect to the physical object it represents.\n\nWhen ARKit detects a reference object, the transform of the resulting ARObjectAnchor is based on the orgin of the reference object's coordinate system. For example, if a reference object represents a physical item that sits on a horizontal surface, virtual content should appear to sit on whatever surface the physical object does. As such, it's typically useful to align a reference object's coordinate origin with the bottom of the physical object.\n\nSee Also\nCreating Derivative Reference Objects\nfunc merging(ARReferenceObject) -> ARReferenceObject\nReturns a new reference object that combines spatial information from both this reference object and another."
  },
  {
    "title": "rendersMotionBlur | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/3182993-rendersmotionblur",
    "html": "Discussion\n\nThis property is enabled by default. When set, the view automatically adds motion blur to rendered content which matches the visual characteristics of the motion blur ARKit observes in the camera feed.\n\nThe value of this property overwrites the motionBlurIntensity of SCNCamera.\n\nSee Also\nManaging Rendering Effects\nvar rendersCameraGrain: Bool\nA flag that determines whether SceneKit applies image noise characteristics to your app's virtual content."
  },
  {
    "title": "rendersCameraGrain | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview/3255193-renderscameragrain",
    "html": "Discussion\n\nEnabled by default. When set, SceneKit adds a camera grain effect to your app's virtual content that matches the image noise characteristics ARKit observes in the camera feed.\n\nSee Also\nManaging Rendering Effects\nvar rendersMotionBlur: Bool\nDetermines whether the view renders motion blur."
  },
  {
    "title": "ARFaceAnchor.BlendShapeLocation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation",
    "html": "Overview\n\nThe blendShapes dictionary provided by an ARFaceAnchor object describes the facial expression of a detected face in terms of the movements of specific facial features. For each key in the dictionary, the corresponding value is a floating point number indicating the current position of that feature relative to its neutral configuration, ranging from 0.0 (neutral) to 1.0 (maximum movement).\n\nARKit provides many blend shape coefficients, resulting in a detailed model of a facial expression; however, you can use as many or as few of the coefficients as you desire to create a visual effect. For example, you might animate a simple cartoon character using only the jawOpen, eyeBlinkLeft, and eyeBlinkRight coefficients. A professional 3D artist could create a detailed character model rigged for realistic animation using a larger set, or the entire set, of coefficients.\n\nNote\n\nIn the naming of blend shape coefficients, the left and right directions are relative to the face. That is, the eyeBlinkRight coefficient refers to the face's right eye. ARKit views running a face-tracking session mirror the camera image, so the face's right eye appears on the right side in the view.\n\nTopics\nLeft Eye\nstatic let eyeBlinkLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the eyelids over the left eye.\nstatic let eyeLookDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a downward gaze.\nstatic let eyeLookInLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a rightward gaze.\nstatic let eyeLookOutLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with a leftward gaze.\nstatic let eyeLookUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the left eyelids consistent with an upward gaze.\nstatic let eyeSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of the face around the left eye.\nstatic let eyeWideLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a widening of the eyelids around the left eye.\nRight Eye\nstatic let eyeBlinkRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the eyelids over the right eye.\nstatic let eyeLookDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a downward gaze.\nstatic let eyeLookInRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a leftward gaze.\nstatic let eyeLookOutRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with a rightward gaze.\nstatic let eyeLookUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the right eyelids consistent with an upward gaze.\nstatic let eyeSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of the face around the right eye.\nstatic let eyeWideRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a widening of the eyelids around the right eye.\nMouth and Jaw\nstatic let jawForward: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing forward movement of the lower jaw.\nstatic let jawLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the lower jaw.\nstatic let jawRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the lower jaw.\nstatic let jawOpen: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing an opening of the lower jaw.\nstatic let mouthClose: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing closure of the lips independent of jaw position.\nstatic let mouthFunnel: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction of both lips into an open shape.\nstatic let mouthPucker: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing contraction and compression of both closed lips.\nstatic let mouthLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of both lips together.\nstatic let mouthRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of both lips together.\nstatic let mouthSmileLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the left corner of the mouth.\nstatic let mouthSmileRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the right corner of the mouth.\nstatic let mouthFrownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the left corner of the mouth.\nstatic let mouthFrownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the right corner of the mouth.\nstatic let mouthDimpleLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the left corner of the mouth.\nstatic let mouthDimpleRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing backward movement of the right corner of the mouth.\nstatic let mouthStretchLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing leftward movement of the left corner of the mouth.\nstatic let mouthStretchRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing rightward movement of the left corner of the mouth.\nstatic let mouthRollLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the lower lip toward the inside of the mouth.\nstatic let mouthRollUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing movement of the upper lip toward the inside of the mouth.\nstatic let mouthShrugLower: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the lower lip.\nstatic let mouthShrugUpper: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of the upper lip.\nstatic let mouthPressLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the left side.\nstatic let mouthPressRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward compression of the lower lip on the right side.\nstatic let mouthLowerDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the left side.\nstatic let mouthLowerDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the lower lip on the right side.\nstatic let mouthUpperUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the left side.\nstatic let mouthUpperUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the upper lip on the right side.\nEyebrows, Cheeks, and Nose\nstatic let browDownLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the left eyebrow.\nstatic let browDownRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing downward movement of the outer portion of the right eyebrow.\nstatic let browInnerUp: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the inner portion of both eyebrows.\nstatic let browOuterUpLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the left eyebrow.\nstatic let browOuterUpRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the outer portion of the right eyebrow.\nstatic let cheekPuff: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing outward movement of both cheeks.\nstatic let cheekSquintLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the left eye.\nstatic let cheekSquintRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing upward movement of the cheek around and below the right eye.\nstatic let noseSneerLeft: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the left side of the nose around the nostril.\nstatic let noseSneerRight: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing a raising of the right side of the nose around the nostril.\nTongue\nstatic let tongueOut: ARFaceAnchor.BlendShapeLocation\nThe coefficient describing extension of the tongue.\nCreating a Blend Shape Location\ninit(rawValue: String)\nCreates a blend shape location.\nRelationships\nConforms To\nHashable\nRawRepresentable\nSendable\nSee Also\nUsing Blend Shapes\nvar blendShapes: [ARFaceAnchor.BlendShapeLocation : NSNumber]\nA dictionary of named coefficients representing the detected facial expression in terms of the movement of specific facial features."
  },
  {
    "title": "isTracked | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imageanchor/4108431-istracked",
    "html": "Relationships\nFrom Protocol\nTrackableAnchor\nSee Also\nGetting image information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of the image in world space.\nvar referenceImage: ReferenceImage\nThe reference image that this image anchor tracks.\nvar estimatedScaleFactor: Float\nThe estimated scale factor between the tracked image’s physical size and the reference image’s size.\nvar description: String\nA textual description of an image anchor."
  },
  {
    "title": "id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imageanchor/4108429-id",
    "html": "Relationships\nFrom Protocol\nAnchor\nIdentifiable\nSee Also\nIdentifying image anchors\ntypealias ImageAnchor.ID\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when ImageAnchor conforms to AnyObject."
  },
  {
    "title": "MeshAnchor.MeshClassification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/meshclassification",
    "html": "Topics\nGetting architecture classifications\ncase ceiling\nA ceiling.\ncase door\nA door.\ncase floor\nA floor.\ncase stairs\nA set of stairs.\ncase wall\nA wall.\ncase window\nA window.\nGetting furniture classifications\ncase bed\nA bed.\ncase cabinet\nA cabinet.\ncase homeAppliance\nA home appliance.\ncase seat\nA seat.\ncase table\nA table.\nGetting decoration classifications\ncase plant\nA plant.\ncase tv\nA television.\nGetting unknown classifications\ncase none\nAn unknown classification.\nInspecting classifications\ninit?(rawValue: NSInteger)\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar rawValue: NSInteger\ntypealias MeshAnchor.MeshClassification.RawValue\nstatic func != (MeshAnchor.MeshClassification, MeshAnchor.MeshClassification) -> Bool\nRelationships\nConforms To\nSendable\nSee Also\nGetting mesh information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a mesh in world space.\nvar geometry: MeshAnchor.Geometry\nThe shape of a mesh anchor.\nstruct MeshAnchor.Geometry\nThe shapes that make up a mesh anchor."
  },
  {
    "title": "neutralPose | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/4284990-neutralpose",
    "html": "Discussion\n\nFor example, you might use this property as part of visualizing all of the available joints in a skeleton."
  },
  {
    "title": "OS_ar_world_tracking_provider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_world_tracking_provider",
    "html": "Relationships\nInherits From\nOS_ar_data_provider\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/4108460-id",
    "html": "Relationships\nFrom Protocol\nAnchor\nIdentifiable\nSee Also\nInspecting mesh anchors\ntypealias MeshAnchor.ID\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when MeshAnchor conforms to AnyObject.\nvar description: String"
  },
  {
    "title": "geometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/4108459-geometry",
    "html": "See Also\nGetting mesh information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a mesh in world space.\nstruct MeshAnchor.Geometry\nThe shapes that make up a mesh anchor.\nenum MeshAnchor.MeshClassification\nThe kinds of classification a face of a mesh can have."
  },
  {
    "title": "OS_ar_reference_image | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_reference_image",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_plane_anchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_plane_anchors",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "leftEyeTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/2968191-lefteyetransform",
    "html": "Discussion\n\nThe translation aspect of this matrix indicates the position of the center of the eyeball, relative to the position represented by the anchor's transform. The positive z-axis points from the center of the eyeball in the direction of the pupil.\n\nRotational aspects of the matrix indicate the orientation of the eyeball—for example, a rotation about the x-axis directs the pupil upward or downward. The eye does not rotate about the z-axis.\n\nSee Also\nTracking Eye Movement\nvar rightEyeTransform: simd_float4x4\nA transform matrix indicating the position and orientation of the face's right eye.\nvar lookAtPoint: simd_float3\nA position in face coordinate space estimating the direction of the face's gaze."
  },
  {
    "title": "center | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/2968197-center",
    "html": "Discussion\n\nThe extent and center properties together define a bounding box for the data recorded in the reference object in its local coordinate system. You define that coordinate system with the transform parameter when calling extractReferenceObject(transform:center:extent:), and can modify it by creating another reference object with applyingTransform(_:).\n\nSee Also\nExamining a Reference Object\nvar name: String?\nA descriptive name for the reference object.\nvar resourceGroupName: String?\nvar extent: simd_float3\nThe size of the reference object's space-mapping data.\nvar scale: simd_float3\nA scale factor for the local coordinate space the reference object defines."
  },
  {
    "title": "rightEyeTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/2968193-righteyetransform",
    "html": "Discussion\n\nThe translation aspect of this matrix indicates the position of the center of the eyeball, relative to the position represented by the anchor's transform. The positive z-axis points from the center of the eyeball in the direction of the pupil.\n\nRotational aspects of the matrix indicate the orientation of the eyeball—for example, a rotation about the x-axis directs the pupil upward or downward. The eye does not rotate about the z-axis.\n\nSee Also\nTracking Eye Movement\nvar leftEyeTransform: simd_float4x4\nA transform matrix indicating the position and orientation of the face's left eye.\nvar lookAtPoint: simd_float3\nA position in face coordinate space estimating the direction of the face's gaze."
  },
  {
    "title": "lookAtPoint | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor/2968192-lookatpoint",
    "html": "Discussion\n\nThis vector abstracts from the leftEyeTransform and rightEyeTransform matrices to estimate what point, relative to the face, the user's eyes are focused upon. For example:\n\nIf the user is looking to the left, the vector has a positive x-axis component.\n\nIf the user is focused on a nearby object, the vector's length is shorter.\n\nIf the user is focused on a faraway object, the vector's length is longer.\n\nSee Also\nTracking Eye Movement\nvar leftEyeTransform: simd_float4x4\nA transform matrix indicating the position and orientation of the face's left eye.\nvar rightEyeTransform: simd_float4x4\nA transform matrix indicating the position and orientation of the face's right eye."
  },
  {
    "title": "extent | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject/2968199-extent",
    "html": "Discussion\n\nThe extent and center properties together define a bounding box for the data recorded in the reference object in its local coordinate system. You define that coordinate system with the transform parameter when calling extractReferenceObject(transform:center:extent:), and can modify it by creating another reference object with applyingTransform(_:).\n\nSee Also\nExamining a Reference Object\nvar name: String?\nA descriptive name for the reference object.\nvar resourceGroupName: String?\nvar center: simd_float3\nThe center point of the reference object's space-mapping data.\nvar scale: simd_float3\nA scale factor for the local coordinate space the reference object defines."
  },
  {
    "title": "ARReferenceObject | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceobject",
    "html": "Overview\n\nObject detection in ARKit lets you trigger AR content when the session recognizes a known 3D object. For example, your app could detect sculptures in an art museum and provide a virtual curator, or detect tabletop gaming figures and create visual effects for the game.\n\nTo provide a known 3D object for detection, you scan a real-world object using ARKit:\n\nRun an AR session using ARObjectScanningConfiguration to enable collection of high-fidelity spatial mapping data.\n\nIn that session, point the device camera at the real-world object from various angles, allowing ARKit to build up an internal map of the object and its surroundings. For an example of guiding user interactions to produce good scan data, see Scanning and detecting 3D objects.\n\nDetermine the portion of the session's world coordinate space representing the object to be recognized, and call createReferenceObject(transform:center:extent:completionHandler:) to get that portion as an ARReferenceObject ready for use in object detection.\n\nTo save the reference object for use later or elsewhere, use the export(to:previewImage:) method to create an .arobject file.\n\nTo detect objects in an AR session, pass a collection of reference objects to your session configuration's detectionObjects property. You need not scan and detect objects in the same app: For example, you might create one app for scanning museum collections that outputs .arobject files, then bundle those files into another app meant for museum visitors.\n\nTo bundle reference objects into an app, use your Xcode project's asset catalog:\n\nIn your asset catalog, use the Add (+) button to create an AR Resource Group.\n\nDrag .arobject into the resource group to create AR Reference Object entries in the asset catalog.\n\nOptionally, use the Xcode inspector panel to provide a descriptive name for the object, which appears as the name property at runtime and can be useful for debugging.\n\nTopics\nLoading Reference Objects\ninit(archiveURL: URL)\nLoads a reference object from the specified file URL.\nclass func referenceObjects(inGroupNamed: String, bundle: Bundle?) -> Set<ARReferenceObject>?\nLoads all reference objects in the specified AR Resource Group in your Xcode project's asset catalog.\nExamining a Reference Object\nvar name: String?\nA descriptive name for the reference object.\nvar resourceGroupName: String?\nvar center: simd_float3\nThe center point of the reference object's space-mapping data.\nvar extent: simd_float3\nThe size of the reference object's space-mapping data.\nvar scale: simd_float3\nA scale factor for the local coordinate space the reference object defines.\nSaving Recorded Objects\nfunc export(to: URL, previewImage: UIImage?)\nWrites a binary representation of the object to the specified file URL.\nclass let archiveExtension: String\nThe standard filename extension for exported ARReferenceObject instances.\nCreating Derivative Reference Objects\nfunc applyingTransform(simd_float4x4) -> ARReferenceObject\nReturns a new reference object created by applying the specified transform to this reference object's geometric data.\nfunc merging(ARReferenceObject) -> ARReferenceObject\nReturns a new reference object that combines spatial information from both this reference object and another.\nDebugging a Reference Object\nvar rawFeaturePoints: ARPointCloud\nA coarse representation of the space-mapping data contained in the reference object.\nRelationships\nInherits From\nNSObject\nConforms To\nNSSecureCoding\nSee Also\nPhysical Objects\nVisualizing and Interacting with a Reconstructed Scene\nEstimate the shape of the physical environment using a polygonal mesh.\nScanning and detecting 3D objects\nRecord spatial features of real-world objects, then use the results to find those objects in the user’s environment and trigger AR content.\nclass ARObjectAnchor\nAn anchor for a real-world 3D object that ARKit detects in the physical environment."
  },
  {
    "title": "loadReferenceImages(inGroupNamed:bundle:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/referenceimage/4131836-loadreferenceimages",
    "html": "Parameters\ngroupName\n\nThe name of the group of assets in an asset catalog.\n\nbundle\n\nThe bundle that contains the image assets. If nil, this method loads reference images from the main bundle.\n\nReturn Value\n\nAn array of reference images loaded from the group you specify.\n\nSee Also\nCreating a reference image\ninit(cgimage: CGImage, physicalSize: CGSize, orientation: CGImagePropertyOrientation)\nCreates a reference image from a Core Graphics image.\ninit(pixelBuffer: CVPixelBuffer, physicalSize: CGSize, orientation: CGImagePropertyOrientation)\nCreates a reference image from a pixel buffer."
  },
  {
    "title": "Capturing Body Motion in 3D | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/capturing_body_motion_in_3d",
    "html": "Overview\n\nNote\n\nThis sample code project is associated with WWDC 2019 session 607: Bringing People into AR.\n\nNote\n\nTo run the app, use an iOS device with A12 chip or later.\n\nSee Also\nBody Position Tracking\nRigging a Model for Motion Capture\nConfigure custom 3D models so ARKit’s human body-tracking feature can control them.\nValidating a Model for Motion Capture\nVerify that your character model matches ARKit’s Motion Capture requirements.\nclass ARBodyAnchor\nAn anchor that tracks the position and movement of a human body in the rear-facing camera."
  },
  {
    "title": "detectionObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/2968177-detectionobjects",
    "html": "Discussion\n\nUse this property to choose known 3D objects for ARKit to find in the user's environment and present as ARObjectAnchor for use in your AR experience.\n\nTo create reference objects for detection, scan them in a world-tracking session and use ARWorldMap to extract ARReferenceObject instances. You can then save reference objects as files and package them in any ARKit app you create using an Xcode asset catalog."
  },
  {
    "title": "Validating a Model for Motion Capture | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/validating_a_model_for_motion_capture",
    "html": "Overview\n\nARKit’s body-tracking functionality requires models to be in a specific format. Models that don’t match the expected format may work incorrectly, or not work at all. Use the detailed information in this article to verify that your character’s scene coordinate system and orientation match ARKit’s expectations, and ensure that your skeleton matches ARKit’s expected joint names and hierarchy for Motion Capture.\n\nAdditionally, you can verify models that you’ve rigged according to Rigging a Model for Motion Capture, and use the information to modify your existing rigged humanoid models to match the expected format.\n\nConfirm Scene and Model Coordinate Systems\n\nYour exported model must use a scene coordinate system that uses +Y for up, +Z for forward, and +X for right. Your model also must face forward on the +Z axis.\n\nIf your 3D software package doesn’t use this coordinate system by default, you may be able to reconfigure your scene settings or transform the coordinate system as part of the export process.\n\nValidate Joint Names, Relationships, and Orientation\n\nThe joints that make up your model’s skeleton must exactly match ARKit’s required joint names. Additionally, the relationship between joints must also match ARKit’s layout.\n\nAlthough your skeleton must contain all the expected joints in the correct hierarchy, you don’t have to bind vertices to every bone. If your character doesn’t need moving eyes, for example, its skeleton must still have the bones that control the eyes, but you can simply bind no part of your model to those bones so they have no affect on your model.\n\nMake sure to match the orientation of every joint in your model in its bind post to match the values in the model biped skeleton.\n\nNote\n\nJoints in a skeletal model represent any potential point of articulation. While many of the joints correspond to anatomical joints, others simply represent a portion of the model that can be moved or deformed in some way.\n\nConfirm Bind Pose and Binding\n\nYour characters should be rigged in a standard T-pose. Although some software packages allow you to bind your character mesh in multiple poses, your character model shouldn’t have any additional bind poses. No more than four joints should contribute to the movement of any single vertex in your model and you shouldn’t add any animation keyframes to your character.\n\nMatch Torso Joints\n\nThe torso of a puppeteering skeleton should contain eight joints: the hip joint, which is the root of the joint hierarchy, and seven spine joints. The hip joint and first spine joint overlap when the character is in the T-pose position.\n\nJoint name\n\n\t\n\nParent joint\n\n\n\n\nhips_joint\n\n\t\n\nroot\n\n\n\n\nspine_1_joint\n\n\t\n\nhips_joint\n\n\n\n\nspine_2_joint\n\n\t\n\nspine_1_joint\n\n\n\n\nspine_3_joint\n\n\t\n\nspine_2_joint\n\n\n\n\nspine_4_joint\n\n\t\n\nspine_3_joint\n\n\n\n\nspine_5_joint\n\n\t\n\nspine_4_joint\n\n\n\n\nspine_6_joint\n\n\t\n\nspine_5_joint\n\n\n\n\nspine_7_joint\n\n\t\n\nspine_6_joint\n\nReview Head and Neck Joints\n\nThe head contains four neck joints extending from the spine, as well as joints for controlling the specific parts of the head, including the eyes and eyelids, nose, chin, and jaw.\n\nJoint name\n\n\t\n\nParent joint\n\n\n\n\nneck_1_joint\n\n\t\n\nspine_7_joint\n\n\n\n\nneck_2_joint\n\n\t\n\nneck_1_joint\n\n\n\n\nneck_3_joint\n\n\t\n\nneck_2_joint\n\n\n\n\nneck_4_joint\n\n\t\n\nneck_3_joint\n\n\n\n\nhead_joint\n\n\t\n\nneck_4_joint\n\n\n\n\njaw_joint\n\n\t\n\nhead_joint\n\n\n\n\nchin_joint\n\n\t\n\njaw_joint\n\n\n\n\nnose_joint\n\n\t\n\nhead_joint\n\n\n\n\nright_eye_joint\n\n\t\n\nhead_joint\n\n\n\n\nright_eyeUpperLid_joint\n\n\t\n\nright_eye_joint\n\n\n\n\nright_eyeLowerLid_joint\n\n\t\n\nright_eye_joint\n\n\n\n\nright_eyeball_joint\n\n\t\n\nright_eye_joint\n\n\n\n\nleft_eye_joint\n\n\t\n\nhead_joint\n\n\n\n\nleft_eyeUpperLid_joint\n\n\t\n\nleft_eye_joint\n\n\n\n\nleft_eyeLowerLid_joint\n\n\t\n\nleft_eye_joint\n\n\n\n\nleft_eyeball_joint\n\n\t\n\nleft_eye_joint\n\nReview Arm and Shoulder Joints\n\nBoth arms descend from the seventh spine joint and are comprised of three joints, representing the shoulder, elbow, and wrist.\n\nJoint name\n\n\t\n\nParent joint\n\n\n\n\nright_shoulder_1_joint\n\n\t\n\nspine_7_joint\n\n\n\n\nright_arm_joint\n\n\t\n\nright_shoulder_1_joint\n\n\n\n\nright_forearm_joint\n\n\t\n\nright_arm_joint\n\n\n\n\nleft_shoulder_1_joint\n\n\t\n\nspine_7_joint\n\n\n\n\nleft_arm_joint\n\n\t\n\nleft_shoulder_1_joint\n\n\n\n\nleft_forearm_joint\n\n\t\n\nleft_arm_joint\n\nReview Leg and Foot Joints\n\nThe leg and foot joints of the model descend from the hips, with joints for moving the upper legs, lower legs, feet, and toes.\n\nJoint name\n\n\t\n\nParent joint\n\n\n\n\nleft_upLeg_joint\n\n\t\n\nhips_joint\n\n\n\n\nleft_leg_joint\n\n\t\n\nleft_upLeg_joint\n\n\n\n\nleft_foot_joint\n\n\t\n\nleft_leg_joint\n\n\n\n\nleft_toes_joint\n\n\t\n\nleft_foot_joint\n\n\n\n\nleft_toesEnd_joint\n\n\t\n\nleft_toes_joint\n\n\n\n\nright_upLeg_joint\n\n\t\n\nhips_joint\n\n\n\n\nright_leg_joint\n\n\t\n\nright_upLeg_joint\n\n\n\n\nright_foot_joint\n\n\t\n\nright_leg_joint\n\n\n\n\nright_toes_joint\n\n\t\n\nright_foot_joint\n\n\n\n\nright_toesEnd_joint\n\n\t\n\nright_toes_joint\n\nReview Right-Hand Joints\n\nThe hands contain many joints, with each of the four fingers comprised of five joints descending from the hand. The thumb has only four joints, but also descends from the hand joint.\n\nJoint name\n\n\t\n\nParent joint\n\n\n\n\nright_hand_joint\n\n\t\n\nright_forearm_joint\n\n\n\n\nright_handPinkyStart_joint\n\n\t\n\nright_hand_joint\n\n\n\n\nright_handPinky_1_joint\n\n\t\n\nright_handPinkyStart_joint\n\n\n\n\nright_handPinky_2_joint\n\n\t\n\nright_handPinky_1_joint\n\n\n\n\nright_handPinky_3_joint\n\n\t\n\nright_handPinky_2_joint\n\n\n\n\nright_handPinkyEnd_joint\n\n\t\n\nright_handPinky_3_joint\n\n\n\n\nright_handRingStart_joint\n\n\t\n\nright_hand_joint\n\n\n\n\nright_handRing_1_joint\n\n\t\n\nright_handRingStart_joint\n\n\n\n\nright_handRing_2_joint\n\n\t\n\nright_handRing_1_joint\n\n\n\n\nright_handRing_3_joint\n\n\t\n\nright_handRing_2_joint\n\n\n\n\nright_handRingEnd_joint\n\n\t\n\nright_handRing_3_joint\n\n\n\n\nright_handMidStart_joint\n\n\t\n\nright_hand_joint\n\n\n\n\nright_handMid_1_joint\n\n\t\n\nright_handMidStart_joint\n\n\n\n\nright_handMid_2_joint\n\n\t\n\nright_handMid_1_joint\n\n\n\n\nright_handMid_3_joint\n\n\t\n\nright_handMid_2_joint\n\n\n\n\nright_handMidEnd_joint\n\n\t\n\nright_handMid_3_joint\n\n\n\n\nright_handIndexStart_joint\n\n\t\n\nright_hand_joint\n\n\n\n\nright_handIndex_1_joint\n\n\t\n\nright_handIndexStart_joint\n\n\n\n\nright_handIndex_2_joint\n\n\t\n\nright_handIndex_1_joint\n\n\n\n\nright_handIndex_3_joint\n\n\t\n\nright_handIndex_2_joint\n\n\n\n\nright_handIndexEnd_joint\n\n\t\n\nright_handIndex_3_joint\n\n\n\n\nright_handThumbStart_joint\n\n\t\n\nright_hand_joint\n\n\n\n\nright_handThumb_1_joint\n\n\t\n\nright_handThumbStart_joint\n\n\n\n\nright_handThumb_2_joint\n\n\t\n\nright_handThumb_1_joint\n\n\n\n\nright_handThumbEnd_joint\n\n\t\n\nright_handThumb_2_joint\n\nReview Left-Hand Joints\n\nThe structure and naming convention of the joints of the left hand are similar to those of the right.\n\nJoint name\n\n\t\n\nParent joint\n\n\n\n\nleft_hand_joint\n\n\t\n\nleft_forearm_joint\n\n\n\n\nleft_handPinkyStart_joint\n\n\t\n\nleft_hand_joint\n\n\n\n\nleft_handPinky_1_joint\n\n\t\n\nleft_handPinkyStart_joint\n\n\n\n\nleft_handPinky_2_joint\n\n\t\n\nleft_handPinky_1_joint\n\n\n\n\nleft_handPinky_3_joint\n\n\t\n\nleft_handPinky_2_joint\n\n\n\n\nleft_handPinkyEnd_joint\n\n\t\n\nleft_handPinky_3_joint\n\n\n\n\nleft_handRingStart_joint\n\n\t\n\nleft_hand_joint\n\n\n\n\nleft_handRing_1_joint\n\n\t\n\nleft_handRingStart_joint\n\n\n\n\nleft_handRing_2_joint\n\n\t\n\nleft_handRing_1_joint\n\n\n\n\nleft_handRing_3_joint\n\n\t\n\nleft_handRing_2_joint\n\n\n\n\nleft_handRingEnd_joint\n\n\t\n\nleft_handRing_3_joint\n\n\n\n\nleft_handMidStart_joint\n\n\t\n\nleft_hand_joint\n\n\n\n\nleft_handMid_1_joint\n\n\t\n\nleft_handMidStart_joint\n\n\n\n\nleft_handMid_2_joint\n\n\t\n\nleft_handMid_1_joint\n\n\n\n\nleft_handMid_3_joint\n\n\t\n\nleft_handMid_2_joint\n\n\n\n\nleft_handMidEnd_joint\n\n\t\n\nleft_handMid_3_joint\n\n\n\n\nleft_handIndexStart_joint\n\n\t\n\nleft_hand_joint\n\n\n\n\nleft_handIndex_1_joint\n\n\t\n\nleft_handIndexStart_joint\n\n\n\n\nleft_handIndex_2_joint\n\n\t\n\nleft_handIndex_1_joint\n\n\n\n\nleft_handIndex_3_joint\n\n\t\n\nleft_handIndex_2_joint\n\n\n\n\nleft_handIndexEnd_joint\n\n\t\n\nleft_handIndex_3_joint\n\n\n\n\nleft_handThumbStart_joint\n\n\t\n\nleft_hand_joint\n\n\n\n\nleft_handThumb_1_joint\n\n\t\n\nleft_handThumbStart_joint\n\n\n\n\nleft_handThumb_2_joint\n\n\t\n\nleft_handThumb_1_joint\n\n\n\n\nleft_handThumbEnd_joint\n\n\t\n\nleft_handThumb_2_joint\n\nSee Also\nBody Position Tracking\nCapturing Body Motion in 3D\nTrack a person in the physical environment and visualize their motion by applying the same body movements to a virtual character.\nRigging a Model for Motion Capture\nConfigure custom 3D models so ARKit’s human body-tracking feature can control them.\nclass ARBodyAnchor\nAn anchor that tracks the position and movement of a human body in the rear-facing camera."
  },
  {
    "title": "Rigging a Model for Motion Capture | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/rigging_a_model_for_motion_capture",
    "html": "Overview\n\nARKit recognizes and tracks a person’s movements using an iOS device’s rear camera. RealityKit applies the detected motion to a 3D character model in real time, allowing the person on camera to control the movement of the 3D model, much like a virtual puppet. You can try out this feature by downloading sample code in Capturing Body Motion in 3D.\n\nConfigure Your Model\n\nYou can configure your own models so RealityKit’s puppeteering functionality can control them. Your character model must be a skeletal mesh exported as a USDZ file and must conform to a specific joint hierarchy and naming convention. Models that don’t conform may behave unexpectedly or fail to work at all. Learn more about the specific joint hierarchy and required names for puppeteering models at Validating a Model for Motion Capture.\n\nDownload the Robot Model\n\nThe easiest way to configure your model for puppeteering is to rig it to the skeleton from the robot model in the Capturing Body Motion in 3D sample. By using the robot model's skeleton to rig your own model, you’ll start with the correct bone names and hierarchy. You can download a zip file containing both the USDZ and FBX versions of the robot character.\n\nNote\n\nIf you already have a rigged humanoid model and prefer to modify your existing skeleton to match RealityKit’s expected format instead of rebinding your model to the provided skeleton, you can find detailed information on the joint names and hierarchy required by ARKit’s motion capture functionality in Validating a Model for Motion Capture.\n\nImport the Skeleton into Your 3D-Modeling Program\n\nIn your 3D-modeling software package (such as Maya, Cinema4D, or Modo), import the provided skeleton and the custom mesh model that you want to use with ARKit’s Motion Capture functionality. You should model your mesh in a standard T-pose.\n\nIt’s very important that you configure your import settings so that they don’t change the orientation of the imported character or any of the skeleton’s individual joints. After import, the character should be oriented facing the +Z axis, with the top of its head oriented toward the +Y axis, and the character’s left hand pointing along the +X axis. Your scene should also be configured with +Y as the up axis. In some software packages, this orientation requires changing the default scene configuration.\n\nNote\n\nSome 3D software packages predefine which axis runs the length of the bone. You may not be able to export a compatible model from these programs without additional scripting or conversion work.\n\nOnce you’ve imported the robot character file, unbind and delete the robot mesh. You only need the skeleton from the imported file, since you’ll be binding your own mesh to it. Make sure you don’t change the joint names or relationships, and hang your custom geometry to the same node as the he skeleton and your custom geometry must be parented in separate hierarchies in order to export a valid USDZ for puppeteering. There should be no geometry descending from joints, and no joints descending from geometry.\n\nMatch the T-Pose Rest Position\n\nNext, align your mesh to the imported skeleton, then scale, translate, and rotate it until it matches the imported skeleton as closely as you can get it. Finally, freeze transformations on the mesh.\n\nFinish matching the mesh and armature by moving any joints of the armature that don’t line up correctly with the mesh, making sure that the X axis still points down the length of the bone after you’re done moving it. Many 3D software packages include tools to automatically re-orient joints based on the location of their children. If a re-orienting feature is available in your software package, use it when you're done moving joints into new locations.\n\nImportant\n\nThe Motion Capture feature doesn’t retarget motion before applying it to your model. If your custom character’s proportions differ substantially from the provided skeleton, the puppeteering functionality may not perform as expected unless you adjust for the differences in your own code.\n\nBind Your Mesh to the Imported Skeleton\n\nOnce you’ve aligned your mesh with the skeleton, bind your mesh to it. For best performance, you should use no more than four skin influences per vertex. You character should be modeled in a T-pose, your scene should contain only one bind pose, and the rotational values of each joint in your hierarchy should match the values in the provided example skeleton.\n\nExport the Model\n\nAfter you’ve tested your bound mesh and are happy with your vertex weights and joint deformations, export your model to a USDZ file. If your 3D package doesn’t support exporting directly to USDZ, you can export as a GLTF or USD file and then use Reality Converter to convert that file to USDZ. Reality Converter will also convert FBX files if you manually install the Autodesk FBX Python SDK available from Autodesk’s website.\n\nXcode automatically configures USDZ files imported into an AR application target and adds them to your app bundle when building so they become available to load at runtime.\n\nFor more information on loading and using the model once it’s in your Xcode project, see the Capturing Body Motion in 3D sample code project, which demonstrates how to load and display a skeletal model contained in a USDZ file as a BodyTrackedEntity.\n\nSee Also\nBody Position Tracking\nCapturing Body Motion in 3D\nTrack a person in the physical environment and visualize their motion by applying the same body movements to a virtual character.\nValidating a Model for Motion Capture\nVerify that your character model matches ARKit’s Motion Capture requirements.\nclass ARBodyAnchor\nAn anchor that tracks the position and movement of a human body in the rear-facing camera."
  },
  {
    "title": "Tracking Geographic Locations in AR | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/tracking_geographic_locations_in_ar",
    "html": "Overview\n\nIn this sample app, the user marks spots on a map or camera feed to create a collection of anchors they view in augmented reality (AR). By rendering those anchors as virtual content in an AR view, the user can see a nearby anchor through the camera feed, move to its physical location, and continue to move to any subsequent anchors in the collection. If a virtual anchor that the user is moving toward isn’t visible in the camera feed, the user can refer to its pin in the map view and advance until the virtual anchor becomes visible.\n\nGeotracking configuration (ARGeoTrackingConfiguration) combines GPS, the device’s compass, and world-tracking features in AR to track specific geographic locations. By giving ARKit a latitude and longitude (and optionally, altitude), the sample app declares interest in a specific location on the map.\n\nDuring a geotracking session, ARKit marks this location in the form of a location anchor (ARGeoAnchor) and continually refines its position in the camera feed as the user moves about. ARKit provides the location anchor’s coordinates with respect to the scene, which allows the app to render virtual content at its real-world location or trigger other interactions.\n\nFor example, when the user approaches a location anchor, an app may reveal a virtual signpost that explains a historic event that occurred there. Or, to form a street route, an app could render a virtual anchor in a series of location anchors that connect.\n\nNote\n\nARKit supports geotracking only with the device’s rear camera.\n\nConfigure the Sample Code Project\n\nThe sample app demonstrates geotracking coaching, which requires iOS 15. The Xcode project defines a deployment target of iOS 15.\n\nGeotracking requires a device with A12 Bionic chip or later, and cellular (GPS) capability. Set the project’s run destination to a device. ARKit doesn’t support iOS Simulator.\n\nEnsure Device Support\n\nThe sample app checks whether a device supports geotracking at the application entry point, AppDelegate.swift:\n\nif !ARGeoTrackingConfiguration.isSupported {\n    let storyboard = UIStoryboard(name: \"Main\", bundle: nil)\n    window?.rootViewController = storyboard.instantiateViewController(withIdentifier: \"unsupportedDeviceMessage\")\n}\n\n\nIf the device doesn’t support geotracking, the sample project stops. Optionally, an app can present an error message and continue the session at a limited capacity without geotracking.\n\nDisplay an AR View and Map View\n\nThe sample project renders location anchors using an ARView. To reinforce the correspondence between geographic locations and positions in the session’s local space, the sample project also displays a map view (MKMapView) that marks the anchors from a top-down perspective. The app displays both views simultaneously by using a stack view (UIStackView) with the camera feed on top. See the sample’s View Controller Scene within the project’s Main.storyboard.\n\nCheck Availability and Run a Session\n\nTo place location anchors with precision, geotracking requires a better understanding of the user’s geographic location than is possible with GPS alone. Based on a particular GPS coordinate, ARKit downloads batches of imagery that depict the physical environment in that area and assist the session with determining the user’s precise geographic location.\n\nThis localization imagery captures the view mostly from public streets and routes accessible by car. As a result, geotracking doesn’t support areas within the city that are gated or accessible only to pedestrians.\n\nBecause localization imagery depicts specific regions on the map, geotracking only supports areas where Apple has collected localization imagery in advance. Before starting a session, the sample project checks whether geotracking supports the user’s location by calling checkAvailability(completionHandler:).\n\nARGeoTrackingConfiguration.checkAvailability { (available, error) in\n    if !available {\n        let errorDescription = error?.localizedDescription ?? \"\"\n        let recommendation = \"Please try again in an area where geotracking is supported.\"\n        let restartSession = UIAlertAction(title: \"Restart Session\", style: .default) { (_) in\n            self.restartSession()\n        }\n        self.alertUser(withTitle: \"Geotracking unavailable\",\n                       message: \"\\(errorDescription)\\n\\(recommendation)\",\n                       actions: [restartSession])\n    }\n}\n\n\nARKit requires a network connection to download localization imagery. The checkAvailability(completionHandler:) function returns false if a network connection is unavailable. If geotracking is available, the sample project runs a session.\n\nlet geoTrackingConfig = ARGeoTrackingConfiguration()\ngeoTrackingConfig.planeDetection = [.horizontal]\narView.session.run(geoTrackingConfig, options: .removeExistingAnchors)\n\n\nNote\n\nIf geotracking is unavailable in the user’s current location, an app can suggest an alternative area if checkAvailability(at:completionHandler:) returns true for a nearby location.\n\nCoach the User for Geotracking Status\n\nTo begin a geotracking session, the framework undergoes several geotracking states. At any point, the session can require action from the user to progress to the next state. To instruct the user on what to do, the sample project uses a ARCoachingOverlayView with the ARCoachingOverlayView.Goal.geoTracking goal.\n\nfunc setupCoachingOverlay() {\n    coachingOverlay.delegate = self\n    arView.addSubview(coachingOverlay)\n    coachingOverlay.goal = .geoTracking\n\nInstruct the User Based on Geotracking State\n\nAfter the app localizes and begins a geotracking session, the sample app monitors the geotracking state and instructs the user by presenting text with a label.\n\nself.trackingStateLabel.text = text\n\n\nAs the user moves along a street, the framework continues to download localization imagery as needed to maintain a precise understanding of the user’s position in the world. If the ARGeoTrackingStatus.StateReason.geoDataNotLoaded error occurs after the session localizes, it may indicate a network issue. If this error persists, the app may ask the user to check the internet connection.\n\nWhile the session runs, the status reason ARGeoTrackingStatus.StateReason.notAvailableAtLocation occurs if the user crosses into an area where ARKit lacks geotracking support. To enable the session to continue, the sample project presents text to guide the user back to a supported area.\n\ncase .notAvailableAtLocation: return \"Geotracking is unavailable here. Please return to your previous location to continue\"\n\nCoach the User as the Session Runs\n\nA geotracking session maps geographic coordinates to ARKit’s world-tracking local space, which requires basic world-tracking support. If environmental circumstances impair the device’s world-tracking condition, the geotracking coaching overlay alerts the user and displays instructions to resolve the problem.\n\nFor example, if the user travels too quickly, the device’s camera feed may not contain sufficient features that ARKit requires to model the environment. In this case:\n\nThe framework sets world-tracking state to ARCamera.TrackingState.limited(_:).\n\nThe geotracking session observes the world-tracking status change and sets the geoTrackingStatus reason to ARGeoTrackingStatus.StateReason.worldTrackingUnstable.\n\nCoaching overlay activates and displays the text: “Slow down”.\n\nThe sample app disables the user interface until the user responds to the coaching.\n\nfunc coachingOverlayViewWillActivate(_ coachingOverlayView: ARCoachingOverlayView) {\n    mapView.isUserInteractionEnabled = false\n    undoButton.isEnabled = false\n    hideUIForCoaching(true)\n}\n\n\nARKit dismisses the coaching overlay when the tracking status improves, and the app reenables the user interface.\n\nfunc coachingOverlayViewDidDeactivate(_ coachingOverlayView: ARCoachingOverlayView) {\n    mapView.isUserInteractionEnabled = true\n    undoButton.isEnabled = true\n    hideUIForCoaching(false)\n}\n\nCreate an Anchor When the User Taps the Map\n\nThe sample project acquires the user’s geographic coordinate (CLLocationCoordinate2D) from the map view at the screen location where the user tapped.\n\nfunc handleTapOnMapView(_ sender: UITapGestureRecognizer) {\n    let point = sender.location(in: mapView)\n    let location = mapView.convert(point, toCoordinateFrom: mapView)\n\n\nWith the user’s latitude and longitude, the sample project creates a location anchor.\n\ngeoAnchor = ARGeoAnchor(coordinate: location)\n\n\nBecause the map view returns a 2D coordinate with no altitude, the sample calls init(coordinate:), which defaults the location anchor’s altitude to ground level.\n\nTo begin tracking the anchor, the sample project adds it to the session.\n\narView.session.add(anchor: geoAnchor)\n\n\nThe sample project listens for the location anchor in session(_:didAdd:) and visualizes it in AR by adding a placemark entity to the scene.\n\nfunc session(_ session: ARSession, didAdd anchors: [ARAnchor]) {\n    for geoAnchor in anchors.compactMap({ $0 as? ARGeoAnchor }) {\n        // Effect a spatial-based delay to avoid blocking the main thread.\n        DispatchQueue.main.asyncAfter(deadline: .now() + (distanceFromDevice(geoAnchor.coordinate) / 10)) {\n            // Add an AR placemark visualization for the geo anchor.\n            self.arView.scene.addAnchor(Entity.placemarkEntity(for: geoAnchor))\n\n\nTo establish visual correspondence in the map view, the sample project adds an MKOverlay that represents the anchor on the map.\n\nlet anchorIndicator = AnchorIndicator(center: geoAnchor.coordinate)\nself.mapView.addOverlay(anchorIndicator)\n\nCreate an Anchor When the User Taps the AR View\n\nWhen the user taps the camera feed, the sample project casts a ray at the screen-tap location to determine its intersection with a real-world surface.\n\nif let result = arView.raycast(from: point, allowing: .estimatedPlane, alignment: .any).first {\n\n\nThe ray cast result’s translation describes the intersection’s position in ARKit’s local coordinate space. To convert that point to a geographic location, the sample project calls the session-provided utility getGeoLocation(forPoint:completionHandler:).\n\narView.session.getGeoLocation(forPoint: worldPosition) { (location, altitude, error) in\n\n\nThen, the sample project creates a location anchor with the result. Because the result includes altitude, the sample project calls the init(coordinate:altitude:) anchor initializer.\n\nNote\n\nFor more on ray casting, see Environmental Analysis.\n\nAssess Geotracking Accuracy\n\nTo ensure the best possible user experience, an app must monitor and react to the geotracking accuracy. When possible, the sample project displays the accuracy as part of its state messaging to the user. The session populates accuracy in its geoTrackingStatus in state ARGeoTrackingStatus.State.localized.\n\nif geoTrackingStatus.state == .localized {\n    text += \"Accuracy: \\(geoTrackingStatus.accuracy.description)\"\n\n\nAn app renders location anchors using an asset that’s less exact if geotracking is off by a small distance, such as when accuracy is ARGeoTrackingStatus.Accuracy.low. For example, the sample app renders a location anchor as a large ball several meters in the air rather than an arrow pointing to a real-world surface.\n\nCenter the Map as the User Moves\n\nThe sample project uses updates from Core Location to center the user in the map view. When the user moves around, Core Location notifies the delegate of any updates in geographic position. The sample project monitors this event by implementing the relevant callback.\n\nfunc locationManager(_ manager: CLLocationManager, didUpdateLocations locations: [CLLocation]) {\n\n\nWhen the user’s position changes, the sample project pans the map to center the user.\n\nlet camera = MKMapCamera(lookingAtCenter: location.coordinate,\n                         fromDistance: CLLocationDistance(250),\n                         pitch: 0,\n                         heading: mapView.camera.heading)\nmapView.setCamera(camera, animated: false)\n\nSee Also\nGeotracking\nclass ARGeoAnchor\nAn anchor that identifies a geographic location using latitude, longitude, and altitude data."
  },
  {
    "title": "buffer | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometrysource/4108387-buffer",
    "html": "See Also\nInspecting geometry data\nvar count: Int\nThe number of vectors in a geometry source.\nvar format: MTLVertexFormat\nThe vertex format for data in a geometry source’s buffer.\nvar componentsPerVector: Int\nThe number of scalar components in each vector in a geometry source.\nvar offset: Int\nThe offset, in bytes, from the beginning of a geometry source’s buffer.\nvar stride: Int\nThe number of bytes between one vector and another in a geometry source’s buffer.\nvar description: String\nA textual description of a geometry source."
  },
  {
    "title": "format | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometrysource/4108390-format",
    "html": "See Also\nInspecting geometry data\nvar buffer: MTLBuffer\nA Metal buffer that contains per-vector data for a geometry source.\nvar count: Int\nThe number of vectors in a geometry source.\nvar componentsPerVector: Int\nThe number of scalar components in each vector in a geometry source.\nvar offset: Int\nThe offset, in bytes, from the beginning of a geometry source’s buffer.\nvar stride: Int\nThe number of bytes between one vector and another in a geometry source’s buffer.\nvar description: String\nA textual description of a geometry source."
  },
  {
    "title": "physicalSize | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/referenceimage/4108519-physicalsize",
    "html": "Discussion\n\nFor example, set the physicalSize of a reference image of a yard stick to 0.9144 meters.\n\nSee Also\nInspecting a reference image\nvar name: String?\nThe name of a reference image.\nvar description: String\nA textual description of a reference image."
  },
  {
    "title": "name | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/referenceimage/4108518-name",
    "html": "See Also\nInspecting a reference image\nvar physicalSize: CGSize\nThe size, in meters, of a reference image in the real world.\nvar description: String\nA textual description of a reference image."
  },
  {
    "title": "resetSceneReconstruction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/runoptions/3526323-resetscenereconstruction",
    "html": "Discussion\n\nWhen you reset scene reconstruction, ARKit removes any existing mesh anchors (ARMeshAnchor) from the session.\n\nSee Also\nRun Options\nstatic var resetTracking: ARSession.RunOptions\nAn option to reset the device's position from the session's previous run.\nstatic var removeExistingAnchors: ARSession.RunOptions\nAn option to remove any anchor objects associated with the session's previous run.\nstatic var stopTrackedRaycasts: ARSession.RunOptions\nAn option to stop all active tracked raycasts."
  },
  {
    "title": "removeExistingAnchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/runoptions/2878307-removeexistinganchors",
    "html": "Discussion\n\nBy default, when you call the run(_:options:) method on a session that has run before or is already running, the session keeps any ARAnchor objects that you previously added. That is, objects in the AR scene keep their apparent real-world positions relative to the device (unless you enable the resetTracking option).\n\nEnable the removeExistingAnchors option if changing session configurations should invalidate the apparent real-world positions of objects in the AR scene. For example, if you've added virtual content to the AR scene whose positions are correlated to real-world objects, remove those anchors so you can reevaluate appropriate real-world positions. On the other hand, if the virtual content in your scene needs to track real-world positions only when that content first appears and can move freely thereafter, you can disable this option to keep the anchors.\n\nSee Also\nRun Options\nstatic var resetTracking: ARSession.RunOptions\nAn option to reset the device's position from the session's previous run.\nstatic var stopTrackedRaycasts: ARSession.RunOptions\nAn option to stop all active tracked raycasts.\nstatic var resetSceneReconstruction: ARSession.RunOptions\nAn option to reset the scene mesh."
  },
  {
    "title": "timestamp | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdate/4131682-timestamp",
    "html": "See Also\nInspecting anchor updates\nlet anchor: AnchorType\nThe anchor that this anchor update contains information about.\nlet event: AnchorUpdate<AnchorType>.Event\nAn event that indicates whether an anchor was added, updated, or removed.\nenum AnchorUpdate.Event\nThe events that cause anchor updates.\nvar description: String\nA textual representation of this anchor update."
  },
  {
    "title": "ARGeoTrackingStatus | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingstatus",
    "html": "Overview\n\nGeo tracking requires coordination with the user at various phases of the geo-tracking lifecycle. To elicit the right user actions, an app needs to provide clear instructions to the user based on the current frame’s geoTrackingStatus:\n\nGeo-tracking state most notably regards the important process in which ARKit acquires a better understanding of the user’s geographic location and orientation than is possible with GPS and the compass heading alone. See ARGeoTrackingStatus.State.localizing for more information.\n\nGiven a particular state, the app needs to tailor its user messaging according to the stateReason.\n\nAn app may need to monitor accuracy closely if it requires high-precision localization.\n\nTopics\nChecking State\nvar state: ARGeoTrackingStatus.State\nA value that describes the session’s current geo-tracking state.\nenum ARGeoTrackingStatus.State\nValues that are possible for the current state of geo-tracking.\nDetermining the Reason\nvar stateReason: ARGeoTrackingStatus.StateReason\nThe reason for the frame’s geo-tracking state.\nenum ARGeoTrackingStatus.StateReason\nThe reasons for the app's geotracking status.\nJudging Accuracy\nvar accuracy: ARGeoTrackingStatus.Accuracy\nThe accuracy of geo tracking at the time the session captured the frame.\nenum ARGeoTrackingStatus.Accuracy\nValues that are possible for the current accuracy of geo tracking.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nSee Also\nAssessing geo-tracking condition\nvar geoTrackingStatus: ARGeoTrackingStatus?\nThe session’s condition with respect to geographic tracking at the time the session captured the frame."
  },
  {
    "title": "capturedImage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/2867984-capturedimage",
    "html": "Discussion\n\nARKit captures pixel buffers in a full-range planar YCbCr format (also known as YUV) format according to the ITU R. 601-4 standard. (You can verify this by checking the kCVImageBufferYCbCrMatrixKey pixel buffer attachment.)\n\nUnlike some uses of that standard, ARKit captures full-range color space values, not video-range values. To correctly render these images on a device display, you'll need to access the luma and chroma planes of the pixel buffer and convert full-range YCbCr values to an sRGB (or ITU R. 709) format according to the ITU-T T.871 specification.\n\nThe following matrix (shown in Metal shader syntax) performs this conversion when multiplied by a 4-element vector (containing Y', Cb, Cr values and an \"alpha\" value of 1.0):\n\nconst float4x4 ycbcrToRGBTransform = float4x4(\n    float4(+1.0000f, +1.0000f, +1.0000f, +0.0000f),\n    float4(+0.0000f, -0.3441f, +1.7720f, +0.0000f),\n    float4(+1.4020f, -0.7141f, +0.0000f, +0.0000f),\n    float4(-0.7010f, +0.5291f, -0.8860f, +1.0000f)\n);\n\n\nFor more details, see Displaying an AR Experience with Metal, or use the Metal variant of the AR app template when creating a new project in Xcode.\n\nSee Also\nAccessing camera data\nvar camera: ARCamera\nInformation about the camera position, orientation, and imaging parameters used to capture the frame.\nvar timestamp: TimeInterval\nThe time at which the frame was captured.\nvar cameraGrainIntensity: Float\nA value that specifies the amount of grain present in the camera grain texture.\nvar cameraGrainTexture: MTLTexture?\nA tileable Metal texture created by ARKit to match the visual characteristics of the current video stream.\nvar exifData: [String : Any]\nAuxiliary data for the captured image."
  },
  {
    "title": "camera | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/2867980-camera",
    "html": "See Also\nAccessing camera data\nvar capturedImage: CVPixelBuffer\nA pixel buffer containing the image captured by the camera.\nvar timestamp: TimeInterval\nThe time at which the frame was captured.\nvar cameraGrainIntensity: Float\nA value that specifies the amount of grain present in the camera grain texture.\nvar cameraGrainTexture: MTLTexture?\nA tileable Metal texture created by ARKit to match the visual characteristics of the current video stream.\nvar exifData: [String : Any]\nAuxiliary data for the captured image."
  },
  {
    "title": "sessionWasInterrupted(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionobserver/2891620-sessionwasinterrupted",
    "html": "Parameters\nsession\n\nThe session providing information.\n\nDiscussion\n\nA session is interrupted when it fails to receive camera or motion sensing data. Session interruptions occur whenever camera capture is not available—for example, when your app is in the background or there are multiple foreground apps—or when the device is too busy to process motion sensor data.\n\nImportant\n\nAn interruption is equivalent to manually pausing the session. Do not call pause() in response to this callback, as that prevents your app from being notified when the interruption ends.\n\nSee Also\nHandling Interruptions\nfunc sessionInterruptionEnded(ARSession)\nTells the delegate that the session has resumed processing frames and tracking device position.\nfunc sessionShouldAttemptRelocalization(ARSession) -> Bool\nAsks the delegate whether to attempt recovery of world-tracking state after an interruption."
  },
  {
    "title": "session(_:didAdd:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessiondelegate/2865617-session",
    "html": "Parameters\nsession\n\nThe session providing information.\n\nanchors\n\nThe anchors newly added to the session.\n\nDiscussion\n\nDepending on the session configuration, ARKit may automatically add anchors to a session.\n\nIf you display an AR experience using SceneKit or SpriteKit, you can instead implement one of the following methods instead to track not only the addition of anchors to the session but also how to add SceneKit or SpriteKit content to the corresponding scene:\n\nARSCNView: renderer(_:nodeFor:) or renderer(_:didAdd:for:)\n\nARSKView: node(for:) or view(_:didAdd:for:)\n\nSee Also\nHandling Content Updates\nfunc session(ARSession, didUpdate: [ARAnchor])\nTells the delegate that the session has adjusted the properties of one or more anchors.\nfunc session(ARSession, didRemove: [ARAnchor])\nTells the delegate that one or more anchors have been removed from the session."
  },
  {
    "title": "session(_:didUpdate:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessiondelegate/2865611-session",
    "html": "Parameters\nsession\n\nThe session providing information.\n\nframe\n\nAn object containing the new camera image and AR information.\n\nDiscussion\n\nImplement this method if you provide your own display for rendering an AR experience. The provided ARFrame object contains the latest image captured from the device camera, which you can render as a scene background, as well as information about camera parameters and anchor transforms you can use for rendering virtual content on top of the camera image."
  },
  {
    "title": "ARSkeletonDefinition | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskeletondefinition",
    "html": "Overview\n\nA skeleton definition establishes the relationship of joints that make up a 3D or 2D body's skeleton, in which joints connect to other joints to compose a single skeleton in a parent-child hierarchy. Use parentIndices to identify the hierarchy for a given skeleton definition.\n\nARKit names particular joints that are crucial to body tracking. You can access named joints by calling index(forJointName:) and passing in one of the available jointNames identifiers.\n\nTopics\nLocating in the Physical Environment\nvar neutralBodySkeleton3D: ARSkeleton3D?\nThe 3D skeleton in neutral pose.\nclass var defaultBody3D: ARSkeletonDefinition\nThe default skeleton definition for bodies defined in 3D.\nLocating in Screen Space\nclass var defaultBody2D: ARSkeletonDefinition\nThe default skeleton definition for bodies defined in 2D.\nGetting Joint Information\nvar jointNames: [String]\nA collection of unique joint names.\nvar jointCount: Int\nThe skeleton's total number of joints.\nfunc index(for: ARSkeleton.JointName) -> Int\nReturns the index for a given joint identifier.\nvar parentIndices: [Int]\nThe parent index for each joint.\nRelationships\nInherits From\nNSObject\nSee Also\nBody Data\nCapturing Body Motion in 3D\nTrack a person in the physical environment and visualize their motion by applying the same body movements to a virtual character.\nclass ARBody2D\nThe screen-space representation of a person ARKit recognizes in the camera feed.\nclass ARSkeleton3D\nThe skeleton of a human body that ARKit tracks in 3D space.\nclass ARSkeleton2D\nAn object that describes the locations of a body’s joints in the camera feed.\nclass ARSkeleton\nThe interface for the skeleton of a tracked body."
  },
  {
    "title": "OS_ar_anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_anchor",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nInherited By\nOS_ar_mesh_anchor\nOS_ar_plane_anchor\nOS_ar_trackable_anchor\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_trackable_anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_trackable_anchor",
    "html": "Relationships\nInherits From\nOS_ar_anchor\nInherited By\nOS_ar_device_anchor\nOS_ar_hand_anchor\nOS_ar_image_anchor\nOS_ar_world_anchor\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "hash(into:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/dataproviderstate/4278290-hash",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nComparing data provider states\nvar description: String\nvar hashValue: Int\nstatic func == (DataProviderState, DataProviderState) -> Bool\nstatic func != (DataProviderState, DataProviderState) -> Bool"
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/deviceanchor/4293511-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting a device anchor\nvar originFromAnchorTransform: simd_float4x4\nThe transform from the device to the origin coordinate system.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is tracking the device.\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when DeviceAnchor conforms to AnyObject.\nvar id: UUID\nA globally unique anchor for a device anchor.\ntypealias DeviceAnchor.ID\nThe type that identifies device anchors."
  },
  {
    "title": "ARSKViewDelegate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskviewdelegate",
    "html": "Overview\n\nImplement this protocol to provide SpriteKit content corresponding to ARAnchor objects tracked by the view's AR session, or to manage the view's automatic updating of such content.\n\nThis protocol extends the ARSessionObserver protocol, so your session delegate can also implement those methods to respond to changes in session status.\n\nTopics\nHandling Content Updates\nfunc view(ARSKView, nodeFor: ARAnchor) -> SKNode?\nAsks the delegate to provide a SpriteKit node corresponding to a newly added anchor.\nfunc view(ARSKView, didAdd: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node corresponding to a new AR anchor has been added to the scene.\nfunc view(ARSKView, willUpdate: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node's properties will be updated to match the current state of its corresponding anchor.\nfunc view(ARSKView, didUpdate: SKNode, for: ARAnchor)\nTells the delegate that a SpriteKit node's properties have been updated to match the current state of its corresponding anchor.\nfunc view(ARSKView, didRemove: SKNode, for: ARAnchor)\nTells the delegate that the SpriteKit node corresponding to an AR anchor has been removed from the scene.\nRelationships\nInherits From\nARSessionObserver\nSKViewDelegate\nSee Also\nResponding to AR Updates\nvar delegate: ARSKViewDelegate?\nAn object you provide to mediate synchronization of the view's AR scene information with SpriteKit content."
  },
  {
    "title": "ARSCNViewDelegate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnviewdelegate",
    "html": "Overview\n\nImplement this protocol to provide SceneKit content corresponding to ARAnchor objects tracked by the view's AR session, or to manage the view's automatic updating of such content.\n\nThis protocol extends the ARSessionObserver protocol, so your session delegate can also implement those methods to respond to changes in session status.\n\nTopics\nHandling Content Updates\nfunc renderer(SCNSceneRenderer, nodeFor: ARAnchor) -> SCNNode?\nAsks the delegate to provide a SceneKit node corresponding to a newly added anchor.\nfunc renderer(SCNSceneRenderer, didAdd: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node corresponding to a new AR anchor has been added to the scene.\nfunc renderer(SCNSceneRenderer, willUpdate: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node's properties will be updated to match the current state of its corresponding anchor.\nfunc renderer(SCNSceneRenderer, didUpdate: SCNNode, for: ARAnchor)\nTells the delegate that a SceneKit node's properties have been updated to match the current state of its corresponding anchor.\nfunc renderer(SCNSceneRenderer, didRemove: SCNNode, for: ARAnchor)\nTells the delegate that the SceneKit node corresponding to a removed AR anchor has been removed from the scene.\nRelationships\nInherits From\nARSessionObserver\nSCNSceneRendererDelegate\nSee Also\nResponding to AR Updates\nvar delegate: ARSCNViewDelegate?\nAn object you provide to mediate synchronization of the view's AR scene information with SceneKit content."
  },
  {
    "title": "originFromAnchorTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/deviceanchor/4293515-originfromanchortransform",
    "html": "Relationships\nFrom Protocol\nAnchor\nSee Also\nInspecting a device anchor\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is tracking the device.\nvar description: String\nA textual description of a device anchor.\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when DeviceAnchor conforms to AnyObject.\nvar id: UUID\nA globally unique anchor for a device anchor.\ntypealias DeviceAnchor.ID\nThe type that identifies device anchors."
  },
  {
    "title": "DataProviderState.initialized | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/dataproviderstate/initialized",
    "html": "See Also\nGetting the state of a data provider\ncase running\nThe data provider is running.\ncase stopped\nThe data provider is stopped.\ncase paused\nThe data provider is paused."
  },
  {
    "title": "originFromAnchorTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imageanchor/4293519-originfromanchortransform",
    "html": "Relationships\nFrom Protocol\nAnchor\nSee Also\nGetting image information\nvar referenceImage: ReferenceImage\nThe reference image that this image anchor tracks.\nvar estimatedScaleFactor: Float\nThe estimated scale factor between the tracked image’s physical size and the reference image’s size.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking this image.\nvar description: String\nA textual description of an image anchor."
  },
  {
    "title": "referenceImage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imageanchor/4108433-referenceimage",
    "html": "See Also\nGetting image information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of the image in world space.\nvar estimatedScaleFactor: Float\nThe estimated scale factor between the tracked image’s physical size and the reference image’s size.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking this image.\nvar description: String\nA textual description of an image anchor."
  },
  {
    "title": "estimatedScaleFactor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imageanchor/4108428-estimatedscalefactor",
    "html": "Discussion\n\nThe scale factor is between the tracked image’s size and the physicalSize property on the reference image you supply when you create an image tracking provider. For example, if you supply a reference image and the version that appears in front of someone is three times larger, this property is 3.0.\n\nSee Also\nGetting image information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of the image in world space.\nvar referenceImage: ReferenceImage\nThe reference image that this image anchor tracks.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking this image.\nvar description: String\nA textual description of an image anchor."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imageanchor/4139381-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nGetting image information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of the image in world space.\nvar referenceImage: ReferenceImage\nThe reference image that this image anchor tracks.\nvar estimatedScaleFactor: Float\nThe estimated scale factor between the tracked image’s physical size and the reference image’s size.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking this image."
  },
  {
    "title": "id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imageanchor/4108430-id",
    "html": "Discussion\n\nNote\n\nThis documentation comment was inherited from Identifiable.\n\nSee Also\nIdentifying image anchors\nvar id: UUID\nA globally unique identifier for an image anchor.\ntypealias ImageAnchor.ID"
  },
  {
    "title": "ImageAnchor.ID | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imageanchor/id",
    "html": "See Also\nIdentifying image anchors\nvar id: UUID\nA globally unique identifier for an image anchor.\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when ImageAnchor conforms to AnyObject."
  },
  {
    "title": "HandAnchor.Chirality | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/chirality",
    "html": "Topics\nGetting hand chirality\ncase left\nA left hand.\ncase right\nA right hand.\nComparing hand chirality\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (HandAnchor.Chirality, HandAnchor.Chirality) -> Bool\nstatic func != (HandAnchor.Chirality, HandAnchor.Chirality) -> Bool\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nGetting hand information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a hand in world space.\nvar handSkeleton: HandSkeleton?\nThe current position and orientation of joints on a hand.\nvar chirality: HandAnchor.Chirality\nA value that indicates a left or right hand.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking this hand.\nvar description: String\nA textual description of a hand anchor."
  },
  {
    "title": "isTracked | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/4108408-istracked",
    "html": "Relationships\nFrom Protocol\nTrackableAnchor\nSee Also\nGetting hand information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a hand in world space.\nvar handSkeleton: HandSkeleton?\nThe current position and orientation of joints on a hand.\nvar chirality: HandAnchor.Chirality\nA value that indicates a left or right hand.\nenum HandAnchor.Chirality\nThe values identifying hand chirality.\nvar description: String\nA textual description of a hand anchor."
  },
  {
    "title": "HandAnchor.ID | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/id",
    "html": "See Also\nIdentifying hand anchors\nvar id: UUID\nA globally unique identifier for a hand anchor.\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when HandAnchor conforms to AnyObject."
  },
  {
    "title": "joint(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/4284989-joint",
    "html": "Parameters\nnamed\n\nThe name of the hand joint to retrieve.\n\nReturn Value\n\nA hand joint referred to by the named parameter.\n\nSee Also\nRetrieving specific hand joints\nstruct HandSkeleton.Joint\nThe name and position of an individual hand joint.\nenum HandSkeleton.JointName\nThe names of different hand joints."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/4139377-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nGetting hand information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a hand in world space.\nvar handSkeleton: HandSkeleton?\nThe current position and orientation of joints on a hand.\nvar chirality: HandAnchor.Chirality\nA value that indicates a left or right hand.\nenum HandAnchor.Chirality\nThe values identifying hand chirality.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking this hand."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/4139386-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting mesh anchors\nvar id: UUID\ntypealias MeshAnchor.ID\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when MeshAnchor conforms to AnyObject."
  },
  {
    "title": "id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/4108461-id",
    "html": "Discussion\n\nNote\n\nThis documentation comment was inherited from Identifiable.\n\nSee Also\nInspecting mesh anchors\nvar id: UUID\ntypealias MeshAnchor.ID\nvar description: String"
  },
  {
    "title": "MeshAnchor.ID | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/id",
    "html": "See Also\nInspecting mesh anchors\nvar id: UUID\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when MeshAnchor conforms to AnyObject.\nvar description: String"
  },
  {
    "title": "MeshAnchor.Geometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/geometry",
    "html": "Topics\nInspecting mesh geometry\nvar faces: GeometryElement\nThe faces of the mesh.\nvar vertices: GeometrySource\nThe vertices of the mesh.\nvar normals: GeometrySource\nThe normals of the mesh.\nvar classifications: GeometrySource?\nThe classification of each face in the mesh.\nvar description: String\nA textual description of the mesh geometry.\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nGetting mesh information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a mesh in world space.\nvar geometry: MeshAnchor.Geometry\nThe shape of a mesh anchor.\nenum MeshAnchor.MeshClassification\nThe kinds of classification a face of a mesh can have."
  },
  {
    "title": "AR_OBJECT_USE_OBJC | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ar_object_use_objc",
    "html": "See Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta"
  },
  {
    "title": "originFromAnchorTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor/4293520-originfromanchortransform",
    "html": "Relationships\nFrom Protocol\nAnchor\nSee Also\nGetting mesh information\nvar geometry: MeshAnchor.Geometry\nThe shape of a mesh anchor.\nstruct MeshAnchor.Geometry\nThe shapes that make up a mesh anchor.\nenum MeshAnchor.MeshClassification\nThe kinds of classification a face of a mesh can have."
  },
  {
    "title": "OS_ar_world_anchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_world_anchors",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_world_anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_world_anchor",
    "html": "Relationships\nInherits From\nOS_ar_trackable_anchor\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_scene_reconstruction_provider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_scene_reconstruction_provider",
    "html": "Relationships\nInherits From\nOS_ar_data_provider\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_pose | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_pose",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_plane_geometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_plane_geometry",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_plane_anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_plane_anchor",
    "html": "Relationships\nInherits From\nOS_ar_anchor\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_plane_detection_provider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_plane_detection_provider",
    "html": "Relationships\nInherits From\nOS_ar_data_provider\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_mesh_geometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_mesh_geometry",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_mesh_anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_mesh_anchor",
    "html": "Relationships\nInherits From\nOS_ar_anchor\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_mesh_anchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_mesh_anchors",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_image_anchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_image_anchors",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_image_anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_image_anchor",
    "html": "Relationships\nInherits From\nOS_ar_trackable_anchor\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_image_tracking_provider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_image_tracking_provider",
    "html": "Relationships\nInherits From\nOS_ar_data_provider\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_world_tracking_configuration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_world_tracking_configuration",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_scene_reconstruction_configuration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_scene_reconstruction_configuration",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_session | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_session",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "PlaneAnchor.Classification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/classification",
    "html": "Topics\nGetting known classifications\ncase ceiling\nA ceiling.\ncase door\nA door.\ncase floor\nA floor.\ncase seat\nA seat.\ncase table\nA table.\ncase wall\nA wall.\ncase window\nA window.\nGetting unknown classifications\ncase notAvailable\nA plane classification is currently unavailable.\ncase undetermined\nA plane classification hasn’t been determined yet.\ncase unknown\nA plane classification isn’t one of the known classes.\nComparing plane classifications\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (PlaneAnchor.Classification, PlaneAnchor.Classification) -> Bool\nstatic func != (PlaneAnchor.Classification, PlaneAnchor.Classification) -> Bool\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nInspecting a plane anchor\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a plane in world space.\nvar alignment: PlaneAnchor.Alignment\nThe alignment — horizontal or vertical — of a plane anchor relative to gravity.\nenum PlaneAnchor.Alignment\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nvar classification: PlaneAnchor.Classification\nThe kind of real-world object that ARKit determined this plane anchor might be.\nvar description: String\nA textual description of a plane anchor."
  },
  {
    "title": "PlaneAnchor.Geometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/geometry",
    "html": "Topics\nInspecting plane geometry\nvar extent: PlaneAnchor.Geometry.Extent\nThe size of a plane.\nstruct PlaneAnchor.Geometry.Extent\nThe size of a plane.\nvar meshVertices: GeometrySource\nThe vertices in the mesh that describes a plane.\nvar meshFaces: GeometryElement\nThe faces in the mesh that describes a plane.\nvar description: String\nA textual description of the shape of a plane.\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nGetting the shape of a plane anchor\nvar geometry: PlaneAnchor.Geometry\nThe shape of a plane anchor."
  },
  {
    "title": "id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/4108495-id",
    "html": "Relationships\nFrom Protocol\nAnchor\nIdentifiable\nSee Also\nIdentifying a plane anchor\ntypealias PlaneAnchor.ID\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when PlaneAnchor conforms to AnyObject."
  },
  {
    "title": "modes | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider/4131878-modes",
    "html": "See Also\nCreating a scene reconstruction provider\ninit(modes: [SceneReconstructionProvider.Mode])\nCreates a provider that reconstructs the person’s surroundings.\nenum SceneReconstructionProvider.Mode\nThe additional kinds of information you can request about a person’s surroundings.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports scene reconstruction providers."
  },
  {
    "title": "referenceObject | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arobjectanchor/2968195-referenceobject",
    "html": "Discussion\n\nThis object is always one of the ARReferenceObject objects you provided in the detectionObjects array when configuring the session."
  },
  {
    "title": "count | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometrysource/4108389-count",
    "html": "See Also\nInspecting geometry data\nvar buffer: MTLBuffer\nA Metal buffer that contains per-vector data for a geometry source.\nvar format: MTLVertexFormat\nThe vertex format for data in a geometry source’s buffer.\nvar componentsPerVector: Int\nThe number of scalar components in each vector in a geometry source.\nvar offset: Int\nThe offset, in bytes, from the beginning of a geometry source’s buffer.\nvar stride: Int\nThe number of bytes between one vector and another in a geometry source’s buffer.\nvar description: String\nA textual description of a geometry source."
  },
  {
    "title": "componentsPerVector | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometrysource/4108388-componentspervector",
    "html": "See Also\nInspecting geometry data\nvar buffer: MTLBuffer\nA Metal buffer that contains per-vector data for a geometry source.\nvar count: Int\nThe number of vectors in a geometry source.\nvar format: MTLVertexFormat\nThe vertex format for data in a geometry source’s buffer.\nvar offset: Int\nThe offset, in bytes, from the beginning of a geometry source’s buffer.\nvar stride: Int\nThe number of bytes between one vector and another in a geometry source’s buffer.\nvar description: String\nA textual description of a geometry source."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometrysource/4139376-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting geometry data\nvar buffer: MTLBuffer\nA Metal buffer that contains per-vector data for a geometry source.\nvar count: Int\nThe number of vectors in a geometry source.\nvar format: MTLVertexFormat\nThe vertex format for data in a geometry source’s buffer.\nvar componentsPerVector: Int\nThe number of scalar components in each vector in a geometry source.\nvar offset: Int\nThe offset, in bytes, from the beginning of a geometry source’s buffer.\nvar stride: Int\nThe number of bytes between one vector and another in a geometry source’s buffer."
  },
  {
    "title": "stride | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometrysource/4108392-stride",
    "html": "See Also\nInspecting geometry data\nvar buffer: MTLBuffer\nA Metal buffer that contains per-vector data for a geometry source.\nvar count: Int\nThe number of vectors in a geometry source.\nvar format: MTLVertexFormat\nThe vertex format for data in a geometry source’s buffer.\nvar componentsPerVector: Int\nThe number of scalar components in each vector in a geometry source.\nvar offset: Int\nThe offset, in bytes, from the beginning of a geometry source’s buffer.\nvar description: String\nA textual description of a geometry source."
  },
  {
    "title": "offset | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometrysource/4108391-offset",
    "html": "See Also\nInspecting geometry data\nvar buffer: MTLBuffer\nA Metal buffer that contains per-vector data for a geometry source.\nvar count: Int\nThe number of vectors in a geometry source.\nvar format: MTLVertexFormat\nThe vertex format for data in a geometry source’s buffer.\nvar componentsPerVector: Int\nThe number of scalar components in each vector in a geometry source.\nvar stride: Int\nThe number of bytes between one vector and another in a geometry source’s buffer.\nvar description: String\nA textual description of a geometry source."
  },
  {
    "title": "init(cgimage:physicalSize:orientation:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/referenceimage/4108516-init",
    "html": "Parameters\ncgimage\n\nThe image to use as a reference during tracking.\n\nphysicalSize\n\nThe size of the image in meters.\n\norientation\n\nThe orientation of the image asset.\n\nSee Also\nCreating a reference image\ninit(pixelBuffer: CVPixelBuffer, physicalSize: CGSize, orientation: CGImagePropertyOrientation)\nCreates a reference image from a pixel buffer.\nstatic func loadReferenceImages(inGroupNamed: String, bundle: Bundle?) -> [ReferenceImage]\nCreates multiple reference images based on their group name in an asset catalog."
  },
  {
    "title": "init(pixelBuffer:physicalSize:orientation:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/referenceimage/4108517-init",
    "html": "Parameters\npixelBuffer\n\nThe image to use as a reference during tracking.\n\nphysicalSize\n\nThe size of the image in meters.\n\norientation\n\nThe orientation of the image asset.\n\nSee Also\nCreating a reference image\ninit(cgimage: CGImage, physicalSize: CGSize, orientation: CGImagePropertyOrientation)\nCreates a reference image from a Core Graphics image.\nstatic func loadReferenceImages(inGroupNamed: String, bundle: Bundle?) -> [ReferenceImage]\nCreates multiple reference images based on their group name in an asset catalog."
  },
  {
    "title": "imageResolution | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/videoformat/2942257-imageresolution",
    "html": "Discussion\n\nVideo format sizes are relative to the native sensor orientation of the device camera, and as such are always landscape-oriented.\n\nSee Also\nAccessing format information\nvar framesPerSecond: Int\nThe rate at which the session captures video and provides AR frame information.\nvar isRecommendedForHighResolutionFrameCapturing: Bool\nDetermines whether the framework considers a format suitable for high-resolution frame capture.\nvar isVideoHDRSupported: Bool\nDetermines whether the format supports high dynamic range (HDR)."
  },
  {
    "title": "isVideoHDRSupported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/videoformat/3930053-isvideohdrsupported",
    "html": "Discussion\n\nCall this function before setting videoHDRAllowed to true to first check whether a video format supports HDR.\n\nSee Also\nAccessing format information\nvar framesPerSecond: Int\nThe rate at which the session captures video and provides AR frame information.\nvar imageResolution: CGSize\nThe size, in pixels, of video images captured in the session.\nvar isRecommendedForHighResolutionFrameCapturing: Bool\nDetermines whether the framework considers a format suitable for high-resolution frame capture."
  },
  {
    "title": "classification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/4108493-classification",
    "html": "See Also\nInspecting a plane anchor\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a plane in world space.\nvar alignment: PlaneAnchor.Alignment\nThe alignment — horizontal or vertical — of a plane anchor relative to gravity.\nenum PlaneAnchor.Alignment\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nenum PlaneAnchor.Classification\nThe kinds of object classification a plane anchor can have.\nvar description: String\nA textual description of a plane anchor."
  },
  {
    "title": "OS_ar_plane_extent | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_plane_extent",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_plane_detection_configuration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_plane_detection_configuration",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_image_tracking_configuration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_image_tracking_configuration",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_geometry_element | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_geometry_element",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_hand_tracking_configuration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_hand_tracking_configuration",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_hand_tracking_provider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_hand_tracking_provider",
    "html": "Relationships\nInherits From\nOS_ar_data_provider\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_device_anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_device_anchor",
    "html": "Relationships\nInherits From\nOS_ar_trackable_anchor\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_data_provider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_data_provider",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nInherited By\nOS_ar_hand_tracking_provider\nOS_ar_image_tracking_provider\nOS_ar_plane_detection_provider\nOS_ar_scene_reconstruction_provider\nOS_ar_world_tracking_provider\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_authorization_results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_authorization_results",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldanchor/4108562-id",
    "html": "Discussion\n\nNote\n\nThis documentation comment was inherited from Identifiable.\n\nSee Also\nIdentifying a world anchor\nvar id: UUID\nA unique identifier for a world anchor.\ntypealias WorldAnchor.ID\nThe type that identifies a world anchor."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldanchor/4139435-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting a world anchor\nvar originFromAnchorTransform: simd_float4x4\nThe position and orientation of a world anchor.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking a world anchor."
  },
  {
    "title": "isTracked | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldanchor/4131919-istracked",
    "html": "Relationships\nFrom Protocol\nTrackableAnchor\nSee Also\nInspecting a world anchor\nvar originFromAnchorTransform: simd_float4x4\nThe position and orientation of a world anchor.\nvar description: String\nA textual description of a world anchor."
  },
  {
    "title": "!=(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/dataproviderstate/4278287",
    "html": "See Also\nComparing data provider states\nvar description: String\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (DataProviderState, DataProviderState) -> Bool"
  },
  {
    "title": "WorldAnchor.ID | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldanchor/id",
    "html": "See Also\nIdentifying a world anchor\nvar id: UUID\nA unique identifier for a world anchor.\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when WorldAnchor conforms to AnyObject."
  },
  {
    "title": "originFromAnchorTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldanchor/4293524-originfromanchortransform",
    "html": "Relationships\nFrom Protocol\nAnchor\nSee Also\nInspecting a world anchor\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking a world anchor.\nvar description: String\nA textual description of a world anchor."
  },
  {
    "title": "==(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/dataproviderstate/4278288",
    "html": "See Also\nComparing data provider states\nvar description: String\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func != (DataProviderState, DataProviderState) -> Bool"
  },
  {
    "title": "hashValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/dataproviderstate/4278291-hashvalue",
    "html": "Relationships\nFrom Protocol\nHashable\nSee Also\nComparing data provider states\nvar description: String\nfunc hash(into: inout Hasher)\nstatic func == (DataProviderState, DataProviderState) -> Bool\nstatic func != (DataProviderState, DataProviderState) -> Bool"
  },
  {
    "title": "DeviceAnchor.ID | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/deviceanchor/id",
    "html": "See Also\nInspecting a device anchor\nvar originFromAnchorTransform: simd_float4x4\nThe transform from the device to the origin coordinate system.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is tracking the device.\nvar description: String\nA textual description of a device anchor.\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when DeviceAnchor conforms to AnyObject.\nvar id: UUID\nA globally unique anchor for a device anchor."
  },
  {
    "title": "PlaneAnchor.ID | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/id",
    "html": "See Also\nIdentifying a plane anchor\nvar id: UUID\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when PlaneAnchor conforms to AnyObject."
  },
  {
    "title": "id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/4108496-id",
    "html": "Discussion\n\nNote\n\nThis documentation comment was inherited from Identifiable.\n\nSee Also\nIdentifying a plane anchor\nvar id: UUID\ntypealias PlaneAnchor.ID"
  },
  {
    "title": "id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/deviceanchor/4293512-id",
    "html": "Relationships\nFrom Protocol\nAnchor\nIdentifiable\nSee Also\nInspecting a device anchor\nvar originFromAnchorTransform: simd_float4x4\nThe transform from the device to the origin coordinate system.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is tracking the device.\nvar description: String\nA textual description of a device anchor.\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when DeviceAnchor conforms to AnyObject.\ntypealias DeviceAnchor.ID\nThe type that identifies device anchors."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/dataproviderstate/4278289-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nComparing data provider states\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (DataProviderState, DataProviderState) -> Bool\nstatic func != (DataProviderState, DataProviderState) -> Bool"
  },
  {
    "title": "id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/deviceanchor/4293513-id",
    "html": "Discussion\n\nNote\n\nThis documentation comment was inherited from Identifiable.\n\nSee Also\nInspecting a device anchor\nvar originFromAnchorTransform: simd_float4x4\nThe transform from the device to the origin coordinate system.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is tracking the device.\nvar description: String\nA textual description of a device anchor.\nvar id: UUID\nA globally unique anchor for a device anchor.\ntypealias DeviceAnchor.ID\nThe type that identifies device anchors."
  },
  {
    "title": "DataProviderState.paused | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/dataproviderstate/paused",
    "html": "See Also\nGetting the state of a data provider\ncase initialized\nThe data provider has been created.\ncase running\nThe data provider is running.\ncase stopped\nThe data provider is stopped."
  },
  {
    "title": "isTracked | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/deviceanchor/4293514-istracked",
    "html": "Relationships\nFrom Protocol\nTrackableAnchor\nSee Also\nInspecting a device anchor\nvar originFromAnchorTransform: simd_float4x4\nThe transform from the device to the origin coordinate system.\nvar description: String\nA textual description of a device anchor.\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when DeviceAnchor conforms to AnyObject.\nvar id: UUID\nA globally unique anchor for a device anchor.\ntypealias DeviceAnchor.ID\nThe type that identifies device anchors."
  },
  {
    "title": "DataProviderState.running | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/dataproviderstate/running",
    "html": "See Also\nGetting the state of a data provider\ncase initialized\nThe data provider has been created.\ncase stopped\nThe data provider is stopped.\ncase paused\nThe data provider is paused."
  },
  {
    "title": "handSkeleton | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/4284943-handskeleton",
    "html": "See Also\nGetting hand information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a hand in world space.\nvar chirality: HandAnchor.Chirality\nA value that indicates a left or right hand.\nenum HandAnchor.Chirality\nThe values identifying hand chirality.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking this hand.\nvar description: String\nA textual description of a hand anchor."
  },
  {
    "title": "chirality | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/4108405-chirality",
    "html": "See Also\nGetting hand information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a hand in world space.\nvar handSkeleton: HandSkeleton?\nThe current position and orientation of joints on a hand.\nenum HandAnchor.Chirality\nThe values identifying hand chirality.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking this hand.\nvar description: String\nA textual description of a hand anchor."
  },
  {
    "title": "originFromAnchorTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/4293516-originfromanchortransform",
    "html": "Discussion\n\nThe transform of a hand anchor is positioned at the base of the wrist. ARKit provides transforms of other joints on the hand relative to this root transform.\n\nRelationships\nFrom Protocol\nAnchor\nSee Also\nGetting hand information\nvar handSkeleton: HandSkeleton?\nThe current position and orientation of joints on a hand.\nvar chirality: HandAnchor.Chirality\nA value that indicates a left or right hand.\nenum HandAnchor.Chirality\nThe values identifying hand chirality.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking this hand.\nvar description: String\nA textual description of a hand anchor."
  },
  {
    "title": "HandSkeleton.JointName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/jointname",
    "html": "Topics\nForearm joints\ncase forearmArm\ncase forearmWrist\ncase wrist\nThumb joints\ncase thumbIntermediateBase\ncase thumbIntermediateTip\ncase thumbKnuckle\ncase thumbTip\nIndex finger joints\ncase indexFingerIntermediateBase\ncase indexFingerIntermediateTip\ncase indexFingerKnuckle\ncase indexFingerMetacarpal\ncase indexFingerTip\nMiddle finger joints\ncase middleFingerIntermediateBase\ncase middleFingerIntermediateTip\ncase middleFingerKnuckle\ncase middleFingerMetacarpal\ncase middleFingerTip\nRing finger joints\ncase ringFingerIntermediateBase\ncase ringFingerIntermediateTip\ncase ringFingerKnuckle\ncase ringFingerMetacarpal\ncase ringFingerTip\nLittle finger joints\ncase littleFingerIntermediateBase\ncase littleFingerIntermediateTip\ncase littleFingerKnuckle\ncase littleFingerMetacarpal\ncase littleFingerTip\nInspecting hand joints\nvar description: String\nstatic var allCases: [HandSkeleton.JointName]\ntypealias HandSkeleton.JointName.AllCases\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (HandSkeleton.JointName, HandSkeleton.JointName) -> Bool\nstatic func != (HandSkeleton.JointName, HandSkeleton.JointName) -> Bool\nRelationships\nConforms To\nCaseIterable\nCustomStringConvertible\nHashable\nSendable\nSee Also\nRetrieving specific hand joints\nfunc joint(HandSkeleton.JointName) -> HandSkeleton.Joint\nRetrieves a hand joint based on the joint name you specify.\nstruct HandSkeleton.Joint\nThe name and position of an individual hand joint."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/4284988-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting hand skeletons\nvar allJoints: [HandSkeleton.Joint]\nAll of the joints in a hand skeleton."
  },
  {
    "title": "HandSkeleton.Joint | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/joint",
    "html": "Topics\nInspecting hand joints\nvar name: HandSkeleton.JointName\nA name that uniquely identifies this joint among others on the same skeleton.\nvar parentJoint: HandSkeleton.Joint?\nThe joint that’s connected to this joint and more closely connected to the base of the skeleton.\nvar description: String\nA textual description of this joint.\nTracking the position of hand joints\nvar anchorFromJointTransform: simd_float4x4\nThe position and orientation of this joint relative to the base joint of the skeleton.\nvar parentFromJointTransform: simd_float4x4\nThe transform from the joint to its parent joint’s coordinate system.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking this joint.\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nRetrieving specific hand joints\nfunc joint(HandSkeleton.JointName) -> HandSkeleton.Joint\nRetrieves a hand joint based on the joint name you specify.\nenum HandSkeleton.JointName\nThe names of different hand joints."
  },
  {
    "title": "id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/4108407-id",
    "html": "Discussion\n\nNote\n\nThis documentation comment was inherited from Identifiable.\n\nSee Also\nIdentifying hand anchors\nvar id: UUID\nA globally unique identifier for a hand anchor.\ntypealias HandAnchor.ID\nThe type that identifies a hand anchor."
  },
  {
    "title": "id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor/4108406-id",
    "html": "Relationships\nFrom Protocol\nAnchor\nIdentifiable\nSee Also\nIdentifying hand anchors\ntypealias HandAnchor.ID\nThe type that identifies a hand anchor.\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when HandAnchor conforms to AnyObject."
  },
  {
    "title": "allJoints | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton/4284987-alljoints",
    "html": "See Also\nInspecting hand skeletons\nvar description: String\nA textual representation of a hand skeleton."
  },
  {
    "title": "requiredAuthorizations | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handtrackingprovider/4131743-requiredauthorizations",
    "html": "Relationships\nFrom Protocol\nDataProvider\nSee Also\nCreating a hand tracking provider\ninit()\nCreates a hand tracking provider.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports hand tracking providers."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/4131960-init",
    "html": "See Also\nTracking objects\nvar anchorUpdates: AnchorUpdateSequence<WorldAnchor>\nA sequence of updates to anchors this provider tracks.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to track world anchors.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports world tracking providers.\nfunc addAnchor(WorldAnchor)\nAdds a world anchor you supply to the set of currently tracked anchors.\nstruct WorldTrackingProvider.Error\nAn error that can occur during a world-tracking session."
  },
  {
    "title": "geometry | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/4108494-geometry",
    "html": "See Also\nGetting the shape of a plane anchor\nstruct PlaneAnchor.Geometry\nThe geometry of a plane anchor."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/4139388-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting a plane anchor\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a plane in world space.\nvar alignment: PlaneAnchor.Alignment\nThe alignment — horizontal or vertical — of a plane anchor relative to gravity.\nenum PlaneAnchor.Alignment\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nvar classification: PlaneAnchor.Classification\nThe kind of real-world object that ARKit determined this plane anchor might be.\nenum PlaneAnchor.Classification\nThe kinds of object classification a plane anchor can have."
  },
  {
    "title": "framesPerSecond | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/videoformat/2942259-framespersecond",
    "html": "See Also\nAccessing format information\nvar imageResolution: CGSize\nThe size, in pixels, of video images captured in the session.\nvar isRecommendedForHighResolutionFrameCapturing: Bool\nDetermines whether the framework considers a format suitable for high-resolution frame capture.\nvar isVideoHDRSupported: Bool\nDetermines whether the format supports high dynamic range (HDR)."
  },
  {
    "title": "alignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/4108492-alignment",
    "html": "See Also\nInspecting a plane anchor\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a plane in world space.\nenum PlaneAnchor.Alignment\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nvar classification: PlaneAnchor.Classification\nThe kind of real-world object that ARKit determined this plane anchor might be.\nenum PlaneAnchor.Classification\nThe kinds of object classification a plane anchor can have.\nvar description: String\nA textual description of a plane anchor."
  },
  {
    "title": "isRecommendedForHighResolutionFrameCapturing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/videoformat/3930052-isrecommendedforhighresolutionfr",
    "html": "See Also\nAccessing format information\nvar framesPerSecond: Int\nThe rate at which the session captures video and provides AR frame information.\nvar imageResolution: CGSize\nThe size, in pixels, of video images captured in the session.\nvar isVideoHDRSupported: Bool\nDetermines whether the format supports high dynamic range (HDR)."
  },
  {
    "title": "PlaneAnchor.Alignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/alignment",
    "html": "Topics\nGetting plane alignment\ncase horizontal\nThe plane is positioned horizontally.\ncase vertical\nThe plane is positioned vertically.\nComparing plane alignment\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (PlaneAnchor.Alignment, PlaneAnchor.Alignment) -> Bool\nstatic func != (PlaneAnchor.Alignment, PlaneAnchor.Alignment) -> Bool\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nInspecting a plane anchor\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a plane in world space.\nvar alignment: PlaneAnchor.Alignment\nThe alignment — horizontal or vertical — of a plane anchor relative to gravity.\nvar classification: PlaneAnchor.Classification\nThe kind of real-world object that ARKit determined this plane anchor might be.\nenum PlaneAnchor.Classification\nThe kinds of object classification a plane anchor can have.\nvar description: String\nA textual description of a plane anchor."
  },
  {
    "title": "originFromAnchorTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor/4293522-originfromanchortransform",
    "html": "Relationships\nFrom Protocol\nAnchor\nSee Also\nInspecting a plane anchor\nvar alignment: PlaneAnchor.Alignment\nThe alignment — horizontal or vertical — of a plane anchor relative to gravity.\nenum PlaneAnchor.Alignment\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nvar classification: PlaneAnchor.Classification\nThe kind of real-world object that ARKit determined this plane anchor might be.\nenum PlaneAnchor.Classification\nThe kinds of object classification a plane anchor can have.\nvar description: String\nA textual description of a plane anchor."
  },
  {
    "title": "isTracked | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/trackableanchor/4108558-istracked",
    "html": "Required"
  },
  {
    "title": "captureDevicePosition | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/videoformat/3112799-capturedeviceposition",
    "html": "See Also\nInspecting the video source\nenum AVCaptureDevice.Position\nConstants that indicate the physical position of a capture device.\nvar captureDeviceType: AVCaptureDevice.DeviceType\nThe camera that supplies the video format."
  },
  {
    "title": "captureDeviceType | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/videoformat/3738412-capturedevicetype",
    "html": "Discussion\n\nTo specify a particular video format, select from your configuration's supportedVideoFormats and set the desired format to the configuration's videoFormat property.\n\nFor example, to specify the ultra-wide camera in a face-tracking session, search the supported video formats for the builtInUltraWideCamera capture device.\n\nlet config = ARFaceTrackingConfiguration()\nfor videoFormat in ARFaceTrackingConfiguration.supportedVideoFormats {\n    if videoFormat.captureDeviceType == .builtInUltraWideCamera {\n        config.videoFormat = videoFormat\n        break\n    }\n}\nsession.run(config)\n\n\nImportant\n\nAR frames only contain depth data (capturedDepthData) in face-tracking sessions that use the TrueDepth camera.\n\nSee Also\nInspecting the video source\nvar captureDevicePosition: AVCaptureDevice.Position\nThe position of the capture device.\nenum AVCaptureDevice.Position\nConstants that indicate the physical position of a capture device."
  },
  {
    "title": "sceneDepth | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/framesemantics/3516902-scenedepth",
    "html": "Discussion\n\nEnable this option on a world-tracking configuration (ARWorldTrackingConfiguration) to instruct ARKit to provide your app with the distance between the user's device and the real-world objects in the camera feed. ARKit samples this distance using the LiDAR scanner and provides the results through the sceneDepth property on the session's currentFrame.\n\nARKit creates this object from LiDAR readings at same time as the current frame. The data in sceneDepth reflects the distance from the device to real-world objects pictured in the frame's capturedImage. Alternatively, ARKit provides a smoothedSceneDepth property that minimizes the difference in LiDAR readings across frames.\n\nARKit supports scene depth only on LiDAR-capable devices, so call supportsFrameSemantics(_:) to ensure device support before attempting to enable scene depth.\n\nSee Also\nAccessing Depth\nstatic var smoothedSceneDepth: ARConfiguration.FrameSemantics\nAn option that provides the distance from the device to real-world objects, averaged across several frames."
  },
  {
    "title": "ARConfiguration.WorldAlignment.gravity | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/worldalignment/gravity",
    "html": "Discussion\n\nThe y-axis matches the direction of gravity as detected by the device's motion sensing hardware; that is, the vector (0,-1,0) points downward.\n\nThe position and orientation of the device as of when the session configuration is first run determine the rest of the coordinate system: For the z-axis, ARKit chooses a basis vector (0,0,-1) pointing in the direction the device camera faces and perpendicular to the gravity axis. ARKit chooses a x-axis based on the z- and y-axes using the right hand rule—that is, the basis vector (1,0,0) is orthogonal to the other two axes, and (for a viewer looking in the negative-z direction) points toward the right.\n\nFigure 1 In gravity-only alignment, X and Z directions are relative to the device's initial orientation.\n\nSee Also\nAlignments\ncase gravityAndHeading\nThe coordinate system's y-axis is parallel to gravity, its x- and z-axes are oriented to compass heading, and its origin is the initial position of the device.\ncase camera\nThe scene coordinate system is locked to match the orientation of the camera."
  },
  {
    "title": "AnchorUpdate.Event | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdate/event",
    "html": "Topics\nInspecting anchor update events\ncase added\nAn event that occurs when ARKit starts tracking an anchor.\ncase updated\nAn event that occurs when an existing anchor updates data.\ncase removed\nAn event that occurs when ARKit stops tracking an anchor.\nComparing anchor update events\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (AnchorUpdate<AnchorType>.Event, AnchorUpdate<AnchorType>.Event) -> Bool\nstatic func != (AnchorUpdate<AnchorType>.Event, AnchorUpdate<AnchorType>.Event) -> Bool\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nInspecting anchor updates\nlet anchor: AnchorType\nThe anchor that this anchor update contains information about.\nvar timestamp: TimeInterval\nThe time when this anchor update occurred.\nlet event: AnchorUpdate<AnchorType>.Event\nAn event that indicates whether an anchor was added, updated, or removed.\nvar description: String\nA textual representation of this anchor update."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/framesemantics/3089199-init",
    "html": "Discussion\n\nInitialize an ARConfiguration.FrameSemantics with the value of zero to indicate that no optional frame features are enabled.\n\nYou enable a feature by adding it to your configuration's frameSemantics option set."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdate/4139374-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting anchor updates\nlet anchor: AnchorType\nThe anchor that this anchor update contains information about.\nvar timestamp: TimeInterval\nThe time when this anchor update occurred.\nlet event: AnchorUpdate<AnchorType>.Event\nAn event that indicates whether an anchor was added, updated, or removed.\nenum AnchorUpdate.Event\nThe events that cause anchor updates."
  },
  {
    "title": "event | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdate/4111175-event",
    "html": "See Also\nInspecting anchor updates\nlet anchor: AnchorType\nThe anchor that this anchor update contains information about.\nvar timestamp: TimeInterval\nThe time when this anchor update occurred.\nenum AnchorUpdate.Event\nThe events that cause anchor updates.\nvar description: String\nA textual representation of this anchor update."
  },
  {
    "title": "anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdate/4111174-anchor",
    "html": "See Also\nInspecting anchor updates\nvar timestamp: TimeInterval\nThe time when this anchor update occurred.\nlet event: AnchorUpdate<AnchorType>.Event\nAn event that indicates whether an anchor was added, updated, or removed.\nenum AnchorUpdate.Event\nThe events that cause anchor updates.\nvar description: String\nA textual representation of this anchor update."
  },
  {
    "title": "estimatedDepthData | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/3152989-estimateddepthdata",
    "html": "Discussion\n\nEach non-background pixel in the segmentation buffer maps to a depth value in this buffer. Use this to occlude people from the rendering of your app's virtual content.\n\nIf you implement a custom renderer, you apply this property by using alpha and depth mattes provided with ARMatteGenerator.\n\nApps using one of the standard renderers don't need this property to occlude virtual content with people. The standard renderers (ARView, ARSCNView, and ARSKView) enable people occlusion when you add personSegmentation or personSegmentationWithDepth to your configuration's frameSemantics.\n\nSee Also\nChecking for people\nvar detectedBody: ARBody2D?\nThe screen position information of a body that ARKit recognizes in the camera image.\nclass ARBody2D\nThe screen-space representation of a person ARKit recognizes in the camera feed.\nvar segmentationBuffer: CVPixelBuffer?\nA buffer that contains pixel information identifying the shape of objects from the camera feed that you use to occlude virtual content.\nenum ARFrame.SegmentationClass\nA categorization of a pixel that defines a type of content you use to occlude your app's virtual content."
  },
  {
    "title": "Visualizing and Interacting with a Reconstructed Scene | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/visualizing_and_interacting_with_a_reconstructed_scene",
    "html": "Overview\n\nOn a fourth-generation iPad Pro running iPad OS 13.4 or later, ARKit uses the LiDAR Scanner to create a polygonal model of the physical environment. The LiDAR Scanner quickly retrieves depth information from a wide area in front of the user, so ARKit can estimate the shape of the real world without requiring the user to move. ARKit converts the depth information into a series of vertices that connect to form a mesh. To partition the information, ARKit makes multiple anchors, each assigned a unique portion of the mesh. Collectively, the mesh anchors represent the real-world scene around the user.\n\nWith these meshes, you can:\n\nMore accurately locate points on real-world surfaces.\n\nClassify real-world objects that ARKit can recognize.\n\nOcclude your app’s virtual content with real-world objects that are in front of it.\n\nHave virtual content interact with the physical environment realistically, for example, by bouncing a virtual ball off a real-world wall and having the ball follow the laws of physics.\n\nThis sample app presents an AR experience using RealityKit. The figure below illustrates how RealityKit leverages real-world information from ARKit, and creates a debug visualization when you run this app and point the device at a real-world chair.\n\nVisualize the Shape of the Physical Environment\n\nTo enable scene meshes, the sample sets a world-configuration’s sceneReconstruction property to one of the mesh options.\n\narView.automaticallyConfigureSession = false\nlet configuration = ARWorldTrackingConfiguration()\nconfiguration.sceneReconstruction = .meshWithClassification\n\n\nThe sample uses RealityKit’s ARView to render its graphics. To visualize meshes at runtime, ARView offers the sceneUnderstanding debugging option.\n\narView.debugOptions.insert(.showSceneUnderstanding)\n\n\nNote\n\nThe sample enables mesh visualization only to demonstrate the mesh feature; similarly, you normally enable mesh visualization only for debugging purposes.\n\nTo begin the AR experience, the sample configures and runs the session when the app first starts, in the main view controller’s viewDidLoad callback.\n\narView.session.run(configuration)\n\nAdd Plane Detection\n\nWhen an app enables plane detection with scene reconstruction, ARKit considers that information when making the mesh. Where the LiDAR scanner may produce a slightly uneven mesh on a real-world surface, ARKit smooths out the mesh where it detects a plane on that surface.\n\nTo demonstrate the difference that plane detection makes on meshes, this app displays a toggle button. In the button handler, the sample adjusts the plane-detection configuration and restarts the session to effect the change.\n\n@IBAction func togglePlaneDetectionButtonPressed(_ button: UIButton) {\n    guard let configuration = arView.session.configuration as? ARWorldTrackingConfiguration else {\n        return\n    }\n    if configuration.planeDetection == [] {\n        configuration.planeDetection = [.horizontal, .vertical]\n        button.setTitle(\"Stop Plane Detection\", for: [])\n    } else {\n        configuration.planeDetection = []\n        button.setTitle(\"Start Plane Detection\", for: [])\n    }\n    arView.session.run(configuration)\n}\n\n\nLocate a Point on an Object’s Surface\n\nApps that retrieve surface locations using meshes can achieve unprecedented accuracy. By considering the mesh, raycasts can intersect with nonplanar surfaces, or surfaces with little or no features, like white walls.\n\nTo demonstrate accurate raycast results, this app casts a ray when the user taps the screen. The sample specifies the ARRaycastQuery.Target.estimatedPlane allowable-target, and ARRaycastQuery.TargetAlignment.any alignment option, as required to retrieve a point on a meshed, real-world object.\n\nlet tapLocation = sender.location(in: arView)\nif let result = arView.raycast(from: tapLocation, allowing: .estimatedPlane, alignment: .any).first {\n    // ...\n\n\nWhen the user’s raycast returns a result, this app gives visual feedback by placing a small sphere at the intersection point.\n\nlet resultAnchor = AnchorEntity(world: result.worldTransform)\nresultAnchor.addChild(sphere(radius: 0.01, color: .lightGray))\narView.scene.addAnchor(resultAnchor, removeAfter: 3)\n\n\nClassify Real-World Objects\n\nARKit has a classification feature that analyzes its meshed model of the world to recognize specific, real-world objects. Within the mesh, ARKit can classify floors, tables, seats, windows, and ceilings. See ARMeshClassification for the full list.\n\nIf the user taps the screen and the raycast intersects with a meshed, real-world object, this app displays text of the mesh’s classification.\n\nWhen the automaticallyConfigureSession property of ARView is true, RealityKit disables classification by default because it isn’t required for occlusion and physics. To enable mesh classification, the sample overrides the default by setting the sceneReconstruction property to meshWithClassification.\n\narView.automaticallyConfigureSession = false\nlet configuration = ARWorldTrackingConfiguration()\nconfiguration.sceneReconstruction = .meshWithClassification\n\n\nThis app attempts to retrieve a classification for the intersection point from the mesh.\n\nnearbyFaceWithClassification(to: result.worldTransform.position) { (centerOfFace, classification) in\n    // ...\n\n\nEvery three vertices in the mesh form a triangle, called a face. ARKit assigns a classification for each face, so the sample searches through the mesh for a face near the intersection point. If the face has a classification, this app displays it on screen. Because this routine involves extensive processing, the sample does the work asynchronously, so the renderer does not stall.\n\nDispatchQueue.global().async {\n    for anchor in meshAnchors {\n        for index in 0..<anchor.geometry.faces.count {\n            // Get the center of the face so that we can compare it to the given location.\n            let geometricCenterOfFace = anchor.geometry.centerOf(faceWithIndex: index)\n            \n            // Convert the face's center to world coordinates.\n            var centerLocalTransform = matrix_identity_float4x4\n            centerLocalTransform.columns.3 = SIMD4<Float>(geometricCenterOfFace.0, geometricCenterOfFace.1, geometricCenterOfFace.2, 1)\n            let centerWorldPosition = (anchor.transform * centerLocalTransform).position\n             \n            // We're interested in a classification that is sufficiently close to the given location––within 5 cm.\n            let distanceToFace = distance(centerWorldPosition, location)\n            if distanceToFace <= 0.05 {\n                // Get the semantic classification of the face and finish the search.\n                let classification: ARMeshClassification = anchor.geometry.classificationOf(faceWithIndex: index)\n                completionBlock(centerWorldPosition, classification)\n                return\n            }\n        }\n    }\n\n\nWith the classification in-hand, the sample creates 3D text to display it.\n\nlet textEntity = self.model(for: classification)\n\n\nTo prevent the mesh from partially occluding the text, the sample offsets the text slightly to help readability. The sample calculates the offset in the negative direction of the ray, which effectively moves the text slightly toward the camera, which is away from the surface.\n\nlet rayDirection = normalize(result.worldTransform.position - self.arView.cameraTransform.translation)\nlet textPositionInWorldCoordinates = result.worldTransform.position - (rayDirection * 0.1)\n\n\nTo make the text always appear the same size on screen, the sample applies a scale based on text’s distance from the camera.\n\nlet raycastDistance = distance(result.worldTransform.position, self.arView.cameraTransform.translation)\ntextEntity.scale = .one * raycastDistance\n\n\nTo display the text, the sample puts it in an anchored entity at the adjusted intersection-point, which is oriented to face the camera.\n\nvar resultWithCameraOrientation = self.arView.cameraTransform\nresultWithCameraOrientation.translation = textPositionInWorldCoordinates\nlet textAnchor = AnchorEntity(world: resultWithCameraOrientation.matrix)\ntextAnchor.addChild(textEntity)\nself.arView.scene.addAnchor(textAnchor, removeAfter: 3)\n\n\nTo visualize the location of the face’s vertex from which the classification was retrieved, the sample creates a small sphere at the vertex’s real-world position.\n\nif let centerOfFace = centerOfFace {\n    let faceAnchor = AnchorEntity(world: centerOfFace)\n    faceAnchor.addChild(self.sphere(radius: 0.01, color: classification.color))\n    self.arView.scene.addAnchor(faceAnchor, removeAfter: 3)\n}\n\n\nOcclude Virtual Content with a Mesh\n\nOcclusion is a feature where parts of the real world cover an app’s virtual content, from the camera’s perspective. To achieve this illusion, RealityKit checks for any meshes in front of virtual content, viewed by the user, and omits drawing any part of the virtual content obscured by those meshes. The sample enables occlusion by adding the occlusion option to the environment’s sceneUnderstanding property.\n\narView.environment.sceneUnderstanding.options.insert(.occlusion)\n\n\nAt runtime, this app omits drawing portions of the virtual text that are behind any part of the meshed, real world.\n\nInteract with Real-World Objects Using Physics\n\nWith scene meshes, virtual content can interact with the physical environment realistically because the meshes give RealityKit’s physics engine an accurate model of the world. The sample enables physics by adding the physics option to the environment’s sceneUnderstanding property.\n\narView.environment.sceneUnderstanding.options.insert(.physics)\n\n\nTo detect when virtual content comes in contact with a meshed, real-world object, the sample defines the text’s proportions using a collision shape in the addAnchor(_:,removeAfter:) Scene extension.\n\nif model.collision == nil {\n    model.generateCollisionShapes(recursive: true)\n    model.physicsBody = .init()\n}\n\n\nWhen this app classifies an object and displays some text, it waits three seconds before dropping the virtual text. When the sample sets the text’s physicsBody’s mode to PhysicsBodyMode.dynamic, the text reacts to gravity by falling.\n\nTimer.scheduledTimer(withTimeInterval: seconds, repeats: false) { (timer) in\n    model.physicsBody?.mode = .dynamic\n}\n\n\nAs the text falls, it reacts when colliding with a meshed, real-world object, such as landing on the floor.\n\nSee Also\nPhysical Objects\nScanning and detecting 3D objects\nRecord spatial features of real-world objects, then use the results to find those objects in the user’s environment and trigger AR content.\nclass ARObjectAnchor\nAn anchor for a real-world 3D object that ARKit detects in the physical environment.\nclass ARReferenceObject\nThe description of a 3D object that you want ARKit to detect in the physical environment."
  },
  {
    "title": "segmentationBuffer | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/2984226-segmentationbuffer",
    "html": "Discussion\n\nARKit generates the contents of this buffer by processing the camera feed.\n\nIf you implement a custom renderer, you apply this property by using alpha and depth mattes provided with ARMatteGenerator.\n\nApps using one of the standard renderers don't need this this property to occlude virtual content with people. The standard renderers (ARView, ARSCNView, and ARSKView) enable people occlusion when you add personSegmentation or personSegmentationWithDepth to your configuration's frameSemantics.\n\nSee Also\nChecking for people\nvar detectedBody: ARBody2D?\nThe screen position information of a body that ARKit recognizes in the camera image.\nclass ARBody2D\nThe screen-space representation of a person ARKit recognizes in the camera feed.\nvar estimatedDepthData: CVPixelBuffer?\nA buffer that represents the estimated depth values from the camera feed that you use to occlude virtual content.\nenum ARFrame.SegmentationClass\nA categorization of a pixel that defines a type of content you use to occlude your app's virtual content."
  },
  {
    "title": "Occluding Virtual Content with People | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/camera_lighting_and_effects/occluding_virtual_content_with_people",
    "html": "Overview\n\nBy default, virtual content covers anything in the camera feed. For example, when a person passes in front of a virtual object, the object is drawn on top of the person, which can break the illusion of the AR experience.\n\nTo cover your app’s virtual content with people that ARKit perceives in the camera feed, you enable people occlusion. Your app can then render a virtual object behind people who pass in front of the camera. ARKit accomplishes the occlusion by identifying regions in the camera feed where people reside, and preventing virtual content from drawing into that region’s pixels.\n\nThis sample renders its graphics using RealityKit, but you can follow the same steps to use people occlusion with SceneKit. To enable people occlusion in Metal apps, see Effecting People Occlusion in Custom Renderers.\n\nVerify Device Support for People Occlusion\n\nPeople occlusion is supported on Apple A12 and later devices. Before attempting to enable people occlusion, verify that the user’s device supports it.\n\nguard ARWorldTrackingConfiguration.supportsFrameSemantics(.personSegmentationWithDepth) else {\n    fatalError(\"People occlusion is not supported on this device.\")\n}\n\n\nNote\n\nIf your device doesn’t support people occlusion, the sample stops. However, if the user’s device doesn’t support people occlusion, you should continue your AR experience without it.\n\nEnable People Occlusion\n\nIf the user’s device supports people occlusion, enable it by adding the personSegmentationWithDepth option to your configuration’s frame semantics.\n\nconfig.frameSemantics.insert(.personSegmentationWithDepth)\n\n\nAny time you change your session’s configuration, rerun the session to effect the configuration change.\n\narView.session.run(config)\n\n\nThe personSegmentationWithDepth option specifies that a person occludes a virtual object only when the person is closer to the camera than the virtual object.\n\nAlternatively, the personSegmentation frame semantic gives you the option of always occluding virtual content with any people that ARKit perceives in the camera feed irrespective of depth. This technique is useful, for example, in green-screen scenarios.\n\nDisable People Occlusion\n\nYou might choose to disable people occlusion for performance reasons if, for example, no virtual content is present in the scene, or if the device has reached a serious or critical thermalState (see ProcessInfo.ThermalState). To temporarily disable people occlusion, remove that option from your app’s frameSemantics.\n\nconfig.frameSemantics.remove(.personSegmentationWithDepth)\n\n\nThen, rerun your session to effect the configuration change.\n\narView.session.run(config)\n\nSee Also\nOcclusion\nEffecting People Occlusion in Custom Renderers\nOcclude your app’s virtual content where ARKit recognizes people in the camera feed by using matte generator.\nVisualizing and Interacting with a Reconstructed Scene\nEstimate the shape of the physical environment using a polygonal mesh.\nclass ARMatteGenerator\nAn object that creates matte textures you use to occlude your app's virtual content with people, that ARKit recognizes in the camera feed."
  },
  {
    "title": "Effecting People Occlusion in Custom Renderers | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/camera_lighting_and_effects/effecting_people_occlusion_in_custom_renderers",
    "html": "Overview\n\nNote\n\nThis sample code project is associated with WWDC 2019 session 607: Bringing People into AR.\n\nNote\n\nTo run the app, use an iOS device with A12 chip or later.\n\nSee Also\nOcclusion\nOccluding Virtual Content with People\nCover your app’s virtual content with people that ARKit perceives in the camera feed.\nVisualizing and Interacting with a Reconstructed Scene\nEstimate the shape of the physical environment using a polygonal mesh.\nclass ARMatteGenerator\nAn object that creates matte textures you use to occlude your app's virtual content with people, that ARKit recognizes in the camera feed."
  },
  {
    "title": "ARMatteGenerator | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armattegenerator",
    "html": "Overview\n\nUse this class when you want full control over occluding your app's virtual content, based on people ARKit recognizes in the camera feed.\n\nNote\n\nApps using one of the standard renderers (ARView or ARSCNView) don't need this class to effect people occlusion. See frameSemantics for more information.\n\nTo assist your custom renderer with people occlusion, matte generator processes alpha and depth information in a frame's segmentationBuffer and estimatedDepthData to provide you with matte and depth textures. You use these textures to layer people on top of your app's virtual content.\n\nTopics\nCreating a Matte Generator\ninit(device: MTLDevice, matteResolution: ARMatteGenerator.Resolution)\nCreates an AR matte generator.\nCreating an Alpha Matte Texture\nfunc generateMatte(from: ARFrame, commandBuffer: MTLCommandBuffer) -> MTLTexture\nGenerates alpha matte at either full resolution or half the resolution of the captured image.\nCreating a Depth Texture\nfunc generateDilatedDepth(from: ARFrame, commandBuffer: MTLCommandBuffer) -> MTLTexture\nGenerates dilated depth at the resolution of the segmentation stencil.\nControlling Resolution\nenum ARMatteGenerator.Resolution\nA resolution for a matte texture.\nRelationships\nInherits From\nNSObject\nSee Also\nOcclusion\nOccluding Virtual Content with People\nCover your app’s virtual content with people that ARKit perceives in the camera feed.\nEffecting People Occlusion in Custom Renderers\nOcclude your app’s virtual content where ARKit recognizes people in the camera feed by using matte generator.\nVisualizing and Interacting with a Reconstructed Scene\nEstimate the shape of the physical environment using a polygonal mesh."
  },
  {
    "title": "personSegmentation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/framesemantics/3089125-personsegmentation",
    "html": "Discussion\n\nThe personSegmentation frame semantic specifies that any person ARKit detects in the camera feed occludes virtual content, regardless of the person's depth in the scene.\n\nWhen this option is enabled, ARKit sets the estimatedDepthData and segmentationBuffer properties to serve as a foundation for people occlusion. The standard renderers (ARView, and ARSCNView) use those properties to implement people occlusion for you. See frameSemantics for more information.\n\nSee Also\nOccluding Virtual Content with People\nstatic var personSegmentationWithDepth: ARConfiguration.FrameSemantics\nAn option that indicates that people occlude your app's virtual content depending on depth."
  },
  {
    "title": "runWithConfiguration: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2865608-runwithconfiguration",
    "html": "Parameters\nconfiguration\n\nAn object that defines motion and scene tracking behaviors for the session.\n\nDiscussion\n\nThe session tracks device motion, captures and processes scene imagery from the device camera, and coordinates with your delegate object or ARSCNView or ARSKView view only when running.\n\nCalling this method on a session that has already started transitions immediately to the new session configuration. After you call this method, the session runs asynchronously.\n\nTo determine how existing session state transitions to the new configuration, use the runWithConfiguration:options: method instead. Calling runWithConfiguration: is equivalent to calling runWithConfiguration:options: with no options enabled.\n\nSee Also\nConfiguring and running a session\n- runWithConfiguration:options:\nStarts AR processing for the session with the specified configuration and options.\nidentifier\nA unique identifier of the running session.\nARSessionRunOptions\nOptions for transitioning an AR session's current state when you change its configuration.\nconfiguration\nAn object that defines motion and scene tracking behaviors for the session.\n- pause\nPauses processing in the session."
  },
  {
    "title": "bodyDetection | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/framesemantics/3214027-bodydetection",
    "html": "Discussion\n\nWhen you set this option in your configuration's frameSemantics property, ARKit describes the joint positions of a body it detects in the camera image, using normalized coordinates. See detectedBody for more information."
  },
  {
    "title": "personSegmentationWithDepth | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/framesemantics/3194576-personsegmentationwithdepth",
    "html": "Discussion\n\nThe personSegmentationWithDepth frame semantic specifies that any person ARKit detects in the camera feed should occlude virtual content, depending on the person's depth in the scene.\n\nWhen this option is enabled, ARKit sets the estimatedDepthData and segmentationBuffer properties to serve as a foundation for people occlusion. The standard renderers (ARView, and ARSCNView) use those properties to implement people occlusion for you. See frameSemantics for more information.\n\nSee Also\nOccluding Virtual Content with People\nstatic var personSegmentation: ARConfiguration.FrameSemantics\nAn option that indicates that people occlude your app's virtual content."
  },
  {
    "title": "initialWorldMap | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/2968180-initialworldmap",
    "html": "Discussion\n\nAn ARWorldMap encapsulates the state of a running ARSession. This state includes ARKit's awareness of the physical space the user moves the device in (which ARKit uses to determine the device's position and orientation), as well as any ARAnchor objects added to the session (which can represent detected real-world features or virtual content placed by your app). After you use getCurrentWorldMap(completionHandler:) to save a session's world map, you can assign it to a configuration's initialWorldMap property and use run(_:options:) to start another session with the same spatial awareness and anchors.\n\nBy saving world maps and using them to start new sessions, your app can add new AR capabilities:\n\nMultiuser AR experiences. Create a shared frame of reference by sending archived ARWorldMap objects to a nearby user's device. With two devices tracking the same world map, you can build a networked experience where both users can see and interact with the same virtual content.\n\nPersistent AR experiences. Save a world map when your app becomes inactive, then restore it the next time your app launches in the same physical environment. You can use anchors from the resumed world map to place the same virtual content at the same positions from the saved session.\n\nWhen you run a session with an initial world map, the session starts in the ARCamera.TrackingState.limited(_:) (ARCamera.TrackingState.Reason.relocalizing) tracking state while ARKit attempts to reconcile the recorded world map with the current environment. If successful, the tracking state becomes ARCamera.TrackingState.normal after a short time, indicating that the current world coordinate system and anchors match those from the recorded world map.\n\nIf ARKit cannot reconcile the recorded world map with the current environment (for example, if the device is in an entirely different place from where the world map was recorded), the session remains in the ARCamera.TrackingState.Reason.relocalizing state indefinitely.\n\nSee Also\nCreating a Configuration\ninit()\nInitializes a new world-tracking configuration."
  },
  {
    "title": "ARFrame.WorldMappingStatus.mapped | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/worldmappingstatus/mapped",
    "html": "Discussion\n\nWhen the worldMappingStatus of the session's currentFrame is ARFrame.WorldMappingStatus.mapped, the session has produced a high-fidelity internal map of the real-world space around the device's current position and the scene visible to the camera.\n\nThis status provides the highest reliability for relocalizing to a saved world map, provided that:\n\nYou call getCurrentWorldMap(completionHandler:) to save the world map while the status of the currentFrame is ARFrame.WorldMappingStatus.mapped.\n\nWhen you run a new session (later or on another device) from that ARWorldMap, the device running the new session is at a real-world position and orientation similar to that when the world map was saved."
  },
  {
    "title": "resetTracking | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/runoptions/2875727-resettracking",
    "html": "Discussion\n\nBy default, when you call the run(_:options:) method on a session that has run before or is already running, the session resumes device position tracking from its last known state. (For example, an ARAnchor object keeps its apparent position relative to the camera.) When you call the run(_:options:) method with a configuration of the same type as the session's current configuration, you can add this option to force device position tracking to return to its initial state.\n\nWhen you call the run(_:options:) method with a configuration of a different type than the session's current configuration, the session always resets tracking (that is, this option is implicitly enabled).\n\nIn either case, when you reset tracking, ARKit also removes any existing anchors from the session.\n\nSee Also\nRun Options\nstatic var removeExistingAnchors: ARSession.RunOptions\nAn option to remove any anchor objects associated with the session's previous run.\nstatic var stopTrackedRaycasts: ARSession.RunOptions\nAn option to stop all active tracked raycasts.\nstatic var resetSceneReconstruction: ARSession.RunOptions\nAn option to reset the scene mesh."
  },
  {
    "title": "ARCamera.TrackingState.Reason.relocalizing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate/reason/relocalizing",
    "html": "Discussion\n\nARKit cannot track device position or orientation when the session has been interrupted (for example, by dismissing the view hosting an AR session or switching to another app). When resuming the session after an interruption, the world coordinate system (used for placing anchors) likely no longer match the device's real-world environment.\n\nIf your session or view delegate implements the sessionShouldAttemptRelocalization(_:) method and returns true, ARKit attempts to reconcile pre- and post-interruption world tracking state. During this process, called relocalization, world tracking quality is ARCamera.TrackingState.limited(_:), with a resaon value of ARCamera.TrackingState.Reason.relocalizing, indicating that hit tests and anchor placement are less accurate.\n\nIf successful, relocalization ends after a short time, tracking quality returns to the ARCamera.TrackingState.normal state, and the world coordinate system and anchor positions reflect their state before the interruption.\n\nFor relocalization to succeed, the device must be returned to a position and orientation approximately near where it was when the session was interrupted. If these conditions never occur (or cannot occur; for example, if the device has moved to an entirely different environment), the session will remain in the ARCamera.TrackingState.Reason.relocalizing state indefinitely.\n\nImportant\n\nWhen in the ARCamera.TrackingState.Reason.relocalizing state, offer the user a way out in case relocalization never succeeds. For example, offer a button for resetting the session, which appears after the relocalizing state has remained for a fixed amount of time.\n\nSee Also\nInhibitors of Tracking Quality\ncase initializing\nThe AR session has not gathered enough camera or motion data to provide tracking information.\ncase excessiveMotion\nThe device is moving too fast for accurate image-based position tracking.\ncase insufficientFeatures\nThe scene visible to the camera doesn't contain enough distinguishable features for image-based position tracking."
  },
  {
    "title": "sessionShouldAttemptRelocalization(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionobserver/2941046-sessionshouldattemptrelocalizati",
    "html": "Parameters\nsession\n\nThe session providing information.\n\nDiscussion\n\nWhen ARKit loses track of the device's position or orientation, ARKit calls sessionShouldAttemptRelocalization(_:) to try to resume the experience where the user left off. This process is called relocalization. ARKit relocalizes when it loses access to the camera or other sensors on the device, such as when your app calls pause(), or when the user switches to another app and back.\n\nWhen relocalization succeeds, your app's virtual content appears in the same position it was in before the interruption. But if your app fails to relocalize, the world coordinate system and anchor positions are likely out of sync with the real world.\n\nHere are the possible scenarios:\n\nIf by default you don't implement this function, ARKit spends a few seconds trying to relocalize before restarting your session.\n\nIf you implement this function and return false, ARKit does not attempt to relocalize after an interruption, and the session restarts immediately.\n\nIf you implement this function and return true, you allow the user more time to find the location they were in and resume where they left off, but you're responsible for restarting the session when it's prudent to do so.\n\nWhen you return true, guide the user to facilitate relocalization; for example, by presenting a UI that asks them to return to their previous location. Relocalization works when ARKit recognizes the physical environment it observed before the interruption. If the user does not return to their previous location, the app stays in ARCamera.TrackingState.Reason.relocalizing state indefinitely. It's important to give the user a way to restart if they choose to. Using ARCoachingOverlayView makes much of this functionality available.\n\nTo respond to failed relocalization, call the session's run(_:options:) method with the resetTracking option. Resetting tracking during relocalization discards all world-tracking state from before the interruption, but keeps world-tracking results obtained during the relocalization attempt.\n\nSee Also\nHandling Interruptions\nfunc sessionWasInterrupted(ARSession)\nTells the delegate that the session has temporarily stopped processing frames and tracking device position.\nfunc sessionInterruptionEnded(ARSession)\nTells the delegate that the session has resumed processing frames and tracking device position.\nRelated Documentation\nARTrackingStateReasonRelocalizing\nThe AR session is attempting to resume after an interruption.\ncase relocalizing\nThe AR session is attempting to resume after an interruption."
  },
  {
    "title": "ARCamera.TrackingState.Reason | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate/reason",
    "html": "Topics\nInhibitors of Tracking Quality\ncase initializing\nThe AR session has not gathered enough camera or motion data to provide tracking information.\ncase relocalizing\nThe AR session is attempting to resume after an interruption.\ncase excessiveMotion\nThe device is moving too fast for accurate image-based position tracking.\ncase insufficientFeatures\nThe scene visible to the camera doesn't contain enough distinguishable features for image-based position tracking.\nHashes of Tracking Quality\nfunc hash(into: inout Hasher)\nHashes the reason by passing it to the given hash function.\nstatic func == (ARCamera.TrackingState.Reason, ARCamera.TrackingState.Reason) -> Bool\nIndicates whether two reasons are equal.\nstatic func != (ARCamera.TrackingState.Reason, ARCamera.TrackingState.Reason) -> Bool\nReturns a Boolean value indicating whether two values are not equal.\nvar hashValue: Int\nA value that identifies an object uniquely as compared to other instances of the same type.\nSee Also\nDetermining the camera tracking status\ncase notAvailable\nCamera position tracking is not available.\ncase limited(ARCamera.TrackingState.Reason)\nTracking is available, but the quality of results is questionable.\ncase normal\nCamera position tracking is providing optimal results."
  },
  {
    "title": "ARCamera.TrackingState.Reason.insufficientFeatures | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate/reason/insufficientfeatures",
    "html": "See Also\nInhibitors of Tracking Quality\ncase initializing\nThe AR session has not gathered enough camera or motion data to provide tracking information.\ncase relocalizing\nThe AR session is attempting to resume after an interruption.\ncase excessiveMotion\nThe device is moving too fast for accurate image-based position tracking."
  },
  {
    "title": "session | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionproviding/3192184-session",
    "html": "Required\n\nDiscussion\n\nSome clients may use key-value observation (KVO) to be notified when this property changes values. To support KVO, Swift classes that adopt ARSessionProviding should mark its session as @objc and dynamic."
  },
  {
    "title": "isCollaborationEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration/3152987-iscollaborationenabled",
    "html": "Discussion\n\nThe default value of this property is false. When you enable collaboration, ARKit invokes session(_:didOutputCollaborationData:) periodically, providing you with collaboration data to share with peers. Collaboration data contains information about the real-world surfaces ARKit detects, your position in relation to them, and any anchors you may have created.\n\nMultiple users sharing collaboration data with each other results in an AR experience in which the users interact by sharing and manipulating anchors. By including information that describes a user's unique view of the world, collaboration data enhances ARKit's understanding of the layout of the physical environment much more quickly than is possible with only one user.\n\nFor more information, see Creating a Collaborative Session.\n\nImportant\n\nCollaborative sessions work best with up to four participants.\n\nSharing Collaboration Data Over the Network\n\nYou are responsible for sending collaboration data over the network, including choosing the network framework and implementing the code. See Creating a Multiuser AR Experience for an example app that shares a world map among users via Multipeer Connectivity. Although Creating a Multiuser AR Experience demonstrates sharing world data among peer users, it does so using a host-guest model. The primary advantage of collaboration data is that it enables you to share world data peer-to-peer.\n\nThe data you send is a serialized version of the ARSession.CollaborationData object provided by your session. You serialize it using NSKeyedArchiver.\n\nfunc session(_ session: ARSession, didOutputCollaborationData data: ARSession.CollaborationData) {    \n    if let collaborationDataEncoded = try? NSKeyedArchiver.archivedData(withRootObject: data, requiringSecureCoding: true) {\n        multipeerSession.sendToAllPeers(collaborationDataEncoded)\n    } else {\n        fatalError(\"An error occurred while encoding collaboration data.\")\n    }\n}\n\n\nUpdating Your Session with Collaboration Data\n\nWhen you receive collaboration data from other users, you instantiate an ARSession.CollaborationData object with it, and pass the object to your session via update(with:).\n\nfunc receivedData(_ data: Data) {        \n    if let collaborationData = try? NSKeyedUnarchiver.unarchivedObject(ofClass: ARSession.CollaborationData.self, from: data) {\n        session.update(with: collaborationData)\n    } else {\n        fatalError(\"An error occurred while decoding collaboration data.\")\n    }\n}\n\n\n"
  },
  {
    "title": "OS_ar_reference_images | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_reference_images",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_hand_anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_hand_anchor",
    "html": "Relationships\nInherits From\nOS_ar_trackable_anchor\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_geometry_source | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_geometry_source",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_error | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_error",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "OS_ar_data_providers | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_data_providers",
    "html": "Relationships\nInherits From\nNSObjectProtocol\nSee Also\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldanchor/4108561-id",
    "html": "Relationships\nFrom Protocol\nAnchor\nIdentifiable\nSee Also\nIdentifying a world anchor\ntypealias WorldAnchor.ID\nThe type that identifies a world anchor.\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when WorldAnchor conforms to AnyObject."
  },
  {
    "title": "init(originFromAnchorTransform:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldanchor/4293523-init",
    "html": "Parameters\noriginFromAnchorTransform\n\nThe transform from the world anchor to the origin coordinate system."
  },
  {
    "title": "DataProviderState.stopped | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/dataproviderstate/stopped",
    "html": "See Also\nGetting the state of a data provider\ncase initialized\nThe data provider has been created.\ncase running\nThe data provider is running.\ncase paused\nThe data provider is paused."
  },
  {
    "title": "Tracking and altering images | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/tracking_and_altering_images",
    "html": "Overview\n\nTo demonstrate general image recognition, this sample app uses Vision to detect rectangular shapes in the user’s environment that are most likely artwork or photos. Run the app on an iPhone or iPad, and point the device’s camera at a movie poster or wall-mounted picture frame. When the app detects a rectangular shape, you extract the pixel data defined by that shape from the camera feed to create an image.\n\nThe sample app changes the appearance of the image by applying a Core ML model that performs a stylistic alteration. By repeating this action in succession, you achieve real-time image processing using a trained neural network.\n\nTo complete the effect of augmenting an image in the user’s environment, you use ARKit’s image tracking feature. ARKit can hold an altered image steady over the original image as the user moves the device in their environment. ARKit also tracks the image if it moves on its own, as when the app recognizes a banner on the side of a bus, and the bus begins to drive away.\n\nThis sample app uses SceneKit to render its graphics.\n\nDetect rectangular shapes in the user’s environment\n\nAs shown below, you can use Vision in real-time to check the camera feed for rectangles. You perform this check up to 10 times a second by using RectangleDetector to schedule a repeating timer with an updateInterval of 0.1 seconds.\n\ninit() {\n    self.updateTimer = Timer.scheduledTimer(withTimeInterval: updateInterval, repeats: true) { [weak self] _ in\n        if let capturedImage = ViewController.instance?.sceneView.session.currentFrame?.capturedImage {\n            self?.search(in: capturedImage)\n        }\n    }\n}\n\n\nBecause Vision requests can be taxing on the processor, check the camera feed no more than 10 times a second. Checking for rectangles more frequently may cause the app’s frame rate to decrease, without noticeably improving the app’s results.\n\nWhen you make Vision requests in real-time with an ARKit–based app, you should do so serially. By waiting for one request to finish before invoking another, you ensure that the AR experience remains smooth and free of interruptions. In the search function, you use the isBusy flag to ensure you’re only checking for one rectangle at a time:\n\nprivate func search(in pixelBuffer: CVPixelBuffer) {\n       guard !isBusy else { return }\n       isBusy = true\n\n\nThe sample sets the isBusy flag to false when a Vision request completes or fails.\n\nCrop the camera feed to an observed rectangle\n\nWhen Vision finds a rectangle in the camera feed, it provides you with the rectangle’s precise coordinates through a VNRectangleObservation. You apply those coordinates to a Core Image perspective correction filter to crop it, leaving you with just the image data inside the rectangular shape.\n\nguard let rectangle = request?.results?.first as? VNRectangleObservation else {\n    guard let error = error else { return }\n    print(\"Error: Rectangle detection failed - Vision request returned an error. \\(error.localizedDescription)\")\n    return\n}\nguard let filter = CIFilter(name: \"CIPerspectiveCorrection\") else {\n    print(\"Error: Rectangle detection failed - Could not create perspective correction filter.\")\n    return\n}\nlet width = CGFloat(CVPixelBufferGetWidth(currentCameraImage))\nlet height = CGFloat(CVPixelBufferGetHeight(currentCameraImage))\nlet topLeft = CGPoint(x: rectangle.topLeft.x * width, y: rectangle.topLeft.y * height)\nlet topRight = CGPoint(x: rectangle.topRight.x * width, y: rectangle.topRight.y * height)\nlet bottomLeft = CGPoint(x: rectangle.bottomLeft.x * width, y: rectangle.bottomLeft.y * height)\nlet bottomRight = CGPoint(x: rectangle.bottomRight.x * width, y: rectangle.bottomRight.y * height)\n\n\nfilter.setValue(CIVector(cgPoint: topLeft), forKey: \"inputTopLeft\")\nfilter.setValue(CIVector(cgPoint: topRight), forKey: \"inputTopRight\")\nfilter.setValue(CIVector(cgPoint: bottomLeft), forKey: \"inputBottomLeft\")\nfilter.setValue(CIVector(cgPoint: bottomRight), forKey: \"inputBottomRight\")\n\n\nlet ciImage = CIImage(cvPixelBuffer: currentCameraImage).oriented(.up)\nfilter.setValue(ciImage, forKey: kCIInputImageKey)\n\n\nguard let perspectiveImage: CIImage = filter.value(forKey: kCIOutputImageKey) as? CIImage else {\n    print(\"Error: Rectangle detection failed - perspective correction filter has no output image.\")\n    return\n}\ndelegate?.rectangleFound(rectangleContent: perspectiveImage)\n\n\nUsing the first image in the Overview, the camera image is:\n\nThe cropped result is:\n\nCreate a reference image\n\nTo prepare to track the cropped image, you create an ARReferenceImage, which provides ARKit with everything it needs, like its look and physical size, to locate that image in the physical environment.\n\nlet possibleReferenceImage = ARReferenceImage(referenceImagePixelBuffer, orientation: .up, physicalWidth: CGFloat(0.5))\n\n\nARKit requires that reference images contain sufficient detail to be recognizable; for example, ARKit can’t track an image that is a solid color with no features. To ensure ARKit can track a reference image, you validate it first before attempting to use it.\n\npossibleReferenceImage.validate { [weak self] (error) in\n    if let error = error {\n        print(\"Reference image validation failed: \\(error.localizedDescription)\")\n        return\n    }\n\n\nTrack the image using ARKit\n\nProvide the reference image to ARKit to get updates on where the image lies in the camera feed when the user moves their device. Do that by creating an image tracking session and passing the reference image in to the configuration’s trackingImages property.\n\nlet configuration = ARImageTrackingConfiguration()\nconfiguration.maximumNumberOfTrackedImages = 1\nconfiguration.trackingImages = trackingImages\nsceneView.session.run(configuration, options: runOptions)\n\n\nVision made the initial observation about where the image lies in 2D space in the camera feed, but ARKit resolves its location in 3D space, in the physical environment. When ARKit succeeds in recognizing the image, it creates an ARImageAnchor and a SceneKit node at the right position. You save the anchor and node that ARKit gives you by passing them to an AlteredImage object.\n\nfunc renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor) {\n    alteredImage?.add(anchor, node: node)\n    setMessageHidden(true)\n}\n\n\nAlter the image’s appearance using Core ML\n\nThis sample app is bundled with a Core ML model that performs image processing. Given an input image and an integer index, the model outputs a visually modified version of that image in one of eight different styles. The particular style of the output depends on the value of the index you pass in. The first style resembles burned paper, the second style resembles a mosaic, and there are six other styles as shown in the following image.\n\nWhen Vision finds a rectangular shape in the user’s environment, you pass the camera’s image data defined by that rectangle into a new AlteredImage.\n\nguard let newAlteredImage = AlteredImage(rectangleContent, referenceImage: possibleReferenceImage) else { return }\n\n\nThe following code shows how you choose the artistic style to apply to the image by inputting the integer index to the Core ML model. Then, you process the image by calling the Core ML model’s predictions(from:options:) routine.\n\nlet input = StyleTransferModelInput(image: self.modelInputImage, index: self.styleIndexArray)\nlet output = try AlteredImage.styleTransferModel.prediction(input: input, options: options)\n\n\nThe following figure shows the result when you process the input image with a style index of 2.\n\nDisplay the altered image in augmented reality\n\nTo complete the augmented reality effect, you cover the original image with the altered image. First, add a visualization node to hold the altered image as a child of the node provided by ARKit.\n\nnode.addChildNode(visualizationNode)\n\n\nWhen Core ML produces the output image, you call imageAlteringComplete(_:) to pass the model’s output image into the visualization node’s display function, where you set the image as the visualization node’s contents.\n\nfunc imageAlteringComplete(_ createdImage: CVPixelBuffer) {\n    guard fadeBetweenStyles else { return }\n    modelOutputImage = createdImage\n    visualizationNode.display(createdImage)\n}\n\n\nThe visualization node’s contents overlap the original image when SceneKit displays it. In the case of the image above, the following screenshot shows the end result as seen through a user’s device:\n\nContinually update the image’s appearance\n\nThis sample demonstrates real-time image processing by switching artistic styles over time. By calling selectNextStyle, you can make successive alterations of the original image. styleIndex is the integer input to the Core ML model that determines the style of the output.\n\nfunc selectNextStyle() {\n    styleIndex = (styleIndex + 1) % numberOfStyles\n}\n\n\nThe sample’s VisualizationNode fades between two images of differing style, which creates the effect that the tracked image is constantly transforming into a new look. You accomplish this effect by defining two SceneKit nodes. One node displays the current altered image, and the other displays the previous altered image.\n\nprivate let currentImage: SCNNode\nprivate let previousImage: SCNNode\n\n\nYou fade between these two nodes by running an opacity animation:\n\nSCNTransaction.begin()\nSCNTransaction.animationDuration = fadeDuration\ncurrentImage.opacity = 1.0\npreviousImage.opacity = 0.0\nSCNTransaction.completionBlock = {\n    self.delegate?.visualizationNodeDidFinishFade(self)\n}\nSCNTransaction.commit()\n\n\nWhen the animation finishes, you begin altering the original image with the next artistic style by calling createAlteredImage again:\n\nfunc visualizationNodeDidFinishFade(_ visualizationNode: VisualizationNode) {\n    guard fadeBetweenStyles, anchor != nil else { return }\n    selectNextStyle()\n    createAlteredImage()\n}\n\n\nRespond to image tracking updates\n\nAs part of the image tracking feature, ARKit continues to look for the image throughout the AR session. If the image itself moves, ARKit updates the ARImageAnchor with its corresponding image’s new location in the physical environment, and calls your delegate’s renderer(_:didUpdate:for:) to notify your app of the change.\n\nfunc renderer(_ renderer: SCNSceneRenderer, didUpdate node: SCNNode, for anchor: ARAnchor) {\n    alteredImage?.update(anchor)\n}\n\n\nThe sample app tracks a single image at a time. To do that, you invalidate the current image tracking session if an image the app was tracking is no longer visible. This, in turn, enables Vision to start looking for a new rectangular shape in the camera feed.\n\nfunc update(_ anchor: ARAnchor) {\n    if let imageAnchor = anchor as? ARImageAnchor, self.anchor == anchor {\n        self.anchor = imageAnchor\n        // Reset the timeout if the app is still tracking an image.\n        if imageAnchor.isTracked {\n            resetImageTrackingTimeout()\n        }\n    }\n}\n\n\nSee Also\nImage Detection\nDetecting Images in an AR Experience\nReact to known 2D images in the user’s environment, and use their positions to place AR content.\nclass ARImageAnchor\nAn anchor for a known image that ARKit detects in the physical environment.\nclass ARReferenceImage\nA 2D image that you want ARKit to detect in the physical environment."
  },
  {
    "title": "ARObjectScanningConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arobjectscanningconfiguration",
    "html": "Overview\n\nTo create an app that recognizes objects in the physical environment, first you scan them during development using ARObjectScanningConfiguration. After you’ve scanned an object, call createReferenceObject(transform:center:extent:completionHandler:) to turn it into an ARReferenceObject that you can use to detect it again at run-time. When users run your app, you ask ARKit to look for your scanned obects by running a world tracking configuration and assigning reference objects to its detectionObjects property.\n\nImportant\n\nARObjectScanningConfiguration is for use only in development scenarios. Because the high-fidelity spatial mapping required by object scanning has a high performance and energy cost, many ARKit features are disabled that aren't required for object scanning.\n\nTopics\nCreating a Configuration\ninit()\nInitializes a new object scanning configuration.\nEnabling Plane Detection\nvar planeDetection: ARWorldTrackingConfiguration.PlaneDetection\nA value specifying whether and how the session attempts to automatically detect flat surfaces in the camera-captured image.\nstruct ARWorldTrackingConfiguration.PlaneDetection\nOptions for whether and how the framework detects flat surfaces in captured images.\nManaging Device Camera Behavior\nvar isAutoFocusEnabled: Bool\nA Boolean value that determines whether the device camera uses fixed focus or autofocus behavior.\nRelationships\nInherits From\nARConfiguration"
  },
  {
    "title": "ARCamera.TrackingState.Reason.initializing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate/reason/initializing",
    "html": "Discussion\n\nThis value occurs temporarily after starting a new AR session or changing configurations.\n\nSee Also\nInhibitors of Tracking Quality\ncase relocalizing\nThe AR session is attempting to resume after an interruption.\ncase excessiveMotion\nThe device is moving too fast for accurate image-based position tracking.\ncase insufficientFeatures\nThe scene visible to the camera doesn't contain enough distinguishable features for image-based position tracking."
  },
  {
    "title": "supportedNumberOfTrackedFaces | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration/3194575-supportednumberoftrackedfaces",
    "html": "Discussion\n\nDo not exceed this value when you set maximumNumberOfTrackedFaces.\n\nSee Also\nTracking Multiple Faces\nvar maximumNumberOfTrackedFaces: Int\nThe number of faces to track during the session."
  },
  {
    "title": "isWorldTrackingEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration/3175413-isworldtrackingenabled",
    "html": "Discussion\n\nBefore attempting to enable this property, check whether the iOS device supports user-face tracking in a world-tracking session, by calling supportsWorldTracking.\n\nSee Also\nEnabling World Tracking\nclass var supportsWorldTracking: Bool\nA Boolean value that indicates whether the iOS device supports tracking the user's facial features in a world-tracking session."
  },
  {
    "title": "Combining User Face-Tracking and World Tracking | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/combining_user_face-tracking_and_world_tracking",
    "html": "Overview\n\nWhen tracking users’ faces in a world-tracking session, ARKit incorporates information from both the front and rear camera feeds in the AR experience. In addition to tracking the physical environment using the rear camera, ARKit uses the front camera to deliver an anchor that provides the position and expression of the user’s face.\n\nTo demonstrate applying the user’s face while world-tracking, this sample app lets the user place robot heads that reflect the user’s facial expression.\n\nConfigure and Start the Session\n\nThis app tracks the user’s face in a world-tracking session on iOS 13 and iPad OS 13 or later, on devices with a front TrueDepth camera that return true to supportsUserFaceTracking. To prevent the app from running an unsupported configuration, check whether the iOS device supports simultaneous world and user face-tracking.\n\nguard ARWorldTrackingConfiguration.supportsUserFaceTracking else {\n    fatalError(\"This sample code requires iOS 13 / iPad OS 13, and an iOS device with a front TrueDepth camera. Note: 2020 iPads do not support user face-tracking while world tracking.\")\n}\n\n\nIf the device running the app doesn’t support user face-tracking in a world-tracking session, the sample project will stop. In your app, consider gracefully degrading the AR experience in this case, such as by presenting the user with an error message and continuing the experience without it.\n\nThe sample app sets the userFaceTrackingEnabled property to true on the world-tracking configuration when app loads the view controller.\n\nconfiguration.userFaceTrackingEnabled = true\n\n\nThe sample app then starts the session by running the configuration when the view controller is about to appear onscreen.\n\noverride func viewWillAppear(_ animated: Bool) {\n    super.viewWillAppear(animated)\n    arView.session.run(configuration)\n}\n\n\nPreview Virtual Content in the Physical Environment\n\nThe app checks whether a robot head preview exists and creates one if not. ARKit calls the implementation of session(_:didUpdate:) every frame, which makes it a good location for a periodic check.\n\nfunc session(_ session: ARSession, didUpdate frame: ARFrame) {\n    if headPreview == nil, case .normal = frame.camera.trackingState {\n        addHeadPreview()\n    }\n    //...\n\n\nDetect Changes in the User’s Expression\n\nARKit provides the app with an updated anchor when the user changes their expression, position, or orientation with respect to the world. If there’s an active robot head preview, the app applies these changes to the head.\n\nfunc session(_ session: ARSession, didUpdate anchors: [ARAnchor]) {\n    anchors.compactMap { $0 as? ARFaceAnchor }.forEach { headPreview?.update(with: $0) }\n}\n\n\nInspect Expression Information\n\nIn the robot head’s update(with faceAnchor:) function, the app reads the user’s current expression by interpreting the anchor’s blend shapes.\n\nlet blendShapes = faceAnchor.blendShapes\n\n\nBlend shapes are Float values normalized within the range [0..1], with 0 representing the facial feature’s rest position, and 1 representing the opposite––the feature in its most pronounced state. To begin processing the values, the app stores them locally by accessing the anchor’s blendShapes array.\n\nguard let eyeBlinkLeft = blendShapes[.eyeBlinkLeft] as? Float,\n    let eyeBlinkRight = blendShapes[.eyeBlinkRight] as? Float,\n    let eyeBrowLeft = blendShapes[.browOuterUpLeft] as? Float,\n    let eyeBrowRight = blendShapes[.browOuterUpRight] as? Float,\n    let jawOpen = blendShapes[.jawOpen] as? Float,\n    let upperLip = blendShapes[.mouthUpperUpLeft] as? Float,\n    let tongueOut = blendShapes[.tongueOut] as? Float\n    else { return }\n\nReact to the User’s Expression\n\nBlend shape values can apply in unique ways depending on an app’s requirements. The sample app uses blend shapes to make the robot head appear to mimic the user’s expression, such as applying the brow and lip values to offset the robot’s brow and lip positions.\n\neyebrowLeftEntity.position.y = originalEyebrowY + 0.03 * eyeBrowLeft\neyebrowRightEntity.position.y = originalEyebrowY + 0.03 * eyeBrowRight\ntongueEntity.position.z = 0.1 * tongueOut\njawEntity.position.y = originalJawY - jawHeight * jawOpen\nupperLipEntity.position.y = originalUpperLipY + 0.05 * upperLip\n\n\nThe entity for the robot’s eye opens or closes when the sample app applies the corresponding blend shape value as a scale factor.\n\neyeLeftEntity.scale.z = 1 - eyeBlinkLeft\neyeRightEntity.scale.z = 1 - eyeBlinkRight\n\nPosition the Robot Head\n\nIn addition to capturing the user’s expression using the front camera, ARKit records the position of the user’s face with respect to the world. By design, the user’s face anchor is always located behind the rear camera. To serve the goal of mimicking the user with the robot head, the sample app applies the face anchor’s position to make the robot head always visible. First, it sets the robot head’s initial position equal to that of the camera.\n\nlet camera = AnchorEntity(.camera)\narView.scene.addAnchor(camera)\n\n\n// Attach a robot head to the camera anchor.\nlet robotHead = RobotHead()\ncamera.addChild(robotHead)\n\n\nThen the app offsets its z-position in the same amount as the camera’s distance from the user’s face.\n\nlet cameraTransform = parent.transformMatrix(relativeTo: nil)\nlet faceTransformFromCamera = simd_mul(simd_inverse(cameraTransform), faceAnchor.transform)\nself.position.z = -faceTransformFromCamera.columns.3.z\n\nOrient the Robot Head\n\nThe sample app also uses the anchor’s orientation to direct the front of the robot’s head continually toward the camera. It starts by accessing the anchor’s orientation.\n\nlet rotationEulers = faceTransformFromCamera.eulerAngles\n\n\nThen it adds pi to the y-Euler angle to turn it on the y-axis.\n\nlet mirroredRotation = Transform(pitch: rotationEulers.x, yaw: -rotationEulers.y + .pi, roll: rotationEulers.z)\n\n\nTo effect the change, the app applies the updated Euler angles to the robot head’s orientation.\n\nself.orientation = mirroredRotation.rotation\n\nCapture the Expression by Placing the Head\n\nTo demonstrate the variety of expressions tracked during the session, the sample app places the robot head in the physical environment when the user taps the screen. When the app initially previews the expressions, it positions the robot head at a fixed offset from the camera. When the user taps the screen, the app reanchors the robot head by updating its position to its current world location.\n\n@objc\nfunc handleTap(recognizer: UITapGestureRecognizer) {\n    guard let robotHeadPreview = headPreview, robotHeadPreview.isEnabled, robotHeadPreview.appearance == .tracked else {\n        return\n    }\n    let headWorldTransform = robotHeadPreview.transformMatrix(relativeTo: nil)\n    robotHeadPreview.anchor?.reanchor(.world(transform: headWorldTransform))\n    robotHeadPreview.appearance = .anchored\n    // ...\n\n\nSetting the headPreview to nil prevents the app from updating the facial expression in session(didUpdate anchors:), which freezes that expression on the placed robot head.\n\nself.headPreview = nil\n\n\nWhen ARKit calls session(didUpdate frame:) again, the app checks whether a robot head preview exists, and creates one if not.\n\nfunc session(_ session: ARSession, didUpdate frame: ARFrame) {\n    if headPreview == nil, case .normal = frame.camera.trackingState {\n        addHeadPreview()\n    }\n    //...\n\n\nWhen the app sets headPreview to nil, it creates another robot head preview, continuing the user’s ability to place objects and archive additional facial expressions.\n\nNote\n\nFor more detail about placing virtual content in the real world, see Placing Objects and Handling 3D Interaction.\n\nSee Also\nFace Tracking\nTracking and Visualizing Faces\nDetect faces in a front-camera AR experience, overlay virtual content, and animate facial expressions in real-time.\nclass ARFaceAnchor\nAn anchor for a unique face that is visible in the front-facing camera."
  },
  {
    "title": "queryAuthorization(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/4193443-queryauthorization",
    "html": "Parameters\nauthorizationTypes\n\nThe authorization types you want to check.\n\nReturn Value\n\nA list of the authorization statuses for each authorization type you passed in authorizationTypes.\n\nSee Also\nGetting authorization\nfunc requestAuthorization(for: [ARKitSession.AuthorizationType]) -> [ARKitSession.AuthorizationType : ARKitSession.AuthorizationStatus]\nRequests authorization from the user to use the specified kinds of ARKit data.\nenum ARKitSession.AuthorizationType\nThe authorization types you can request from ARKit.\nenum ARKitSession.AuthorizationStatus\nThe authorization states for a type of ARKit data."
  },
  {
    "title": "requestAuthorization(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/4131678-requestauthorization",
    "html": "Parameters\nauthorizationTypes\n\nThe types of authorizations your app needs to run.\n\nReturn Value\n\nA dictionary that contains the result of the authorization request for each authorization type you requested.\n\nDiscussion\n\nYou can use the requiredAuthorizations property on any of the types that conform to the DataProvider protocol to get the list of authorizations specific to that data provider and pass it to this method.\n\nSee Also\nGetting authorization\nenum ARKitSession.AuthorizationType\nThe authorization types you can request from ARKit.\nfunc queryAuthorization(for: [ARKitSession.AuthorizationType]) -> [ARKitSession.AuthorizationType : ARKitSession.AuthorizationStatus]\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nenum ARKitSession.AuthorizationStatus\nThe authorization states for a type of ARKit data."
  },
  {
    "title": "ARCamera.TrackingState.limited(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate/limited",
    "html": "Discussion\n\nIn this state, the positions and transforms of anchors in the scene (especially detected planes) may not be accurate or consistent from one captured frame to the next.\n\nSee the associated ARCamera.TrackingState.Reason value for information you can present to the user for improving tracking quality.\n\nSee Also\nDetermining the camera tracking status\ncase notAvailable\nCamera position tracking is not available.\nenum ARCamera.TrackingState.Reason\nCauses of limited position-tracking quality.\ncase normal\nCamera position tracking is providing optimal results."
  },
  {
    "title": "ARCamera.TrackingState.notAvailable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate/notavailable",
    "html": "See Also\nDetermining the camera tracking status\ncase limited(ARCamera.TrackingState.Reason)\nTracking is available, but the quality of results is questionable.\nenum ARCamera.TrackingState.Reason\nCauses of limited position-tracking quality.\ncase normal\nCamera position tracking is providing optimal results."
  },
  {
    "title": "ARCamera | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera",
    "html": "Overview\n\nYou get camera information from the camera property of each ARFrame ARKit delivers.\n\nTopics\nHandling Tracking Status\nvar trackingState: ARCamera.TrackingState\nThe general quality of position tracking available when the camera captured a frame.\nenum ARCamera.TrackingState\nValues for position tracking quality, with possible causes when tracking quality is limited.\nExamining Camera Geometry\nvar transform: simd_float4x4\nThe position and orientation of the camera in world coordinate space.\nvar eulerAngles: simd_float3\nThe orientation of the camera, expressed as roll, pitch, and yaw values.\nExamining Imaging Parameters\nvar imageResolution: CGSize\nThe width and height, in pixels, of the captured camera image.\nvar intrinsics: simd_float3x3\nA matrix that converts between the 2D camera plane and 3D world coordinate space.\nApplying Camera Geometry\nvar projectionMatrix: simd_float4x4\nA transform matrix appropriate for rendering 3D content to match the image captured by the camera.\nfunc projectionMatrix(for: UIInterfaceOrientation, viewportSize: CGSize, zNear: CGFloat, zFar: CGFloat) -> simd_float4x4\nReturns a transform matrix appropriate for rendering 3D content to match the image captured by the camera, using the specified parameters.\nfunc viewMatrix(for: UIInterfaceOrientation) -> simd_float4x4\nReturns a transform matrix for converting from world space to camera space.\nfunc projectPoint(simd_float3, orientation: UIInterfaceOrientation, viewportSize: CGSize) -> CGPoint\nReturns the projection of a point from the 3D world space detected by ARKit into the 2D space of a view rendering the scene.\nfunc unprojectPoint(CGPoint, ontoPlane: simd_float4x4, orientation: UIInterfaceOrientation, viewportSize: CGSize) -> simd_float3?\nReturns the projection of a point from the 2D space of a view rendering the scene onto a plane in the 3D world space detected by ARKit.\nApplying Motion Blur\nvar exposureDuration: TimeInterval\nA value you use to effect motion blur when rendering your app's virtual content.\nApplying Post-Processed Lighting\nvar exposureOffset: Float\nA value you supply to your custom renderer to light your scene.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying"
  },
  {
    "title": "init(modes:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider/4131877-init",
    "html": "Parameters\nmodes\n\nThe modes of scene reconstruction your app requires.\n\nDiscussion\n\nYou can pass additional modes, such as SceneReconstructionProvider.Mode.classification, if you need more than the default mesh data.\n\nSee Also\nCreating a scene reconstruction provider\nlet modes: [SceneReconstructionProvider.Mode]\nThe modes of scene reconstruction this provider supplies.\nenum SceneReconstructionProvider.Mode\nThe additional kinds of information you can request about a person’s surroundings.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports scene reconstruction providers."
  },
  {
    "title": "requiredAuthorizations | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider/4131879-requiredauthorizations",
    "html": "Relationships\nFrom Protocol\nDataProvider\nSee Also\nInspecting a scene reconstruction provider\nvar description: String\nA textual description of a scene reconstruction provider."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider/4139430-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting a scene reconstruction provider\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations needed to run scene reconstruction."
  },
  {
    "title": "SceneReconstructionProvider.Mode | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider/mode",
    "html": "Topics\nScene reconstruction modes\ncase classification\nThe reconstruction mode that classifies each face of a mesh anchor.\nInspecting scene reconstruction modes\nvar description: String\nA textual description of a scene reconstruction mode.\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nComparing scene reconstruction modes\nstatic func == (SceneReconstructionProvider.Mode, SceneReconstructionProvider.Mode) -> Bool\nstatic func != (SceneReconstructionProvider.Mode, SceneReconstructionProvider.Mode) -> Bool\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nCreating a scene reconstruction provider\ninit(modes: [SceneReconstructionProvider.Mode])\nCreates a provider that reconstructs the person’s surroundings.\nlet modes: [SceneReconstructionProvider.Mode]\nThe modes of scene reconstruction this provider supplies.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports scene reconstruction providers."
  },
  {
    "title": "isSupported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider/4218772-issupported",
    "html": "Relationships\nFrom Protocol\nDataProvider\nSee Also\nCreating a scene reconstruction provider\ninit(modes: [SceneReconstructionProvider.Mode])\nCreates a provider that reconstructs the person’s surroundings.\nlet modes: [SceneReconstructionProvider.Mode]\nThe modes of scene reconstruction this provider supplies.\nenum SceneReconstructionProvider.Mode\nThe additional kinds of information you can request about a person’s surroundings."
  },
  {
    "title": "anchorUpdates | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider/4180468-anchorupdates",
    "html": "See Also\nObserving scene reconstruction\nvar state: DataProviderState\nA value that indicates whether the scene reconstruction provider is currently supplying anchor updates."
  },
  {
    "title": "state | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider/4278300-state",
    "html": "Relationships\nFrom Protocol\nDataProvider\nSee Also\nObserving scene reconstruction\nvar anchorUpdates: AnchorUpdateSequence<MeshAnchor>\nAn asynchronous sequence of updates to scene meshes that the scene reconstruction provider detects."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/4139438-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting a world tracking provider\nvar state: DataProviderState\nThe current status of data coming from this provider."
  },
  {
    "title": "state | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/4278301-state",
    "html": "Relationships\nFrom Protocol\nDataProvider\nSee Also\nInspecting a world tracking provider\nvar description: String\nA textual description of a world tracking provider."
  },
  {
    "title": "removeAnchor(forID:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/4180470-removeanchor",
    "html": "Parameters\nid\n\nThe unique ID of the world anchor to remove.\n\nSee Also\nStopping object tracking\nfunc removeAnchor(WorldAnchor)\nRemoves a world anchor from a world tracking provider."
  },
  {
    "title": "removeAnchor(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/4108572-removeanchor",
    "html": "Parameters\nworldAnchor\n\nThe world anchor to remove.\n\nSee Also\nStopping object tracking\nfunc removeAnchor(forID: UUID)\nRemoves a world anchor from a world tracking provider based on its ID."
  },
  {
    "title": "WorldTrackingProvider.Error | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/error",
    "html": "Topics\nInspecting world-tracking errors\nlet anchor: WorldAnchor?\nThe anchor that caused a world-tracking error.\nvar code: WorldTrackingProvider.Error.Code\nThe error code for a world-tracking error.\nenum WorldTrackingProvider.Error.Code\nThe error codes for errors that world tracking providers throw.\nvar description: String\nA textual description of the error that occurred.\nvar localizedDescription: String\nA localized description of the error.\nvar errorDescription: String?\nA localized message that describes the error that occurred.\nProviding recovery suggestions\nvar recoverySuggestion: String?\nA localized message that describes how someone might recover from the error.\nvar failureReason: String?\nA localized message that describes why the error occurred.\nvar helpAnchor: String?\nRelationships\nConforms To\nCustomStringConvertible\nLocalizedError\nSendable\nSee Also\nTracking objects\ninit()\nCreates a world tracking provider.\nvar anchorUpdates: AnchorUpdateSequence<WorldAnchor>\nA sequence of updates to anchors this provider tracks.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to track world anchors.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports world tracking providers.\nfunc addAnchor(WorldAnchor)\nAdds a world anchor you supply to the set of currently tracked anchors."
  },
  {
    "title": "isSupported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/4218773-issupported",
    "html": "Relationships\nFrom Protocol\nDataProvider\nSee Also\nTracking objects\ninit()\nCreates a world tracking provider.\nvar anchorUpdates: AnchorUpdateSequence<WorldAnchor>\nA sequence of updates to anchors this provider tracks.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to track world anchors.\nfunc addAnchor(WorldAnchor)\nAdds a world anchor you supply to the set of currently tracked anchors.\nstruct WorldTrackingProvider.Error\nAn error that can occur during a world-tracking session."
  },
  {
    "title": "addAnchor(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/4108569-addanchor",
    "html": "Parameters\nworldAnchor\n\nA position and orientation in the world you want to track.\n\nSee Also\nTracking objects\ninit()\nCreates a world tracking provider.\nvar anchorUpdates: AnchorUpdateSequence<WorldAnchor>\nA sequence of updates to anchors this provider tracks.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to track world anchors.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports world tracking providers.\nstruct WorldTrackingProvider.Error\nAn error that can occur during a world-tracking session."
  },
  {
    "title": "requiredAuthorizations | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/4131961-requiredauthorizations",
    "html": "Relationships\nFrom Protocol\nDataProvider\nSee Also\nTracking objects\ninit()\nCreates a world tracking provider.\nvar anchorUpdates: AnchorUpdateSequence<WorldAnchor>\nA sequence of updates to anchors this provider tracks.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports world tracking providers.\nfunc addAnchor(WorldAnchor)\nAdds a world anchor you supply to the set of currently tracked anchors.\nstruct WorldTrackingProvider.Error\nAn error that can occur during a world-tracking session."
  },
  {
    "title": "anchorUpdates | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/4180469-anchorupdates",
    "html": "See Also\nTracking objects\ninit()\nCreates a world tracking provider.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to track world anchors.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports world tracking providers.\nfunc addAnchor(WorldAnchor)\nAdds a world anchor you supply to the set of currently tracked anchors.\nstruct WorldTrackingProvider.Error\nAn error that can occur during a world-tracking session."
  },
  {
    "title": "state | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imagetrackingprovider/4278297-state",
    "html": "Relationships\nFrom Protocol\nDataProvider\nSee Also\nInspecting an image tracking provider\nvar description: String\nA textual description of an image tracking provider."
  },
  {
    "title": "isSupported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planedetectionprovider/4218771-issupported",
    "html": "Relationships\nFrom Protocol\nDataProvider\nSee Also\nDetecting planes\ninit(alignments: [PlaneAnchor.Alignment])\nCreates a plane detection provider for the types of planes you want to detect.\nvar anchorUpdates: AnchorUpdateSequence<PlaneAnchor>\nA sequence of updates to planes this provider detects.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to detect planes."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imagetrackingprovider/4139384-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting an image tracking provider\nvar state: DataProviderState\nThe current status of data coming from this provider."
  },
  {
    "title": "init(referenceImages:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imagetrackingprovider/4131785-init",
    "html": "Parameters\nreferenceImages\n\nAn array of known images to track in a person’s surroundings.\n\nSee Also\nCreating an image tracking provider\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports image tracking providers.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to track images."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handtrackingprovider/4139380-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting a hand tracking provider\nvar state: DataProviderState\nThe current status of data coming from this provider."
  },
  {
    "title": "Displaying an AR Experience with Metal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/displaying_an_ar_experience_with_metal",
    "html": "Overview\n\nARKit includes view classes for easily displaying AR experiences with SceneKit or SpriteKit. However, if instead you build your own rendering engine using Metal, ARKit also provides all the support necessary to display an AR experience with your custom view.\n\nIn any AR experience, the first step is to configure an ARSession object to manage camera capture and motion processing. A session defines and maintains a correspondence between the real-world space the device inhabits and a virtual space where you model AR content. To display your AR experience in a custom view, you’ll need to:\n\nRetrieve video frames and tracking information from the session.\n\nRender those frame images as the backdrop for your view.\n\nUse the tracking information to position and draw AR content atop the camera image.\n\nNote\n\nThis article covers code found in Xcode project templates. For complete example code, create a new iOS application with the Augmented Reality template, and choose Metal from the Content Technology popup menu.\n\nGet Video Frames and Tracking Data from the Session\n\nCreate and maintain your own ARSession instance, and run it with a session configuration appropriate for the kind of AR experience you want to support. The session captures video from the camera, tracks the device’s position and orientation in a modeled 3D space, and provides ARFrame objects. Each such object contains both an individual video frame image and position tracking information from the moment that frame was captured.\n\nThere are two ways to access ARFrame objects produced by an AR session, depending on whether your app favors a pull or a push design pattern.\n\nIf you prefer to control frame timing (the pull design pattern), use the session’s currentFrame property to get the current frame image and tracking information each time you redraw your view’s contents. For example, see the following function that a custom renderer calls as part of its regular update process:\n\nfunc updateGameState() {        \n    guard let currentFrame = session.currentFrame else {\n        return\n    }    \n    updateSharedUniforms(frame: currentFrame)\n    updateAnchors(frame: currentFrame)\n    updateCapturedImageTextures(frame: currentFrame)\n    \n    if viewportSizeDidChange {\n        viewportSizeDidChange = false\n        \n        updateImagePlane(frame: currentFrame)\n    }\n}\n\n\nAlternatively, if your app design favors a push pattern, implement the session(_:didUpdate:) delegate method, and the session will call it once for each video frame it captures (at 60 frames per second by default).\n\nUpon obtaining a frame, you’ll need to draw the camera image, and update and render any overlay content your AR experience includes.\n\nDraw the Camera Image\n\nEach ARFrame object’s capturedImage property contains a pixel buffer captured from the device camera. To draw this image as the backdrop for your custom view, you’ll need to create textures from the image content and submit GPU rendering commands that use those textures.\n\nThe pixel buffer’s contents are encoded in a biplanar YCbCr (also called YUV) data format; to render the image you’ll need to convert this pixel data to a drawable RGB format. For rendering with Metal, you can perform this conversion most efficiently in GPU shader code. Use CVMetalTextureCache APIs to create two Metal textures from the pixel buffer—one each for the buffer’s luma (Y) and chroma (CbCr) planes:\n\nfunc updateCapturedImageTextures(frame: ARFrame) {\n    // Create two textures (Y and CbCr) from the provided frame's captured image.\n    let pixelBuffer = frame.capturedImage\n    if (CVPixelBufferGetPlaneCount(pixelBuffer) < 2) {\n        return\n    }\n    capturedImageTextureY = createTexture(fromPixelBuffer: pixelBuffer, pixelFormat:.r8Unorm, planeIndex:0)!\n    capturedImageTextureCbCr = createTexture(fromPixelBuffer: pixelBuffer, pixelFormat:.rg8Unorm, planeIndex:1)!\n}\n\n\nfunc createTexture(fromPixelBuffer pixelBuffer: CVPixelBuffer, pixelFormat: MTLPixelFormat, planeIndex: Int) -> MTLTexture? {\n    var mtlTexture: MTLTexture? = nil\n    let width = CVPixelBufferGetWidthOfPlane(pixelBuffer, planeIndex)\n    let height = CVPixelBufferGetHeightOfPlane(pixelBuffer, planeIndex)\n    \n    var texture: CVMetalTexture? = nil\n    let status = CVMetalTextureCacheCreateTextureFromImage(nil, capturedImageTextureCache, pixelBuffer, nil, pixelFormat, width, height, planeIndex, &texture)\n    if status == kCVReturnSuccess {\n        mtlTexture = CVMetalTextureGetTexture(texture!)\n    }\n    \n    return mtlTexture\n}\n\n\nNext, encode render commands that draw those two textures using a fragment function that performs YCbCr to RGB conversion with a color transform matrix:\n\nfragment float4 capturedImageFragmentShader(ImageColorInOut in [[stage_in]],\n                                            texture2d<float, access::sample> capturedImageTextureY [[ texture(kTextureIndexY) ]],\n                                            texture2d<float, access::sample> capturedImageTextureCbCr [[ texture(kTextureIndexCbCr) ]]) {\n    \n    constexpr sampler colorSampler(mip_filter::linear,\n                                   mag_filter::linear,\n                                   min_filter::linear);\n    \n    const float4x4 ycbcrToRGBTransform = float4x4(\n        float4(+1.0000f, +1.0000f, +1.0000f, +0.0000f),\n        float4(+0.0000f, -0.3441f, +1.7720f, +0.0000f),\n        float4(+1.4020f, -0.7141f, +0.0000f, +0.0000f),\n        float4(-0.7010f, +0.5291f, -0.8860f, +1.0000f)\n    );\n    \n    // Sample Y and CbCr textures to get the YCbCr color at the given texture coordinate.\n    float4 ycbcr = float4(capturedImageTextureY.sample(colorSampler, in.texCoord).r,\n                          capturedImageTextureCbCr.sample(colorSampler, in.texCoord).rg, 1.0);\n    \n    // Return the converted RGB color.\n    return ycbcrToRGBTransform * ycbcr;\n}\n\n\nNote\n\nUse the displayTransform(for:viewportSize:) method to make sure the camera image covers the entire view. For example use of this method, as well as complete Metal pipeline setup code, see the full Xcode template. (Create a new iOS application with the Augmented Reality template, and choose Metal from the Content Technology popup menu.)\n\nTrack and Render Overlay Content\n\nAR experiences typically focus on rendering 3D overlay content so that the content appears to be part of the real world seen in the camera image. To achieve this illusion, use the ARAnchor class to model the position and orientation of your own 3D content relative to real-world space. Anchors provide transforms that you can reference during rendering.\n\nFor example, the Xcode template creates an anchor located about 20 cm in front of the device whenever a user taps on the screen:\n\nfunc handleTap(gestureRecognize: UITapGestureRecognizer) {\n    // Create an anchor using the camera's current position.\n    if let currentFrame = session.currentFrame {\n        \n        // Create a transform with a translation of 0.2 meters in front of the camera.\n        var translation = matrix_identity_float4x4\n        translation.columns.3.z = -0.2\n        let transform = simd_mul(currentFrame.camera.transform, translation)\n        \n        // Add a new anchor to the session.\n        let anchor = ARAnchor(transform: transform)\n        session.add(anchor: anchor)\n    }\n}\n\n\nIn your rendering engine, use the transform property of each ARAnchor object to place visual content. The Xcode template uses each of the anchors added to the session in its handleTap method to position a simple cube mesh:\n\nfunc updateAnchors(frame: ARFrame) {\n    // Update the anchor's uniform buffer with transforms of the current frame's anchors.\n    anchorInstanceCount = min(frame.anchors.count, kMaxAnchorInstanceCount)\n    \n    var anchorOffset: Int = 0\n    if anchorInstanceCount == kMaxAnchorInstanceCount {\n        anchorOffset = max(frame.anchors.count - kMaxAnchorInstanceCount, 0)\n    }\n    \n    for index in 0..<anchorInstanceCount {\n        let anchor = frame.anchors[index + anchorOffset]\n        \n        // Flip the Z axis to convert geometry from right handed to left handed.\n        var coordinateSpaceTransform = matrix_identity_float4x4\n        coordinateSpaceTransform.columns.2.z = -1.0\n        \n        let modelMatrix = simd_mul(anchor.transform, coordinateSpaceTransform)\n        \n        let anchorUniforms = anchorUniformBufferAddress.assumingMemoryBound(to: InstanceUniforms.self).advanced(by: index)\n        anchorUniforms.pointee.modelMatrix = modelMatrix\n    }\n}\n\n\nNote\n\nIn a more complex AR experience, you can use hit testing or plane detection to find the positions of real-world surfaces. For details, see the planeDetection property and the hitTest(_:types:) method. In both cases, ARKit provides results as ARAnchor objects, so you still use anchor transforms to place visual content.\n\nRender with Realistic Lighting\n\nWhen you configure shaders for drawing 3D content in your scene, use the estimated lighting information in each ARFrame object to produce more realistic shading. See the following code that an app's custom renderer performs while updating its shared uniforms:\n\n// Set up the scene's lighting with the ambient intensity, if available.\nvar ambientIntensity: Float = 1.0\nif let lightEstimate = frame.lightEstimate {\n    ambientIntensity = Float(lightEstimate.ambientIntensity) / 1000.0\n}\nlet ambientLightColor: vector_float3 = vector3(0.5, 0.5, 0.5)\nuniforms.pointee.ambientLightColor = ambientLightColor * ambientIntensity\n\n\nNote\n\nFor the complete set of Metal setup and rendering commands that go with this example, see the full Xcode template. (Create a new iOS application with the Augmented Reality template, and choose Metal from the Content Technology popup menu.)\n\nSee Also\nSetup\nChoosing Which Camera Feed to Augment\nAdd visual effects to the user's environment in an AR experience through the front or rear camera.\nManaging Session Life Cycle and Tracking Quality\nKeep the user informed on the current session state and recover from interruptions.\nclass ARSession\nThe object that manages the major tasks associated with every AR experience, such as motion tracking, camera passthrough, and image analysis.\nConfiguration Objects\nConfigure your augmented reality session to detect and track specific types of content."
  },
  {
    "title": "state | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handtrackingprovider/4278296-state",
    "html": "Relationships\nFrom Protocol\nDataProvider\nSee Also\nInspecting a hand tracking provider\nvar description: String\nA textual description of a hand tracking provider."
  },
  {
    "title": "latestAnchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handtrackingprovider/4189752-latestanchors",
    "html": "Discussion\n\nAccessing this tuple consumes its values and sets them to nil until the next anchor update. Both elements of this tuple are nil when the associated HandTrackingProvider isn’t running.\n\nSee Also\nObserving hand anchor data\nvar anchorUpdates: AnchorUpdateSequence<HandAnchor>\nA sequence of updates for all hands that this provider tracks."
  },
  {
    "title": "state | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planedetectionprovider/4278299-state",
    "html": "Relationships\nFrom Protocol\nDataProvider\nSee Also\nInspecting a plane detection provider\nvar alignments: [PlaneAnchor.Alignment]\nThe plane alignments that you configured this provider to detect.\nvar description: String\nA textual description of this provider."
  },
  {
    "title": "alignments | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planedetectionprovider/4131831-alignments",
    "html": "See Also\nInspecting a plane detection provider\nvar state: DataProviderState\nThe current status of data coming from this provider.\nvar description: String\nA textual description of this provider."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planedetectionprovider/4139391-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nInspecting a plane detection provider\nvar alignments: [PlaneAnchor.Alignment]\nThe plane alignments that you configured this provider to detect.\nvar state: DataProviderState\nThe current status of data coming from this provider."
  },
  {
    "title": "stop() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/4131680-stop",
    "html": "Discussion\n\nARKit also automatically stops sessions when they’re deinitialized.\n\nSee Also\nStarting and stopping a session\ninit()\nCreates a new session.\nfunc run([DataProvider])\nRuns a session with the data providers you supply.\nstruct ARKitSession.Error\nAn error that might occur when running data providers on an ARKit session."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/4131677-init",
    "html": "Discussion\n\nARKit stops sessions when they’re deinitialized.\n\nSee Also\nStarting and stopping a session\nfunc run([DataProvider])\nRuns a session with the data providers you supply.\nfunc stop()\nStops all data providers running in this session.\nstruct ARKitSession.Error\nAn error that might occur when running data providers on an ARKit session."
  },
  {
    "title": "requiredAuthorizations | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planedetectionprovider/4131834-requiredauthorizations",
    "html": "Discussion\n\nYou can use this property to pass plane detection requirements to the requestAuthorization(for:) method.\n\nRelationships\nFrom Protocol\nDataProvider\nSee Also\nDetecting planes\ninit(alignments: [PlaneAnchor.Alignment])\nCreates a plane detection provider for the types of planes you want to detect.\nvar anchorUpdates: AnchorUpdateSequence<PlaneAnchor>\nA sequence of updates to planes this provider detects.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports plane detection providers."
  },
  {
    "title": "lightEstimate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe/2878306-lightestimate",
    "html": "Discussion\n\nIf you render your own overlay graphics for the AR scene, you can use this information in shading algorithms to help make those graphics match the real-world lighting conditions of the scene captured by the camera. (The ARSCNView class automatically uses this information to configure SceneKit lighting.)\n\nThis property's value is nil if the isLightEstimationEnabled property of the session configuration that captured this frame is false.\n\nSee Also\nAccessing scene data\nfunc displayTransform(for: UIInterfaceOrientation, viewportSize: CGSize) -> CGAffineTransform\nReturns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.\nvar rawFeaturePoints: ARPointCloud?\nThe current intermediate results of the scene analysis ARKit uses to perform world tracking.\nvar capturedDepthData: AVDepthData?\nDepth data captured in front-camera experiences.\nvar capturedDepthDataTimestamp: TimeInterval\nThe time at which depth data for the frame (if any) was captured.\nvar sceneDepth: ARDepthData?\nData on the distance between a device's rear camera and real-world objects in an AR experience.\nvar smoothedSceneDepth: ARDepthData?\nAn average of distance measurements between a device's rear camera and real-world objects that creates smoother visuals in an AR experience."
  },
  {
    "title": "smoothedSceneDepth | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/framesemantics/3674208-smoothedscenedepth",
    "html": "Discussion\n\nEnable this option on a world-tracking configuration (ARWorldTrackingConfiguration) to instruct ARKit to provide your app with the distance between the user’s device and the real-world objects pictured in the frame's capturedImage. ARKit samples this distance using the LiDAR scanner and provides the results through the smoothedSceneDepth property on the session’s currentFrame.\n\nTo minimize the difference in LiDAR readings across frames, ARKit processes the data as an average. The averaged readings reduce flickering to create a smoother motion effect when depicting objects with depth, as demonstrated in Creating a Fog Effect Using Scene Depth. Alternatively, to access a discrete LiDAR reading at the instant the framework creates the current frame, use sceneDepth.\n\nARKit supports scene depth only on LiDAR-capable devices, so call supportsFrameSemantics(_:) to ensure device support before attempting to enable scene depth.\n\nSee Also\nAccessing Depth\nstatic var sceneDepth: ARConfiguration.FrameSemantics\nAn option that provides the distance from the device to real-world objects viewed through the camera."
  },
  {
    "title": "ARConfiguration.WorldAlignment.gravityAndHeading | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/worldalignment/gravityandheading",
    "html": "Discussion\n\nThe y-axis matches the direction of gravity as detected by the device's motion sensing hardware; that is, the vector (0,-1,0) points downward.\n\nThe x- and z-axes match the longitude and latitude directions as measured by Location Services. The vector (0,0,-1) points to true north and the vector (-1,0,0) points west. (That is, the positive x-, y-, and z-axes point east, up, and south, respectively.)\n\nFigure 1 In gravity and heading alignment, all directions are fixed to a real-world reference frame.\n\nAlthough this option fixes the directions of the three coordinate axes to real-world directions, the location of the coordinate system's origin is still relative to the device, matching the device's position as of when the session configuration is first run.\n\nNote\n\nUsing gravity and heading alignment requires tracking the device's geographic location. Your app's Info.plist must include user-facing text for the NSLocationUsageDescription or NSLocationWhenInUseUsageDescription key so that the user can grant your app permission for location tracking.\n\nSee Also\nAlignments\ncase gravity\nThe coordinate system's y-axis is parallel to gravity, and its origin is the initial position of the device.\ncase camera\nThe scene coordinate system is locked to match the orientation of the camera."
  },
  {
    "title": "ARConfiguration.WorldAlignment.camera | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/worldalignment/camera",
    "html": "Discussion\n\nCamera alignment defines a coordinate system based on the native sensor orientation of the device camera. Relative to a AVCaptureVideoOrientation.landscapeRight-oriented camera image, the x-axis points to the right, the y-axis points up, and the z-axis points out the front of the device (toward the user).\n\nNote\n\nThis coordinate system is always the same regardless of device or user interface orientation. That is, the x-axis always points along the long axis of the device, even if that direction is \"down\" relative to the user.\n\nWhen this alignment is active, ARKit performs no device motion tracking. That is, world-space positions are effectively always relative to the current position and orientation of the device. (For example, a SceneKit object placed in an ARSCNView will thus maintain the same position on screen, even as the camera image changes while the device moves.)\n\nSee Also\nAlignments\ncase gravity\nThe coordinate system's y-axis is parallel to gravity, and its origin is the initial position of the device.\ncase gravityAndHeading\nThe coordinate system's y-axis is parallel to gravity, its x- and z-axes are oriented to compass heading, and its origin is the initial position of the device."
  },
  {
    "title": "ARSession.CollaborationData.Priority | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/collaborationdata/priority",
    "html": "Overview\n\nWhen you send ARSession.CollaborationData over the network by using a protocol that allows you to specify varying reliability, this property provides you with a hint about which reliability setting to use for a given collaboration data instance. Depending on its priority, you may also choose to send a given collaboration data instance using different protocols.\n\nTopics\nData Sensitivity\ncase critical\nA priority that indicates that collaboration depends on this data.\ncase optional\nA priority that indicates that collaboration can continue without this data.\nRelationships\nConforms To\nSendable\nSee Also\nObserving Priority\nvar priority: ARSession.CollaborationData.Priority\nA property that gives you a hint about how to send a given data instance over the network."
  },
  {
    "title": "priority | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/collaborationdata/3340466-priority",
    "html": "Discussion\n\nIf you have reliability options in the network protocol you choose to transport ARSession.CollaborationData among peers, the priority property gives you a hint about which reliability option to choose for a given collaboration data instance. For example, if you use MultipeerConnectivity to send collaboration data over the network, choose MCSessionSendDataMode.reliable when calling send(_:toPeers:with:) after ARKit gives you a collaboration data instance with priority ARSession.CollaborationData.Priority.critical.\n\nSee Also\nObserving Priority\nenum ARSession.CollaborationData.Priority\nOptions that help you choose the appropriate network protocol or settings for a given data instance."
  },
  {
    "title": "anchorUpdates | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planedetectionprovider/4180466-anchorupdates",
    "html": "Discussion\n\nThe system adds, updates, or removes plane anchors when this sequence provides updates.\n\nSee Also\nDetecting planes\ninit(alignments: [PlaneAnchor.Alignment])\nCreates a plane detection provider for the types of planes you want to detect.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to detect planes.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports plane detection providers."
  },
  {
    "title": "init(alignments:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planedetectionprovider/4131833-init",
    "html": "Parameters\nalignments\n\nThe types of planes you want to detect — horizontal, vertical, or both.\n\nSee Also\nDetecting planes\nvar anchorUpdates: AnchorUpdateSequence<PlaneAnchor>\nA sequence of updates to planes this provider detects.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to detect planes.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports plane detection providers."
  },
  {
    "title": "Specifying a lighting environment in AR Quick Look | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/specifying_a_lighting_environment_in_ar_quick_look",
    "html": "Overview\n\nAR Quick Look in iOS 16 and later enhances lighting to deliver more brightness, contrast, and visual definition for your scene's virtual content.\n\nYou can set an asset's lighting environment, or image-based lighting (IBL), by adding the preferredIblVersion metadata to the file’s .usda textual definition, or by generating the asset with Apple-provided tools.\n\nSet the lighting metadata\n\nTo define the lighting environment in the asset's .usda textual format using a tool like USD Toolset, add the following metadata:\n\n// asset.usda\n#usda 1.0\n(\n    customLayerData = {\n        dictionary Apple = {\n            int preferredIblVersion = 2\n        }\n    }\n)\n\n\nA value of 1 indicates the classic lighting environment, and a value of 2 indicates the new lighting environment.\n\nIf you omit the preferredIblVersion metadata or give it a value of 0, the system checks the asset’s creation timestamp. A timestamp of July 1, 2022, or later results in the new lighting environment; otherwise, the scene features classic lighting for backward compatibility. The system checks the timestamp of the .usd asset within the .usdz archive, not the archive's file creation date.\n\nSet the lighting environment with Apple-provided tools\n\nWith Reality Converter, you can choose a lighting preferrence for your 3D asset by previewing the available options one after the other. By default, Reality Converter previews an imported 3D asset with preferredIblVersion = 2. You can select the Use Classic Lighting option to set preferredIblVersion to 1 in the file.\n\nAlternatively, you can use usdzconvert in the Apple USDZ Tools suite to output from another file format. Pass an integer value between 0 and 2 for the --preferrediblversion argument to add this metadata in the file, as the following example shows:\n\nusdzconvert fireHydrant.obj --useObjMtl --preferrediblversion 2 \n\n\nMatch AR Quick Look lighting in third-party apps and tools\n\nTo design your content in a third-party digital content creation tool (DCC) under the same lighting conditions as AR Quick Look's lighting environment, configure the tool to use one of the following .exr files. Alternatively, you can apply an .exr file in your third-party app, such as one that renders with RealityKit, to accommodate the new lighting environment in your app's runtime experience.\n\nThe studio_lighting_objectmode_v002.exr file provides reflections that match AR Quick Look’s studio object mode. This IBL is appropriate for asset creation in a third-party DCC. Consult the DCC documentation about enabling custom image-based lighting in the tool.\n\nThe studio_lighting_armode_v002.exr file provides enhanced highlights according to AR Quick Look's new lighting environment in AR mode. This IBL is appropriate for use in your AR app as a combination with an environment texture that composes captures of the user's environment. Together, the combination enhances reflections on your app's virtual content that feature the particular tints and hues of the real world.\n\nTip\n\nFor apps that render virtual content using RealityKit, set up a skybox with the AR mode IBL to use the new lighting environment. Although AR Quick Look observes the preferredIblVersion metadata, RealityKit doesn’t. See EnvironmentResource for more information about defining image-based lighting in RealityKit.\n\nSee Also\nAR Quick Look\nPreviewing a Model with AR Quick Look\nDisplay a model or scene that the user can move, scale, and share with others.\nAdding Visual Effects in AR Quick Look and RealityKit\nBalance the appearance and performance of your AR experiences with modeling strategies.\nAdding an Apple Pay Button or a Custom Action in AR Quick Look\nProvide a banner that users can tap to make a purchase or perform a custom action in an AR experience.\nclass ARQuickLookPreviewItem\nAn object for customizing the AR Quick Look experience.\nUSDZ schemas for AR\nAdd augmented reality functionality to your 3D content using USDZ schemas."
  },
  {
    "title": "sessionProvider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayview/3192181-sessionprovider",
    "html": "Discussion\n\nUse this property to set the coaching overlay's session when loading from a storyboard. If you set this property at runtime, the coaching overlay keeps its session property up to date for you. If your app recreates its ARSession at any point, you may find it convienient to set the sessionProvider once rather than update the coaching overlay's session manually.\n\nSee Also\nProviding the Session\nvar session: ARSession?\nThe session this view uses to provide coaching."
  },
  {
    "title": "Content Anchors | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors",
    "html": "Overview\n\nAnchors identify the position of items in your augmented reality session. Use anchors to obtain information about the item itself, or about the thing it represents. For example, use an ARPlaneAnchor to determine the location of a planar surface.\n\nTopics\nSurface Detection\nTracking and Visualizing Planes\nDetect surfaces in the physical environment and visualize their shape and location in 3D space.\nclass ARPlaneAnchor\nAn anchor for a 2D planar surface that ARKit detects in the physical environment.\nclass ARMeshAnchor\nAn anchor for a physical object that ARKit detects and recreates virtually using a polygonal mesh.\nImage Detection\nTracking and altering images\nCreate images from rectangular shapes found in the user’s environment, and augment their appearance.\nDetecting Images in an AR Experience\nReact to known 2D images in the user’s environment, and use their positions to place AR content.\nclass ARImageAnchor\nAn anchor for a known image that ARKit detects in the physical environment.\nclass ARReferenceImage\nA 2D image that you want ARKit to detect in the physical environment.\nPhysical Objects\nVisualizing and Interacting with a Reconstructed Scene\nEstimate the shape of the physical environment using a polygonal mesh.\nScanning and detecting 3D objects\nRecord spatial features of real-world objects, then use the results to find those objects in the user’s environment and trigger AR content.\nclass ARObjectAnchor\nAn anchor for a real-world 3D object that ARKit detects in the physical environment.\nclass ARReferenceObject\nThe description of a 3D object that you want ARKit to detect in the physical environment.\nBody Position Tracking\nCapturing Body Motion in 3D\nTrack a person in the physical environment and visualize their motion by applying the same body movements to a virtual character.\nRigging a Model for Motion Capture\nConfigure custom 3D models so ARKit’s human body-tracking feature can control them.\nValidating a Model for Motion Capture\nVerify that your character model matches ARKit’s Motion Capture requirements.\nclass ARBodyAnchor\nAn anchor that tracks the position and movement of a human body in the rear-facing camera.\nFace Tracking\nTracking and Visualizing Faces\nDetect faces in a front-camera AR experience, overlay virtual content, and animate facial expressions in real-time.\nCombining User Face-Tracking and World Tracking\nTrack the user’s face in an app that displays an AR experience with the rear camera.\nclass ARFaceAnchor\nAn anchor for a unique face that is visible in the front-facing camera.\nGeotracking\nTracking Geographic Locations in AR\nTrack specific geographic areas of interest and render them in an AR experience.\nclass ARGeoAnchor\nAn anchor that identifies a geographic location using latitude, longitude, and altitude data.\nMultiuser Experiences\nclass ARParticipantAnchor\nAn anchor for another user in multiuser augmented reality experiences.\nApp Clip Codes\nInteracting with App Clip Codes in AR\nDisplay content and provide services in an AR experience with App Clip Codes.\nclass ARAppClipCodeAnchor\nAn anchor that tracks the position and orientation of an App Clip Code in the physical environment.\nText Annotations\nCreating Screen Annotations for Objects in an AR Experience\nAnnotate an AR experience with virtual sticky notes that you display onscreen over real and virtual objects.\nRecognizing and Labeling Arbitrary Objects\nCreate anchors that track objects you recognize in the camera feed, using a custom optical-recognition algorithm.\nCommon Types\nclass ARAnchor\nAn object that specifies the position and orientation of an item in the physical environment.\nprotocol ARAnchorCopying\nSupport for custom anchor subclasses.\nprotocol ARTrackable\nAn interface for objects that track the location of real-world objects or locations.\nSee Also\nVirtual Content\nEnvironmental Analysis\nAnalyze the video from the cameras and the accompanying data, and use ray-casting and depth-map information to determine the location of items.\nCamera, Lighting, and Effects\nDetermine the camera position and lighting for the current session, and apply effects, such as occlusion, to elements of the environment.\nData Management\nObtain detailed information about skeletal and face geometry, and saved world data.\nCreating USD files for Apple devices\nGenerate 3D assets that render as expected."
  },
  {
    "title": "ARGeoAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeoanchor",
    "html": "Overview\n\nA geographic anchor (also known as location anchor) identifies a specific area in the world that the app can refer to in an AR experience. As a user moves around the scene, the session updates a location anchor’s transform based on the anchor’s coordinate and the device’s compass heading.\n\nARKit aligns location anchors to an East-North-Up orientation, with its x- and z-axes matching the longitude and latitude directions. For more information, see ARConfiguration.WorldAlignment.gravityAndHeading.\n\nARKit sets the anchor’s vertical position to the altitude you pass in to init(coordinate:altitude:). If you initialize a location anchor using init(coordinate:) instead, ARKit sets the anchor’s altitude to ground level.\n\nImportant\n\nLocation anchors are available only in geotracking sessions, and geotracking is available in specific areas; for more information, see ARGeoTrackingConfiguration.\n\nCommunicate Data Usage\n\nLocation anchors consume data from Apple Maps called localization imagery (for more information, see ARGeoTrackingConfiguration). As the user moves, the framework downloads localization imagery to refine the user's precise geographic position. The amount of data the session requires depends on the user's movement and distance they travel. To make the user aware of potential fees, you can notify the user of their data usage.\n\nManage Location Anchor Availability\n\nWhen an app creates a location anchor, it’s invisible to the user until the framework populates the anchor in the scene. When an anchor populates successfully, the session passes the anchor into the delegate’s session(_:didAdd:) callback.\n\nIf ARKit fails to populate a location anchor, the session calls session(_:didRemove:) to notify your delegate. A location anchor may fail to populate when:\n\nThe network is unavailable. If you create a location anchor without providing an altitude, ARKit defaults the altitude to ground level, and may query the server to check the topography at the anchor’s geographic coordinate. If the network is unavailable, ask the user to restore a connection by disabling Airplane Mode, enabling WiFi, or moving to a location that provides service. If the network is available but slow, an altitude query response may be delayed. Consider pausing a navigation or presenting visual feedback for the anchor’s tentative placement, such as by displaying a status indicator.\n\nThe location anchor is too far from the user. If users can create location anchors in your app, let them know to position the coordinates of each anchor within 0.05 degrees (~5 kilometers) of themselves and their device.\n\nThe server prevents the location anchor's position, such as in a large body of water.\n\nTopics\nCreating a Geo Anchor\ninit(coordinate: CLLocationCoordinate2D, altitude: CLLocationDistance?)\nInitializes a location anchor with the given coordinate and altitude.\ninit(name: String, coordinate: CLLocationCoordinate2D, altitude: CLLocationDistance)\nInitializes a named location anchor with the given coordinates and altitude.\ninit(name: String, coordinate: CLLocationCoordinate2D, altitude: CLLocationDistance?)\nInitializes a named location anchor with the given coordinates and altitude.\nAccessing Latitude and Longitude\nvar coordinate: CLLocationCoordinate2D\nThe lattitude and longitude of the anchor’s geographic location.\nDefining Altitude\n\nvar altitude: CLLocationDistance?\nVertical distance, in meters, between this anchor and sea level.\nvar altitudeSource: ARGeoAnchor.AltitudeSource\nA record of the source from which an altitude came.\nenum ARGeoAnchor.AltitudeSource\nOptions for setting a location anchor’s altitude.\nRelationships\nInherits From\nARAnchor\nConforms To\nARTrackable\nSee Also\nGeotracking\nTracking Geographic Locations in AR\nTrack specific geographic areas of interest and render them in an AR experience."
  },
  {
    "title": "ARBodyAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodyanchor",
    "html": "Overview\n\nThis ARAnchor subclass tracks the movement of a single person. You enable body tracking by running your session using ARBodyTrackingConfiguration.\n\nWhen ARKit recognizes a person in the back camera feed, it calls your delegate's session(_:didAdd:) function with ARBodyAnchor. A body anchor's transform position defines the world position of the body's hip joint.\n\nYou can also check within the frame's anchors for a body that ARKit is tracking.\n\nPlace a Skeleton on a Surface\n\nBecause a body anchor's origin maps to the hip joint, you calculate the current offset of the feet to the hip to place the body's skeleton on a surface. By passing the foot joint index to jointModelTransforms, you get the foot's offset from skeleton's origin.\n\nstatic var hipToFootOffset: Float {\n    // Get an index for a foot. \n    let footIndex = ARSkeletonDefinition.defaultBody3D.index(forJointName: .leftFoot)\n    // Get the foot's world-space offset from the hip. \n    let footTransform = ARSkeletonDefinition.defaultBody3D.neutralBodySkeleton3D!.jointModelTransforms[footIndex]\n    // Return the height by getting just the y-value. \n    let distanceFromHipOnY = abs(footTransform.columns.3.y) \n    return distanceFromHipOnY\n}\n\n\nTopics\nInterpreting 3D Motion\nvar skeleton: ARSkeleton3D\nThe tracked body in 3D.\nGetting Scale Information\nvar estimatedScaleFactor: CGFloat\nA factor that relates the body's default height with the height ARKit estimates at runtime.\nRelationships\nInherits From\nARAnchor\nConforms To\nARTrackable\nSee Also\nBody Position Tracking\nCapturing Body Motion in 3D\nTrack a person in the physical environment and visualize their motion by applying the same body movements to a virtual character.\nRigging a Model for Motion Capture\nConfigure custom 3D models so ARKit’s human body-tracking feature can control them.\nValidating a Model for Motion Capture\nVerify that your character model matches ARKit’s Motion Capture requirements."
  },
  {
    "title": "ARAppClipCodeAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arappclipcodeanchor",
    "html": "Overview\n\nYour App Clip gives users immediate access to critical or context-specific parts of your app’s AR experience, and makes it easy for them to download and launch your full app if they choose.\n\nYou can use physical App Clip Codes in the real world to enable users to discover your App Clip. An App Clip Code includes a unique URL and can incorporate an NFC tag. When users hold their iPhone near the code or scan it with the camera or Code Scanner in Control Center, the system offers to launch the code's associated App Clip.\n\nFor more on App Clip Codes, see App Clips. For an app that reacts to App Clip Codes in AR, see Interacting with App Clip Codes in AR.\n\nDistinguish Between App Clip Codes\n\nThere may be multiple App Clip Codes visible in the camera feed that share the same url, so ARKit also relies on the App Clip Code’s location (see transform) to distinguish different App Clip Codes in the physical environment.\n\nWhen the framework recognizes an App Clip Code, it initializes an ARAppClipCodeAnchor and passes it to the session delegate via session(_:didAdd:). If a recognized App Clip Code becomes obscured or is no longer visible in the camera feed, the framework sets the anchor’s isTracked property to false and passes it into the session(_:didUpdate:) callback. If the same App Clip Code becomes visible once again:\n\nARKit sets isTracked to true if the App Clip Code maintained its relative position in the physical environment.\n\nARKit intializes a new ARAppClipCodeAnchor if the App Clip Code position in the physical environment changed significantly.\n\nRemove App Clip Codes\n\nTo prevent App Clip Codes from accumulating in the session, ARKit removes anchors by passing them in to the session(_:didRemove:) callback. ARKit removes an ARAppClipCodeAnchor if all of the following conditions are true:\n\nThe framework instantiates a new ARAppClipCodeAnchor.\n\nThe new anchor’s url matches one or more existing anchors with substantially different positions in the physical environment.\n\nThe existing anchors are untracked (isTracked is false).\n\nTopics\nDecoding the URL\nvar url: URL?\nThe URL encoded in an App Clip Code.\nvar urlDecodingState: ARAppClipCodeAnchor.URLDecodingState\nA state that indicates the process of decoding an App Clip Code URL.\nenum ARAppClipCodeAnchor.URLDecodingState\nThe states in the process of decoding an App Clip code URL.\nMeasuring Physical Size\nvar radius: Float\nThe App Clip Code's radius in meters.\nRelationships\nInherits From\nARAnchor\nConforms To\nARTrackable\nSee Also\nApp Clip Codes\nInteracting with App Clip Codes in AR\nDisplay content and provide services in an AR experience with App Clip Codes."
  },
  {
    "title": "Recognizing and Labeling Arbitrary Objects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/recognizing_and_labeling_arbitrary_objects",
    "html": "Overview\n\nThis sample app parses the camera feed, using the Vision framework with a Core ML model that recognizes regular desktop items. The app displays a label onscreen that indicates when it recognizes an item. You then tap the screen to place a textual annotation in the physical environment that’s labeled with the name of the recognized object. Because the Core ML model used by this app doesn’t tell you where the object lies within an image, label placement relative to the object depends on where you tap.\n\nNote\n\nARKit requires an iOS device with an A9 or later processor. ARKit is not available in iOS Simulator.\n\nImplement the Vision/Core ML Image Classifier\n\nThe sample code’s classificationRequest property, classifyCurrentImage() method, and processClassifications(for:error:) method manage:\n\nA Core ML image-classifier model, loaded from an mlmodel file bundled with the app using the Swift API that Core ML generates for the model\n\nVNCoreMLRequest and VNImageRequestHandler objects for passing image data to the model for evaluation\n\nFor more details on using VNImageRequestHandler, VNCoreMLRequest, and image classifier models, see the Classifying Images with Vision and Core ML sample-code project.\n\nRun the AR Session and Process Camera Images\n\nThe sample ViewController class manages the AR session and displays AR overlay content in a SpriteKit view. ARKit captures video frames from the camera and provides them to the view controller in the session(_:didUpdate:) method, which then calls the classifyCurrentImage() method to run the Vision image classifier.\n\nfunc session(_ session: ARSession, didUpdate frame: ARFrame) {\n    // Do not enqueue other buffers for processing while another Vision task is still running.\n    // The camera stream has only a finite amount of buffers available; holding too many buffers for analysis would starve the camera.\n    guard currentBuffer == nil, case .normal = frame.camera.trackingState else {\n        return\n    }\n    \n    // Retain the image buffer for Vision processing.\n    self.currentBuffer = frame.capturedImage\n    classifyCurrentImage()\n}\n\n\nSerialize Image Processing for Real-Time Performance\n\nThe classifyCurrentImage() method uses the view controller’s currentBuffer property to track whether Vision is currently processing an image before starting another Vision task.\n\n// Most computer vision tasks are not rotation agnostic so it is important to pass in the orientation of the image with respect to device.\nlet orientation = CGImagePropertyOrientation(UIDevice.current.orientation)\n\n\nlet requestHandler = VNImageRequestHandler(cvPixelBuffer: currentBuffer!, orientation: orientation)\nvisionQueue.async {\n    do {\n        // Release the pixel buffer when done, allowing the next buffer to be processed.\n        defer { self.currentBuffer = nil }\n        try requestHandler.perform([self.classificationRequest])\n    } catch {\n        print(\"Error: Vision request failed with error \\\"\\(error)\\\"\")\n    }\n}\n\n\nImportant\n\nLimit your processing to one buffer at a time for performance. The camera recycles a finite pool of pixel buffers, so retaining too many buffers for processing could starve the camera and shut down the capture session. Passing multiple buffers to Vision for processing would slow down processing of each image, adding latency and reducing the amount of CPU and GPU overhead for rendering AR visualizations.\n\nIn addition, the sample app enables the usesCPUOnly setting for its Vision request, freeing the GPU for use in rendering.\n\nVisualize Results in AR\n\nThe processClassifications(for:error:) method stores the best-match result label produced by the image classifier and displays it in the corner of the screen. The user can then tap in the AR scene to place that label at a real-world position. Placing a label requires two main steps.\n\nFirst, a tap gesture recognizer fires the placeLabelAtLocation(sender:) action. This method uses the ARKit hitTest(_:types:) method to estimate the 3D real-world position corresponding to the tap, and adds an anchor to the AR session at that position.\n\n@IBAction func placeLabelAtLocation(sender: UITapGestureRecognizer) {\n    let hitLocationInView = sender.location(in: sceneView)\n    let hitTestResults = sceneView.hitTest(hitLocationInView, types: [.featurePoint, .estimatedHorizontalPlane])\n    if let result = hitTestResults.first {\n        \n        // Add a new anchor at the tap location.\n        let anchor = ARAnchor(transform: result.worldTransform)\n        sceneView.session.add(anchor: anchor)\n        \n        // Track anchor ID to associate text with the anchor after ARKit creates a corresponding SKNode.\n        anchorLabels[anchor.identifier] = identifierString\n    }\n}\n\n\nNext, after ARKit automatically creates a SpriteKit node for the newly added anchor, the view(_:didAdd:for:) delegate method provides content for that node. In this case, the sample TemplateLabelNode class creates a styled text label using the string provided by the image classifier.\n\nfunc view(_ view: ARSKView, didAdd node: SKNode, for anchor: ARAnchor) {\n    guard let labelText = anchorLabels[anchor.identifier] else {\n        fatalError(\"missing expected associated label for anchor\")\n    }\n    let label = TemplateLabelNode(text: labelText)\n    node.addChild(label)\n}\n\n\nSee Also\nText Annotations\nCreating Screen Annotations for Objects in an AR Experience\nAnnotate an AR experience with virtual sticky notes that you display onscreen over real and virtual objects."
  },
  {
    "title": "currentFrame | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2865621-currentframe",
    "html": "See Also\nAccessing the camera frame\nclass ARFrame\nA video image captured as part of a session with position-tracking information.\nfunc captureHighResolutionFrame(completion: (ARFrame?, Error?) -> Void)\nRequests a frame outside of the normal frequency that contains a high-resolution captured image."
  },
  {
    "title": "Creating Screen Annotations for Objects in an AR Experience | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/creating_screen_annotations_for_objects_in_an_ar_experience",
    "html": "Overview\n\nAt times, the user may want to annotate real or virtual objects in your AR experience. For example, they might want to place a virtual name plate on paintings at a museum. By fixing annotations to the screen, you enable the user to annotate their AR experience in screen space. To demonstrate screen-space annotations, this sample app enables the capability to tap the screen to place one or more virtual sticky notes with text in the real world.\n\nText displayed in screen space remains readable at all viewing angles and distances. The sample app implements sticky notes using a UITextView that’s flush with the screen, which allows the user to quickly define the note’s text using regular touch input. Using UIKit to annotate an AR experience also provides the benefits of localization and accessibility.\n\nNote\n\nThis sample uses RealityKit to anchor virtual content in the real world. RealityKit requires iOS 13. ARKit is not available in iOS Simulator.\n\nTo display text that’s anchored in world space instead, see Recognizing and Labeling Arbitrary Objects.\n\nResolve the User’s Tap to a 3D Location\n\nTo annotate an object in an AR experience, you first determine where it is in the physical environment. This sample app enables the user to tap the screen to place a sticky note by first adding a tap gesture recognizer to the view.\n\nfunc arViewGestureSetup() {\n    let tapGesture = UITapGestureRecognizer(target: self, action: #selector(tappedOnARView))\n    arView.addGestureRecognizer(tapGesture)\n    \n    let swipeGesture = UISwipeGestureRecognizer(target: self, action: #selector(swipedDownOnARView))\n    swipeGesture.direction = .down\n    arView.addGestureRecognizer(swipeGesture)\n}\n\n\nWhen the input handler is called, you read the tap screen coordinates by calling location(in:).\n\nlet touchLocation = sender.location(in: arView)\n\n\nTo get a 3D world position that corresponds to the tap location, cast a ray from the camera’s origin through the touch location to check for intersection with any real-world surfaces along that ray.\n\nguard let raycastResult = arView.raycast(from: touchLocation, allowing: .estimatedPlane, alignment: .any).first else {\n    messageLabel.displayMessage(\"No surface detected, try getting closer.\", duration: 2.0)\n    return\n}\n\n\nIf ARKit finds a planar surface where the user tapped, the ray-cast result provides you the 3D intersection point in worldTransform.\n\nAnchor a Sticky Note in the Environment\n\nTo keep track of a real-world location, you create an anchor positioned there. RealityKit implements an anchor as an Entity conforming to HasAnchoring. Thus, you implement those protocols when designing a sticky note in RealityKit.\n\nclass StickyNoteEntity: Entity, HasAnchoring, HasScreenSpaceView {\n    // ...\n\n\nCreate the entity by calling its initializer and passing in the ray-cast result’s worldTransform.\n\nlet note = StickyNoteEntity(frame: frame, worldTransform: raycastResult.worldTransform)\n\n\nIn the sticky note entity’s init function, position the entity at the tap location by setting its transformation matrix to the argument worldTransform.\n\ninit(frame: CGRect, worldTransform: simd_float4x4) {\n    super.init()\n    self.transform.matrix = worldTransform\n    // ...\n\n\nLet RealityKit know about your entity by adding it to the scene hierarchy. RealityKit then registers an ARAnchor for your entity with ARKit.\n\n// Add the sticky note to the scene's entity hierarchy.\narView.scene.addAnchor(note)\n\nDisplay the Sticky Note’s Annotation\n\nFor the purposes of this sample app, the sticky note entity has no geometry and thus, no appearance. Its anchor provides a 3D location only, and it’s the sticky note’s screen-space annotation that has an appearance. To display it, you define the sticky note’s annotation. Following RealityKit’s entity-component model, design a component that houses the annotation, which in this case is a view. See ScreenSpaceComponent.\n\nstruct ScreenSpaceComponent: Component {\n    var view: StickyNoteView?\n    //...\n\n\nAs a prepackaged UI element that renders text for you, UITextView is useful as a screen-space annotation.\n\nclass StickyNoteView: UIView {\n    var textView: UITextView!\n    // ...\n\n\nExpose the screen-space component in its own protocol.\n\nprotocol HasScreenSpaceView: Entity {\n    var screenSpaceComponent: ScreenSpaceComponent { get set }\n}\n\n\nImplement the protocol in your entity; see StickyNoteEntity.\n\nclass StickyNoteEntity: Entity, HasAnchoring, HasScreenSpaceView {\n    // ...\n\n\nTo display the entity’s annotation, add the sticky-note view to the view hierarchy.\n\n// Add the sticky note's view to the view hierarchy.\nguard let stickyView = note.view else { return }\narView.insertSubview(stickyView, belowSubview: trashZone)\n\n\nTo put the annotation in the right place on the screen, ask the ARView to convert its entity’s world location to a 2D screen point.\n\nguard let projectedPoint = arView.project(note.position) else { return }\n\n\nTo enhance visual accuracy, center the note’s view around the anchor’s projected world location.\n\nsetPositionCenter(projectedPoint)\n\n\nTo do that, calculate the midpoint and set the view’s origin.\n\nview.frame.origin = CGPoint(x: centerPoint.x, y: centerPoint.y)\n\nUpdate the Annotation’s Position\n\nBecause users move their device during an AR experience, the annotation’s screen position quickly becomes out of sync with its anchor’s world position. To keep the annotation’s screen position accurate, call ARView’s project function every frame, updating the annotation’s position with the result.\n\n// Updates the screen position of the note based on its visibility\nnote.projection = Projection(projectedPoint: projectedPoint, isVisible: isVisible)\nnote.updateScreenPosition()\n\nHandle User Interaction\n\nA benefit of using UIView types for screen annotations is that they simplify user interaction. The sample implements sticky notes using UITextView, which enables users to more easily edit their text. The sample implements minimal gesture recognizer code to manage sticky notes.\n\nThe following code enables the capability to create a note by tapping the screen.\n\n@objc\nfunc tappedOnARView(_ sender: UITapGestureRecognizer) {\n    \n    // Ignore the tap if the user is editing a sticky note.\n    for note in stickyNotes where note.isEditing { return }\n    \n    // Create a new sticky note at the tap location.\n    insertNewSticky(sender)\n}\n\n\nBy implementing its own tap gesture recognizer to control editing, UITextView enables the user to tap an existing note to edit its text. To be notified when the user edits a note, override UITextView’s textViewDidBeginEditing(_ textView:) delegate callback.\n\nextension ViewController: UITextViewDelegate {\n    \n    // - Tag: TextViewDidBeginEditing\n    func textViewDidBeginEditing(_ textView: UITextView) {\n\n\n        // Get the main view for this sticky note.\n        guard let stickyView = textView.firstSuperViewOfType(StickyNoteView.self) else { return }\n        // ...\n\n\nThe following code enables the capability to move a note by panning the screen.\n\n@objc\nfunc panOnStickyView(_ sender: UIPanGestureRecognizer) {\n    \n    guard let stickyView = sender.view as? StickyNoteView else { return }\n    \n    let panLocation = sender.location(in: arView)\n    \n    // Ignore the pan if any StickyViews are being edited.\n    for note in stickyNotes where note.isEditing { return }\n    \n    panStickyNote(sender, stickyView, panLocation)\n}\n\n\nWhen the user pans to reposition a sticky note, you convert the screen touch location to a 3D world position using raycast(from:allowing:alignment:). The user can then reposition the sticky note’s anchor in the real world versus simply moving the annotation to a new arbitrary screen location. If a ray cast from the final screen location in the pan gesture doesn’t produce an intersection with a 3D world location, don’t move the sticky note there.\n\n/// - Tag: AttemptRepositioning\nfileprivate func attemptRepositioning(_ stickyView: StickyNoteView) {\n    // Conducts a ray-cast for feature points using the panned position of the StickyNoteView\n    let point = CGPoint(x: stickyView.frame.midX, y: stickyView.frame.midY)\n    if let result = arView.raycast(from: point, allowing: .estimatedPlane, alignment: .any).first {\n        stickyView.stickyNote.transform.matrix = result.worldTransform\n    } else {\n        messageLabel.displayMessage(\"No surface detected, unable to reposition note.\", duration: 2.0)\n        stickyView.stickyNote.shouldAnimate = true\n    }\n}\n\n\nThe following portion of the pan gesture handler enables the capability to remove a sticky note when the user drags it to the text that says “delete” at the top of the screen.\n\nif stickyView.isInTrashZone {\n    deleteStickyNote(stickyView.stickyNote)\n    // ...\n\nEnhance the Experience with Animation\n\nKeeping screen-space annotations to a minimum will maximize the user’s immersion in the AR experience. The sample app makes sticky notes small when the user isn’t editing text, minimizing distractions so they can focus on the real-world environment. But for similar reasons, you should enlarge a sticky note when the user is editing text. To create a seamless transition between editing and nonediting states, animate the sticky note’s size instead of changing it abruptly. See the animateStickyViewToEditingFrame function.\n\nfunc animateStickyViewToEditingFrame(_ stickyView: StickyNoteView, keyboardHeight: Double) {\n    let safeFrame = view.safeAreaLayoutGuide.layoutFrame\n    let height = safeFrame.height - keyboardHeight\n    let inset = height * 0.05\n    let editingFrame = CGRect(origin: safeFrame.origin, size: CGSize(width: safeFrame.width, height: height)).insetBy(dx: inset, dy: inset)\n    UIViewPropertyAnimator(duration: 0.2, curve: .easeIn) {\n        stickyView.frame = editingFrame\n        //...\n\n\nBring even more focus to the editing experience by dimming the background and by lighting the sticky note that the user is editing.\n\nstickyView.blurView.effect = UIBlurEffect(style: .light)\n\n\nTo prevent the user from losing track of a sticky note’s real-world location, animate the note smoothly from one position to the next. For example, if an annotation fails to reposition, animate the sticky note back to its original screen position. This increases the user’s ability to track the annotation if they want to try moving it again.\n\nif shouldAnimate {\n    animateTo(projectedPoint)\n    // ...\n\n\nTo animate the note’s movement, you continually set its location using a UIViewPropertyAnimator.\n\nfunc animateTo(_ point: CGPoint) {\n\n\n    let animator = UIViewPropertyAnimator(duration: 0.3, curve: .linear) {\n        self.setPositionCenter(point)\n    }\n    // ...\n\n\nSee Also\nText Annotations\nRecognizing and Labeling Arbitrary Objects\nCreate anchors that track objects you recognize in the camera feed, using a custom optical-recognition algorithm."
  },
  {
    "title": "raycast(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/3132065-raycast",
    "html": "Parameters\nquery\n\nThe ray you create from a screen point you're interested in.\n\nReturn Value\n\nAn array of ray-cast results, sorted from nearest to furthest from the camera. The array is empty if the ray cast fails to find an intersection between the query's ray and a real-world surface.\n\nDiscussion\n\nRay casting provides a 3D location in physical space that corresponds to a given 2D location on the screen. When you call this function, it succeeds in returning a result when a mathematical ray that ARKit casts outward from the user intersects with any real-world surfaces that ARKit detects in the physical environment.\n\nSee Also\nFinding real-world surfaces\nfunc trackedRaycast(ARRaycastQuery, updateHandler: ([ARRaycastResult]) -> Void) -> ARTrackedRaycast?\nRepeats a ray-cast query over time to notify you of updated surfaces in the physical environment."
  },
  {
    "title": "isTracked | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/artrackable/2928210-istracked",
    "html": "Required\n\nDiscussion\n\nIf this value is true, the object’s transform currently matches the position and orientation of the real-world object it represents.\n\nIf this value is false, the object is not guaranteed to match the movement of its corresponding real-world feature, even if it remains in the visible scene."
  },
  {
    "title": "setWorldOrigin(relativeTransform:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2942278-setworldorigin",
    "html": "Parameters\nrelativeTransform\n\nA transform matrix encoding a translation and orientation relative to the session's current world coordinate space.\n\nDiscussion\n\nARKit defines a world coordinate space for you to use to place virtual content and locate detected objects in an AR experience. By default, this space is based on the initial position and orientation of the device when the session begins. However, after a session begins and has detected useful reference points (such as a plane or image), you may find it helpful to redefine the world coordinate system based on those reference points."
  },
  {
    "title": "Tracking and Visualizing Planes | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/tracking_and_visualizing_planes",
    "html": "Overview\n\nThis sample app runs an ARKit world tracking session with content displayed in a SceneKit view. To demonstrate plane detection, the app visualizes the estimated shape of each detected ARPlaneAnchor object, and a bounding rectangle for it. On supported devices, ARKit can recognize many types of real-world surfaces, so the app also labels each detected plane with identifying text.\n\nNote\n\nARKit requires an iOS device with an A9 or later processor. ARKit is not available in iOS Simulator.\n\nConfigure and Run the AR Session\n\nThe ARSCNView class is a SceneKit view that includes an ARSession object that manages the motion tracking and image processing required to create an augmented reality (AR) experience. However, to run a session you must provide a session configuration.\n\nThe ARWorldTrackingConfiguration class provides high-precision motion tracking and enables features to help you place virtual content in relation to real-world surfaces. To start an AR session, create a session configuration object with the options you want (such as plane detection), then call the run(_:options:) method on the session object of your ARSCNView instance:\n\nlet configuration = ARWorldTrackingConfiguration()\nconfiguration.planeDetection = [.horizontal, .vertical]\nsceneView.session.run(configuration)\n\n\nRun your session only when the view that will display it is onscreen.\n\nImportant\n\nIf your app requires ARKit for its core functionality, use the arkit key in the section of your app’s Info.plist file to make your app available only on devices that support ARKit. If AR is a secondary feature of your app, use the isSupported property to determine whether to offer AR-based features.\n\nPlace 3D Content for Detected Planes\n\nAfter you’ve set up your AR session, you can use SceneKit to place virtual content in the view.\n\nWhen plane detection is enabled, ARKit adds and updates anchors for each detected plane. By default, the ARSCNView class adds an SCNNode object to the SceneKit scene for each anchor. Your view’s delegate can implement the renderer(_:didAdd:for:) method to add content to the scene. When you add content as a child of the node corresponding to the anchor, the ARSCNView class automatically moves that content as ARKit refines its estimate of the plane’s position.\n\nfunc renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor) {\n    // Place content only for anchors found by plane detection.\n    guard let planeAnchor = anchor as? ARPlaneAnchor else { return }\n    \n    // Create a custom object to visualize the plane geometry and extent.\n    let plane = Plane(anchor: planeAnchor, in: sceneView)\n    \n    // Add the visualization to the ARKit-managed node so that it tracks\n    // changes in the plane anchor as plane estimation continues.\n    node.addChildNode(plane)\n}\n\n\nARKit offers two ways to track the area of an estimated plane. A plane anchor’s geometry describes a convex polygon tightly enclosing all points that ARKit currently estimates to be part of the same plane (easily visualized using ARSCNPlaneGeometry). ARKit also provides a simpler estimate in a plane anchor’s extent and center, which together describe a rectangular boundary (easily visualized using SCNPlane).\n\n// Create a mesh to visualize the estimated shape of the plane.\nguard let meshGeometry = ARSCNPlaneGeometry(device: sceneView.device!)\n    else { fatalError(\"Can't create plane geometry\") }\nmeshGeometry.update(from: anchor.geometry)\nmeshNode = SCNNode(geometry: meshGeometry)\n\n\n// Create a node to visualize the plane's bounding rectangle.\nlet extentPlane: SCNPlane = SCNPlane(width: CGFloat(anchor.extent.x), height: CGFloat(anchor.extent.z))\nextentNode = SCNNode(geometry: extentPlane)\nextentNode.simdPosition = anchor.center\n\n\n// `SCNPlane` is vertically oriented in its local coordinate space, so\n// rotate it to match the orientation of `ARPlaneAnchor`.\nextentNode.eulerAngles.x = -.pi / 2\n\n\nARKit continually updates its estimates of each detected plane’s shape and extent. To show the current estimated shape for each plane, this sample app also implements the renderer(_:didUpdate:for:) method, updating the ARSCNPlaneGeometry and SCNPlane objects to reflect the latest information from ARKit.\n\nfunc renderer(_ renderer: SCNSceneRenderer, didUpdate node: SCNNode, for anchor: ARAnchor) {\n    // Update only anchors and nodes set up by `renderer(_:didAdd:for:)`.\n    guard let planeAnchor = anchor as? ARPlaneAnchor,\n        let plane = node.childNodes.first as? Plane\n        else { return }\n    \n    // Update ARSCNPlaneGeometry to the anchor's new estimated shape.\n    if let planeGeometry = plane.meshNode.geometry as? ARSCNPlaneGeometry {\n        planeGeometry.update(from: planeAnchor.geometry)\n    }\n\n\n    // Update extent visualization to the anchor's new bounding rectangle.\n    if let extentGeometry = plane.extentNode.geometry as? SCNPlane {\n        extentGeometry.width = CGFloat(planeAnchor.extent.x)\n        extentGeometry.height = CGFloat(planeAnchor.extent.z)\n        plane.extentNode.simdPosition = planeAnchor.center\n    }\n    \n    // Update the plane's classification and the text position\n    if #available(iOS 12.0, *),\n        let classificationNode = plane.classificationNode,\n        let classificationGeometry = classificationNode.geometry as? SCNText {\n        let currentClassification = planeAnchor.classification.description\n        if let oldClassification = classificationGeometry.string as? String, oldClassification != currentClassification {\n            classificationGeometry.string = currentClassification\n            classificationNode.centerAlign()\n        }\n    }\n    \n}\n\n\nOn iPhone XS, iPhone XS Max, and iPhone XR, ARKit can also classify detected planes, reporting which kind of common real-world surface that plane represents (for example, a table, floor, or wall). In this example, the renderer(_:didUpdate:for:) method also displays and updates a text label to show that information.\n\nSee Also\nSurface Detection\nclass ARPlaneAnchor\nAn anchor for a 2D planar surface that ARKit detects in the physical environment.\nclass ARMeshAnchor\nAn anchor for a physical object that ARKit detects and recreates virtually using a polygonal mesh."
  },
  {
    "title": "ARMeshAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/armeshanchor",
    "html": "Overview\n\nARKit subdivides the reconstructed, real-world scene surrounding the user into mesh anchors.\n\nMesh anchors constantly update their data as ARKit refines its understanding of the real world. Although ARKit updates a mesh to reflect a change in the physical environment (such as when a person pulls out a chair), the mesh's subsequent change is not intended to reflect in real time.\n\nTopics\nAccessing the Mesh\nvar geometry: ARMeshGeometry\n3D information about the mesh such as its shape and classifications.\nclass ARMeshGeometry\nMesh information stored in an efficient, array-based format.\nRelationships\nInherits From\nARAnchor\nSee Also\nSurface Detection\nTracking and Visualizing Planes\nDetect surfaces in the physical environment and visualize their shape and location in 3D space.\nclass ARPlaneAnchor\nAn anchor for a 2D planar surface that ARKit detects in the physical environment."
  },
  {
    "title": "Detecting Images in an AR Experience | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/detecting_images_in_an_ar_experience",
    "html": "Overview\n\nMany AR experiences can be enhanced by using known features of the user’s environment to trigger the appearance of virtual content. For example, a museum app might show a virtual curator when the user points their device at a painting, or a board game might place virtual pieces when the player points their device at a game board. In iOS 11.3 and later, you can add such features to your AR experience by enabling image detection in ARKit: Your app provides known 2D images, and ARKit tells you when and where those images are detected during an AR session.\n\nThis example app looks for any of the several reference images included in the app’s asset catalog. When ARKit detects one of those images, the app shows a message identifying the detected image and a brief animation showing its position in the scene.\n\nImportant\n\nThe images included with this sample are designed to fit the screen sizes of various Apple devices. To try the app using these images, choose an image that fits any spare device you have, and display the image full screen on that device. Then run the sample code project on a different device, and point its camera at the device displaying the image. Alternatively, you can add your own images; see the steps in Provide Your Own Reference Images, below.\n\nNote\n\nARKit requires an iOS device with an A9 (or later) processor. ARKit isn’t available in iOS Simulator.\n\nEnable Image Detection\n\nImage detection is an add-on feature for world-tracking AR sessions. (For more details on world tracking, see Tracking and Visualizing Planes.)\n\nTo enable image detection:\n\nLoad one or more ARReferenceImage resources from your app’s asset catalog.\n\nCreate a world-tracking configuration and pass those reference images to its detectionImages property.\n\nUse the run(_:options:) method to run a session with your configuration.\n\nThe code below shows how the sample app performs these steps when starting or restarting the AR experience.\n\nguard let referenceImages = ARReferenceImage.referenceImages(inGroupNamed: \"AR Resources\", bundle: nil) else {\n    fatalError(\"Missing expected asset catalog resources.\")\n}\n\n\nlet configuration = ARWorldTrackingConfiguration()\nconfiguration.detectionImages = referenceImages\nsession.run(configuration, options: [.resetTracking, .removeExistingAnchors])\n\n\nVisualize Image Detection Results\n\nWhen ARKit detects one of your reference images, the session automatically adds a corresponding ARImageAnchor to its list of anchors. To respond to an image being detected, implement an appropriate ARSessionDelegate, ARSKViewDelegate, or ARSCNViewDelegate method that reports the new anchor being added to the session. (This example app uses the renderer(_:didAdd:for:) method for the code shown below.)\n\nTo use the detected image as a trigger for AR content, you’ll need to know its position and orientation, its size, and which reference image it is. The anchor’s inherited transform property provides position and orientation, and its referenceImage property tells you which ARReferenceImage object was detected. If your AR content depends on the extent of the image in the scene, you can then use the reference image’s physicalSize to set up your content, as shown in the code below.\n\nguard let imageAnchor = anchor as? ARImageAnchor else { return }\nlet referenceImage = imageAnchor.referenceImage\nupdateQueue.async {\n    \n    // Create a plane to visualize the initial position of the detected image.\n    let plane = SCNPlane(width: referenceImage.physicalSize.width,\n                         height: referenceImage.physicalSize.height)\n    let planeNode = SCNNode(geometry: plane)\n    planeNode.opacity = 0.25\n    \n    /*\n     `SCNPlane` is vertically oriented in its local coordinate space, but\n     `ARImageAnchor` assumes the image is horizontal in its local space, so\n     rotate the plane to match.\n     */\n    planeNode.eulerAngles.x = -.pi / 2\n    \n    /*\n     Image anchors are not tracked after initial detection, so create an\n     animation that limits the duration for which the plane visualization appears.\n     */\n    planeNode.runAction(self.imageHighlightAction)\n    \n    // Add the plane visualization to the scene.\n    node.addChildNode(planeNode)\n}\n\n\nProvide Your Own Reference Images\n\nTo use your own images for detection (in this sample or in your own project), you’ll need to add them to your asset catalog in Xcode.\n\nOpen your project’s asset catalog, then use the Add button (+) to add a new AR resource group.\n\nDrag image files from the Finder into the newly created resource group.\n\nFor each image, use the inspector to describe the physical size of the image as you’d expect to find it in the user’s real-world environment, and optionally include a descriptive name for your own use.\n\nNote\n\nPut all the images you want to look for in the same session into a resource group. Use separate resource groups to hold sets of images for use in separate sessions. For example, an art museum app might use separate sessions (and thus separate resource groups) for detecting paintings in different wings of the museum.\n\nBe aware of image detection capabilities. Choose, design, and configure reference images for optimal reliability and performance:\n\nEnter the physical size of the image in Xcode as accurately as possible. ARKit relies on this information to determine the distance of the image from the camera. Entering an incorrect physical size will result in an ARImageAnchor that’s the wrong distance from the camera.\n\nWhen you add reference images to your asset catalog in Xcode, pay attention to the quality estimation warnings Xcode provides. Images with high contrast work best for image detection.\n\nUse only images on flat surfaces for detection. If an image to be detected is on a nonplanar surface, like a label on a wine bottle, ARKit might not detect it at all, or might create an image anchor at the wrong location.\n\nConsider how your image appears under different lighting conditions. If an image is printed on glossy paper or displayed on a device screen, reflections on those surfaces can interfere with detection.\n\nApply Best Practices\n\nThis example app simply visualizes where ARKit detects each reference image in the user’s environment, but your app can do much more. Follow the tips below to design AR experiences that use image detection well.\n\nUse detected images to set a frame of reference for the AR scene. Instead of requiring the user to choose a place for virtual content, or arbitrarily placing content in the user’s environment, use detected images to anchor the virtual scene. You can even use multiple detected images. For example, an app for a retail store could make a virtual character appear to emerge from a store’s front door by detecting posters placed on either side of the door and then calculating a position for the character directly between the posters.\n\nNote\n\nUse the ARSession setWorldOrigin(relativeTransform:) method to redefine the world coordinate system so that you can place all anchors and other content relative to the reference point you choose.\n\nDesign your AR experience to use detected images as a starting point for virtual content. ARKit doesn’t track changes to the position or orientation of each detected image. If you try to place virtual content that stays attached to a detected image, that content may not appear to stay in place correctly. Instead, use detected images as a frame of reference for starting a dynamic scene. For example, your app might detect theater posters for a sci-fi film and then have virtual spaceships appear to emerge from the posters and fly around the environment.\n\nConsider when to allow detection of each image to trigger (or repeat) AR interactions. ARKit adds an image anchor to a session exactly once for each reference image in the session configuration’s detectionImages array. If your AR experience adds virtual content to the scene when an image is detected, that action will by default happen only once. To allow the user to experience that content again without restarting your app, call the session’s remove(anchor:) method to remove the corresponding ARImageAnchor. After the anchor is removed, ARKit will add a new anchor the next time it detects the image.\n\nFor example, in the case described above, where spaceships appear to fly out of a movie poster, you might not want an extra copy of that animation to appear while the first one is still playing. Wait until the animation ends to remove the anchor, so that the user can trigger it again by pointing their device at the image.\n\nSee Also\nImage Detection\nTracking and altering images\nCreate images from rectangular shapes found in the user’s environment, and augment their appearance.\nclass ARImageAnchor\nAn anchor for a known image that ARKit detects in the physical environment.\nclass ARReferenceImage\nA 2D image that you want ARKit to detect in the physical environment."
  },
  {
    "title": "ARReferenceImage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arreferenceimage",
    "html": "Overview\n\nTo accurately detect the position and orientation of a 2D image in the real world, ARKit requires preprocessed image data and knowledge of the image's real-world dimensions. The ARReferenceImage class encapsulates this information. To enable image detection in an AR session, pass a collection of reference images to your session configuration's detectionImages property.\n\nTypically, you create reference images in your Xcode project's asset catalog:\n\nIn your asset catalog, use the Add (+) button to create an AR Resource Group.\n\nDrag image files into the resource group to create AR Reference Image entries in the asset catalog.\n\nFor each reference image, use the Xcode inspector panel to provide the real-world size at which you want ARKit to recognize the image. (You can also provide a descriptive name, which appears as the name property at runtime and can be useful for debugging.)\n\nTopics\nLoading Reference Images\nclass func referenceImages(inGroupNamed: String, bundle: Bundle?) -> Set<ARReferenceImage>?\nLoads all reference images in the specified AR Resource Group in your Xcode project's asset catalog.\nExamining a Reference Image\nvar name: String?\nA descriptive name for the image.\nvar physicalSize: CGSize\nThe real-world dimensions, in meters, of the image.\nvar resourceGroupName: String?\nThe AR resource group name for this image.\nCreating Reference Images\ninit(CGImage, orientation: CGImagePropertyOrientation, physicalWidth: CGFloat)\nCreates a new reference image from a Core Graphics image object.\ninit(CVPixelBuffer, orientation: CGImagePropertyOrientation, physicalWidth: CGFloat)\nCreates a new reference image from a Core Video pixel buffer.\nValidating Reference Images\nfunc validate(completionHandler: (Error?) -> Void)\nDetermines whether the reference image is valid.\nRelationships\nInherits From\nNSObject\nSee Also\nImage Detection\nTracking and altering images\nCreate images from rectangular shapes found in the user’s environment, and augment their appearance.\nDetecting Images in an AR Experience\nReact to known 2D images in the user’s environment, and use their positions to place AR content.\nclass ARImageAnchor\nAn anchor for a known image that ARKit detects in the physical environment."
  },
  {
    "title": "ARDirectionalLightEstimate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/ardirectionallightestimate",
    "html": "Overview\n\nWhen you run a face tracking AR session (see ARFaceTrackingConfiguration) with the isLightEstimationEnabled property set to true, ARKit uses the detected face as a light probe to estimate the directional lighting environment in the scene. The lightEstimate property of each frame vended by the session contains an ARDirectionalLightEstimate instance containing this information.\n\nIf you render your own overlay graphics for the AR scene, you can use this information in shading algorithms to help make those graphics match the real-world lighting conditions of the scene captured by the camera. (The ARSCNView class automatically uses this information to configure SceneKit lighting.)\n\nTopics\nExamining Light Parameters\nvar sphericalHarmonicsCoefficients: Data\nData describing the estimated lighting environment in all directions.\nvar primaryLightDirection: simd_float3\nA vector indicating the orientation of the strongest directional light source in the scene.\nvar primaryLightIntensity: CGFloat\nThe estimated intensity, in lumens, of the strongest directional light source in the scene.\nRelationships\nInherits From\nARLightEstimate\nSee Also\nLighting Effects\nAdding Realistic Reflections to an AR Experience\nUse ARKit to generate environment probe textures from camera imagery and render reflective virtual objects.\nclass AREnvironmentProbeAnchor\nAn object that provides environmental lighting information for a specific area of space in a world-tracking AR session.\nclass ARLightEstimate\nEstimated scene lighting information associated with a captured video frame in an AR session."
  },
  {
    "title": "ARImageTrackingConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arimagetrackingconfiguration",
    "html": "Overview\n\nAll AR configurations establish a correspondence between the real world the device inhabits and a virtual 3D coordinate space where you can model content. When your app displays that content together with a live camera image, the user experiences the illusion that your virtual content is part of the real world.\n\nWith ARImageTrackingConfiguration, ARKit establishes a 3D space not by tracking the motion of the device relative to the world, but solely by detecting and tracking the motion of known 2D images in view of the camera. ARWorldTrackingConfiguration can also detect images, but each configuration has its own strengths:\n\nWorld tracking has a higher performance cost than image-only tracking, so your session can reliably track more images at once with ARImageTrackingConfiguration.\n\nImage-only tracking lets you anchor virtual content to known images only when those images are in view of the camera. World tracking with image detection lets you use known images to add virtual content to the 3D world, and continues to track the position of that content in world space even after the image is no longer in view.\n\nWorld tracking works best in a stable, nonmoving environment. You can use image-only tracking to add virtual content to known images in more situations—for example, an advertisement inside a moving subway car.\n\nWhen an image-tracking configuration detects known images, it tracks their movement with six degrees of freedom (6DOF): specifically, the three rotation axes (roll, pitch, and yaw), and three translation axes (movement in x, y, and z).\n\nTo use ARImageTrackingConfiguration, define ARReferenceImage objects (either at runtime or by bundling them in your Xcode asset catalog) and assign them to the configuration's trackingImages property. Then, as with any AR configuration, pass the configuration to your session's run(_:options:) method.\n\nTopics\nCreating a Configuration\ninit()\nInitializes a new image-tracking configuration.\nChoosing Images to Track\nvar trackingImages: Set<ARReferenceImage>\nA set of images that ARKit searches for and tracks in the user's environment.\nvar maximumNumberOfTrackedImages: Int\nThe number of image anchors to monitor closely for position and orientation updates.\nManaging Device Camera Behavior\nvar isAutoFocusEnabled: Bool\nA Boolean value that determines whether the device camera uses fixed focus or autofocus behavior.\nRelationships\nInherits From\nARConfiguration"
  },
  {
    "title": "ARConfiguration.SceneReconstruction | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/scenereconstruction",
    "html": "Overview\n\nWhen you set one of the these values onto a world-tracking configuration's sceneReconstruction property, ARKit provides you with a mesh that models the real-world surrounding the user.\n\nTopics\nModeling the Environment\ninit(rawValue: UInt)\nInitializes a scene-reconstruction object.\nstatic var mesh: ARConfiguration.SceneReconstruction\nA polygonal mesh approximation of the physical environment.\nstatic var meshWithClassification: ARConfiguration.SceneReconstruction\nAn approximate shape of the physical environment, including classification of the real-world objects within it.\nRelationships\nConforms To\nOptionSet\nSendable"
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration/2928280-init",
    "html": "Discussion\n\nTo use the configuration in an AR session, pass it to the ARSession run(_:options:) method."
  },
  {
    "title": "ARGeoTrackingConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/argeotrackingconfiguration",
    "html": "Overview\n\nThis configuration creates location anchors (ARGeoAnchor) that specify a particular latitude, longitude, and optionally, altitude to enable an app to track geographic areas of interest in an AR experience.\n\nImportant\n\nThe isSupported property returns true for this class on iOS 14 & iPadOS 14 devices that have an A12 chip or later and cellular (GPS) capability. Geotracking is available in specific geographic locations. To determine availability at the user’s location at runtime, call checkAvailability(completionHandler:).\n\nGeotracking occurs exclusively outdoors. If a geotracking app navigates users between waypoints, your app needs to handle any events along a route. The user must have an internet connection, and you can provide them information about data usage, as described in ARGeoAnchor.\n\nEncourage User Safety\n\nTo keep your users' focus on the road while traveling, discourage them from looking at the device when in motion, such as while riding a bike. Keep users informed when navigating through unfamiliar territory. For instance, you can recommend they steer clear of private property, or remind them to check their device's battery level before beginning a long route.\n\nRefine the User's Position with Imagery\n\nTo place location anchors with precision, geotracking requires a better understanding of the user’s geographic location than is possible with GPS alone. Based on the user's GPS coordinates, ARKit downloads imagery that depicts the physical environment in that area. Apple collects this localization imagery in advance by capturing photos of the view from the street and recording the geographic position at each photo. By comparing the device's current camera image with this imagery, the session matches the user’s precise geographic location with the scene's local coordinates. For information about the user's position in local space, see transform.\n\nLocalization imagery captures views from public streets and routes accessible by car, but doesn't include images of gated or pedestrian-only areas.\n\nGeotracking sessions use localization imagery in the ARGeoTrackingStatus.State.localizing state.\n\nSupported Areas and Cities\n\nLocalization imagery is available for specific areas, including the following U.S. cities:\n\nArizona\n\nPhoenix\n\nCalifornia\n\nAlameda, Arden Arcade, Contra Costa, Los Angeles County, Marin County, Napa County, Orange County, Riverside, Roseville, Sacramento, San Bernadino, San Diego, San Francisco, San Jose, San Mateo, Santa Clara, Santa Cruz, Solano County, Sonoma County\n\nColorado\n\nDenver\n\nFlorida\n\nClearwater, Miami, St. Petersburg, Tampa\n\nGeorgia\n\nAtlanta\n\nIndiana\n\nBloomington\n\nMaryland\n\nBaltimore, Towson\n\nMassachusetts\n\nBoston\n\nMichigan\n\nDetroit, Livonia, Warren\n\nMinnesota\n\nMinneapolis, St. Paul\n\nMissouri\n\nSt. Louis\n\nNevada\n\nLas Vegas\n\nNew Jersey\n\nJersey City\n\nNew York\n\nNew York City\n\nOregon\n\nHillsboro, Portland\n\nPennsylvania\n\nPhiladelphia, Pittsburgh\n\nTexas\n\nAustin, Dallas, Houston, New Braunfels, San Antonio\n\nWashington, D.C.\n\nWashington\n\nSeattle, Vancouver\n\nLocalization imagery is also available here:\n\nAustralia\n\nSydney, Melbourne\n\nCanada\n\nVancouver, Montreal, Toronto\n\nJapan\n\nFukuoka, Hiroshima, Kyoto, Nagoya, Osaka, Tokyo, Yokohama\n\nSingapore\n\nUnited Kingdom\n\nLondon\n\nTip\n\nYou can share an experience of geotracking with developers who live outside an area that supports it. Record a session in your app in an area that supports localization imagery for developers to create and test their geotracking app. For more information, see Recording and Replaying AR Session Data .\n\nTopics\nCreating a Configuration\ninit()\nInitializes a new geotracking configuration.\nChecking Availability\nclass func checkAvailability(completionHandler: (Bool, Error?) -> Void)\nDetermines if geotracking supports the user’s current location.\nclass func checkAvailability(at: CLLocationCoordinate2D, completionHandler: (Bool, Error?) -> Void)\nDetermines if geotracking supports a particular location.\nTracking Surfaces\nvar planeDetection: ARWorldTrackingConfiguration.PlaneDetection\nA value that specifies whether and how the session automatically attempts to detect flat surfaces in the camera-captured image.\nstruct ARWorldTrackingConfiguration.PlaneDetection\nOptions for whether and how the framework detects flat surfaces in captured images.\nDetecting or Tracking Images\nvar detectionImages: Set<ARReferenceImage>!\nA set of images that ARKit searches for in the user's environment.\nvar maximumNumberOfTrackedImages: Int\nThe number of image anchors to monitor closely for position and orientation updates.\nvar automaticImageScaleEstimationEnabled: Bool\nA flag that instructs the framework to estimate and set the scale of a detected or tracked image on your behalf.\nDetecting Real-World Objects\nvar detectionObjects: Set<ARReferenceObject>\nA set of 3D objects that the framework attempts to detect in the user's environment.\nCreating Realistic Reflections\nvar environmentTexturing: ARWorldTrackingConfiguration.EnvironmentTexturing\nAn option that determines how the framework generates environment textures.\nenum ARWorldTrackingConfiguration.EnvironmentTexturing\nOptions to generate environment textures in a world-tracking AR session.\nclass AREnvironmentProbeAnchor\nAn object that provides environmental lighting information for a specific area of space in a world-tracking AR session.\nvar wantsHDREnvironmentTextures: Bool\nA flag that instructs the framework to create environment textures in HDR format.\nAccessing App Clip Codes\nInteracting with App Clip Codes in AR\nDisplay content and provide services in an AR experience with App Clip Codes.\nclass var supportsAppClipCodeTracking: Bool\nA flag that indicates if the device tracks App Clip Codes.\nvar appClipCodeTrackingEnabled: Bool\nA Boolean value that indicates if the framework searches the physical environment for App Clip Codes.\nclass ARAppClipCodeAnchor\nAn anchor that tracks the position and orientation of an App Clip Code in the physical environment.\nRelationships\nInherits From\nARConfiguration\nSee Also\nSpatial Tracking\nUnderstanding World Tracking\nDiscover features and best practices for building rear-camera AR experiences.\nclass ARWorldTrackingConfiguration\nA configuration that tracks the position of a device in relation to objects in the environment.\nclass AROrientationTrackingConfiguration\nA configuration that tracks only the device’s orientation using the rear-facing camera.\nclass ARPositionalTrackingConfiguration\nA configuration that tracks only the device’s position in 3D space."
  },
  {
    "title": "maximumNumberOfTrackedFaces | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration/3192187-maximumnumberoftrackedfaces",
    "html": "Discussion\n\nThe default value is one. Set the maximum number of tracked faces to limit the number of faces that can be tracked in a given frame. Check the value of supportedNumberOfTrackedFaces before setting this property.\n\nNo new anchors will be provided to your delegate's session(_:didAdd:) if more than the maximum number of faces are visible in the camera feed. In that case, ARKit continues to track the faces that already have associated face anchors.\n\nSee Also\nTracking Multiple Faces\nclass var supportedNumberOfTrackedFaces: Int\nThe maximum number of faces that the framework can track."
  },
  {
    "title": "Understanding World Tracking | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/configuration_objects/understanding_world_tracking",
    "html": "Overview\n\nThe basic requirement for any AR experience—and the defining feature of ARKit—is the ability to create and track a correspondence between the real-world space the user inhabits and a virtual space where you can model visual content. When your app displays that content together with a live camera image, the user experiences augmented reality: the illusion that your virtual content is part of the real world.\n\nIn all AR experiences, ARKit uses world and camera coordinate systems following a right-handed convention: the y-axis points upward, and (when relevant) the z-axis points toward the viewer and the x-axis points toward the viewer's right.\n\nSession configurations can change the origin and orientation of the coordinate system with respect to the real world (see worldAlignment). Each anchor in an AR session defines its own local coordinate system, also following the right-handed, z-towards-viewer convention; for example, the ARFaceAnchor class defines a system for locating facial features.\n\nCombine Motion Sensing and Scene Analysis\n\nTo create a correspondence between real and virtual spaces, ARKit uses a technique called visual-inertial odometry. This process combines information from the iOS device’s motion sensing hardware with computer vision analysis of the scene visible to the device’s camera. ARKit recognizes notable features in the scene image, tracks differences in the positions of those features across video frames, and compares that information with motion sensing data. The result is a high-precision model of the device’s position and motion.\n\nWorld tracking also analyzes and understands the contents of a scene. Use ray-casting methods (see Raycasting and Hit-Testing) to find real-world surfaces corresponding to a point in the camera image. If you enable the planeDetection setting in your session configuration, ARKit detects flat surfaces in the camera image and reports their position and sizes. You can use ray-cast results or detected planes to place or interact with virtual content in your scene.\n\nCraft an Exceptional AR Experience\n\nAlthough world tracking can produce realistic AR experiences with accuracy and precision, it relies on details of the device’s physical environment that are not always consistent and can be difficult to measure in real time without a degree of error. To build high-quality AR experiences, be aware of these caveats and tips.\n\nDesign AR experiences for predictable lighting conditions. World tracking involves image analysis, which requires a clear image. Tracking quality is reduced when the camera can’t see details, such as when the camera is pointed at a blank wall or the scene is too dark.\n\nUse tracking quality information to provide user feedback. World tracking correlates image analysis with device motion. ARKit develops a better understanding of the scene if the device is moving, even if the device moves only subtly. Excessive motion—too far, too fast, or shaking too vigorously—results in a blurred image or too much distance for tracking features between video frames, reducing tracking quality. The ARCamera class provides tracking state reason information, which you can use to develop UI that tells a user how to resolve low-quality tracking situations.\n\nAllow time for plane detection to produce clear results, and disable plane detection when you have the results you need. Plane detection results vary over time—when a plane is first detected, its position and extent may be inaccurate. As the plane remains in the scene over time, ARKit refines its estimate of position and extent. When a large flat surface is in the scene, ARKit may continue changing the plane anchor’s position, extent, and transform after you’ve already used the plane to place content.\n\nSee Also\nSpatial Tracking\nclass ARWorldTrackingConfiguration\nA configuration that tracks the position of a device in relation to objects in the environment.\nclass ARGeoTrackingConfiguration\nA configuration that tracks locations with GPS, map data, and a device's compass.\nclass AROrientationTrackingConfiguration\nA configuration that tracks only the device’s orientation using the rear-facing camera.\nclass ARPositionalTrackingConfiguration\nA configuration that tracks only the device’s position in 3D space.\nRelated Documentation\nTracking and Visualizing Planes\nDetect surfaces in the physical environment and visualize their shape and location in 3D space."
  },
  {
    "title": "ARKitSession.Events | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/events",
    "html": "Topics\nType Aliases\ntypealias ARKitSession.Events.AsyncIterator\ntypealias ARKitSession.Events.Element\nInstance Methods\nfunc allSatisfy((ARKitSession.Event) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((ARKitSession.Event) -> ElementOfResult?) -> AsyncCompactMapSequence<ARKitSession.Events, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((ARKitSession.Event) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<ARKitSession.Events, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (ARKitSession.Event) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (ARKitSession.Event) -> Bool) -> AsyncDropWhileSequence<ARKitSession.Events>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<ARKitSession.Events>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((ARKitSession.Event) -> Bool) -> AsyncFilterSequence<ARKitSession.Events>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (ARKitSession.Event) -> Bool) -> ARKitSession.Event?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((ARKitSession.Event) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<ARKitSession.Events, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((ARKitSession.Event) -> SegmentOfResult) -> AsyncFlatMapSequence<ARKitSession.Events, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> ARKitSession.Events.Iterator\nfunc map<Transformed>((ARKitSession.Event) -> Transformed) -> AsyncMapSequence<ARKitSession.Events, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((ARKitSession.Event) -> Transformed) -> AsyncThrowingMapSequence<ARKitSession.Events, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (ARKitSession.Event, ARKitSession.Event) -> Bool) -> ARKitSession.Event?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (ARKitSession.Event, ARKitSession.Event) -> Bool) -> ARKitSession.Event?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<ARKitSession.Events>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (ARKitSession.Event) -> Bool) -> AsyncPrefixWhileSequence<ARKitSession.Events>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, ARKitSession.Event) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, ARKitSession.Event) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\nStructures\nstruct ARKitSession.Events.Iterator\nRelationships\nConforms To\nAsyncSequence\nSee Also\nObserving a session\nvar events: ARKitSession.Events\nAn asynchronous sequence of events that provide updates to the current authorization status of the session.\nenum ARKitSession.Event\nThe kinds of events that can occur in a session.\nvar description: String\nA string that describes this session."
  },
  {
    "title": "AROrientationTrackingConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arorientationtrackingconfiguration",
    "html": "Overview\n\nAll AR configurations establish a correspondence between the real world the device inhabits and a virtual 3D coordinate space where you can model content. When your app displays that content together with a live camera image, the user experiences the illusion that your virtual content is part of the real world.\n\nCreating and maintaining this correspondence between spaces requires tracking the device's motion. The AROrientationTrackingConfiguration class tracks the device's movement with three degrees of freedom (3DOF): specifically, the three rotation axes (roll, pitch, and yaw).\n\nThis basic level of motion tracking can create limited AR experiences: A virtual object can appear to be part of the real world, even as the user rotates the device to look above, below, or beside that object. However, this configuration cannot track movement of the device: non-trivially changing the device's position breaks the AR illusion, causing virtual content to appear to drift relative to the real world. For example, the user cannot walk around to see the sides and back of a virtual object. Additionally, 3DOF tracking does not support plane detection or hit testing.\n\nImportant\n\nBecause 3DOF tracking creates limited AR experiences, you should generally not use the AROrientationTrackingConfiguration class directly. Instead, use ARWorldTrackingConfiguration for six degrees of freedom (6DOF) plane detection and hit testing. Use 3DOF tracking only as a fallback in situations where 6DOF tracking is temporarily unavailable.\n\nFigure 1 3DOF tracking maintains an AR illusion when the the device pivots, but not when the device's position moves\n\nTopics\nCreating a Configuration\ninit()\nInitializes a new orientation tracking configuration.\nManaging Device Camera Behavior\nvar isAutoFocusEnabled: Bool\nA Boolean value that determines whether the device camera uses fixed focus or autofocus behavior.\nRelationships\nInherits From\nARConfiguration\nSee Also\nSpatial Tracking\nUnderstanding World Tracking\nDiscover features and best practices for building rear-camera AR experiences.\nclass ARWorldTrackingConfiguration\nA configuration that tracks the position of a device in relation to objects in the environment.\nclass ARGeoTrackingConfiguration\nA configuration that tracks locations with GPS, map data, and a device's compass.\nclass ARPositionalTrackingConfiguration\nA configuration that tracks only the device’s position in 3D space."
  },
  {
    "title": "ARPositionalTrackingConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arpositionaltrackingconfiguration",
    "html": "Overview\n\nEnables 6 degrees of freedom tracking of the iOS device by running the camera at lowest possible resolution and frame rate. Use this configuration when you don't need to parse the camera feed, such as for example, virtual reality scenarios.\n\nTopics\nCreating a Configuration\ninit()\nCreates a new positional tracking configuration.\nvar initialWorldMap: ARWorldMap?\nThe state from a previous AR session to attempt to resume with this session configuration.\nDetecting Real-World Surfaces\nvar planeDetection: ARWorldTrackingConfiguration.PlaneDetection\nA value that specifies if and how the session automatically attempts to detect flat surfaces in the camera-captured image.\nRelationships\nInherits From\nARConfiguration\nSee Also\nSpatial Tracking\nUnderstanding World Tracking\nDiscover features and best practices for building rear-camera AR experiences.\nclass ARWorldTrackingConfiguration\nA configuration that tracks the position of a device in relation to objects in the environment.\nclass ARGeoTrackingConfiguration\nA configuration that tracks locations with GPS, map data, and a device's compass.\nclass AROrientationTrackingConfiguration\nA configuration that tracks only the device’s orientation using the rear-facing camera."
  },
  {
    "title": "ARKitSession.AuthorizationStatus | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationstatus",
    "html": "Topics\nGetting authorization states\ncase notDetermined\nThe user hasn’t yet granted or denied permission.\ncase allowed\nThe user granted your app permission to use the associated kind of ARKit data.\ncase denied\nThe user denied your app permission to use the associated kind of ARKit data.\nComparing authorization states\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nvar description: String\nstatic func == (ARKitSession.AuthorizationStatus, ARKitSession.AuthorizationStatus) -> Bool\nstatic func != (ARKitSession.AuthorizationStatus, ARKitSession.AuthorizationStatus) -> Bool\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nGetting authorization\nfunc requestAuthorization(for: [ARKitSession.AuthorizationType]) -> [ARKitSession.AuthorizationType : ARKitSession.AuthorizationStatus]\nRequests authorization from the user to use the specified kinds of ARKit data.\nenum ARKitSession.AuthorizationType\nThe authorization types you can request from ARKit.\nfunc queryAuthorization(for: [ARKitSession.AuthorizationType]) -> [ARKitSession.AuthorizationType : ARKitSession.AuthorizationStatus]\nChecks whether the current session is authorized for particular authorization types without requesting authorization."
  },
  {
    "title": "ARKitSession.AuthorizationType.worldSensing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationtype/worldsensing",
    "html": "See Also\nRequesting authorization\ncase handTracking\nThe authorization for access to detailed hand-tracking data."
  },
  {
    "title": "ARKitSession.Event | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/event",
    "html": "Topics\nObserving session events\ncase authorizationChanged(type: ARKitSession.AuthorizationType, status: ARKitSession.AuthorizationStatus)\nAn event that represents a change in authorization status for a specific authorization type.\ncase dataProviderStateChanged(dataProviders: [DataProvider], newState: DataProviderState, error: ARKitSession.Error?)\nAn event that represents a change in state of one of the data providers associated with a session.\nvar description: String\nA textual description of the authorization status.\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nObserving a session\nvar events: ARKitSession.Events\nAn asynchronous sequence of events that provide updates to the current authorization status of the session.\nstruct ARKitSession.Events\nAn asynchronous sequence of session events.\nvar description: String\nA string that describes this session."
  },
  {
    "title": "description | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/4139356-description",
    "html": "Relationships\nFrom Protocol\nCustomStringConvertible\nSee Also\nObserving a session\nvar events: ARKitSession.Events\nAn asynchronous sequence of events that provide updates to the current authorization status of the session.\nstruct ARKitSession.Events\nAn asynchronous sequence of session events.\nenum ARKitSession.Event\nThe kinds of events that can occur in a session."
  },
  {
    "title": "events | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/4218761-events",
    "html": "Discussion\n\nThe following example detects changes in the current session’s authorization status.\n\nlet session = ARKitSession()\nTask {\n    for await update in session.events {\n        if case .authorizationChanged(let type, let status) = update {\n            print(\"Authorization. Status of \\(type) changed to \\(status).\")\n        } else {\n            print(\"Another session event \\(update).\")\n        }\n    }\n}\n\n\nSee Also\nObserving a session\nstruct ARKitSession.Events\nAn asynchronous sequence of session events.\nenum ARKitSession.Event\nThe kinds of events that can occur in a session.\nvar description: String\nA string that describes this session."
  },
  {
    "title": "ARKitSession.AuthorizationType | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/authorizationtype",
    "html": "Topics\nRequesting authorization\ncase handTracking\nThe authorization for access to detailed hand-tracking data.\ncase worldSensing\nThe authorization for access to plane detection, scene reconstruction, and image tracking.\nComparing authorizations\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (ARKitSession.AuthorizationType, ARKitSession.AuthorizationType) -> Bool\nstatic func != (ARKitSession.AuthorizationType, ARKitSession.AuthorizationType) -> Bool\nvar description: String\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nGetting authorization\nfunc requestAuthorization(for: [ARKitSession.AuthorizationType]) -> [ARKitSession.AuthorizationType : ARKitSession.AuthorizationStatus]\nRequests authorization from the user to use the specified kinds of ARKit data.\nfunc queryAuthorization(for: [ARKitSession.AuthorizationType]) -> [ARKitSession.AuthorizationType : ARKitSession.AuthorizationStatus]\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nenum ARKitSession.AuthorizationStatus\nThe authorization states for a type of ARKit data."
  },
  {
    "title": "ARKitSession.Error | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/error",
    "html": "Topics\nInspecting ARKit errors\nlet dataProvider: (DataProvider)?\nThe data provider that caused an error in a session.\nvar code: ARKitSession.Error.Code\nThe error code for an ARKit session error.\nenum ARKitSession.Error.Code\nThe error codes for ARKit sessions.\nvar description: String\nA textual description of the error that occurred.\nvar localizedDescription: String\nA localized description of the error.\nvar errorDescription: String?\nA localized message that describes the error that occurred.\nProviding recovery suggestions\nvar recoverySuggestion: String?\nA localized message that describes how someone might recover from the error.\nvar failureReason: String?\nA localized message that describes why the error occurred.\nvar helpAnchor: String?\nRelationships\nConforms To\nCustomStringConvertible\nLocalizedError\nSendable\nSee Also\nStarting and stopping a session\ninit()\nCreates a new session.\nfunc run([DataProvider])\nRuns a session with the data providers you supply.\nfunc stop()\nStops all data providers running in this session."
  },
  {
    "title": "ARCamera.TrackingState.normal | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/trackingstate/normal",
    "html": "See Also\nDetermining the camera tracking status\ncase notAvailable\nCamera position tracking is not available.\ncase limited(ARCamera.TrackingState.Reason)\nTracking is available, but the quality of results is questionable.\nenum ARCamera.TrackingState.Reason\nCauses of limited position-tracking quality."
  },
  {
    "title": "transform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcamera/2866108-transform",
    "html": "Discussion\n\nWorld coordinate space in ARKit always follows a right-handed convention, but is oriented based on the session configuration. For details, see Understanding World Tracking.\n\nThis transform creates a local coordinate space for the camera that is constant with respect to device orientation. In camera space, the x-axis points to the right when the device is in UIDeviceOrientation.landscapeRight orientation—that is, the x-axis always points along the long axis of the device, from the front-facing camera toward the Home button. The y-axis points upward (with respect to UIDeviceOrientation.landscapeRight orientation), and the z-axis points away from the device on the screen side.\n\nSee Also\nExamining Camera Geometry\nvar eulerAngles: simd_float3\nThe orientation of the camera, expressed as roll, pitch, and yaw values."
  },
  {
    "title": "queryDeviceAnchor(atTimestamp:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider/4293525-querydeviceanchor",
    "html": "Parameters\ntimestamp\n\nA time — now or in the future — to predict the device pose.\n\nReturn Value\n\nThe predicted position and orientation of the device at the time you specify.\n\nDiscussion\n\nPass the timestamp parameter as absolute time in seconds. For example, to get the device’s current pose, pass CACurrentMediaTime() as the timestamp.\n\nImportant\n\nPredicting future device pose is a computationally expensive operation. You typically only use this method when implementing your own rendering with the Compositor Services framework."
  },
  {
    "title": "isSupported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imagetrackingprovider/4218764-issupported",
    "html": "Relationships\nFrom Protocol\nDataProvider\nSee Also\nCreating an image tracking provider\ninit(referenceImages: [ReferenceImage])\nCreates an image tracking provider that tracks the reference images you supply.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to track images."
  },
  {
    "title": "requiredAuthorizations | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imagetrackingprovider/4131786-requiredauthorizations",
    "html": "Relationships\nFrom Protocol\nDataProvider\nSee Also\nCreating an image tracking provider\ninit(referenceImages: [ReferenceImage])\nCreates an image tracking provider that tracks the reference images you supply.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports image tracking providers."
  },
  {
    "title": "init(anchor:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/aranchorcopying/3020708-init",
    "html": "Required\n\nParameters\nanchor\n\nThe other anchor from which to copy information.\n\nDiscussion\n\nEach time ARKit generates a new ARFrame object (corresponding to an incoming frame of live camera video at 60 fps), ARKit calls this initializer to copy each of the anchors associated with the previous frame.\n\nNote\n\nARKit always calls this initializer with an anchor parameter of the same class as self.\n\nIf you subclass ARAnchor to add extra properties, your implementation of this initializer should copy the values of those properties, then chain to the superclass initializer. For example, an AR game might define a BoardAnchor class to encode the position and size of a game board, so its version of this initializer would copy that size property:\n\nrequired init(anchor: ARAnchor) {\n    let other = anchor as! BoardAnchor\n    self.size = other.size\n    super.init(anchor: other)\n}\n\n\nImportant\n\nCarefully consider performance and your app's data model when storing references to other objects in anchors. Copying custom values might be expensive, but multiple references to the same data might or might not be correct for your app"
  },
  {
    "title": "ARPlaneAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arplaneanchor",
    "html": "Overview\n\nWhen you enable planeDetection in a world tracking session, ARKit notifies your app of all the surfaces it observes using the device's back camera. ARKit calls your delegate's session(_:didAdd:) with an ARPlaneAnchor for each unique surface. Each plane anchor provides details about the surface, like its real-world position and shape.\n\nThe width and length of a plane (the planeExtent) span the xz-plane of an ARPlaneAnchor instance's local coordinate system. The y-axis of the plane anchor is the plane’s normal vector.\n\nTopics\nOrientation\nvar alignment: ARPlaneAnchor.Alignment\nThe general orientation of the detected plane with respect to gravity.\nenum ARPlaneAnchor.Alignment\nValues describing possible general orientations of a detected plane with respect to gravity.\nGeometry\nvar geometry: ARPlaneGeometry\nA coarse triangle mesh representing the general shape of the detected plane.\nclass ARPlaneGeometry\nA 3D mesh describing the shape of a detected plane in world-tracking AR sessions.\nclass ARSCNPlaneGeometry\nA SceneKit representation of the 2D shape of a plane, for use with plane detection results in an AR session.\nDimensions\nvar center: simd_float3\nThe center point of the plane relative to its anchor position.\nvar planeExtent: ARPlaneExtent\nThe estimated width, length, and y-axis rotation of the detected plane.\nclass ARPlaneExtent\nThe size and y-axis rotation of a detected plane.\nvar extent: simd_float3\nThe estimated width and length of the detected plane.\nDeprecated\nClassifying a Plane\nclass var isClassificationSupported: Bool\nA Boolean value that indicates whether plane classification is available on the current device.\nvar classification: ARPlaneAnchor.Classification\nA general characterization of what kind of real-world surface the plane anchor represents.\nenum ARPlaneAnchor.Classification\nPossible characterizations of real-world surfaces represented by plane anchors.\nRelationships\nInherits From\nARAnchor\nSee Also\nSurface Detection\nTracking and Visualizing Planes\nDetect surfaces in the physical environment and visualize their shape and location in 3D space.\nclass ARMeshAnchor\nAn anchor for a physical object that ARKit detects and recreates virtually using a polygonal mesh."
  },
  {
    "title": "ARImageAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arimageanchor",
    "html": "Overview\n\nWhen you run a world-tracking AR session and specify ARReferenceImage objects for the session configuration's detectionImages property, ARKit searches for those images in the real-world environment. When the session recognizes an image, it automatically adds an ARImageAnchor for each detected image to its list of anchors.\n\nTo find the extent of a recognized image in the scene, use the inherited transform property together with the physicalSize of the anchor's referenceImage.\n\nTopics\nIdentifying Detected Images\nvar referenceImage: ARReferenceImage\nThe detected image referenced by the image anchor.\nEstimating Scale\nvar estimatedScaleFactor: CGFloat\nA factor between the initial size and the estimated physical size.\nRelationships\nInherits From\nARAnchor\nConforms To\nARTrackable\nSee Also\nImage Detection\nTracking and altering images\nCreate images from rectangular shapes found in the user’s environment, and augment their appearance.\nDetecting Images in an AR Experience\nReact to known 2D images in the user’s environment, and use their positions to place AR content.\nclass ARReferenceImage\nA 2D image that you want ARKit to detect in the physical environment."
  },
  {
    "title": "GeometryElement | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometryelement",
    "html": "Topics\nRendering geometry elements\nvar buffer: MTLBuffer\nA Metal buffer that contains index data that defines the geometry of an object.\nvar primitive: GeometryElement.Primitive\nThe kind of primitive, lines or triangles, that a geometry element contains.\nenum GeometryElement.Primitive\nThe kinds of geometry primitives that a geometry element can contain.\nvar count: Int\nThe number of primitives in the Metal buffer for a geometry element.\nvar bytesPerIndex: Int\nThe number of bytes that represent an index value.\nInspecting geometry elements\nvar description: String\nA textual description of a geometry element.\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nGeometry\nstruct GeometrySource\nA container for geometrical vector data."
  },
  {
    "title": "isSupported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handtrackingprovider/4218763-issupported",
    "html": "Relationships\nFrom Protocol\nDataProvider\nSee Also\nCreating a hand tracking provider\ninit()\nCreates a hand tracking provider.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to track hands."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handtrackingprovider/4131742-init",
    "html": "See Also\nCreating a hand tracking provider\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to track hands."
  },
  {
    "title": "AnchorUpdateSequence | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdatesequence",
    "html": "Topics\nPerforming sequence operations\nfunc allSatisfy((AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether all elements produced by the asynchronous sequence satisfy the given predicate.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements, omitting results that don’t return a value.\nfunc compactMap<ElementOfResult>((AnchorUpdate<AnchorType>) -> ElementOfResult?) -> AsyncThrowingCompactMapSequence<AnchorUpdateSequence<AnchorType>, ElementOfResult>\nCreates an asynchronous sequence that maps an error-throwing closure over the base sequence’s elements, omitting results that don’t return a value.\nfunc contains(where: (AnchorUpdate<AnchorType>) -> Bool) -> Bool\nReturns a Boolean value that indicates whether the asynchronous sequence contains an element that satisfies the given predicate.\nfunc drop(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncDropWhileSequence<AnchorUpdateSequence<AnchorType>>\nOmits elements from the base asynchronous sequence until a given closure returns false, after which it passes through all remaining elements.\nfunc dropFirst(Int) -> AsyncDropFirstSequence<AnchorUpdateSequence<AnchorType>>\nOmits a specified number of elements from the base asynchronous sequence, then passes through all remaining elements.\nfunc filter((AnchorUpdate<AnchorType>) -> Bool) -> AsyncFilterSequence<AnchorUpdateSequence<AnchorType>>\nCreates an asynchronous sequence that contains, in order, the elements of the base sequence that satisfy the given predicate.\nfunc first(where: (AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the first element of the sequence that satisfies the given predicate.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncThrowingFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given error-throwing transformation with each element of this sequence.\nfunc flatMap<SegmentOfResult>((AnchorUpdate<AnchorType>) -> SegmentOfResult) -> AsyncFlatMapSequence<AnchorUpdateSequence<AnchorType>, SegmentOfResult>\nCreates an asynchronous sequence that concatenates the results of calling the given transformation with each element of this sequence.\nfunc makeAsyncIterator() -> AnchorUpdateSequence<AnchorType>.Iterator<AnchorType>\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given closure over the asynchronous sequence’s elements.\nfunc map<Transformed>((AnchorUpdate<AnchorType>) -> Transformed) -> AsyncThrowingMapSequence<AnchorUpdateSequence<AnchorType>, Transformed>\nCreates an asynchronous sequence that maps the given error-throwing closure over the asynchronous sequence’s elements.\nfunc max(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the maximum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc min(by: (AnchorUpdate<AnchorType>, AnchorUpdate<AnchorType>) -> Bool) -> AnchorUpdate<AnchorType>?\nReturns the minimum element in the asynchronous sequence, using the given predicate as the comparison between elements.\nfunc prefix(Int) -> AsyncPrefixSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, up to the specified maximum length, containing the initial elements of the base asynchronous sequence.\nfunc prefix(while: (AnchorUpdate<AnchorType>) -> Bool) -> AsyncPrefixWhileSequence<AnchorUpdateSequence<AnchorType>>\nReturns an asynchronous sequence, containing the initial, consecutive elements of the base sequence that satisfy the given predicate.\nfunc reduce<Result>(Result, (Result, AnchorUpdate<AnchorType>) -> Result) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure.\nfunc reduce<Result>(into: Result, (inout Result, AnchorUpdate<AnchorType>) -> Void) -> Result\nReturns the result of combining the elements of the asynchronous sequence using the given closure, given a mutable initial value.\ntypealias AnchorUpdateSequence.AsyncIterator\ntypealias AnchorUpdateSequence.Element\nstruct AnchorUpdateSequence.Iterator\nRelationships\nConforms To\nAsyncSequence\nSee Also\nTracking anchors over time\nstruct AnchorUpdate\nInformation about the event that updated an anchor."
  },
  {
    "title": "ARError.Code | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror/code",
    "html": "Topics\nErrors\ncase requestFailed\nAn error that indicates a request fails.\ncase cameraUnauthorized\nAn error that indicates the app lacks user permission for the camera.\ncase fileIOFailed\nAn error that indicates a file access fails to read or write.\ncase insufficientFeatures\nAn error that indicates the framework requires more features to complete a task.\ncase invalidCollaborationData\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\ncase invalidConfiguration\nAn error that indicates the configuration contains ambiguous or erroneous data.\ncase invalidReferenceImage\nAn error that indicates the framework fails to process a reference image.\ncase invalidReferenceObject\nAn error that indicates the framework fails to process a reference object.\ncase invalidWorldMap\nAn error that indicates the framework fails to process a world map.\ncase microphoneUnauthorized\nAn error that indicates the app lacks user permission for the microphone.\ncase objectMergeFailed\nAn error that indicates the framework fails to merge a detected object.\ncase sensorFailed\nAn error that indicates a sensor fails to provide required input.\ncase sensorUnavailable\nAn error that indicates the framework fails to access a required sensor.\ncase unsupportedConfiguration\nAn error that indicates the device lacks support for the session’s configuration.\ncase worldTrackingFailed\nAn error that indicates when world tracking experiences an unrecoverable problem.\ncase geoTrackingFailed\nAn error that indicates when localization imagery fails to match the device’s camera captures.\ncase geoTrackingNotAvailableAtLocation\nAn error that indicates a location lacks geotracking support.\ncase locationUnauthorized\nAn error that indicates the app lacks user permission for the device’s current location.\ncase highResolutionFrameCaptureFailed\nAn error that indicates a problem in the system's capture pipeline.\ncase highResolutionFrameCaptureInProgress\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request.\nRelationships\nConforms To\nSendable\nSee Also\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request."
  },
  {
    "title": "run(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession/4131679-run",
    "html": "Parameters\nproviders\n\nThe providers that supply data during this session.\n\nDiscussion\n\nIf you call this method without previously calling the requestAuthorization(for:) method, and if any of the data providers you supply require authorization, the system prompts the user for authorization when you call run(_:). If you call this method on an already-running session, ARKit stops the previous providers unless they’re also in the new array of providers.\n\nThis method either throws an ARKitSession.Error or asserts when there’s a problem with the data providers you supply. Potential problems include:\n\nPassing a data provider that’s already in use in another session\n\nPassing a data provider that’s stopped\n\nPassing a data provider that’s not supported in the current context, such as in Simulator\n\nWhen this method throws an error, its session stops all of the associated data providers.\n\nSee Also\nStarting and stopping a session\ninit()\nCreates a new session.\nfunc stop()\nStops all data providers running in this session.\nstruct ARKitSession.Error\nAn error that might occur when running data providers on an ARKit session."
  },
  {
    "title": "USDZ schemas for AR | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/usdz_schemas_for_ar",
    "html": "Overview\n\nLeveraging Pixar’s Universal Scene Description standard, USDZ delivers AR and 3D content to Apple devices. Apple developed a set of new schemas in collaboration with Pixar to further extend the format for AR use cases. Simply add data to a USDZ file to give your 3D assets AR abilities, such as the ability to:\n\nAnchor 3D content at a specific location in the real world.\n\nReact to real-world situations.\n\nParticipate in a physics simulation.\n\nConnect audio effects to a location.\n\nAnnotate the environment by displaying text.\n\nA USDZ file uses these schemas to add features to an augmented reality experience in AR Quick Look or RealityKit in place of .reality files, .rcproject files, or custom code to implement AR functionality. Reality Composer describes AR features in its USDZ export using these schemas, too (see Exporting a Reality Composer Scene to USDZ). To enable AR features in assets from a third-party digital content-creation (DCC) tool such as Maya or Houdini, edit the file in .usda textual format using the USD Toolset.\n\nNote\n\nThese new schemas (see Schema definitions for third-party DCCs) are included in the Universal Scene Description specification as an addendum and are marked as preliminary. The addendum also adds metadata (name-value pairs; see Metadata), and new data properties (Property). To provide feedback on the addendum, use the Feedback Assistant.\n\nImplement AR functionality\n\nThe following illustration depicts a virtual castle rendered by a runtime, the app or system framework that implements the AR functionality described in the schemas. The prim for the virtual castle (USD refers to individual units of 3D content as prims; see UsdPrim) instructs the runtime to place the castle on a known image in the physical environment, called the image anchor. When the user comes into proximity with the anchor, the runtime displays the 3D visualization of the castle. Falling snowflakes represent additional prims that behave as if in accordance with gravity, and disappear as they approach a real-world surface.\n\nTopics\nAnimation\nControl animation playback with metadata.\nautoPlay\nMetadata that specifies whether an animation plays automatically on load.\nplaybackMode\nMetadata that controls animation idling until a behavior takes over.\nAnchoring\nPlace a prim at the physical location of a real-world object.\nPlacing a prim in the real world\nAnchor a prim to a real-world object that the runtime recognizes in the physical environment.\nPreliminary_AnchoringAPI\nA schema that defines the placement of a prim and its children at a real-world location.\nPreliminary_ReferenceImage\nA schema that defines the properties of an image in the physical environment.\nBehaviors\nActions and triggers\nEnable visual and audible responses to programmatic or environmental events.\nText\nPreliminary_Text\nA prim that renders 3D text in a scene.\nScenes and lighting\nSpecifying a lighting environment in AR Quick Look\nAdd metadata to your USDZ file to specify its lighting characteristics.\npreferredIblVersion\nMetadata that determines the lighting environment of virtual content.\nsceneLibrary\nMetadata that partitions an asset into scene-based units.\nDigital content creation\nSchema definitions for third-party DCCs\nUpdate your local USD library to add interactive and augmented reality features.\nSee Also\nEssentials\nVerifying Device Support and User Permission\nCheck whether your app can use ARKit and respect user privacy at runtime."
  },
  {
    "title": "ARError | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arerror",
    "html": "Topics\nCreating an Error\ninit(Code, userInfo: [String : Any])\nInitializes an error.\nComparing errors\nstatic func == (ARError, ARError) -> Bool\nDetermines whether two errors are equal.\nstatic func != (ARError, ARError) -> Bool\nReturns a Boolean value indicating whether two values aren't equal.\nfunc hash(into: inout Hasher)\nHashes the essential components of a value by feeding them into the given hasher.\nvar hashValue: Int\nThe hash value.\nInspecting error information\nvar errorUserInfo: [String : Any]\nThe error user info dictionary.\nvar userInfo: [String : Any]\nThe user info dictionary.\nvar localizedDescription: String\nA string that contains a description of the error.\nstatic var errorDomain: String\nA string that indicates the error domain in Core Foundation.\nIdentifying an error cause\nvar errorCode: Int\nAn integer value for an error.\nvar code: Code\nA code that indicates the cause of the error.\nenum ARError.Code\nCodes that identify errors in ARKit.\nstatic var requestFailed: ARError.Code\nAn error that indicates a request fails.\nstatic var cameraUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the camera.\nstatic var fileIOFailed: ARError.Code\nAn error that indicates a file access fails to read or write.\nstatic var insufficientFeatures: ARError.Code\nAn error that indicates the framework requires more features to complete a task.\nstatic var invalidCollaborationData: ARError.Code\nAn error that indicates the framework fails to parse collaboration data the app receives over the network.\nstatic var invalidConfiguration: ARError.Code\nAn error that indicates the configuration contains ambiguous or erroneous data.\nstatic var invalidReferenceImage: ARError.Code\nAn error that indicates the framework fails to process a reference image.\nstatic var invalidReferenceObject: ARError.Code\nAn error that indicates the framework fails to process a reference object.\nstatic var invalidWorldMap: ARError.Code\nAn error that indicates the framework fails to process a world map.\nstatic var microphoneUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the microphone.\nstatic var objectMergeFailed: ARError.Code\nAn error that indicates the framework fails to merge a detected object.\nstatic var sensorFailed: ARError.Code\nAn error that indicates a sensor fails to provide required input.\nstatic var sensorUnavailable: ARError.Code\nAn error that indicates the framework fails to access a required sensor.\nstatic var unsupportedConfiguration: ARError.Code\nAn error that indicates the device lacks support for the session’s configuration.\nstatic var worldTrackingFailed: ARError.Code\nAn error that indicates when world tracking experiences an unrecoverable problem.\nstatic var geoTrackingFailed: ARError.Code\nAn error that indicates when localization imagery fails to match the device’s camera captures.\nstatic var geoTrackingNotAvailableAtLocation: ARError.Code\nAn error that indicates a location lacks geotracking support.\nstatic var locationUnauthorized: ARError.Code\nAn error that indicates the app lacks user permission for the device's current location.\nstatic var highResolutionFrameCaptureFailed: ARError.Code\nAn error that indicates a problem in the system's capture pipeline.\nstatic var highResolutionFrameCaptureInProgress: ARError.Code\nAn error that indicates the system needs to finish a high-resolution frame request before accepting another request.\nSee Also\nErrors\nenum ARError.Code\nCodes that identify errors in ARKit."
  },
  {
    "title": "Creating a Multiuser AR Experience | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/creating_a_multiuser_ar_experience",
    "html": "Overview\n\nThis sample app demonstrates a simple shared AR experience for two or more iOS 12 devices. Before exploring the code, try building and running the app to familiarize yourself with the user experience it demonstrates:\n\nRun the app on one device. You can look around the local environment, and tap to place a virtual 3D character on real-world surfaces. (Tap again to place multiple copies of the character.)\n\nRun the app on a second device. On both device screens, a message indicates that they have automatically joined a shared session.\n\nTap the Send World Map button on one device. Make sure the other device is in an area that the first device visited before sending the map, or has a similar view of the surrounding environment.\n\nThe other device displays a message indicating that it has received the map and is attempting to use it. When that process succeeds, both devices show virtual content at the same real-world positions, and tapping on either device places virtual content visible to both.\n\nFollow the steps below to see how this app uses the ARWorldMap class to save and restore ARKit’s spatial mapping state, and the Multipeer Connectivity framework to send world maps between nearby devices.\n\nGetting Started\n\nRequires Xcode 10.0, iOS 12.0 and two or more iOS devices with A9 or later processors.\n\nRun the AR Session and Place AR Content\n\nThis app extends the basic workflow for building an ARKit app. (For details, see Tracking and Visualizing Planes.) It defines an ARWorldTrackingConfiguration with plane detection enabled, then runs that configuration in the ARSession attached to the ARSCNView that displays the AR experience.\n\nWhen UITapGestureRecognizer detects a tap on the screen, the handleSceneTap method uses ARKit hit-testing to find a 3D point on a real-world surface, then places an ARAnchor marking that position. When ARKit calls the delegate method renderer(_:didAdd:for:), the app loads a 3D model for ARSCNView to display at the anchor’s position.\n\nConnect to Peer Devices\n\nThe sample MultipeerSession class provides a simple abstraction around the Multipeer Connectivity features this app uses. After the main view controller creates a MultipeerSession instance (at app launch), it starts running an MCNearbyServiceAdvertiser to broadcast the device’s ability to join multipeer sessions and an MCNearbyServiceBrowser to find other devices:\n\nsession = MCSession(peer: myPeerID, securityIdentity: nil, encryptionPreference: .required)\nsession.delegate = self\n\n\nserviceAdvertiser = MCNearbyServiceAdvertiser(peer: myPeerID, discoveryInfo: nil, serviceType: MultipeerSession.serviceType)\nserviceAdvertiser.delegate = self\nserviceAdvertiser.startAdvertisingPeer()\n\n\nserviceBrowser = MCNearbyServiceBrowser(peer: myPeerID, serviceType: MultipeerSession.serviceType)\nserviceBrowser.delegate = self\nserviceBrowser.startBrowsingForPeers()\n\n\nWhen the MCNearbyServiceBrowser finds another device, it calls the browser(_:foundPeer:withDiscoveryInfo:) delegate method. To invite that other device to a shared session, call the browser’s invitePeer(_:to:withContext:timeout:) method:\n\npublic func browser(_ browser: MCNearbyServiceBrowser, foundPeer peerID: MCPeerID, withDiscoveryInfo info: [String: String]?) {\n    // Invite the new peer to the session.\n    browser.invitePeer(peerID, to: session, withContext: nil, timeout: 10)\n}\n\n\nWhen the other device receives that invitation, MCNearbyServiceAdvertiser calls the advertiser(_:didReceiveInvitationFromPeer:withContext:invitationHandler:) delegate method. To accept the invitation, call the provided invitationHandler:\n\nfunc advertiser(_ advertiser: MCNearbyServiceAdvertiser, didReceiveInvitationFromPeer peerID: MCPeerID, withContext context: Data?, invitationHandler: @escaping (Bool, MCSession?) -> Void) {\n    // Call handler to accept invitation and join the session.\n    invitationHandler(true, self.session)\n}\n\n\nImportant\n\nThis app automatically joins the first nearby session it finds. Depending on the kind of shared AR experience you want to create, you may want to more precisely control broadcasting, invitation, and acceptance behavior. See the Multipeer Connectivity documentation for details.\n\nIn a multipeer session, all participants are by definition equal peers; there is no explicit separation of devices into host and guest roles. However, you may wish to define such roles for your own AR experience. For example, a multiplayer game design might require a host role to arbitrate gameplay. If you need to separate peers by role, you can choose a way to do so that fits the design of your app. For example:\n\nHave the user choose whether to act as a host or guest before starting a session. The host uses MCNearbyServiceAdvertiser to broadcast availability, and guests use MCNearbyServiceBrowser to find a host to join.\n\nJoin a session as peers, then negotiate between peers to nominate a host. (This approach can be helpful for designs that need a host role but also allow peers to join or leave at any time.)\n\nCapture and Send the AR World Map\n\nAn ARWorldMap object contains a snapshot of all the spatial mapping information that ARKit uses to locate the user’s device in real-world space. Reliably sharing a map to another device requires two key steps: finding a good time to capture a map, and capturing and sending it.\n\nARKit provides a worldMappingStatus value that indicates whether it’s currently a good time to capture a world map (or if it’s better to wait until ARKit has mapped more of the local environment). This app uses that value to provide visual feedback on its Send World Map button:\n\nswitch frame.worldMappingStatus {\ncase .notAvailable, .limited:\n    sendMapButton.isEnabled = false\ncase .extending:\n    sendMapButton.isEnabled = !multipeerSession.connectedPeers.isEmpty\ncase .mapped:\n    sendMapButton.isEnabled = !multipeerSession.connectedPeers.isEmpty\n@unknown default:\n    sendMapButton.isEnabled = false\n}\nmappingStatusLabel.text = frame.worldMappingStatus.description\n\n\nWhen the user taps the Send World Map button, the app calls getCurrentWorldMap(completionHandler:) to capture the map from the running ARSession, then serializes it to a Data object with NSKeyedArchiver and sends it to other devices in the multipeer session:\n\nsceneView.session.getCurrentWorldMap { worldMap, error in\n    guard let map = worldMap\n        else { print(\"Error: \\(error!.localizedDescription)\"); return }\n    guard let data = try? NSKeyedArchiver.archivedData(withRootObject: map, requiringSecureCoding: true)\n        else { fatalError(\"can't encode map\") }\n    self.multipeerSession.sendToAllPeers(data)\n}\n\n\nReceive and Relocalize to the Shared Map\n\nWhen a device receives data sent by another participant in the multipeer session, the session(_:didReceive:fromPeer:) delegate method provides that data. To make use of it, the app uses NSKeyedUnarchiver to deserialize an ARWorldMap object, then creates and runs a new ARWorldTrackingConfiguration using that map as the initialWorldMap:\n\nif let worldMap = try NSKeyedUnarchiver.unarchivedObject(ofClass: ARWorldMap.self, from: data) {\n    // Run the session with the received world map.\n    let configuration = ARWorldTrackingConfiguration()\n    configuration.planeDetection = .horizontal\n    configuration.initialWorldMap = worldMap\n    sceneView.session.run(configuration, options: [.resetTracking, .removeExistingAnchors])\n    \n    // Remember who provided the map for showing UI feedback.\n    mapProvider = peer\n}\n\n\nARKit then attempts to relocalize to the new world map—that is, to reconcile the received spatial-mapping information with what it senses of the local environment. For best results:\n\nThoroughly scan the local environment on the sending device before sharing a world map.\n\nPlace the receiving device next to the sending device, so that both see the same view of the environment.\n\nShare AR Content and User Actions\n\nSharing the world map also shares all existing anchors. In this app, this means that as soon as a receiving device relocalizes to the world map, it shows all the 3D characters that were placed by the sending device before it captured and sent a world map. However, recording and transmitting a world map and relocalizing to a world map are time-consuming, bandwidth-intensive operations, so you should take those steps only once, when a new device joins a session.\n\nTo create an ongoing shared AR experience, where each user’s actions affect the AR scene visible to other users, after each device relocalizes to the same world map you should share only the information needed to recreate each user action. For example, in this app the user can tap to place a virtual 3D character in the scene. That character is static, so all that is needed to place the character on another participating device is the character’s position and orientation in world space.\n\nThis app communicates virtual character positions by sharing ARAnchor objects between peers. When one user taps in the scene, the app creates an anchor and adds it to the local ARSession, then serializes that ARAnchor using Data and sends it to other devices in the multipeer session:\n\n// Place an anchor for a virtual character. The model appears in renderer(_:didAdd:for:).\nlet anchor = ARAnchor(name: \"panda\", transform: hitTestResult.worldTransform)\nsceneView.session.add(anchor: anchor)\n\n\n// Send the anchor info to peers, so they can place the same content.\nguard let data = try? NSKeyedArchiver.archivedData(withRootObject: anchor, requiringSecureCoding: true)\n    else { fatalError(\"can't encode anchor\") }\nself.multipeerSession.sendToAllPeers(data)\n\n\nWhen other peers receive data from the multipeer session, they test for whether that data contains an archived ARAnchor; if so, they decode it and add it to their session:\n\nif let anchor = try NSKeyedUnarchiver.unarchivedObject(ofClass: ARAnchor.self, from: data) {\n    // Add anchor to the session, ARSCNView delegate adds visible content.\n    sceneView.session.add(anchor: anchor)\n}\n\n\nThis is just one strategy for adding dynamic features to a shared AR experience—many other strategies are possible. Choose one that fits the user interaction, rendering, and networking requirements of your app. For example, a game where users throw projectiles in the AR world space might define custom data types with attributes like initial position and velocity, then use Swift’s Codable protocols to serialize that information to a binary representation for sending over the network.\n\nSee Also\nShared Experiences\nStreaming an AR Experience\nControl an AR experience remotely by transferring sensor and user input over the network.\nCreating a Collaborative Session\nEnable nearby devices to share an AR experience by using a peer-to-peer multiuser strategy.\nSwiftShot: Creating a Game for Augmented Reality\nSee how Apple built the featured demo for WWDC18, and get tips for making your own multiplayer games using ARKit, SceneKit, and Swift.\nclass ARParticipantAnchor\nAn anchor for another user in multiuser augmented reality experiences.\nclass ARSession.CollaborationData\nAn object that holds information that a user has collected about the physical environment."
  },
  {
    "title": "Creating a Collaborative Session | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/creating_a_collaborative_session",
    "html": "Overview\n\nAs an AR app runs, ARKit gathers information about a userʼs physical environment by processing the camera feed from the user’s device. To effect a multiuser AR experience in which users learn more about the environment by sharing the information from their device’s camera feed with other users, you enable collaboration.\n\nThroughout a collaborative session, ARKit periodically provides data for you to share with peer users, and you choose a network protocol to send that data. In addition to information about the layout of the physical environment––the world data––collaboration data includes an anchor for each participant. These anchors indicate each peer’s approximate location, which you can use, for example, to place virtual content that represents the peer user. ARKit also provides you with any anchors that the peer users create themselves.\n\nThis sample app allows multiple users to view a common horizontal surface and place blocks on top of the surface, with a unique color for each user.\n\nAlthough this sample draws its graphics using RealityKit, it doesn’t use RealityKit’s mechanism for over-the-network entity synchronization. Instead, it uses RealityKit as a renderer only as necessary to demonstrate ARKit’s collaborative session.\n\nTo create a shared AR experience using a host-guest approach, see Creating a Multiuser AR Experience.\n\nEnable Collaboration\n\nCollaborative sessions are available when your session uses ARWorldTrackingConfiguration. To enable collaboration, set isCollaborationEnabled to true.\n\nconfiguration = ARWorldTrackingConfiguration()\n\n\n// Enable a collaborative session.\nconfiguration?.isCollaborationEnabled = true\n\n\n// Enable realistic reflections.\nconfiguration?.environmentTexturing = .automatic\n\n\n// Begin the session.\narView.session.run(configuration!)\n\nGather Collaboration Data\n\nWhen collaboration is enabled, ARKit periodically invokes session(_:didOutputCollaborationData:), which provides collaboration data that you can share with nearby users. You are responsible for sending collaboration data over the network, including choosing the network framework and implementing the code. The data you send is a serialized version of the ARSession.CollaborationData object provided by your session. Before you send collaboration data over the network, first serialize it using NSKeyedArchiver.\n\nfunc session(_ session: ARSession, didOutputCollaborationData data: ARSession.CollaborationData) {\n    guard let multipeerSession = multipeerSession else { return }\n    if !multipeerSession.connectedPeers.isEmpty {\n        guard let encodedData = try? NSKeyedArchiver.archivedData(withRootObject: data, requiringSecureCoding: true)\n        else { fatalError(\"Unexpectedly failed to encode collaboration data.\") }\n        // Use reliable mode if the data is critical, and unreliable mode if the data is optional.\n        let dataIsCritical = data.priority == .critical\n        multipeerSession.sendToAllPeers(encodedData, reliably: dataIsCritical)\n    } else {\n        print(\"Deferred sending collaboration to later because there are no peers.\")\n    }\n}\n\n\nIt’s safe to ignore the collaboration data if no peers have joined the session. In that case, ARKit outputs the collaboration data later to try again. The alternative approach of enabling collaboration only after peers have joined is not supported, because doing so restarts the session.\n\nSend Collaboration Data to Others\n\nYou choose the network protocol with which to share collaboration data. This sample app sends collaboration data using Multipeer Connectivity.\n\nfunc sendToPeers(_ data: Data, reliably: Bool, peers: [MCPeerID]) {\n    guard !peers.isEmpty else { return }\n    do {\n        try session.send(data, toPeers: peers, with: reliably ? .reliable : .unreliable)\n    } catch {\n        print(\"error sending data to peers \\(peers): \\(error.localizedDescription)\")\n    }\n}\n\n\nUpdate Your Session with Collaboration Data\n\nWhen you receive collaboration data from peer users, you instantiate an ARSession.CollaborationData object with it, and pass the object to your session via update(with:).\n\nfunc receivedData(_ data: Data, from peer: MCPeerID) {\n    if let collaborationData = try? NSKeyedUnarchiver.unarchivedObject(ofClass: ARSession.CollaborationData.self, from: data) {\n        arView.session.update(with: collaborationData)\n        return\n    }\n    // ...\n\nFacilitate World Map Merging\n\nFor ARKit to know where two users are with respect to each other, it has to recognize overlap across their respective world maps. When ARKit succeeds in fitting the two world maps together, it can begin sharing those users’ respective locations and any anchors they created with each other.\n\nTo aid ARKit with world map merging, a user must point their device near an area that another user has viewed. The sample app accomplishes this by asking the users to hold their devices side by side.\n\nmessageLabel.displayMessage(\"\"\"\n    A peer wants to join the experience.\n    Hold the phones next to each other.\n    \"\"\", duration: 6.0)\n\nIdentify when ARKit Merges World Data\n\nThe first time ARKit successfully merges world data from another user, it calls your app’s session(_:didAdd:), passing in an ARParticipantAnchor that identifies the other user. This action notifies you of the merging event.\n\nfunc session(_ session: ARSession, didAdd anchors: [ARAnchor]) {\n    for anchor in anchors {\n        if let participantAnchor = anchor as? ARParticipantAnchor {\n            messageLabel.displayMessage(\"Established joint experience with a peer.\")\n            // ...\n\nVisualize Users by Displaying Virtual Content\n\nWhen ARKit successfully merges two users’ world data, you can then initiate actions to begin the multiuser experience. The sample adds virtual content in the real-world location of newly joined peer users to visualize them in AR.\n\nlet anchorEntity = AnchorEntity(anchor: participantAnchor)\n\n\nlet coordinateSystem = MeshResource.generateCoordinateSystemAxes()\nanchorEntity.addChild(coordinateSystem)\n\n\nlet color = participantAnchor.sessionIdentifier?.toRandomColor() ?? .white\nlet coloredSphere = ModelEntity(mesh: MeshResource.generateSphere(radius: 0.03),\n                                materials: [SimpleMaterial(color: color, isMetallic: true)])\nanchorEntity.addChild(coloredSphere)\n\n\narView.scene.addAnchor(anchorEntity)\n\n\nThe multicolored coordinate system shown in the following illustration represents the real-world pose of a peer user. ARKit periodically refreshes participant anchors to reflect any updates in the real-world location and orientation of the user it tracks. This process is a part of the collaboration data your app shares and uses to update its session.\n\nCheck an Anchor’s Owner\n\nWhen ARKit merges two users’ world data, it collects all the anchors created by both users and calls session(_:didAdd:) to notify each user of the collection. To check which user created an anchor, you compare the anchor’s sessionIdentifier with the active session’s identifier. If the anchor’s session ID is different from the active session’s ID, the other user created the anchor.\n\nColor Virtual Content Based on the User\n\nTo distinguish virtual content by user, you choose a different color for each user. The sample app uses the toRandomColor function to assign user colors.\n\nlet color = anchor.sessionIdentifier?.toRandomColor() ?? .white\n\n\nThe random color function works by applying a modulo operation to the anchor’s session ID, and interpreting the result as an index into a color array.\n\nfunc toRandomColor() -> UIColor {\n    var firstFourUUIDBytesAsUInt32: UInt32 = 0\n    let data = withUnsafePointer(to: self) {\n        return Data(bytes: $0, count: MemoryLayout.size(ofValue: self))\n    }\n    _ = withUnsafeMutableBytes(of: &firstFourUUIDBytesAsUInt32, { data.copyBytes(to: $0) })\n\n\n    let colors: [UIColor] = [.red, .green, .blue, .yellow, .magenta, .cyan, .purple,\n    .orange, .brown, .lightGray, .gray, .darkGray, .black, .white]\n    \n    let randomNumber = Int(firstFourUUIDBytesAsUInt32) % colors.count\n    return colors[randomNumber]\n}\n\n\nPlace Virtual Content\n\nWhen ARKit notifies you of a new nonparticipant anchor in session(_:didAdd:), place a block geometry tinted with the color calculated in the previous section.\n\nlet coloredCube = ModelEntity(mesh: MeshResource.generateBox(size: boxLength),\n                              materials: [SimpleMaterial(color: color, isMetallic: true)])\n// Offset the cube by half its length to align its bottom with the real-world surface.\ncoloredCube.position = [0, boxLength / 2, 0]\n\n\n// Attach the cube to the ARAnchor via an AnchorEntity.\n//   World origin -> ARAnchor -> AnchorEntity -> ModelEntity\nlet anchorEntity = AnchorEntity(anchor: anchor)\nanchorEntity.addChild(coloredCube)\narView.scene.addAnchor(anchorEntity)\n\n\nThe sample app uses common ARKit techniques to place virtual objects. For more information about mapping screen touches to real-world locations, see Placing Objects and Handling 3D Interaction.\n\nSee Also\nShared Experiences\nStreaming an AR Experience\nControl an AR experience remotely by transferring sensor and user input over the network.\nCreating a Multiuser AR Experience\nEnable nearby devices to share an AR experience by using a host-guest multiuser strategy.\nSwiftShot: Creating a Game for Augmented Reality\nSee how Apple built the featured demo for WWDC18, and get tips for making your own multiplayer games using ARKit, SceneKit, and Swift.\nclass ARParticipantAnchor\nAn anchor for another user in multiuser augmented reality experiences.\nclass ARSession.CollaborationData\nAn object that holds information that a user has collected about the physical environment."
  },
  {
    "title": "SwiftShot: Creating a Game for Augmented Reality | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/swiftshot_creating_a_game_for_augmented_reality",
    "html": "Overview\n\nSwiftShot is an AR game for two to six players, featured in the WWDC18 keynote. Use this sample code project to experience it on your own device, see how it works, and build your own customized version of the game.\n\nTap the Host button to start a game for other nearby players, or the Join button to participate in a game started on another device. If you’re hosting, the app asks you to find a flat surface (like a table) to place the game board on: Drag, rotate, and pinch to position and resize the board, then tap when you’re ready to play, and the game board appears.\n\nWhen the game board appears, you’ll find a landscape of wooden blocks on the table, with three slingshots at each end. Move your device near a slingshot and touch the screen to grab it, then pull back and release to aim and fire the ball. Hit blocks with balls to knock them out of the way, and knock down all three of the other team’s slingshots to win.\n\nGetting Started\n\nRequires Xcode 10.0, iOS 12.0 and an iOS device with an A9 or later processor. ARKit is not supported in iOS Simulator.\n\nDesigning Gameplay for AR\n\nSwiftShot embraces augmented reality as a medium for engaging gameplay.\n\nEncourage player movement to make gameplay more immersive. In SwiftShot, you may find that you can’t get a good shot at an enemy slingshot because blocks are in the way. And you may find a structure of blocks that can’t be easily knocked down from one angle. But you can move to other slingshots and work with your teammates to find the best angle for a winning play.\n\nDon’t encourage too much movement. You have to aim carefully to fire a good shot, so you’re less likely to bump into your teammates and send your device flying across the room.\n\nFoster social engagement. Multiplayer AR games bring players together in the same space, giving them exciting new ways to have fun together. Using AR to watch a game as a spectator provides a different perspective and a new experience.\n\nKeep games short, but add fun through variation. Getting up and waving your device around at arm’s length can make for exciting gameplay, but it can also be tiring. SwiftShot keeps matches short, encouraging party-style gameplay where players can drop into and out of games often. But SwiftShot also provides several game board layouts and special effects so that each game can be different.\n\nUsing Local Multipeer Networking and Sharing World Maps\n\nSwiftShot uses the Multipeer Connectivity framework to establish a connection with other local players and send gameplay data between devices. When you start your own session, the player who starts the session creates an ARWorldMap containing ARKit’s spatial understanding of the area around the game board. Other players joining the session receive a copy of the map and see a photo of the host’s view of the table. Moving their device so they see a similar perspective helps ARKit process the received map and establish a shared frame of reference for the multiplayer game.\n\nFor more details on setting up multiplayer AR sessions, see Creating a Multiuser AR Experience. For details on how this app implements Multipeer Connectivity, see the GameBrowser and GameSession classes.\n\nNote\n\nUsing Multipeer Connectivity helps to ensure user privacy for the local space-mapping data that ARKit collects. Multipeer Connectivity transmits data directly between devices using peer-to-peer wireless networking. When you use the MCEncryptionPreference.required encryption setting, it also protects against eavesdropping.\n\nSynchronizing Gameplay Actions\n\nTo synchronize game events between players—like launching a ball from a slingshot—SwiftShot uses an action queue pattern:\n\nThe GameManager class maintains a list of GameCommand structures, each of which pairs a GameAction enum value describing the event with an identifier for the player responsible for that event.\n\nWhenever the local player performs an action that would trigger a game event (like touching the screen while near a slingshot), the game creates a corresponding GameAction and adds it to the end of the list.\n\nAt the same time, the game encodes that GameAction and sends it through the multipeer session to other players. Each player’s GameSession decodes actions as they are received, adding them to the local GameManager instance’s command queue.\n\nThe GameManager class updates game state for each pass of the SceneKit rendering loop (at 60 frames per second). On each update, it removes commands from the queue in the order they were added and applies the resulting effect for each in the game world (like launching a ball).\n\nDefining the set of game events as a Swift enum brings multiple benefits. The enum can include additional information specific to each game action (like status for a slingshot grab or velocity for a ball launch) as an associated value for each enum case, which means you don’t need to write code elsewhere determining which information is relevant for which action. By implementing the Swift Codable protocol on these enum types, actions can be easily serialized and deserialized for transmission over the local network.\n\nSolving Multiplayer Physics\n\nSceneKit has a built-in physics engine that provides realistic physical behaviors for SwiftShot. SceneKit simulates physics on only one device, so SwiftShot needs to ensure that all players in a session see the same physics results, while still providing realistic smooth animation. SwiftShot supports all ARKit-capable iOS devices and unreliable networking scenarios, so it can’t guarantee that all devices in a session can synchronize at 60 frames per second.\n\nSwiftShot uses two techniques to solve these problems:\n\nEach peer in a session runs its own local physics simulation, but synchronizes physics results. To ensure that gameplay-relevant physics results are consistent for all peers, the game designates the player who started the game as the source of truth. The peer in that “server” role continually sends physics state information to all other peers, who update their local physics simulations accordingly. The physics server doesn’t encode and transmit the entire state of the SceneKit physics simulation, however—it sends updates only for bodies that are relevant to gameplay and whose state has changed since the last update. For implementation details, see the PhysicsSyncSceneData class in the sample code.\n\nDomain-specific data compression minimizes the bandwidth cost of physics synchronization. To transmit physics state information, the server encodes only the minimal information needed for accurate synchronization: position, orientation, velocity, and angular velocity, as well as a Boolean flag indicating whether the body should be treated as in motion or at rest. To send this information efficiently between devices, the PhysicsNodeData and PhysicsPoolNodeData types encode it to a minimal binary representation. For example:\n\nPosition is a three-component vector of 32-bit float values (96 bits total), but the game is constrained to a space 80 units wide, tall, and deep. Applying this constraint provides for encoding position in only 48 bits (16 bits per component).\n\nOrientation can be expressed as a unit quaternion of always-positive magnitude, which in turn can be written as a four-component vector. Additionally, one component of a unit quaternion is always dependent on the other three, and those components’ values are always in the range from -1/sqrt(2) to 1/sqrt(2). Applying these constraints provides for encoding orientation in 38 bits (2 bits to identify the dependent component, and 12 bits each for the other three components).\n\nTo encode and decode structures with this compact packing of bits, SwiftShot defines a BitStreamCodable protocol, extending the pattern of the Swift Codable protocol and providing a way to combine bit-stream-encoded types with other Swift Codable types in the same data stream.\n\nNote\n\nSwiftShot’s bit-stream encoding is purpose-built for minimal data size, so it omits features of a general-purpose encoder such as resilience to schema change.\n\nThe GameSession class sends and receives physics synchronization data in addition to game actions. Physics data synchronization occurs outside the queue used for game actions, so that each peer’s physics world is updated to match the server’s at the earliest opportunity.\n\nSee Also\nShared Experiences\nStreaming an AR Experience\nControl an AR experience remotely by transferring sensor and user input over the network.\nCreating a Collaborative Session\nEnable nearby devices to share an AR experience by using a peer-to-peer multiuser strategy.\nCreating a Multiuser AR Experience\nEnable nearby devices to share an AR experience by using a host-guest multiuser strategy.\nclass ARParticipantAnchor\nAn anchor for another user in multiuser augmented reality experiences.\nclass ARSession.CollaborationData\nAn object that holds information that a user has collected about the physical environment."
  },
  {
    "title": "Creating an Immersive AR Experience with Audio | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/creating_an_immersive_ar_experience_with_audio",
    "html": "Overview\n\nThis sample app uses SceneKit’s node-based audio API to associate environmental sounds with a virtual object that’s placed in the real world. Because audio is 3D positional in SceneKit by default, volume is automatically mixed based on the user’s distance from a node.\n\nGetting Started\n\nThis sample code supports Relocalization and therefore, it requires ARKit 1.5 (iOS 11.3) or greater\n\nARKit is not available in the iOS Simulator\n\nBuilding the sample requires Xcode 9.3 or later\n\nRun an AR Session and Place Virtual Content\n\nBefore you can use audio, you need to set up a session and place the object from which to play sound. For simplicity, this sample runs a world tracking configuration and places a virtual object on the first horizontal plane that it detects. For more detail about this kind of session setup, see Tracking and Visualizing Planes. The object placement approach in this sample is similar to the one demonstrated in Placing Objects and Handling 3D Interaction.\n\nAdd 3D Audio to the Scene\n\nTo play audio from a given position in 3D space, create an SCNAudioSource from an audio file. This sample loads the file from the bundle in viewDidLoad:\n\n// Instantiate the audio source\naudioSource = SCNAudioSource(fileNamed: \"fireplace.mp3\")!\n\n\nThen, the audio source is configured and prepared:\n\n// As an environmental sound layer, audio should play indefinitely\naudioSource.loops = true\n// Decode the audio from disk ahead of time to prevent a delay in playback\naudioSource.load()\n\n\nWhen you’re ready to play the sound, create an SCNAudioPlayer, passing it the audio source:\n\n// Create a player from the source and add it to `objectNode`\nobjectNode.addAudioPlayer(SCNAudioPlayer(source: audioSource))\n\n\nNote\n\nFor best results, use mono audio files. SceneKit’s audio engine uses panning to create 3D positional effects, so stereo audio sources produce less recognizable 3D audio effects."
  },
  {
    "title": "ARParticipantAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arparticipantanchor",
    "html": "Overview\n\nWhen you set isCollaborationEnabled to true, ARKit calls session(_:didAdd:) with an ARParticipantAnchor for every user it detects in your physical environment, providing you with their world position.\n\nRelationships\nInherits From\nARAnchor\nSee Also\nShared Experiences\nStreaming an AR Experience\nControl an AR experience remotely by transferring sensor and user input over the network.\nCreating a Collaborative Session\nEnable nearby devices to share an AR experience by using a peer-to-peer multiuser strategy.\nCreating a Multiuser AR Experience\nEnable nearby devices to share an AR experience by using a host-guest multiuser strategy.\nSwiftShot: Creating a Game for Augmented Reality\nSee how Apple built the featured demo for WWDC18, and get tips for making your own multiplayer games using ARKit, SceneKit, and Swift.\nclass ARSession.CollaborationData\nAn object that holds information that a user has collected about the physical environment."
  },
  {
    "title": "Streaming an AR Experience | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/streaming_an_ar_experience",
    "html": "Overview\n\nThe sample app, AR Stream, shares the augmented camera feed with a peer device, and enables it to take control by interacting with the remote AR experience. For example, a user shares their screen depicting their physical environment with a computer technician who assists the user with troubleshooting a hardware issue. As the user views a broken device resting on a table from different angles, the remote technician interacts with the experience by augmenting the user’s camera feed with textual annotations that describe the necessary steps to repair the device.\n\nTo enable the remote user to see the user’s physical environment, AR Stream shares device sensor information across the network. By compressing camera frames with Video Toolbox, the app provides the peer with good visibility of the user’s view by displaying the remote experience at a high frame rate.\n\nAR Stream also sends mathematical details about the user’s real-world pose to the remote user to process the peer’s touch input. The sample app sends the session’s inverse view and inverse projection matrices to the remote device so it can calculate a location in the user’s environment where the remote user taps. To indicate when the remote user taps the screen, AR Stream places a helpful virtual indicator at the tap location.\n\nDisplay a Camera Feed and Monitor the Session\n\nAR Stream displays the device’s camera feed by configuring a window with a view controller that displays an ARView (see the sample project’s Main.storyboard file). By default, ARView runs a session with a world-tracking configuration (ARWorldTrackingConfiguration). To receive notifications of the view’s session events, the project’s view controller (see ViewController in the sample project) assigns itself as the session delegate.\n\narView.session.delegate = self\n\nCapture Frames\n\nTo show the user’s physical environment to the remote user, AR Stream uses ReplayKit to open a screen-recording session with RPScreenRecorder.\n\nRPScreenRecorder.shared().startCapture {\n\n\nThe screen recording captures the contents of the app’s main window, which includes any augmentations that RealityKit may add to the camera feed. In the startCapture(handler:completionHandler:) closure, the sample project passes the captured screen (sampleBuffer) to the compressAndSend function for eventual transmission over the network. The sample project also passes in the session’s current frame to conform the screen captures to the camera-image size.\n\nif type == .video {\n    guard let currentFrame = arView.session.currentFrame else { return }\n    videoProcessor.compressAndSend(sampleBuffer, arFrame: currentFrame) {\n\n\nNote\n\nAlthough ARView provides a snapshot(saveToHDR:completion:) function to capture the contents of the view, ReplayKit’s screen recording is more conducive to real-time capture.\n\nCompress and Send Frames to the Peer\n\nThe sample project’s VideoProcessor class implements the compressAndSend function, which uses VTCompressionSession to compress the captured video frames.\n\nVTCompressionSessionEncodeFrame(compressionSession,\n    imageBuffer: imageBuffer,\n    presentationTimeStamp: presentationTimeStamp,\n    duration: .invalid,\n    frameProperties: nil,\n    infoFlagsOut: nil) {\n\n\nTo ensure timely compression for the real-time streaming use case of the app, the video processor enables the compression session’s kVTCompressionPropertyKey_RealTime option.\n\nVTSessionSetProperty(compressionSession, key: kVTCompressionPropertyKey_RealTime,\n    value: kCFBooleanTrue)\n\n\nAfter the VTCompressionSession finishes encoding a frame, the app creates a VideoFrameData instance using the compressed frame and the inverse view and projection matrices from the corresponding ARFrame.\n\nlet videoFrameData = VideoFrameData(sampleBuffer: sampleBuffer, arFrame: arFrame)\n\n\nThe project serializes and encodes the VideoFrameData as JSON data, and passes the data to its sendHandler.\n\ndo {\n    let data = try JSONEncoder().encode(videoFrameData)\n    // Invoke the caller's handler to send the data.\n    sendHandler(data)\n} catch {\n    fatalError(\"Failed to encode videoFrameData as JSON with error: \"\n        + error.localizedDescription)\n}\n\n\nThe screen-recording closure defines the send handler to contain code that uses Multipeer Connectivity to transmit the video data over the local network.\n\nmultipeerSession.sendToAllPeers(data, reliably: true)\n\nReceive and Decompress Peer Frames\n\nWhen the app receives VideoFrameData from another device, it decodes the JSON data.\n\nfunc receivedData(_ data: Data, from peer: MCPeerID) {\n    // Try to decode the received data and handle it appropriately.\n    if let videoFrameData = try? JSONDecoder().decode(VideoFrameData.self,\n        from: data) {\n\n\nTo house the transmitted video frame, AR Stream reconstructs a sample buffer.\n\nlet sampleBuffer = videoFrameData.makeSampleBuffer()\n\n\nThe system can display only uncompressed data, so the video processor decompresses the video frame using VTDecompressionSession within its decompress function.\n\nVTDecompressionSessionDecodeFrame(decompressionSession,\n    sampleBuffer: sampleBuffer,\n    flags: [],\n    infoFlagsOut: nil) {\n\n\nAR Stream draws the video frame to the screen using its renderer object (see Renderer in the sample project). The renderer enqueues the frame data for imminent display.\n\n// Update the PipView aspect ratio to match the camera-image dimensions.\nlet width = CGFloat(CVPixelBufferGetWidth(imageBuffer))\nlet height = CGFloat(CVPixelBufferGetHeight(imageBuffer))\noverlayViewController?.setPipViewConstraints(width: width, height: height)\n\n\noverlayViewController?.renderer.enqueueFrame(\n    pixelBuffer: imageBuffer,\n    presentationTimeStamp: presentationTimeStamp,\n    inverseProjectionMatrix: videoFrameData.inverseProjectionMatrix,\n    inverseViewMatrix: videoFrameData.inverseViewMatrix)\n\nDisplay the Remote User’s Camera Feed\n\nAR Stream defines an MTKView subclass, OverlayViewController, that displays the remote user’s camera feed on top of the ARView by placing a picture-in-picture (PiP) view at the bottom left of the screen.\n\nThe sample project’s AppDelegate configures the PiP view in a secondary window. Because ReplayKit’s screen recording captures only the main window, the PiP view displays only the remote user’s camera feed.\n\noverlayWindow = UIWindow(windowScene: windowScene)\n\n\nlet storyBoard = UIStoryboard(name: \"Main\", bundle: nil)\nlet overlayViewController = storyBoard.instantiateViewController(\n    identifier: \"OverlayViewController\")\noverlayWindow.rootViewController = overlayViewController\noverlayWindow.makeKeyAndVisible()\n\n\n// Make sure the overlayWindow is always above the main window.\noverlayWindow.windowLevel = window.windowLevel + 1\n\nSend Gestures to the Peer\n\nWhen the remote user taps the PiP view, the project responds by recording the tap location.\n\n@objc\nfunc tapped(_ sender: UITapGestureRecognizer) {\n    guard let view = sender.view else { return }\n    let location = sender.location(in: view)\n\n\nThe sample project uses the inverse matrices that the user sends to enable the remote user to interact with the user’s AR experience.\n\nguard let inverseProjectionMatrix = renderer.lastDrawnInverseProjectionMatrix,\n    let inverseViewMatrix = renderer.lastDrawnInverseViewMatrix else {\n    return\n\n\nThe project converts the tap location and inverse matrices into a ray cast that describes the location and direction in the user’s ARSession world coordinate system (see the makeRay function in the sample project).\n\nlet rayQuery = makeRay(from: location,\n    viewportSize: view.frame.size,\n    inverseProjectionMatrix: simd_float4x4(inverseProjectionMatrix),\n    inverseViewMatrix: simd_float4x4(inverseViewMatrix))\n\n\nThen, the sample project encodes the ray cast as JSON data and sends it to the connected peer.\n\nlet data = try JSONEncoder().encode(rayQuery)\nmultipeerSession?.sendToAllPeers(data, reliably: true)\n\nHandle Peer Gestures\n\nIn the project’s ViewController, the receivedData function receives a Ray object when the remote user taps the PiP view.\n\n} else if let rayQuery = try? JSONDecoder().decode(Ray.self, from: data) {\n\n\nTo hand the remote user’s tap gesture to ARKit as if the user is tapping the screen, the sample project uses the Ray data to create an ARTrackedRaycast.\n\ntrackedRaycast = arView.session.trackedRaycast(\n    ARRaycastQuery(\n        origin: rayQuery.origin,\n        direction: rayQuery.direction,\n        allowing: .estimatedPlane,\n        alignment: .any)\n    ) {\n\n\nWhen the tracked ray cast intersects with a surface in the user’s environment, the app records the resulting location.\n\nif let result = raycastResults.first {\n    marker.transform.matrix = result.worldTransform\n\nDisplay Virtual Content\n\nTo enable the remote user to interact with the user’s AR experience, the app places a virtual ball at the location in the environment where the remote user taps.\n\nImportant\n\nAR Stream displays a virtual ball for simplicity. An app may require different virtual content, such as an arrow that points to a precise spot, or virtual text that explains the importance of a location. For an example app that displays text at a real-world location, see Creating Screen Annotations for Objects in an AR Experience.\n\nThe project creates this visual marker using a ball-shaped ModelEntity.\n\nlet marker: AnchorEntity = {\n    let entity = AnchorEntity()\n    entity.addChild(ModelEntity(mesh: .generateSphere(radius: 0.05)))\n    entity.isEnabled = false\n    return entity\n}()\n\n\nAt app launch, the marker is invisible by default as the project readies the marker for display by adding it to the scene.\n\narView.scene.addAnchor(marker)\n\n\nWhen the app receives a Ray from the remote user and adjusts the marker’s position, the project displays the marker by enabling it.\n\nmarker.isEnabled = true\n\nSee Also\nShared Experiences\nCreating a Collaborative Session\nEnable nearby devices to share an AR experience by using a peer-to-peer multiuser strategy.\nCreating a Multiuser AR Experience\nEnable nearby devices to share an AR experience by using a host-guest multiuser strategy.\nSwiftShot: Creating a Game for Augmented Reality\nSee how Apple built the featured demo for WWDC18, and get tips for making your own multiplayer games using ARKit, SceneKit, and Swift.\nclass ARParticipantAnchor\nAn anchor for another user in multiuser augmented reality experiences.\nclass ARSession.CollaborationData\nAn object that holds information that a user has collected about the physical environment."
  },
  {
    "title": "ARSKView | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arskview",
    "html": "Overview\n\nUse the ARSKView class to create augmented reality experiences that position 2D elements in 3D space within a device camera view of the real world. When you run the view's provided ARSession object:\n\nThe view automatically renders the live video feed from the device camera as the scene background.\n\nWhen you implement ARSKViewDelegate methods to associate SpriteKit content with real-world positions, the view automatically scales and rotates those SpriteKit nodes so that they appear to track the real world seen by the camera.\n\nTopics\nFirst Steps\nProviding 2D Virtual Content with SpriteKit\nUse SpriteKit to place two-dimensional images in 3D space in your AR experience.\nvar session: ARSession\nThe AR session that manages motion tracking and camera image processing for the view's contents.\nResponding to AR Updates\nvar delegate: ARSKViewDelegate?\nAn object you provide to mediate synchronization of the view's AR scene information with SpriteKit content.\nprotocol ARSKViewDelegate\nMethods you can implement to mediate the automatic synchronization of SpriteKit content with an AR session.\nFinding Real-World Surfaces\nfunc hitTest(CGPoint, types: ARHitTestResult.ResultType) -> [ARHitTestResult]\nSearches for real-world objects or AR anchors in the captured camera image corresponding to a point in the SpriteKit view.\nDeprecated\nMapping Content to Real-World Positions\nfunc anchor(for: SKNode) -> ARAnchor?\nReturns the AR anchor associated with the specified SpriteKit node, if any.\nfunc node(for: ARAnchor) -> SKNode?\nReturns the SpriteKit node associated with the specified AR anchor, if any.\nRelationships\nInherits From\nSKView\nConforms To\nARSessionProviding\nSee Also\nViews\nclass ARView\nA view that enables you to display an AR experience with RealityKit.\nclass ARSCNView\nA view that blends virtual 3D content from SceneKit into your augmented reality experience.\nclass ARCoachingOverlayView\nA view that displays standardized onboarding instructions to direct users toward a specific goal."
  },
  {
    "title": "Adding an Apple Pay Button or a Custom Action in AR Quick Look | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/adding_an_apple_pay_button_or_a_custom_action_in_ar_quick_look",
    "html": "Overview\n\nFor AR experiences initiated through the web in iOS 13.3 or later, you can display an Apple Pay button so users can make purchases from your website.\n\nAlternatively, you can provide text in the banner that users can tap to invoke a custom action in your website, like adding a previewed item to a shopping cart.\n\nIn addition, you can supply AR Quick Look with custom HTML that completely customizes the banner’s graphics.\n\nTo add an Apple Pay button or custom text or graphics in a banner, choose URL parameters to configure AR Quick Look for your website. Finally, detect and react to customer taps to the banner.\n\nChoose an Apple Pay Button Style\n\nTo select a style of Apple Pay button for your AR experience, append the applePayButtonType parameter to your website URL.\n\nhttps://example.com/biplane.usdz#applePayButtonType=plain\n\n\nYou can choose from the button options using the button type values shown here.\n\nProvide Custom Text\n\nInstead of an Apple Pay button, you can supply text that AR Quick Look displays as a custom action button, as in the following image.\n\nAppend the callToAction URL parameter with the custom text as the value. The following example URL renders a banner with the text “Add to cart”:\n\nhttps://example.com/biplane.usdz#callToAction=Add%20to%20cart\n\n\nBecause URLs can’t contain spaces, be sure to URL-encode the custom text before appending it as a URL parameter. If your website supports multiple languages, localize the custom text before URL-encoding it for the URL parameter list.\n\nDefine the Item\n\nWhen you add an Apple Pay button or a custom action button to AR Quick Look, set the description of the previewed items using the checkoutTitle, checkoutSubtitle, and price URL parameters. AR Quick Look displays the subtitle and price separated by a comma below the title.\n\nIf AR Quick Look can’t fit the subtitle and price on one line, it truncates the subtitle with an ellipsis. The following example URL renders the banner.\n\nhttps://example.com/biplane.usdz#applePayButtonType=buy&checkoutTitle=Biplane%20Toy&checkoutSubtitle=Rustic%20finish%20with%20rotating%20propeller&price=$15\n\n\nIf your website supports multiple languages, localize the item title, subtitle, and price before URL-encoding them for the URL parameter list.\n\nDisplay a Custom Banner\n\nTo take full control of the banner’s graphics, supply a custom HTML file through the custom URL parameter. The following example URL renders a banner from a custom file named comingSoonBanner.\n\nhttps://example.com/biplane.usdz#custom=https://example.com/customBanners/comingSoonBanner.html\n\n\n\n\nThis example URL creates the AR experience illustrated below.\n\nIf you use the custom URL parameter, the value must be an absolute URL. To comply with AR Quick Look’s security standards, ensure the server sends the HTML resource over HTTPS.\n\nImportant\n\nAR Quick Look displays the contents of the HTML only. If you embed actions such as links or events, AR Quick Look ignores them.\n\nDefine the Custom Banner's Height\n\nWhen you display a custom banner, you can set the banner height using the customHeight URL parameter.\n\nSupply a value of small, medium, or large to set the banner height to 81, 121, or 161 points, respectively. For example:\n\nhttps://example.com/biplane.usdz#custom=https://example.com/my-custom-page.html&customHeight=large\n\n\nAR Quick Look automatically scales the width of the banner to the size and orientation of the device on which it displays. The maximum width of the custom banner is 450 points. If you omit the customHeight URL parameter, AR Quick Look uses the default value, small.\n\nDetect a Tap\n\nWhen the user taps the Apple Pay button or custom action button, WebKit sends a DOM message to the <a> element of your code that references the 3D asset.\n\n<a id=\"ar-link\" rel=\"ar\" href=\"https://example.com/cool-model.usdz#applePayButtonType=pay....etc\">  <img src=\"poster.jpg\"></a>\n\n\nTo be notified of the tap, define a JavaScript listener for the message event on your anchor.\n\nconst linkElement = document.getElementById(\"ar-link\");\nlinkElement.addEventListener(\"message\", function (event) { ... }, false);\n\n\nWhen WebKit invokes your listener, check the data property. A value of _apple_ar_quicklook_button_tapped confirms the user tapped the banner in AR Quick Look.\n\nconst linkElement = document.getElementById(\"ar-link\");\nlinkElement.addEventListener(\"message\", function (event) {   \n    if (event.data == \"_apple_ar_quicklook_button_tapped\") {\n        // Handle the user tap.   \n    }\n}, false);\n\n\nThe message event follows normal DOM processing rules. Rather than adding a listener for a specific anchor, you can add a listener at the document root for all AR links, and use the event.target to determine which anchor the user invoked.\n\nReact to a Tap\n\nDefine the actions your website takes in response to a user tap in your event listener. When the user taps the custom action button, you might add the previewed item to a shopping cart or take the user to a checkout page, depending on the banner’s text and custom action.\n\nIf your banner displays an Apple Pay button, bring up the Apple Pay prompt using Apple Pay JS API.\n\nIf your banner displays an Apple Messages for Business button, send the user to Messages using your company’s custom Apple Messages for Business URL. For more infomation, see Starting a Chat from a URL.\n\nSee Also\nAR Quick Look\nPreviewing a Model with AR Quick Look\nDisplay a model or scene that the user can move, scale, and share with others.\nAdding Visual Effects in AR Quick Look and RealityKit\nBalance the appearance and performance of your AR experiences with modeling strategies.\nclass ARQuickLookPreviewItem\nAn object for customizing the AR Quick Look experience.\nUSDZ schemas for AR\nAdd augmented reality functionality to your 3D content using USDZ schemas.\nSpecifying a lighting environment in AR Quick Look\nAdd metadata to your USDZ file to specify its lighting characteristics."
  },
  {
    "title": "ARQuickLookPreviewItem | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arquicklookpreviewitem",
    "html": "Overview\n\nUse this class when you want to control the background, designate which content the share sheet shares, or disable scaling in case it's not appropriate to allow the user to scale a particular model.\n\nTopics\nCreating a Preview Item\ninit(fileAt: URL)\nInitializes a new AR Quick Look preview item.\nScaling Content\nvar allowsContentScaling: Bool\nA Boolean value that determines whether the user can scale your virtual content using pinch gestures.\nInteracting with Content\nvar canonicalWebPageURL: URL?\nThe web URL to share when the user invokes the share sheet.\nRelationships\nInherits From\nNSObject\nConforms To\nQLPreviewItem\nSee Also\nAR Quick Look\nPreviewing a Model with AR Quick Look\nDisplay a model or scene that the user can move, scale, and share with others.\nAdding Visual Effects in AR Quick Look and RealityKit\nBalance the appearance and performance of your AR experiences with modeling strategies.\nAdding an Apple Pay Button or a Custom Action in AR Quick Look\nProvide a banner that users can tap to make a purchase or perform a custom action in an AR experience.\nUSDZ schemas for AR\nAdd augmented reality functionality to your 3D content using USDZ schemas.\nSpecifying a lighting environment in AR Quick Look\nAdd metadata to your USDZ file to specify its lighting characteristics."
  },
  {
    "title": "Adding Visual Effects in AR Quick Look and RealityKit | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/adding_visual_effects_in_ar_quick_look_and_realitykit",
    "html": "Overview\n\nAR Quick Look supports modeling features you can use to present your app's virtual content at its most vivid and compelling. Make sure to optimize the performance of your AR experience by managing and testing the features you incorporate.\n\nAR Quick Look uses RealityKit to display AR experiences, and you can design your models to take advantage of features in RealityKit available on iOS 13 and later. Roughness, for instance, is a property that affects how light diffuses on matte surfaces. RealityKit implements roughness using a multiscattering algorithm, enhancing the texture and realism of virtual content. Additionally, you can use the clearcoat option to add a glossy shine to your virtual content.\n\nAdd Gloss to Your 3D Content with Clearcoat\n\nYou can add shine and highlights to the models in your AR experience with the clearcoat option.\n\nRealityKit renders clearcoat by adding a second specular lobe. To enable a clearcoat layer on your app's virtual content, use the following inputs when designing your model:\n\nclearcoat\n\nA scalar float normalized between 0 and 1 that indicates the intensity of the clearcoat specular lobe.\n\nclearcoatRoughness\n\nA float normalized between 0 and 1 that indicates the perceived roughness of the clearcoat surface.\n\nThe following figure shows the clearcoat parameter varying between 0 and 1 vertically, and clearcoatRoughness varying between 0 and 1 horizontally, with (0, 0) at the bottom left.\n\nDesign for Lighting and Reflection Accuracy\n\nRealityKit's multiscattering roughness affects how light reflects from––or diffuses on––the surface of models with a matte finish. By displaying virtual content using RealityKit, AR Quick Look realistically honors the roughness, metallic, and specular components of a model. Configure your 3D assets' roughness, metallic, and specular inputs to create the right blend of reflection and diffusion.\n\nThe roughness component controls the texture of a surface. A roughness value of 0 (smooth) results in a mirror reflection, while a roughness value of 1 (rough) diffuses light to create a surface without shine. The following figure shows the roughness parameter varying between 0 and 1, with 0 on the left.\n\nThe metallic component defines whether your model appears dielectric (0) or metallic (1). The following figure shows the metallic parameter varying from 0-1, with 0 on the left.\n\nFor dielectrics, the specular component enables you to control the reflectiveness of a surface, ranging from 0 (nonreflective) to 1 (fully reflective). The following figure shows a roughness with the specular parameter varying from 0-1, with 0 on the left.\n\nNote\n\nEnvironment lighting in AR Quick Look supports multiple models in iOS 13. To ensure your virtual content displays with the most realistic reflections in your app, check the iOS version at runtime and load multiple models only in iOS 13.\n\nOptimize Model Geometry and Transparency\n\nWhile utilizing features to present virtual content effectively, take steps to keep the AR experience responsive. Because each model mesh results in a draw call, for best results, limit the number of meshes in a USDZ file to around 50. To reduce draw calls while maintaining the same amount of geometry, merge objects that share a material. However, be aware that merging may not afford performance gains in situations where the result prevents the renderer from culling.\n\nThe cost of projective and raytraced shadows depends on the complexity of your model's geometry. To enhance performance and prevent excessive memory use relating to lighting, keep the polygon count of your 3D asset's geometry below 100,000 polygons.\n\nTransparency in models increases the number of times the fragment shader is run per pixel, which is expensive when the user views your model straight on or in full screen. To improve performance by reducing the time your app spends shading pixels, use transparency sparingly when authoring your app's virtual content.\n\nControl Texture Memory\n\nFor best results, size your textures to 1024 x 1024 pixels or less, and don't surpass a size of 2048 x 2048 pixels. If you have multiple small textures, combine them into one larger texture using texture coordinates to specify their location. To ensure your textures fit into memory as required for AR Quick Look to display your model, cap your total number of textures at six.\n\nIf you observe any resolution discrepancies at runtime, switch to testing on a device with more memory and check for any differences. AR Quick Look automatically generates mipmaps for your model's textures. AR Quick Look also scales down your texture at runtime if needed to keep its memory use under a certain budget depending on the device.\n\nTest Features on Various Devices\n\nWhen developing your model for optimal performance, begin with a conservative configuration to ensure AR Quick Look supports your model on earlier iOS devices (iPhone 6s, and first-generation iPad Pro). By starting on the earliest devices that support AR Quick Look, you avoid having to prune back your virtual content's features during the testing phase.\n\nSee Also\nAR Quick Look\nPreviewing a Model with AR Quick Look\nDisplay a model or scene that the user can move, scale, and share with others.\nAdding an Apple Pay Button or a Custom Action in AR Quick Look\nProvide a banner that users can tap to make a purchase or perform a custom action in an AR experience.\nclass ARQuickLookPreviewItem\nAn object for customizing the AR Quick Look experience.\nUSDZ schemas for AR\nAdd augmented reality functionality to your 3D content using USDZ schemas.\nSpecifying a lighting environment in AR Quick Look\nAdd metadata to your USDZ file to specify its lighting characteristics."
  },
  {
    "title": "Data Management | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/data_management",
    "html": "Topics\nBody Data\nCapturing Body Motion in 3D\nTrack a person in the physical environment and visualize their motion by applying the same body movements to a virtual character.\nclass ARBody2D\nThe screen-space representation of a person ARKit recognizes in the camera feed.\nclass ARSkeleton3D\nThe skeleton of a human body that ARKit tracks in 3D space.\nclass ARSkeleton2D\nAn object that describes the locations of a body’s joints in the camera feed.\nclass ARSkeleton\nThe interface for the skeleton of a tracked body.\nclass ARSkeletonDefinition\nThe hierarchy of joints and their names.\nFace Data\nTracking and Visualizing Faces\nDetect faces in a front-camera AR experience, overlay virtual content, and animate facial expressions in real-time.\nCombining User Face-Tracking and World Tracking\nTrack the user’s face in an app that displays an AR experience with the rear camera.\nclass ARFaceGeometry\nA 3D mesh describing face topology for use in face-tracking AR sessions.\nclass ARSCNFaceGeometry\nA SceneKit representation of face topology for use with face information that an AR session provides.\nWorld Data\nSaving and Loading World Data\nSerialize a world-tracking session to resume it later on.\nclass ARWorldMap\nThe state in a world-tracking AR session during which a device maps the user's position in physical space and proximity to anchor objects.\nSee Also\nVirtual Content\nContent Anchors\nIdentify items in the physical environment, including planar surfaces, images, physical objects, body positions, and faces.\nEnvironmental Analysis\nAnalyze the video from the cameras and the accompanying data, and use ray-casting and depth-map information to determine the location of items.\nCamera, Lighting, and Effects\nDetermine the camera position and lighting for the current session, and apply effects, such as occlusion, to elements of the environment.\nCreating USD files for Apple devices\nGenerate 3D assets that render as expected."
  },
  {
    "title": "Previewing a Model with AR Quick Look | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/previewing_a_model_with_ar_quick_look",
    "html": "Overview\n\nAR Quick Look enables the user to place virtual content that you provide on any surface that ARKit finds in the real-world environment. Users can interact with your virtual content by moving and scaling it using touch gestures, or by sharing it with others through the iOS share sheet.\n\nChoose an Input Format\n\nYou provide content for your AR experience in .usdz or .reality format:\n\nTo browse a library of .usdz files, see the AR Quick Look Gallery.\n\nTo browse a library of .reality assets, use Reality Composer. For more information, see Creating 3D Content with Reality Composer.\n\nNote\n\nIf you include a Reality Composer file (.rcproject) in your app's Copy Files build phase, Xcode automatically outputs a converted .reality file in your app bundle at build time.\n\nDisplay an AR Experience in Your App\n\nIn your app, you enable AR Quick Look by providing QLPreviewController with a supported input file. The following code demonstrates previewing a scene named myScene from the app bundle.\n\nimport UIKit\nimport QuickLook\nimport ARKit\n\n\nclass ViewController: UIViewController, QLPreviewControllerDataSource {\n\n\n    override func viewDidAppear(_ animated: Bool) {\n        let previewController = QLPreviewController()\n        previewController.dataSource = self\n        present(previewController, animated: true, completion: nil)\n    }\n\n\n    func numberOfPreviewItems(in controller: QLPreviewController) -> Int { return 1 }\n\n\n    func previewController(_ controller: QLPreviewController, previewItemAt index: Int) -> QLPreviewItem {\n        guard let path = Bundle.main.path(forResource: \"myScene\", ofType: \"reality\") else { fatalError(\"Couldn't find the supported input file.\") }\n        let url = URL(fileURLWithPath: path)\n        return url as QLPreviewItem\n    }    \n}\n\n\nTo prevent the user from scaling your virtual content or to customize the default share sheet behavior, use ARQuickLookPreviewItem instead of QLPreviewItem.\n\nDisplay an AR Experience in Your Web Page\n\nIn your web page, you enable AR Quick Look by linking a supported input file.\n\n<div>\n    <a rel=\"ar\" href=\"/assets/models/my-model.usdz\">\n        <img src=\"/assets/models/my-model-thumbnail.jpg\">\n    </a>\n</div>\n\n\nWhen the user clicks the link in Safari or within a web view that's displayed in your app, iOS presents your scene in an AR Quick Look view on your behalf. For more information, see Viewing Augmented Reality Assets in Safari for iOS.\n\nSee Also\nAR Quick Look\nAdding Visual Effects in AR Quick Look and RealityKit\nBalance the appearance and performance of your AR experiences with modeling strategies.\nAdding an Apple Pay Button or a Custom Action in AR Quick Look\nProvide a banner that users can tap to make a purchase or perform a custom action in an AR experience.\nclass ARQuickLookPreviewItem\nAn object for customizing the AR Quick Look experience.\nUSDZ schemas for AR\nAdd augmented reality functionality to your 3D content using USDZ schemas.\nSpecifying a lighting environment in AR Quick Look\nAdd metadata to your USDZ file to specify its lighting characteristics."
  },
  {
    "title": "Camera, Lighting, and Effects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/camera_lighting_and_effects",
    "html": "Topics\nCamera\nGet details about a user's iOS device, like its position and orientation in 3D space, and the camera's video data and exposure.\nclass ARCamera\nInformation about the camera position and imaging characteristics for a given frame.\nLighting Effects\nAdding Realistic Reflections to an AR Experience\nUse ARKit to generate environment probe textures from camera imagery and render reflective virtual objects.\nclass AREnvironmentProbeAnchor\nAn object that provides environmental lighting information for a specific area of space in a world-tracking AR session.\nclass ARLightEstimate\nEstimated scene lighting information associated with a captured video frame in an AR session.\nclass ARDirectionalLightEstimate\nEstimated environmental lighting information associated with a captured video frame in a face-tracking AR session.\nOcclusion\nOccluding Virtual Content with People\nCover your app’s virtual content with people that ARKit perceives in the camera feed.\nEffecting People Occlusion in Custom Renderers\nOcclude your app’s virtual content where ARKit recognizes people in the camera feed by using matte generator.\nVisualizing and Interacting with a Reconstructed Scene\nEstimate the shape of the physical environment using a polygonal mesh.\nclass ARMatteGenerator\nAn object that creates matte textures you use to occlude your app's virtual content with people, that ARKit recognizes in the camera feed.\nSee Also\nVirtual Content\nContent Anchors\nIdentify items in the physical environment, including planar surfaces, images, physical objects, body positions, and faces.\nEnvironmental Analysis\nAnalyze the video from the cameras and the accompanying data, and use ray-casting and depth-map information to determine the location of items.\nData Management\nObtain detailed information about skeletal and face geometry, and saved world data.\nCreating USD files for Apple devices\nGenerate 3D assets that render as expected."
  },
  {
    "title": "ARWorldTrackingConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration",
    "html": "Overview\n\nThe ARWorldTrackingConfiguration class tracks the device's movement with six degrees of freedom (6DOF): the three rotation axes (roll, pitch, and yaw), and three translation axes (movement in x, y, and z).\n\nThis kind of tracking can create immersive AR experiences: A virtual object can appear to stay in the same place relative to the real world, even as the user tilts the device to look above or below the object, or moves the device around to see the object's sides and back.\n\nFigure 1 6DOF tracking maintains an AR illusion regardless of device rotation or movement\n\nWorld-tracking sessions also provide several ways for your app to recognize or interact with elements of the real-world scene visible to the camera:\n\nFind real-world horizontal or vertical surfaces with planeDetection. Add the surfaces to the session as ARPlaneAnchor objects.\n\nRecognize and track the movement of 2D images with detectionImages. Add 2D images to the scene as ARImageAnchor objects.\n\nRecognize 3D objects with detectionObjects. Add 3D objects to the scene as ARObjectAnchor objects.\n\nFind the 3D positions of real-world features that correspond to a touch point on the device's screen with ray casting.\n\nTopics\nCreating a Configuration\ninit()\nInitializes a new world-tracking configuration.\nvar initialWorldMap: ARWorldMap?\nThe state from a previous AR session to attempt to resume with this session configuration.\nTracking Surfaces\nvar planeDetection: ARWorldTrackingConfiguration.PlaneDetection\nA value that specifies whether and how the session automatically attempts to detect flat surfaces in the camera-captured image.\nstruct ARWorldTrackingConfiguration.PlaneDetection\nOptions for whether and how the framework detects flat surfaces in captured images.\nvar sceneReconstruction: ARConfiguration.SceneReconstruction\nA flag that enables scene reconstruction.\nclass func supportsSceneReconstruction(ARConfiguration.SceneReconstruction) -> Bool\nChecks if the device supports scene reconstruction.\nDetecting or Tracking Images\nvar detectionImages: Set<ARReferenceImage>!\nA set of images that ARKit searches for in the user's environment.\nvar maximumNumberOfTrackedImages: Int\nThe number of image anchors to monitor closely for position and orientation updates.\nvar automaticImageScaleEstimationEnabled: Bool\nA flag that instructs the framework to estimate and set the scale of a detected or tracked image on your behalf.\nDetecting 3D Objects\nvar detectionObjects: Set<ARReferenceObject>\nA set of 3D objects that the framework attempts to detect in the user’s environment.\nTracking the User's Face\nvar userFaceTrackingEnabled: Bool\nA flag that determines whether ARKit tracks the user's face in a world-tracking session.\nclass var supportsUserFaceTracking: Bool\nA Boolean value that tells you whether the iOS device supports tracking the user's face during a world-tracking session.\nCreating Realistic Reflections\nvar environmentTexturing: ARWorldTrackingConfiguration.EnvironmentTexturing\nAn option that determines how the framework generates environment textures.\nenum ARWorldTrackingConfiguration.EnvironmentTexturing\nOptions to generate environment textures in a world-tracking AR session.\nclass AREnvironmentProbeAnchor\nAn object that provides environmental lighting information for a specific area of space in a world-tracking AR session.\nvar wantsHDREnvironmentTextures: Bool\nA flag that instructs the framework to create environment textures in HDR format.\nManaging Device Camera Behavior\nvar isAutoFocusEnabled: Bool\nA Boolean value that determines whether the device camera uses fixed focus or autofocus behavior.\nEnabling Collaboration\nvar isCollaborationEnabled: Bool\nA flag that opts you in to a peer-to-peer multiuser AR experience.\nAccessing App Clip Codes\nInteracting with App Clip Codes in AR\nDisplay content and provide services in an AR experience with App Clip Codes.\nclass var supportsAppClipCodeTracking: Bool\nA flag that indicates if the device tracks App Clip Codes.\nvar appClipCodeTrackingEnabled: Bool\nA Boolean value that indicates if the framework searches the physical environment for App Clip Codes.\nclass ARAppClipCodeAnchor\nAn anchor that tracks the position and orientation of an App Clip Code in the physical environment.\nRelationships\nInherits From\nARConfiguration\nSee Also\nSpatial Tracking\nUnderstanding World Tracking\nDiscover features and best practices for building rear-camera AR experiences.\nclass ARGeoTrackingConfiguration\nA configuration that tracks locations with GPS, map data, and a device's compass.\nclass AROrientationTrackingConfiguration\nA configuration that tracks only the device’s orientation using the rear-facing camera.\nclass ARPositionalTrackingConfiguration\nA configuration that tracks only the device’s position in 3D space."
  },
  {
    "title": "ARCoachingOverlayView | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arcoachingoverlayview",
    "html": "Overview\n\nThis view offers your users a standardized onboarding routine. You can configure this view to automatically display during session initialization and in limited tracking situations, while giving the user specific instructions that best facilitate ARKit's world tracking.\n\nThese illustrations show overlay views with horizontal- and vertical-plane goals, indicating that the user should begin moving the device:\n\nThese illustrations show overlay views indicating that the user should continue moving the phone or change the speed with which they move it:\n\nWhen you start your app, the coaching overlay asks the user to move the device in ways that help ARKit establish tracking. When you choose a specific goal like finding a plane, the view tailors its instructions accordingly. After the coaching overlay determines the goal has been met and no further coaching is required, it hides from the user's view.\n\nFor an example app that uses the coaching overlay, see Placing Objects and Handling 3D Interaction.\n\nSupporting Automatic Coaching\n\nBy default, activatesAutomatically is enabled and therefore you should override coachingOverlayViewWillActivate(_:) to determine whether coaching is in progress. Coordinate your actions to help the user focus on these instructions, for example, by hiding any UI that's not necessary while the session reinitializes.\n\nRelocalizing After an Interruption\n\nIf relocalization is enabled (see sessionShouldAttemptRelocalization(_:)), ARKit attempts to restore your session if any interruptions degrade your app's tracking state. In this event, the coaching overlay presents itself and gives the user instructions to assist ARKit with relocalizing.\n\nDuring this time, the coaching overlay includes a button that lets the user indicate they'd like to start over rather than restore the session.\n\nARKit notifies you when the user presses Start Over by calling your delegate's coachingOverlayViewDidRequestSessionReset(_:) function. Implement this callback if your app requires any custom actions to restart the AR experience.\n\nfunc coachingOverlayViewDidRequestSessionReset(_ coachingOverlayView: ARCoachingOverlayView) {    \n\n\n    // Reset the session.\n    let configuration = ARWorldTrackingConfiguration()\n    configuration.planeDetection = [.horizontal, .vertical]\n    session.run(configuration, options: [.resetTracking])\n\n\n    // Custom actions to restart the AR experience. \n    // ...\n}\n\n\nIf you do not implement coachingOverlayViewDidRequestSessionReset(_:), the coaching overlay responds to the Start Over button by resetting tracking, which also removes any existing anchors.\n\nFor more information about relocalization, see Managing Session Life Cycle and Tracking Quality.\n\nTopics\nDelegating Events\nvar delegate: ARCoachingOverlayViewDelegate?\nAn object you supply that implements coaching event callbacks.\nprotocol ARCoachingOverlayViewDelegate\nA set of callbacks you implement to be notified of coaching events.\nDefining a Goal\nvar goal: ARCoachingOverlayView.Goal\nA field that indicates your app's tracking requirements.\nenum ARCoachingOverlayView.Goal\nThe options that specify your app's tracking requirements.\nActivating the View\nvar activatesAutomatically: Bool\nA flag that indicates whether the coaching view activates automatically, depending on the current session state.\nvar isActive: Bool\nA flag that indicates whether coaching is in progress.\nfunc setActive(Bool, animated: Bool)\nControls whether coaching is in progress.\nProviding the Session\nvar session: ARSession?\nThe session this view uses to provide coaching.\nvar sessionProvider: ARSessionProviding?\nAn object you designate that provides the current session.\nRelationships\nInherits From\nUIView\nSee Also\nViews\nclass ARView\nA view that enables you to display an AR experience with RealityKit.\nclass ARSCNView\nA view that blends virtual 3D content from SceneKit into your augmented reality experience.\nclass ARSKView\nA view that blends virtual 2D content from SpriteKit into the 3D space of an augmented reality experience."
  },
  {
    "title": "Environmental Analysis | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/environmental_analysis",
    "html": "Topics\nVideo Frame Analysis\nDisplaying a Point Cloud Using Scene Depth\nPresent a visualization of the physical environment by placing points based a scene’s depth data.\nCreating a Fog Effect Using Scene Depth\nApply virtual fog to the physical environment.\nclass ARFrame\nA video image captured as part of a session with position-tracking information.\nclass ARPointCloud\nA collection of points in the world coordinate space of the AR session.\nclass ARDepthData\nAn object that describes the distance to regions of the real world from the plane of the camera.\nRaycasting\nPlacing Objects and Handling 3D Interaction\nPlace virtual content at tracked, real-world locations, and enable the user to interact with virtual content by using gestures.\nclass ARRaycastQuery\nA mathematical ray you use to find 3D positions on real-world surfaces.\nclass ARTrackedRaycast\nA raycast query that ARKit repeats in succession to give you refined results over time.\nclass ARRaycastResult\nInformation about a real-world surface found by examining a point on the screen.\nHit-Testing\nclass ARHitTestResult\nInformation about a real-world surface found by examining a point on the screen.\nDeprecated\nSee Also\nVirtual Content\nContent Anchors\nIdentify items in the physical environment, including planar surfaces, images, physical objects, body positions, and faces.\nCamera, Lighting, and Effects\nDetermine the camera position and lighting for the current session, and apply effects, such as occlusion, to elements of the environment.\nData Management\nObtain detailed information about skeletal and face geometry, and saved world data.\nCreating USD files for Apple devices\nGenerate 3D assets that render as expected."
  },
  {
    "title": "captureHighResolutionFrame(completion:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/3975720-capturehighresolutionframe",
    "html": "Parameters\ncompletion\n\nCode you provide that the framework runs after attempting to generate the frame.\n\nDiscussion\n\nConcurrency Note\n\nYou can call this method from synchronous code using a completion handler, as shown on this page, or you can call it as an asynchronous method that has the following declaration:\n\nfunc captureHighResolutionFrame() async throws -> ARFrame\n\n\nFor information about concurrency and asynchronous code in Swift, see Calling Objective-C APIs Asynchronously.\n\nIf the function succeeds, the completion handler's frame contains a high quality, high resolution capturedImage.\n\nIn the event of failure, the completion block receives a non-nil error object. A call may fail if a previous request for a high resolution capture hasn't completed yet, or an underlying problem occurs in the system's capture pipeline. You can identify the failure reason in either case by checking for highResolutionFrameCaptureInProgress or highResolutionFrameCaptureFailed, respectively.\n\nARKit populates the frame's properties other than pixel data, including pose information, anchors, and frame semantics. The system provides the frame to your completion handler asynchronously.\n\nYou can call this function at any time during a session. The system delivers a high-resolution frame out-of-band, which means that it doesn't affect the other frames that the session receives at a regular interval, such as currentFrame or the frame argument to session(_:didUpdate:).\n\nFor the highest resolution captured image, choose a non-binned videoFormat in your session's configuration. You can call recommendedVideoFormatForHighResolutionFrameCapturing to select the best option for you.\n\nFor the highest resolution still images, choose a videoFormat among your configuration's supportedVideoFormats that returns true for isRecommendedForHighResolutionFrameCapturing. If your app doesn't have specific resolution requirements, you can use the framework-recommended format that recommendedVideoFormatForHighResolutionFrameCapturing returns.\n\nSee Also\nAccessing the camera frame\nvar currentFrame: ARFrame?\nThe most recent still frame captured by the active camera feed, including ARKit's interpretation of it.\nclass ARFrame\nA video image captured as part of a session with position-tracking information."
  },
  {
    "title": "getGeoLocation(forPoint:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/3571352-getgeolocation",
    "html": "Parameters\nposition\n\nPosition in local coordinates to convert.\n\ncompletionHandler\n\nCode that control will execute when this function returns. The session runs this code on its delegate queue. The parameters are:\n\ncoordinate\n\nLocation coordinates (latitude, longitude).\n\naltitude\n\nThe altitude.\n\nerror\n\nThe reason, if conversion fails.\n\nReturn Value\n\nA latitude, longitude, and altitude for the argument position in the session’s world coordinate-space.\n\nDiscussion\n\nConcurrency Note\n\nYou can call this method from synchronous code using a completion handler, as shown on this page, or you can call it as an asynchronous method that has the following declaration:\n\nfunc geoLocation(forPoint position: simd_float3) async throws -> (CLLocationCoordinate2D, CLLocationDistance)\n\n\nFor information about concurrency and asynchronous code in Swift, see Calling Objective-C APIs Asynchronously.\n\nARKit refers to its local coordinate space as “world” coordinate space, but this is different from geographic coordinates. For more information on ARKit’s coordinate space, see setWorldOrigin(relativeTransform:).\n\nTo succeed, this function requires an ARGeoTrackingConfiguration session with state equal to ARGeoTrackingStatus.State.localized."
  },
  {
    "title": "trackedRaycast(_:updateHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/3132066-trackedraycast",
    "html": "Parameters\nquery\n\nThe ray-cast query that ARKit will repeat. If you use a standard renderer, you ask the standard renderer to provide the ray-cast query; see Finding Real-World Surfaces of ARSCNView. If you use a custom renderer, you create a ray-cast query by specifying a point on a particular frame; see Finding Real-World Surfaces in ARFrame.\n\nupdateHandler\n\nA closure you provide that ARKit calls every time it has an updated ray-cast result for you. Use this opportunity to update the position of any virtual content your app may have placed using the prior results of the tracked ray cast. ARKit invokes this closure on your session's delegate queue.\n\nDiscussion\n\nA tracked ray cast wraps a ray-cast query that the session calls repeatedly, each time invoking your update handler to provide you with new results.\n\nWhen you're ready to stop a tracked ray cast, call stopTracking().\n\nSee Also\nFinding real-world surfaces\nfunc raycast(ARRaycastQuery) -> [ARRaycastResult]\nChecks once for intersections between a ray and real-world surfaces."
  },
  {
    "title": "createReferenceObject(transform:center:extent:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/3001731-createreferenceobject",
    "html": "Parameters\ntransform\n\nA transform matrix defining the origin and orientation of the local coordinate system for the region to extract.\n\ncenter\n\nA point, relative to the origin specified by transform, that defines the center of the bounding box for the region to extract.\n\nextent\n\nThe width, height, and depth of the region to extract, centered on the center point and oriented to the local coordinate system specified by transform.\n\ncompletionHandler\n\nA handler to be invoked asynchronously after ARKit finishes creating the reference object. The handler takes two parameters:\n\nreferenceObject\n\nThe generated ARReferenceObject, or nil if a reference object could not be created.\n\nerror\n\nIf the referenceObject is nil, an ARError describing the failure.\n\nReturn Value\n\nAn ARReferenceObject representing the specified region of the world map.\n\nDiscussion\n\nConcurrency Note\n\nYou can call this method from synchronous code using a completion handler, as shown on this page, or you can call it as an asynchronous method that has the following declaration:\n\nfunc createReferenceObject(transform: simd_float4x4, center: simd_float3, extent: simd_float3) async throws -> ARReferenceObject\n\n\nFor information about concurrency and asynchronous code in Swift, see Calling Objective-C APIs Asynchronously.\n\nImportant\n\nThis method is valid only when running a session with ARObjectScanningConfiguration, which enables the high-fidelity spatial data collection needed for scanning reference objects. Calling this method on a session with a different configuration immediately invokes your completionHandler with an error.\n\nTo use the extracted reference object for 3D object detection, assign it to the detectionObjects property of a world tracking configuration. You can bundle reference objects in an app by saving them to files and adding them to an Xcode asset catalog.\n\nWhen ARKit detects a reference image, the transform of the resulting ARObjectAnchor is based on the orgin of the reference object's coordinate system—the transform you specify when extracting the reference object. For example, if a reference object represents a physical item that sits on a horizontal surface, virtual content should appear to sit on whatever surface the physical object does. To adjust a reference object's origin after extracting it, use the applyingTransform(_:) method."
  },
  {
    "title": "ARSCNView | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arscnview",
    "html": "Overview\n\nThe ARSCNView class provides the easiest way to create augmented reality experiences that blend virtual 3D content with a device camera view of the real world. When you run the view's provided ARSession object:\n\nThe view automatically renders the live video feed from the device camera as the scene background.\n\nThe world coordinate system of the view's SceneKit scene directly responds to the AR world coordinate system established by the session configuration.\n\nThe view automatically moves its SceneKit camera to match the real-world movement of the device.\n\nARKit automatically matches SceneKit's coordinate space to the real world, so after you place your app's virtual content, it maintains the illusion of resting in the real-world as the user moves the device. See Providing 3D Virtual Content with SceneKit.\n\nYou don't necessarily need to use the ARAnchor class to track positions of objects you add to the scene, but by implementing ARSCNViewDelegate methods, you can add SceneKit content to any anchors that are automatically detected by ARKit.\n\nBecause ARKit requires Metal, use only Metal features of SceneKit. For example:\n\nThis class supports only SCNProgram instances with Metal Shading Language code.\n\nIf you set the preferredRenderingAPI property to SCNRenderingAPI.openGLES2, the framework reverts the value to SCNRenderingAPI.metal.\n\nTopics\nEssentials\nProviding 3D Virtual Content with SceneKit\nUse SceneKit to add realistic three-dimensional objects to your AR experience.\nvar session: ARSession\nThe AR session that manages motion tracking and camera image processing for the view's contents.\nvar scene: SCNScene\nThe SceneKit scene to be displayed in the view.\nResponding to AR Updates\nvar delegate: ARSCNViewDelegate?\nAn object you provide to mediate synchronization of the view's AR scene information with SceneKit content.\nprotocol ARSCNViewDelegate\nMethods you can implement to mediate the automatic synchronization of SceneKit content with an AR session.\nFinding Real-World Surfaces\nfunc hitTest(CGPoint, types: ARHitTestResult.ResultType) -> [ARHitTestResult]\nSearches for real-world objects or AR anchors in the captured camera image corresponding to a point in the SceneKit view.\nDeprecated\nfunc raycastQuery(from: CGPoint, allowing: ARRaycastQuery.Target, alignment: ARRaycastQuery.TargetAlignment) -> ARRaycastQuery?\nCreates a raycast query that originates from a point on the view, aligned with the center of the camera's field of view.\nMapping Content to Real-World Positions\nfunc anchor(for: SCNNode) -> ARAnchor?\nReturns the AR anchor associated with the specified SceneKit node, if any.\nfunc node(for: ARAnchor) -> SCNNode?\nReturns the SceneKit node associated with the specified AR anchor, if any.\nfunc unprojectPoint(CGPoint, ontoPlane: simd_float4x4) -> simd_float3?\nReturns the projection of a point from 2D view onto a plane in the 3D world space detected by ARKit.\nManaging Lighting\nvar automaticallyUpdatesLighting: Bool\nA Boolean value that specifies whether ARKit creates and updates SceneKit lights in the view's scene.\nDebugging AR Display\ntypealias ARSCNDebugOptions\nOptions for drawing overlay content to aid debugging of AR tracking in a SceneKit view.\nManaging Rendering Effects\nvar rendersMotionBlur: Bool\nDetermines whether the view renders motion blur.\nvar rendersCameraGrain: Bool\nA flag that determines whether SceneKit applies image noise characteristics to your app's virtual content.\nRelationships\nInherits From\nSCNView\nConforms To\nARSessionProviding\nSee Also\nViews\nclass ARView\nA view that enables you to display an AR experience with RealityKit.\nclass ARSKView\nA view that blends virtual 2D content from SpriteKit into the 3D space of an augmented reality experience.\nclass ARCoachingOverlayView\nA view that displays standardized onboarding instructions to direct users toward a specific goal."
  },
  {
    "title": "recommendedVideoFormatForHighResolutionFrameCapturing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/3930047-recommendedvideoformatforhighres",
    "html": "Discussion\n\nThe framework determines the resolution of the camera feed and still-image capture for this format. Call this function when your app requires a high-resolution still capture regardless of format specifics. If instead, your app requires a particular resolution, iterate over the supportedVideoFormats array and choose a format with the desired configuration where isRecommendedForHighResolutionFrameCapturing is true.\n\nOther video formats may support still-image capture but at a lower quality or resolution.\n\nSee Also\nManaging video capture options\nvar videoFormat: ARConfiguration.VideoFormat\nA camera type, resolution, and frame rate for an AR session.\nclass var supportedVideoFormats: [ARConfiguration.VideoFormat]\nThe set of video capture formats available on the current device.\nclass ARConfiguration.VideoFormat\nA video size and frame rate specification for use with an AR session.\nvar videoHDRAllowed: Bool\nEnables high dynamic range (HDR) for the session's camera feed.\nclass var configurableCaptureDeviceForPrimaryCamera: AVCaptureDevice?\nAn object that enables you to alter the appearance of a frame's captured image.\nclass var recommendedVideoFormatFor4KResolution: ARConfiguration.VideoFormat?\nProvides a 4K video format if the device and configuration support it."
  },
  {
    "title": "providesAudioData | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/2923559-providesaudiodata",
    "html": "Discussion\n\nTo receive and handle captured audio data, your session delegate must implement the session(_:didOutputAudioSampleBuffer:) method."
  },
  {
    "title": "ARBodyTrackingConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration",
    "html": "Overview\n\nWhen ARKit identifies a person in the rear camera's feed, it calls session(_:didAdd:), passing an ARBodyAnchor you can use to track the body's movement.\n\nWhen you enable plane detection and image detection, you can use a body anchor to display a virtual character and set the character on a surface or image that you choose.\n\nBy default, frameSemantics includes bodyDetection, which gives you access to the joint positions of a person that ARKit detects in the camera feed via the frame's detectedBody.\n\nTopics\nCreating a Configuration\ninit()\nCreates a new body tracking configuration.\nvar initialWorldMap: ARWorldMap?\nThe state from a previous AR session to attempt to resume with this session configuration.\nEstimating Body Scale\nvar automaticSkeletonScaleEstimationEnabled: Bool\nA flag that determines whether ARKit estimates the height of a body that it's tracking.\nEnabling Auto Focus\nvar isAutoFocusEnabled: Bool\nA Boolean value that determines whether the device camera uses fixed focus or autofocus behavior.\nEnabling Plane Detection\nvar planeDetection: ARWorldTrackingConfiguration.PlaneDetection\nA value specifying whether and how the session attempts to automatically detect flat surfaces in the camera-captured image.\nstruct ARWorldTrackingConfiguration.PlaneDetection\nOptions for whether and how the framework detects flat surfaces in captured images.\nEnabling Image Tracking\nvar automaticImageScaleEstimationEnabled: Bool\nA flag that instructs ARKit to estimate and set the scale of a tracked image on your behalf.\nvar detectionImages: Set<ARReferenceImage>\nA set of images that ARKit searches for in the user's environment.\nvar maximumNumberOfTrackedImages: Int\nThe number of image anchors to monitor closely for position and orientation updates.\nAdding Realistic Reflections\nvar wantsHDREnvironmentTextures: Bool\nA flag that instructs ARKit to create environment textures in HDR format.\nvar environmentTexturing: ARWorldTrackingConfiguration.EnvironmentTexturing\nThe behavior ARKit uses for generating environment textures.\nAccessing App Clip Codes\nInteracting with App Clip Codes in AR\nDisplay content and provide services in an AR experience with App Clip Codes.\nclass var supportsAppClipCodeTracking: Bool\nA flag that indicates if the device tracks App Clip Codes.\nvar appClipCodeTrackingEnabled: Bool\nA Boolean value that indicates if the framework searches the physical environment for App Clip Codes.\nclass ARAppClipCodeAnchor\nAn anchor that tracks the position and orientation of an App Clip Code in the physical environment.\nRelationships\nInherits From\nARConfiguration\nSee Also\nBody and Face Tracking\nclass ARFaceTrackingConfiguration\nA configuration that tracks facial movement and expressions using the front camera."
  },
  {
    "title": "videoHDRAllowed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/3930048-videohdrallowed",
    "html": "Discussion\n\nBefore calling this function, check whether the session configuration's video format supports HDR first by calling isVideoHDRSupported.\n\nSee Also\nManaging video capture options\nvar videoFormat: ARConfiguration.VideoFormat\nA camera type, resolution, and frame rate for an AR session.\nclass var supportedVideoFormats: [ARConfiguration.VideoFormat]\nThe set of video capture formats available on the current device.\nclass ARConfiguration.VideoFormat\nA video size and frame rate specification for use with an AR session.\nclass var configurableCaptureDeviceForPrimaryCamera: AVCaptureDevice?\nAn object that enables you to alter the appearance of a frame's captured image.\nclass var recommendedVideoFormatFor4KResolution: ARConfiguration.VideoFormat?\nProvides a 4K video format if the device and configuration support it.\nclass var recommendedVideoFormatForHighResolutionFrameCapturing: ARConfiguration.VideoFormat?\nReturns a video format that the framework recommends for high-resolution-still-image capture."
  },
  {
    "title": "configurableCaptureDeviceForPrimaryCamera | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/3930045-configurablecapturedeviceforprim",
    "html": "Discussion\n\nThis property provides the underlying capture device for the framework's camera feed. By altering the device's configuration, your app indirectly adjusts the visual properties of the each AR frame's capturedImage.\n\nAlter the device's settings with caution, as extreme changes can affect ARKit's features that rely on the capturedImage and depend on its integrity, such as people occlusion that uses personSegmentation.\n\nImportant\n\nThis property is nil on devices that aren’t equiped with an ultra-wide camera.\n\nSee Also\nManaging video capture options\nvar videoFormat: ARConfiguration.VideoFormat\nA camera type, resolution, and frame rate for an AR session.\nclass var supportedVideoFormats: [ARConfiguration.VideoFormat]\nThe set of video capture formats available on the current device.\nclass ARConfiguration.VideoFormat\nA video size and frame rate specification for use with an AR session.\nvar videoHDRAllowed: Bool\nEnables high dynamic range (HDR) for the session's camera feed.\nclass var recommendedVideoFormatFor4KResolution: ARConfiguration.VideoFormat?\nProvides a 4K video format if the device and configuration support it.\nclass var recommendedVideoFormatForHighResolutionFrameCapturing: ARConfiguration.VideoFormat?\nReturns a video format that the framework recommends for high-resolution-still-image capture."
  },
  {
    "title": "recommendedVideoFormatFor4KResolution | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/3930046-recommendedvideoformatfor4kresol",
    "html": "Discussion\n\nIf the device and configuration support 4K, the returned video format is also present in the configuration’s supportedVideoFormats array.\n\nThis function returns nil if the device or configuration doesn't support 4K, so you can call this function to determine whether to enable 4K for your session.\n\nSee Also\nManaging video capture options\nvar videoFormat: ARConfiguration.VideoFormat\nA camera type, resolution, and frame rate for an AR session.\nclass var supportedVideoFormats: [ARConfiguration.VideoFormat]\nThe set of video capture formats available on the current device.\nclass ARConfiguration.VideoFormat\nA video size and frame rate specification for use with an AR session.\nvar videoHDRAllowed: Bool\nEnables high dynamic range (HDR) for the session's camera feed.\nclass var configurableCaptureDeviceForPrimaryCamera: AVCaptureDevice?\nAn object that enables you to alter the appearance of a frame's captured image.\nclass var recommendedVideoFormatForHighResolutionFrameCapturing: ARConfiguration.VideoFormat?\nReturns a video format that the framework recommends for high-resolution-still-image capture."
  },
  {
    "title": "supportsWorldTracking | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration/3175412-supportsworldtracking",
    "html": "Discussion\n\nCall this function before attempting to enable world tracking in a face tracking configuration using isWorldTrackingEnabled.\n\nSee Also\nEnabling World Tracking\nvar isWorldTrackingEnabled: Bool\nA Boolean value that instructs a session to provide the app with user face data during a world-tracking session."
  },
  {
    "title": "getCurrentWorldMap(completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2968206-getcurrentworldmap",
    "html": "Parameters\ncompletionHandler\n\nA closure to be invoked asynchronously after ARKit finishes generating the world map. The closure takes two parameters:\n\nworldMap\n\nThe generated ARWorldMap, or nil if a map could not be generated.\n\nerror\n\nIf the worldMap is nil, an ARError describing the failure.\n\nDiscussion\n\nConcurrency Note\n\nYou can call this method from synchronous code using a completion handler, as shown on this page, or you can call it as an asynchronous method that has the following declaration:\n\nfunc currentWorldMap() async throws -> ARWorldMap\n\n\nFor information about concurrency and asynchronous code in Swift, see Calling Objective-C APIs Asynchronously.\n\nAn ARWorldMap encapsulates the state of a running ARSession. This state includes ARKit's awareness of the physical space the user moves the device in (which ARKit uses to determine the device's position and orientation), as well as any ARAnchor objects added to the session (which can represent detected real-world features or virtual content placed by your app). After you use this method to save a session's world map, you can assign it to a configuration's initialWorldMap property and use run(_:options:) to start another session with the same spatial awareness and anchors.\n\nBy saving world maps and using them to start new sessions, your app can add new AR capabilities:\n\nMultiuser AR experiences. Create a shared frame of reference by sending archived ARWorldMap objects to a nearby user's device. With two devices tracking the same world map, you can build a networked experience where both users can see and interact with the same virtual content.\n\nPersistent AR experiences. Save a world map when your app becomes inactive, then restore it the next time your app launches in the same physical environment. You can use anchors from the resumed world map to place the same virtual content at the same positions from the saved session.\n\nBefore saving a world map, monitor the worldMappingStatus property to verify that ARKit has an adequate understanding of the user's environment, ensuring that you can reliably make use of the saved map on a different device or at a later time.\n\nWorld map generation requires a world-tracking AR session. If you call this method on an ARSession not run with ARWorldTrackingConfiguration, it invokes your completion handler immediately, providing no world map and an error.\n\nImportant\n\nARKit calls your completionHandler on the session's delegateQueue (if set; on the main queue otherwise). If you need to perform expensive work from this handler (such as archiving and saving or sending the world map), do so on an appropriate dispatch queue to avoid disrupting performance.\n\nSee Also\nSaving or sharing state\nRecording and Replaying AR Session Data\nRecord an AR session in Reality Composer and replay it in your ARKit app."
  },
  {
    "title": "Recording and Replaying AR Session Data | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/recording_and_replaying_ar_session_data",
    "html": "Overview\n\nARKit apps use video feeds and sensor data from an iOS device to understand the world around the device. This reliance on real-world input makes the testing of an AR experience challenging because real-world input is never the same across two AR sessions. Differences in lighting conditions, device motion, and the location of nearby objects all change how RealityKit understands and renders the scene each time.\n\nTo provide consistent data to your AR app, you can record a session using Reality Composer, then use the recorded camera and sensor data to drive your app when running from Xcode.\n\nRecord an AR Session in Reality Composer\n\nOn an iOS device, create a Reality Composer scene with the same anchor type as the app you’re recording the session for, then follow these steps to create the recording.\n\nTap the Settings toolbar button. It’s the circle containing three dots in the upper-right corner.\n\nIn the Settings sidebar that appears, tap Developer.\n\nIn the Developer window, tap Record AR Session.\n\nMove your iOS device to the initial location for your recording.\n\nTap the record button to start.\n\nMove your device around until it anchors the Reality Composer scene, then continue moving it around until you’ve captured the desired input. When done, tap the stop button to end recording. Rename the recording by providing a custom name in the Capture Complete window, then tap Done to save the recording to Reality Composer’s library.\n\nReplay an AR Session in Reality Composer\n\nTo replay a session capture right after recording it, tap the Replay button in the Capture Complete window.\n\nTo replay a session later, follow these steps:\n\nTap the Settings toolbar button.\n\nTap Developer in the Settings inspector.\n\nIn the Developer window, tap Replay AR Session.\n\nSelect the session to replay.\n\nPress the Play button to begin playback.\n\nIf the recorded session meets your needs, you can export it to a Quicktime movie file by hitting the Share button from either the Recording Complete window or the Playback Complete window. If your Mac is on and unlocked, you can Airdrop the file directly to your computer.\n\nUse a Recorded Session in an App\n\nIn Xcode, you can specify an exported recording to use when launching your app. To select a recording, edit your project’s scheme and choose the Run phase from the left pane. Select the Options tab, then look for a row labeled ARKit with a “Replay data” checkbox next to it. Check that box, then choose Add Replay Data to Project from the popup button next to it to select the recording.\n\nWhen you run your app with this option selected, it uses the recorded session instead of the device’s camera and sensors.\n\nSee Also\nSaving or sharing state\nfunc getCurrentWorldMap(completionHandler: (ARWorldMap?, Error?) -> Void)\nReturns an object encapsulating the world-tracking session's space-mapping state and set of anchors."
  },
  {
    "title": "remove(anchor:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2865607-remove",
    "html": "Parameters\nanchor\n\nThe anchor to remove.\n\nDiscussion\n\nChanges to anchor tracking take effect when the next frame is captured.\n\nSee Also\nManaging anchors\nfunc add(anchor: ARAnchor)\nAdds the specified anchor to be tracked by the session."
  },
  {
    "title": "ARSessionObserver | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionobserver",
    "html": "Overview\n\nThis protocol defines optional methods common to the ARSessionDelegate, ARSCNViewDelegate, and ARSKViewDelegate protocols. You can implement this protocol's methods when adopting one of those protocols.\n\nTopics\nResponding to Tracking Quality Changes\nfunc session(ARSession, cameraDidChangeTrackingState: ARCamera)\nInforms the delegate of changes to the quality of ARKit's device position tracking.\nfunc session(ARSession, didChange: ARGeoTrackingStatus)\nListen and react to geo-tracking state changes.\nHandling Interruptions\nfunc sessionWasInterrupted(ARSession)\nTells the delegate that the session has temporarily stopped processing frames and tracking device position.\nfunc sessionInterruptionEnded(ARSession)\nTells the delegate that the session has resumed processing frames and tracking device position.\nfunc sessionShouldAttemptRelocalization(ARSession) -> Bool\nAsks the delegate whether to attempt recovery of world-tracking state after an interruption.\nReceiving Audio Data\nfunc session(ARSession, didOutputAudioSampleBuffer: CMSampleBuffer)\nTells the delegate that a new sample buffer of recorded audio is available.\nHandling Errors\nfunc session(ARSession, didFailWithError: Error)\nTells the delegate that the session has stopped running due to an error.\nlet ARErrorDomain: String\nThe error domain for NSError objects produced by an AR session.\nManaging Collaboration\nfunc session(ARSession, didOutputCollaborationData: ARSession.CollaborationData)\nProvides information for nearby users about your perspective in the environment.\nRelationships\nInherits From\nNSObjectProtocol\nInherited By\nARSCNViewDelegate\nARSKViewDelegate\nARSessionDelegate\nSee Also\nResponding to events\nvar delegate: ARSessionDelegate?\nAn object you provide to receive captured video images and tracking information, or to respond to changes in session status.\nvar delegateQueue: dispatch_queue_t?\nThe dispatch queue through which the session calls your delegate methods.\nprotocol ARSessionDelegate\nMethods you can implement to receive captured video frame images and tracking state from an AR session."
  },
  {
    "title": "ARSessionDelegate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessiondelegate",
    "html": "Overview\n\nImplement this protocol if you need to work directly with ARFrame objects captured by the session or directly follow changes to the session's set of tracked ARAnchor objects. Typically, you adopt this protocol when building a custom view for displaying AR content—if you display content with SceneKit or SpriteKit, the ARSCNViewDelegate and ARSKViewDelegate protocols provide similar information and integrate with those technologies.\n\nThis protocol extends the ARSessionObserver protocol, so your session delegate can also implement those methods to respond to changes in session status.\n\nTopics\nReceiving Camera Frames\nfunc session(ARSession, didUpdate: ARFrame)\nProvides a newly captured camera image and accompanying AR information to the delegate.\nHandling Content Updates\nfunc session(ARSession, didAdd: [ARAnchor])\nTells the delegate that one or more anchors have been added to the session.\nfunc session(ARSession, didUpdate: [ARAnchor])\nTells the delegate that the session has adjusted the properties of one or more anchors.\nfunc session(ARSession, didRemove: [ARAnchor])\nTells the delegate that one or more anchors have been removed from the session.\nRelationships\nInherits From\nARSessionObserver\nSee Also\nResponding to events\nvar delegate: ARSessionDelegate?\nAn object you provide to receive captured video images and tracking information, or to respond to changes in session status.\nvar delegateQueue: dispatch_queue_t?\nThe dispatch queue through which the session calls your delegate methods.\nprotocol ARSessionObserver\nMethods you can implement to respond to changes in the state of an AR session."
  },
  {
    "title": "delegateQueue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2875726-delegatequeue",
    "html": "Discussion\n\nIf this value is nil (the default), the session calls your delegate methods on the main queue.\n\nSee Also\nResponding to events\nvar delegate: ARSessionDelegate?\nAn object you provide to receive captured video images and tracking information, or to respond to changes in session status.\nprotocol ARSessionDelegate\nMethods you can implement to receive captured video frame images and tracking state from an AR session.\nprotocol ARSessionObserver\nMethods you can implement to respond to changes in the state of an AR session."
  },
  {
    "title": "ARFrame | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arframe",
    "html": "Overview\n\nA running session continuously captures video frames from the device's camera while ARKit analyzes the captures to determine the user's position in the world. ARKit can provide this information to you in the form of an ARFrame in two ways:\n\nOccasionally, by accessing an ARSession object's currentFrame\n\nConstantly, as a stream of frames through the session(_:didUpdate:) callback\n\nTo automatically receive all frames as ARKit captures them, make one of your objects the delegate of your app's ARSession.\n\nEach frame can contain additional data, for example, EXIF (exifData), or data based on any particular frameSemantics that you enable.\n\nTopics\nAccessing camera data\nvar camera: ARCamera\nInformation about the camera position, orientation, and imaging parameters used to capture the frame.\nvar capturedImage: CVPixelBuffer\nA pixel buffer containing the image captured by the camera.\nvar timestamp: TimeInterval\nThe time at which the frame was captured.\nvar cameraGrainIntensity: Float\nA value that specifies the amount of grain present in the camera grain texture.\nvar cameraGrainTexture: MTLTexture?\nA tileable Metal texture created by ARKit to match the visual characteristics of the current video stream.\nvar exifData: [String : Any]\nAuxiliary data for the captured image.\nAccessing scene data\nvar lightEstimate: ARLightEstimate?\nAn estimate of lighting conditions based on the camera image.\nfunc displayTransform(for: UIInterfaceOrientation, viewportSize: CGSize) -> CGAffineTransform\nReturns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.\nvar rawFeaturePoints: ARPointCloud?\nThe current intermediate results of the scene analysis ARKit uses to perform world tracking.\nvar capturedDepthData: AVDepthData?\nDepth data captured in front-camera experiences.\nvar capturedDepthDataTimestamp: TimeInterval\nThe time at which depth data for the frame (if any) was captured.\nvar sceneDepth: ARDepthData?\nData on the distance between a device's rear camera and real-world objects in an AR experience.\nvar smoothedSceneDepth: ARDepthData?\nAn average of distance measurements between a device's rear camera and real-world objects that creates smoother visuals in an AR experience.\nTracking and interacting with the real world\nvar anchors: [ARAnchor]\nThe list of anchors representing positions tracked or objects detected in the scene.\nfunc raycastQuery(from: CGPoint, allowing: ARRaycastQuery.Target, alignment: ARRaycastQuery.TargetAlignment) -> ARRaycastQuery\nGet a ray-cast query for a screen point.\nfunc hitTest(CGPoint, types: ARHitTestResult.ResultType) -> [ARHitTestResult]\nSearches for real-world objects or AR anchors in the captured camera image.\nDeprecated\nChecking world-mapping status\nvar worldMappingStatus: ARFrame.WorldMappingStatus\nThe feasibility of generating or relocalizing a world map for this frame.\nenum ARFrame.WorldMappingStatus\nPossible values describing how thoroughly ARKit has mapped the the area visible in a given frame.\nChecking for people\nvar detectedBody: ARBody2D?\nThe screen position information of a body that ARKit recognizes in the camera image.\nclass ARBody2D\nThe screen-space representation of a person ARKit recognizes in the camera feed.\nvar segmentationBuffer: CVPixelBuffer?\nA buffer that contains pixel information identifying the shape of objects from the camera feed that you use to occlude virtual content.\nvar estimatedDepthData: CVPixelBuffer?\nA buffer that represents the estimated depth values from the camera feed that you use to occlude virtual content.\nenum ARFrame.SegmentationClass\nA categorization of a pixel that defines a type of content you use to occlude your app's virtual content.\nAssessing geo-tracking condition\nvar geoTrackingStatus: ARGeoTrackingStatus?\nThe session’s condition with respect to geographic tracking at the time the session captured the frame.\nclass ARGeoTrackingStatus\nThe state, accuracy, and reason that are possible for geo-tracking’s current condition.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nSee Also\nAccessing the camera frame\nvar currentFrame: ARFrame?\nThe most recent still frame captured by the active camera feed, including ARKit's interpretation of it.\nfunc captureHighResolutionFrame(completion: (ARFrame?, Error?) -> Void)\nRequests a frame outside of the normal frequency that contains a high-resolution captured image."
  },
  {
    "title": "delegate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2865614-delegate",
    "html": "Discussion\n\nIf you use the ARSCNView or ARSKView class to display your AR experience, a session delegate isn't necessary. Those views automatically display captured video images and coordinate SceneKit or SpriteKit content to track device and camera motion.\n\nIf you create your own visualization for an AR experience using Metal or other rendering technologies, set a session delegate. Your delegate object periodically receives ARFrame objects captured by the session. These objects contain video frame images for you to display and AR scene information you can use to coordinate display of the scene elements you render.\n\nSee Also\nResponding to events\nvar delegateQueue: dispatch_queue_t?\nThe dispatch queue through which the session calls your delegate methods.\nprotocol ARSessionDelegate\nMethods you can implement to receive captured video frame images and tracking state from an AR session.\nprotocol ARSessionObserver\nMethods you can implement to respond to changes in the state of an AR session."
  },
  {
    "title": "pause() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2865619-pause",
    "html": "Discussion\n\nWhile paused, the session doesn't track device motion or capture scene imagery, nor does it coordinate with its delegate object or update any associated ARSCNView or ARSKView object.\n\nSee Also\nConfiguring and running a session\nfunc run(ARConfiguration, options: ARSession.RunOptions)\nStarts AR processing for the session with the specified configuration and options.\nvar identifier: UUID\nA unique identifier of the running session.\nstruct ARSession.RunOptions\nOptions for transitioning an AR session's current state when you change its configuration.\nvar configuration: ARConfiguration?\nAn object that defines motion and scene tracking behaviors for the session."
  },
  {
    "title": "ARWorldMap | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arworldmap",
    "html": "Overview\n\nThe session state in a world map includes ARKit's awareness of the physical space in which the user moves the device. ARKit uses the details of the user's physical space to determine the device's position and orientation, as well as any ARAnchor objects added to the session that can represent detected real-world features or virtual content placed by your app.\n\nSerialize and Deserialize a World Map\n\nWhen your app quits, you can save the current world map (acquired using getCurrentWorldMap(completionHandler:)). Because ARWorldMap conforms to NSSecureCoding, you serialize it using NSKeyedArchiver.\n\nfunc writeWorldMap(_ worldMap: ARWorldMap, to url: URL) throws {\n    let data = try NSKeyedArchiver.archivedData(withRootObject: worldMap, requiringSecureCoding: true)\n    try data.write(to: url)\n}\n\n\nTo restore the world map the next time your app launches, use NSKeyedUnarchiver.\n\nfunc loadWorldMap(from url: URL) throws -> ARWorldMap {\n    let mapData = try Data(contentsOf: url)\n    guard let worldMap = try NSKeyedUnarchiver.unarchivedObject(ofClass: ARWorldMap.self, from: mapData)\n        else { throw ARError(.invalidWorldMap) }\n    return worldMap\n}\n\n\nYou can use anchors from a resumed world map to place the same virtual content at the same positions from the saved session, if the app launches in the same physical environment.\n\nFor more information, see Saving and Loading World Data.\n\nShare a Saved World Map\n\nWith two devices tracking the same world map, you can build a networked experience in which both users can see and interact with the same virtual content. To send an ARWorldMap to another device:\n\nOn one device, use NSKeyedArchiver to convert the world map to a data object. You don't need to write the data to a file to send it over the network.\n\nUse the networking technology of your choice to send the resulting data to another device. For example, in a Multipeer Connectivity session, call send(_:toPeers:with:) to send data, and implement MCSessionDelegate methods on the other device to receive data.\n\nOn the receiving device, use NSKeyedUnarchiver to instantiate an ARWorldMap from the data.\n\nFor more information, see Creating a Multiuser AR Experience.\n\nRun a Deserialized World Map\n\nTo begin a new session from an existing ARWorldMap, set a world-tracking configuration's initialWorldMap property and use run(_:options:). This starts a new session using the same spatial awareness and anchors loaded from the saved world map.\n\nTopics\nExamining a World Map\nvar anchors: [ARAnchor]\nThe set of anchors recorded in the world map.\nvar center: simd_float3\nThe center point of the world map's space-mapping data, relative to the world coordinate origin of the session the map was recorded in.\nvar extent: simd_float3\nThe size of the world map's space-mapping data, relative to the world coordinate origin of the session the map was recorded in.\nDebugging a World Map\nvar rawFeaturePoints: ARPointCloud\nA coarse representation of the space-mapping data recorded in the world map.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nSee Also\nWorld Data\nSaving and Loading World Data\nSerialize a world-tracking session to resume it later on.\nRelated Documentation\nManaging Session Life Cycle and Tracking Quality\nKeep the user informed on the current session state and recover from interruptions."
  },
  {
    "title": "ARAnchorCopying | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/aranchorcopying",
    "html": "Overview\n\nAn ARAnchor (or an instance of any anchor subclass) represents a position and orientation in world space, and optionally associates extra information with that point (like a name, or plane or image detection data). Each time ARKit generates an ARFrame object (describing the current environment as of a specific frame of live camera video), ARKit updates the anchors associated with the session as of that moment. Because anchor objects are immutable, ARKit must copy them to make changes from one ARFrame to the next.\n\nIf you create your own ARAnchor subclass, you must implement the init(anchor:) initializer required by this protocol. To ensure that any custom information in your subclass is maintained between successive frames, your implementation should copy any custom properties it declares.\n\nTopics\nCopying Anchors\ninit(anchor: ARAnchor)\nInitializes a new anchor by copying custom information from another anchor.\n\nRequired\n\nRelationships\nInherits From\nNSCopying\nConforming Types\nARAnchor\nSee Also\nCommon Types\nclass ARAnchor\nAn object that specifies the position and orientation of an item in the physical environment.\nprotocol ARTrackable\nAn interface for objects that track the location of real-world objects or locations."
  },
  {
    "title": "ARFaceAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfaceanchor",
    "html": "Overview\n\nThe session automatically adds to its list of anchors an ARFaceAnchor object when it detects a unique face in the front camera feed.\n\nWhen you track faces using ARFaceTrackingConfiguration, ARKit can track multiple faces simultaneously.\n\nAlternatively, you can enable face tracking with a world tracking configuration by setting .\n\nTracking Face Position and Orientation\n\nThe inherited transform property describes the face’s current position and orientation in world coordinates; that is, in a coordinate space relative to that specified by the worldAlignment property of the session configuration. Use this transform matrix to position virtual content you want to “attach” to the face in your AR scene.\n\nThis transform matrix creates a face coordinates system for positioning other elements relative to the face. Units of face coordinate space are in meters, with the origin centered behind the face as indicated in the figure below.\n\nThe coordinate system is right-handed—the positive x direction points to the viewer’s right (that is, the face’s own left), the positive y direction points up (relative to the face itself, not to the world), and the positive z direction points outward from the face (toward the viewer).\n\nUsing Face Topology\n\nThe geometry property provides an ARFaceGeometry object representing detailed topology for the face, which conforms a generic face model to match the dimensions, shape, and current expression of the detected face.\n\nYou can use this model as the basis for overlaying content that follows the shape of the user’s face—for example, to apply virtual makeup or tattoos. You can also use this model to create occlusion geometry—a 3D model that doesn't render any visible content (allowing the camera image to show through), but that obstructs the camera's view of other virtual content in the scene.\n\nTracking Facial Expressions\n\nThe blendShapes property provides a high-level model of the current facial expression, described via a series of many named coefficients that represent the movement of specific facial features relative to their neutral configurations. You can use blend shape coefficients to animate 2D or 3D content, such as a character or avatar, in ways that follow the user’s facial expressions.\n\nTopics\nUsing Face Geometry\nvar geometry: ARFaceGeometry\nA coarse triangle mesh representing the topology of the detected face.\nUsing Blend Shapes\nvar blendShapes: [ARFaceAnchor.BlendShapeLocation : NSNumber]\nA dictionary of named coefficients representing the detected facial expression in terms of the movement of specific facial features.\nstruct ARFaceAnchor.BlendShapeLocation\nIdentifiers for specific facial features, for use with coefficients describing the relative movements of those features.\nTracking Eye Movement\nvar leftEyeTransform: simd_float4x4\nA transform matrix indicating the position and orientation of the face's left eye.\nvar rightEyeTransform: simd_float4x4\nA transform matrix indicating the position and orientation of the face's right eye.\nvar lookAtPoint: simd_float3\nA position in face coordinate space estimating the direction of the face's gaze.\nRelationships\nInherits From\nARAnchor\nConforms To\nARTrackable\nSee Also\nFace Tracking\nTracking and Visualizing Faces\nDetect faces in a front-camera AR experience, overlay virtual content, and animate facial expressions in real-time.\nCombining User Face-Tracking and World Tracking\nTrack the user’s face in an app that displays an AR experience with the rear camera."
  },
  {
    "title": "ARObjectAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arobjectanchor",
    "html": "Overview\n\nWhen you run a world-tracking AR session and specify ARReferenceObject objects for the session configuration's detectionObjects property, ARKit searches for those objects in the real-world environment. When the session recognizes an object, it automatically adds to its list of anchors an ARObjectAnchor for each detected object.\n\nTo place virtual 3D content that matches the position or size of the detected object, use the anchor's inherited transform property together with the center and extent of the anchor's referenceObject.\n\nTopics\nIdentifying Detected Objects\nvar referenceObject: ARReferenceObject\nThe detected object referenced by the object anchor.\nRelationships\nInherits From\nARAnchor\nSee Also\nPhysical Objects\nVisualizing and Interacting with a Reconstructed Scene\nEstimate the shape of the physical environment using a polygonal mesh.\nScanning and detecting 3D objects\nRecord spatial features of real-world objects, then use the results to find those objects in the user’s environment and trigger AR content.\nclass ARReferenceObject\nThe description of a 3D object that you want ARKit to detect in the physical environment."
  },
  {
    "title": "ARSession.RunOptions | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/runoptions",
    "html": "Topics\nCreating Run Options\ninit(rawValue: UInt)\nCreates a run options.\nRun Options\nstatic var resetTracking: ARSession.RunOptions\nAn option to reset the device's position from the session's previous run.\nstatic var removeExistingAnchors: ARSession.RunOptions\nAn option to remove any anchor objects associated with the session's previous run.\nstatic var stopTrackedRaycasts: ARSession.RunOptions\nAn option to stop all active tracked raycasts.\nstatic var resetSceneReconstruction: ARSession.RunOptions\nAn option to reset the scene mesh.\nRelationships\nConforms To\nOptionSet\nSendable\nSee Also\nConfiguring and running a session\nfunc run(ARConfiguration, options: ARSession.RunOptions)\nStarts AR processing for the session with the specified configuration and options.\nvar identifier: UUID\nA unique identifier of the running session.\nvar configuration: ARConfiguration?\nAn object that defines motion and scene tracking behaviors for the session.\nfunc pause()\nPauses processing in the session."
  },
  {
    "title": "configuration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2865609-configuration",
    "html": "See Also\nConfiguring and running a session\nfunc run(ARConfiguration, options: ARSession.RunOptions)\nStarts AR processing for the session with the specified configuration and options.\nvar identifier: UUID\nA unique identifier of the running session.\nstruct ARSession.RunOptions\nOptions for transitioning an AR session's current state when you change its configuration.\nfunc pause()\nPauses processing in the session."
  },
  {
    "title": "identifier | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/3214028-identifier",
    "html": "Discussion\n\nThis property might change after you call the run function, but not immediately. Therefore, to get the new value, listen for its change using key-value observation.\n\n// Use key-value observation to monitor my ARSession's identifier.\nvar sessionIDObservation: NSKeyValueObservation?\n...\nsessionIDObservation = observe(\n    .arView.session.identifier,\n    options: [.old, .new]) { \n        object, change in\n        print(\"SessionID changed to: \\(change.newValue!)\")\n    }\n\n\nSee Also\nConfiguring and running a session\nfunc run(ARConfiguration, options: ARSession.RunOptions)\nStarts AR processing for the session with the specified configuration and options.\nstruct ARSession.RunOptions\nOptions for transitioning an AR session's current state when you change its configuration.\nvar configuration: ARConfiguration?\nAn object that defines motion and scene tracking behaviors for the session.\nfunc pause()\nPauses processing in the session."
  },
  {
    "title": "add(anchor:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2865612-add",
    "html": "Parameters\nanchor\n\nThe anchor to add.\n\nDiscussion\n\nChanges to anchor tracking take effect when the next frame is captured.\n\nSee Also\nManaging anchors\nfunc remove(anchor: ARAnchor)\nRemoves the specified anchor from tracking by the session."
  },
  {
    "title": "transform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/aranchor/2867981-transform",
    "html": "Discussion\n\nWorld coordinate space in ARKit always follows a right-handed convention, but is oriented based on the session configuration. For details, see Understanding World Tracking.\n\nSee Also\nTracking Anchors\nvar identifier: UUID\nA unique identifier for the anchor.\nvar sessionIdentifier: UUID?\nThe unique identifier of the session that owns this anchor."
  },
  {
    "title": "sessionIdentifier | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/aranchor/3214025-sessionidentifier",
    "html": "Discussion\n\nARKit sets this property when a user first adds an anchor to a session. In multiuser experiences, use this ID to identify the user that created the anchor.\n\nSee Also\nTracking Anchors\nvar identifier: UUID\nA unique identifier for the anchor.\nvar transform: simd_float4x4\nA matrix encoding the position, orientation, and scale of the anchor relative to the world coordinate space of the AR session the anchor is placed in."
  },
  {
    "title": "GeometrySource | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/geometrysource",
    "html": "Overview\n\nMesh-anchor geometry (MeshDescriptor) uses geometry sources to hold 3D data like vertices and normals in an efficent, array-like format. A Metal buffer wraps the data and other properties specify how to interpret that data.\n\nIf componentsPerVector is greater than one, the element type of the geometry-source array is itself a sequence (pairs, triplets, and so on).\n\nTopics\nInspecting geometry data\nvar buffer: MTLBuffer\nA Metal buffer that contains per-vector data for a geometry source.\nvar count: Int\nThe number of vectors in a geometry source.\nvar format: MTLVertexFormat\nThe vertex format for data in a geometry source’s buffer.\nvar componentsPerVector: Int\nThe number of scalar components in each vector in a geometry source.\nvar offset: Int\nThe offset, in bytes, from the beginning of a geometry source’s buffer.\nvar stride: Int\nThe number of bytes between one vector and another in a geometry source’s buffer.\nvar description: String\nA textual description of a geometry source.\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nGeometry\nstruct GeometryElement\nA container for vertex indices of lines or triangles."
  },
  {
    "title": "ReferenceImage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/referenceimage",
    "html": "Topics\nCreating a reference image\ninit(cgimage: CGImage, physicalSize: CGSize, orientation: CGImagePropertyOrientation)\nCreates a reference image from a Core Graphics image.\ninit(pixelBuffer: CVPixelBuffer, physicalSize: CGSize, orientation: CGImagePropertyOrientation)\nCreates a reference image from a pixel buffer.\nstatic func loadReferenceImages(inGroupNamed: String, bundle: Bundle?) -> [ReferenceImage]\nCreates multiple reference images based on their group name in an asset catalog.\nInspecting a reference image\nvar physicalSize: CGSize\nThe size, in meters, of a reference image in the real world.\nvar name: String?\nThe name of a reference image.\nvar description: String\nA textual description of a reference image.\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nImage tracking\nTracking preregistered images in 3D space\nPlace content based on the current position of a known image in a person’s surroundings.\nclass ImageTrackingProvider\nA source of live data about a 2D image’s position in a person’s surroundings.\nstruct ImageAnchor\nA 2D image’s position in a person’s surroundings."
  },
  {
    "title": "name | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/aranchor/2873672-name",
    "html": "Discussion\n\nTo name an anchor, create one with the init(name:transform:) initializer. This property is nil for anchors created otherwise.\n\nARKit does not display the name to users, but your app can use it to identify anchors for debugging.\n\nSee Also\nCreating Anchors\ninit(transform: simd_float4x4)\nCreates a new anchor object with the specified transform.\ninit(name: String, transform: simd_float4x4)\nCreates a new anchor object with the specified transform and a descriptive name."
  },
  {
    "title": "identifier | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/aranchor/2867972-identifier",
    "html": "Discussion\n\nWhether an anchor is created manually (with the init(transform:) initializer) or automatically by ARKit (and provided to you through ARSessionDelegate, ARSCNViewDelegate, or ARSKViewDelegate methods), each anchor automatically receives a unique identifier value.\n\nYou can use this value to determine which anchors accompanying a specific ARFrame capture correspond to anchors in frames captured previously.\n\nSee Also\nTracking Anchors\nvar sessionIdentifier: UUID?\nThe unique identifier of the session that owns this anchor.\nvar transform: simd_float4x4\nA matrix encoding the position, orientation, and scale of the anchor relative to the world coordinate space of the AR session the anchor is placed in."
  },
  {
    "title": "ImageAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imageanchor",
    "html": "Topics\nGetting image information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of the image in world space.\nvar referenceImage: ReferenceImage\nThe reference image that this image anchor tracks.\nvar estimatedScaleFactor: Float\nThe estimated scale factor between the tracked image’s physical size and the reference image’s size.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking this image.\nvar description: String\nA textual description of an image anchor.\nIdentifying image anchors\nvar id: UUID\nA globally unique identifier for an image anchor.\ntypealias ImageAnchor.ID\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when ImageAnchor conforms to AnyObject.\nRelationships\nConforms To\nSendable\nTrackableAnchor\nSee Also\nImage tracking\nTracking preregistered images in 3D space\nPlace content based on the current position of a known image in a person’s surroundings.\nclass ImageTrackingProvider\nA source of live data about a 2D image’s position in a person’s surroundings.\nstruct ReferenceImage\nA 2D image the system uses as a reference to find the same image in a person’s surroundings."
  },
  {
    "title": "init(name:transform:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/aranchor/2968170-init",
    "html": "Parameters\nname\n\nA descriptive name for the anchor. ARKit does not display the name to users, but your app can use it to identify anchors for debugging.\n\ntransform\n\nA matrix encoding the position, orientation, and scale of the anchor relative to the world coordinate space of the AR session the anchor is placed in.\n\nWorld coordinate space in ARKit always follows a right-handed convention, but is oriented based on the session configuration. For details, see Understanding World Tracking.\n\nDiscussion\n\nUse the add(anchor:) method to begin tracking your custom anchor in an AR session.\n\nSee Also\nCreating Anchors\ninit(transform: simd_float4x4)\nCreates a new anchor object with the specified transform.\nvar name: String?\nA descriptive name for the anchor."
  },
  {
    "title": "init(transform:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/aranchor/2867985-init",
    "html": "Parameters\ntransform\n\nA matrix encoding the position, orientation, and scale of the anchor relative to the world coordinate space of the AR session the anchor is placed in.\n\nWorld coordinate space in ARKit always follows a right-handed convention, but is oriented based on the session configuration. For details, see Understanding World Tracking.\n\nDiscussion\n\nUse the add(anchor:) method to begin tracking your custom anchor in an AR session.\n\nSee Also\nCreating Anchors\ninit(name: String, transform: simd_float4x4)\nCreates a new anchor object with the specified transform and a descriptive name.\nvar name: String?\nA descriptive name for the anchor."
  },
  {
    "title": "HandAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handanchor",
    "html": "Topics\nGetting hand information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a hand in world space.\nvar handSkeleton: HandSkeleton?\nThe current position and orientation of joints on a hand.\nvar chirality: HandAnchor.Chirality\nA value that indicates a left or right hand.\nenum HandAnchor.Chirality\nThe values identifying hand chirality.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking this hand.\nvar description: String\nA textual description of a hand anchor.\nIdentifying hand anchors\nvar id: UUID\nA globally unique identifier for a hand anchor.\ntypealias HandAnchor.ID\nThe type that identifies a hand anchor.\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when HandAnchor conforms to AnyObject.\nRelationships\nConforms To\nSendable\nTrackableAnchor\nSee Also\nHand tracking\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nclass HandTrackingProvider\nA source of live data about the position of a person’s hands and hand joints.\nstruct HandSkeleton\nA collection of joints in a hand."
  },
  {
    "title": "HandSkeleton | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handskeleton",
    "html": "Topics\nRetrieving specific hand joints\nfunc joint(HandSkeleton.JointName) -> HandSkeleton.Joint\nRetrieves a hand joint based on the joint name you specify.\nstruct HandSkeleton.Joint\nThe name and position of an individual hand joint.\nenum HandSkeleton.JointName\nThe names of different hand joints.\nInspecting hand skeletons\nvar allJoints: [HandSkeleton.Joint]\nAll of the joints in a hand skeleton.\nvar description: String\nA textual representation of a hand skeleton.\nComparing hand joint positions\nstatic var neutralPose: HandSkeleton\nA hand pose that you can use as a reference.\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nHand tracking\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nclass HandTrackingProvider\nA source of live data about the position of a person’s hands and hand joints.\nstruct HandAnchor\nA hand’s position in a person’s surroundings."
  },
  {
    "title": "MeshAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/meshanchor",
    "html": "Topics\nGetting mesh information\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a mesh in world space.\nvar geometry: MeshAnchor.Geometry\nThe shape of a mesh anchor.\nstruct MeshAnchor.Geometry\nThe shapes that make up a mesh anchor.\nenum MeshAnchor.MeshClassification\nThe kinds of classification a face of a mesh can have.\nInspecting mesh anchors\nvar id: UUID\ntypealias MeshAnchor.ID\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when MeshAnchor conforms to AnyObject.\nvar description: String\nRelationships\nConforms To\nAnchor\nSendable\nSee Also\nScene reconstruction\nIncorporating real-world surroundings in an immersive experience\nCreate an immersive experience by making your app’s content respond to the local shape of the world.\nclass SceneReconstructionProvider\nA source of live data about the shape of a person’s surroundings."
  },
  {
    "title": "Objective-C compatibility | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_visionos_c_api/objective-c_compatibility",
    "html": "Topics\nObjective-C compatibility types\nprotocol OS_ar_anchor\nThe identity, location, and orientation of an object in world space.\nBeta\nprotocol OS_ar_trackable_anchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nBeta\nprotocol OS_ar_authorization_result\nAn authorization result.\nBeta\nprotocol OS_ar_authorization_results\nA collection of authorization results.\nBeta\nprotocol OS_ar_data_provider\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_device_anchor\nThe position and orientation of Apple Vision Pro.\nBeta\nprotocol OS_ar_error\nAn error reported by ARKit.\nBeta\nprotocol OS_ar_data_providers\nA source of live data from ARKit.\nBeta\nprotocol OS_ar_geometry_element\nA container for vertex indices of lines or triangles.\nBeta\nprotocol OS_ar_geometry_source\nA container for geometrical vector data.\nBeta\nprotocol OS_ar_hand_anchor\nA hand’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_hand_tracking_provider\nA source of live data about the position of a person’s hands and hand joints.\nBeta\nprotocol OS_ar_hand_tracking_configuration\nBeta\nprotocol OS_ar_image_tracking_configuration\nBeta\nprotocol OS_ar_plane_detection_configuration\nBeta\nprotocol OS_ar_plane_extent\nThe size of a plane.\nBeta\nprotocol OS_ar_reference_images\nA collection of reference images.\nBeta\nprotocol OS_ar_scene_reconstruction_configuration\nBeta\nprotocol OS_ar_session\nThe main entry point for receiving data from ARKit.\nBeta\nprotocol OS_ar_world_tracking_configuration\nBeta\nprotocol OS_ar_image_anchor\nA 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_image_anchors\nA collection of image anchors.\nBeta\nprotocol OS_ar_image_tracking_provider\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchor\nA source of live data about a 2D image’s position in a person’s surroundings.\nBeta\nprotocol OS_ar_mesh_anchors\nA collection of mech anchors.\nBeta\nprotocol OS_ar_mesh_geometry\nThe shapes that make up a mesh anchor.\nBeta\nprotocol OS_ar_plane_anchor\nAn anchor that represents horizontal and vertical planes.\nBeta\nprotocol OS_ar_plane_anchors\nA collection of plane anchors.\nBeta\nprotocol OS_ar_plane_detection_provider\nA source of live data about planes in a person’s surroundings.\nBeta\nprotocol OS_ar_plane_geometry\nThe geometry of a plane anchor.\nBeta\nprotocol OS_ar_pose\nThe 3D position (x, y, and z) and rotation (yaw, pitch, and roll) of an object.\nBeta\nprotocol OS_ar_reference_image\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nBeta\nprotocol OS_ar_scene_reconstruction_provider\nA source of live data about the shape of a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchor\nA fixed location in a person’s surroundings.\nBeta\nprotocol OS_ar_world_anchors\nA collection of world anchors.\nBeta\nprotocol OS_ar_world_tracking_provider\nA source of live data about the device pose and anchors in a person’s surroundings.\nBeta\nvar AR_OBJECT_USE_OBJC: Int32\nBeta"
  },
  {
    "title": "WorldAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldanchor",
    "html": "Overview\n\nARKit persists world anchor UUIDs and transforms across multiple runs of your app. For more information, see Tracking specific points in world space.\n\nTopics\nCreating a world anchor\ninit(originFromAnchorTransform: simd_float4x4)\nCreates a world anchor from a position and orientation in world space.\nIdentifying a world anchor\nvar id: UUID\nA unique identifier for a world anchor.\ntypealias WorldAnchor.ID\nThe type that identifies a world anchor.\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when WorldAnchor conforms to AnyObject.\nInspecting a world anchor\nvar originFromAnchorTransform: simd_float4x4\nThe position and orientation of a world anchor.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is currently tracking a world anchor.\nvar description: String\nA textual description of a world anchor.\nRelationships\nConforms To\nSendable\nTrackableAnchor\nSee Also\nWorld tracking\nTracking specific points in world space\nRetrieve the position and orientation of anchors your app stores in ARKit.\nclass WorldTrackingProvider\nA source of live data about the device pose and anchors in a person’s surroundings.\nstruct DeviceAnchor\nThe position and orientation of Apple Vision Pro."
  },
  {
    "title": "DataProviderState | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/dataproviderstate",
    "html": "Topics\nGetting the state of a data provider\ncase initialized\nThe data provider has been created.\ncase running\nThe data provider is running.\ncase stopped\nThe data provider is stopped.\ncase paused\nThe data provider is paused.\nComparing data provider states\nvar description: String\nvar hashValue: Int\nfunc hash(into: inout Hasher)\nstatic func == (DataProviderState, DataProviderState) -> Bool\nstatic func != (DataProviderState, DataProviderState) -> Bool\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nSetup\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nclass ARKitSession\nThe main entry point for receiving data from ARKit.\nprotocol DataProvider\nA source of live data from ARKit.\nprotocol Anchor\nThe identity, location, and orientation of an object in world space.\nprotocol TrackableAnchor\nAn anchor that can gain and lose its tracking state over the course of a session."
  },
  {
    "title": "DeviceAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/deviceanchor",
    "html": "Overview\n\nYou create a device anchor by starting an ARKitSession with a WorldTrackingProvider and calling its queryDeviceAnchor(atTimestamp:) method.\n\nTopics\nInspecting a device anchor\nvar originFromAnchorTransform: simd_float4x4\nThe transform from the device to the origin coordinate system.\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is tracking the device.\nvar description: String\nA textual description of a device anchor.\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when DeviceAnchor conforms to AnyObject.\nvar id: UUID\nA globally unique anchor for a device anchor.\ntypealias DeviceAnchor.ID\nThe type that identifies device anchors.\nRelationships\nConforms To\nSendable\nTrackableAnchor\nSee Also\nWorld tracking\nTracking specific points in world space\nRetrieve the position and orientation of anchors your app stores in ARKit.\nclass WorldTrackingProvider\nA source of live data about the device pose and anchors in a person’s surroundings.\nstruct WorldAnchor\nA fixed location in a person’s surroundings."
  },
  {
    "title": "PlaneAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planeanchor",
    "html": "Topics\nInspecting a plane anchor\nvar originFromAnchorTransform: simd_float4x4\nThe location and orientation of a plane in world space.\nvar alignment: PlaneAnchor.Alignment\nThe alignment — horizontal or vertical — of a plane anchor relative to gravity.\nenum PlaneAnchor.Alignment\nThe kinds of alignment — horizontal or vertical — that a plane anchor can have.\nvar classification: PlaneAnchor.Classification\nThe kind of real-world object that ARKit determined this plane anchor might be.\nenum PlaneAnchor.Classification\nThe kinds of object classification a plane anchor can have.\nvar description: String\nA textual description of a plane anchor.\nGetting the shape of a plane anchor\nvar geometry: PlaneAnchor.Geometry\nThe shape of a plane anchor.\nstruct PlaneAnchor.Geometry\nThe geometry of a plane anchor.\nIdentifying a plane anchor\nvar id: UUID\ntypealias PlaneAnchor.ID\nvar id: ObjectIdentifier\nThe stable identity of the entity associated with this instance.\nAvailable when PlaneAnchor conforms to AnyObject.\nRelationships\nConforms To\nAnchor\nSendable\nSee Also\nPlane detection\nPlacing content on detected planes\nDetect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.\nclass PlaneDetectionProvider\nA source of live data about planes in a person’s surroundings."
  },
  {
    "title": "TrackableAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/trackableanchor",
    "html": "Topics\nChecking an anchor’s tracking state\nvar isTracked: Bool\nA Boolean value that indicates whether ARKit is tracking an anchor.\n\nRequired\n\nRelationships\nInherits From\nAnchor\nConforming Types\nDeviceAnchor\nHandAnchor\nImageAnchor\nWorldAnchor\nSee Also\nSetup\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nclass ARKitSession\nThe main entry point for receiving data from ARKit.\nprotocol DataProvider\nA source of live data from ARKit.\nenum DataProviderState\nThe possible states of a data provider.\nprotocol Anchor\nThe identity, location, and orientation of an object in world space."
  },
  {
    "title": "ARConfiguration.VideoFormat | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/videoformat",
    "html": "Overview\n\nThis class is immutable; to set the frame rate and video resolution for an AR session, set your configuration's videoFormat property to one of the formats in the supportedVideoFormats array.\n\nTopics\nAccessing format information\nvar framesPerSecond: Int\nThe rate at which the session captures video and provides AR frame information.\nvar imageResolution: CGSize\nThe size, in pixels, of video images captured in the session.\nvar isRecommendedForHighResolutionFrameCapturing: Bool\nDetermines whether the framework considers a format suitable for high-resolution frame capture.\nvar isVideoHDRSupported: Bool\nDetermines whether the format supports high dynamic range (HDR).\nInspecting the video source\nvar captureDevicePosition: AVCaptureDevice.Position\nThe position of the capture device.\nenum AVCaptureDevice.Position\nConstants that indicate the physical position of a capture device.\nvar captureDeviceType: AVCaptureDevice.DeviceType\nThe camera that supplies the video format.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nSee Also\nManaging video capture options\nvar videoFormat: ARConfiguration.VideoFormat\nA camera type, resolution, and frame rate for an AR session.\nclass var supportedVideoFormats: [ARConfiguration.VideoFormat]\nThe set of video capture formats available on the current device.\nvar videoHDRAllowed: Bool\nEnables high dynamic range (HDR) for the session's camera feed.\nclass var configurableCaptureDeviceForPrimaryCamera: AVCaptureDevice?\nAn object that enables you to alter the appearance of a frame's captured image.\nclass var recommendedVideoFormatFor4KResolution: ARConfiguration.VideoFormat?\nProvides a 4K video format if the device and configuration support it.\nclass var recommendedVideoFormatForHighResolutionFrameCapturing: ARConfiguration.VideoFormat?\nReturns a video format that the framework recommends for high-resolution-still-image capture."
  },
  {
    "title": "supportedVideoFormats | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/2942261-supportedvideoformats",
    "html": "Discussion\n\nBy default, the videoFormat property for a new session configuration matches the first video capture format in this array. To change the video format for a session, change that property's value to one of the other ARConfiguration.VideoFormat objects in this array.\n\nImportant\n\nARConfiguration is an abstract base class, so its implementation of this property always returns an empty array. Read this property from the configuration subclass you plan to use for your AR session, such as ARWorldTrackingConfiguration or ARFaceTrackingConfiguration.\n\nDifferent devices and iOS versions offer different sets of supported video formats, but the order of this array always puts higher-quality formats before lower-quality formats. For best results across all devices and versions, choose formats based on their order in the array rather than on hard-coded minimum resolution or frame rate values.\n\nSee Also\nManaging video capture options\nvar videoFormat: ARConfiguration.VideoFormat\nA camera type, resolution, and frame rate for an AR session.\nclass ARConfiguration.VideoFormat\nA video size and frame rate specification for use with an AR session.\nvar videoHDRAllowed: Bool\nEnables high dynamic range (HDR) for the session's camera feed.\nclass var configurableCaptureDeviceForPrimaryCamera: AVCaptureDevice?\nAn object that enables you to alter the appearance of a frame's captured image.\nclass var recommendedVideoFormatFor4KResolution: ARConfiguration.VideoFormat?\nProvides a 4K video format if the device and configuration support it.\nclass var recommendedVideoFormatForHighResolutionFrameCapturing: ARConfiguration.VideoFormat?\nReturns a video format that the framework recommends for high-resolution-still-image capture."
  },
  {
    "title": "AnchorUpdate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchorupdate",
    "html": "Topics\nInspecting anchor updates\nlet anchor: AnchorType\nThe anchor that this anchor update contains information about.\nvar timestamp: TimeInterval\nThe time when this anchor update occurred.\nlet event: AnchorUpdate<AnchorType>.Event\nAn event that indicates whether an anchor was added, updated, or removed.\nenum AnchorUpdate.Event\nThe events that cause anchor updates.\nvar description: String\nA textual representation of this anchor update.\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nTracking anchors over time\nstruct AnchorUpdateSequence\nAn asynchronous sequence of updates to anchors."
  },
  {
    "title": "videoFormat | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/2942260-videoformat",
    "html": "Discussion\n\nThe ARConfiguration.VideoFormat class is immutable; to set the frame rate and video resolution for an AR session, choose one of the video formats in the supportedVideoFormats array.\n\nSee Also\nManaging video capture options\nclass var supportedVideoFormats: [ARConfiguration.VideoFormat]\nThe set of video capture formats available on the current device.\nclass ARConfiguration.VideoFormat\nA video size and frame rate specification for use with an AR session.\nvar videoHDRAllowed: Bool\nEnables high dynamic range (HDR) for the session's camera feed.\nclass var configurableCaptureDeviceForPrimaryCamera: AVCaptureDevice?\nAn object that enables you to alter the appearance of a frame's captured image.\nclass var recommendedVideoFormatFor4KResolution: ARConfiguration.VideoFormat?\nProvides a 4K video format if the device and configuration support it.\nclass var recommendedVideoFormatForHighResolutionFrameCapturing: ARConfiguration.VideoFormat?\nReturns a video format that the framework recommends for high-resolution-still-image capture."
  },
  {
    "title": "originFromAnchorTransform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchor/4293508-originfromanchortransform",
    "html": "Required\n\nSee Also\nInspecting an anchor\nvar id: UUID\nA unique identifier that distinguishes this anchor from all other anchors.\n\nRequired"
  },
  {
    "title": "ARConfiguration.WorldAlignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/worldalignment",
    "html": "Topics\nAlignments\ncase gravity\nThe coordinate system's y-axis is parallel to gravity, and its origin is the initial position of the device.\ncase gravityAndHeading\nThe coordinate system's y-axis is parallel to gravity, its x- and z-axes are oriented to compass heading, and its origin is the initial position of the device.\ncase camera\nThe scene coordinate system is locked to match the orientation of the camera.\nRelationships\nConforms To\nSendable\nSee Also\nConfiguring the AR session\nvar isLightEstimationEnabled: Bool\nA Boolean value specifying whether ARKit analyzes scene lighting in captured camera images.\nvar worldAlignment: ARConfiguration.WorldAlignment\nA value specifying how the session maps real-world device motion into a 3D scene coordinate system."
  },
  {
    "title": "id | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchor/4108368-id",
    "html": "Required\n\nSee Also\nInspecting an anchor\nvar originFromAnchorTransform: simd_float4x4\nThe position and orientation of this anchor in world space.\n\nRequired"
  },
  {
    "title": "worldAlignment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/2923550-worldalignment",
    "html": "Discussion\n\nCreating an AR experience depends on being able to construct a coordinate system for placing objects in a virtual 3D world that maps to the real-world position and motion of the device. When you run a session configuration, ARKit creates a scene coordinate system based on the position and orientation of the device; any ARAnchor objects you create or that the AR session detects are positioned relative to that coordinate system.\n\nSee ARConfiguration.WorldAlignment for possible values.\n\nSee Also\nConfiguring the AR session\nvar isLightEstimationEnabled: Bool\nA Boolean value specifying whether ARKit analyzes scene lighting in captured camera images.\nenum ARConfiguration.WorldAlignment\nOptions for how ARKit constructs a scene coordinate system based on real-world device motion."
  },
  {
    "title": "isLightEstimationEnabled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/2923546-islightestimationenabled",
    "html": "Discussion\n\nWhen this value is true (the default), a running AR session provides scene lighting information in the lightEstimate property of each ARFrame object it captures.\n\nIf you render your own overlay graphics for the AR scene, you can use this information in shading algorithms to help make those graphics match the real-world lighting conditions of the scene captured by the camera. (The ARSCNView class automatically uses this information to configure SceneKit lighting.)\n\nSee Also\nConfiguring the AR session\nvar worldAlignment: ARConfiguration.WorldAlignment\nA value specifying how the session maps real-world device motion into a 3D scene coordinate system.\nenum ARConfiguration.WorldAlignment\nOptions for how ARKit constructs a scene coordinate system based on real-world device motion."
  },
  {
    "title": "ARConfiguration.FrameSemantics | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/framesemantics",
    "html": "Overview\n\nA frame semantic represents 2D information that ARKit extracts from a frame.\n\nTopics\nCreating a Feature\ninit(rawValue: UInt)\nCreates a frame semantics feature.\nTracking Bodies in 2D\nstatic var bodyDetection: ARConfiguration.FrameSemantics\nAn option that indicates that 2D body detection is enabled.\nOccluding Virtual Content with People\nstatic var personSegmentation: ARConfiguration.FrameSemantics\nAn option that indicates that people occlude your app's virtual content.\nstatic var personSegmentationWithDepth: ARConfiguration.FrameSemantics\nAn option that indicates that people occlude your app's virtual content depending on depth.\nAccessing Depth\nstatic var sceneDepth: ARConfiguration.FrameSemantics\nAn option that provides the distance from the device to real-world objects viewed through the camera.\nstatic var smoothedSceneDepth: ARConfiguration.FrameSemantics\nAn option that provides the distance from the device to real-world objects, averaged across several frames.\nRelationships\nConforms To\nOptionSet\nSendable\nSee Also\nEnabling frame features\nvar frameSemantics: ARConfiguration.FrameSemantics\nThe set of active semantics on the frame.\nclass func supportsFrameSemantics(ARConfiguration.FrameSemantics) -> Bool\nChecks whether a particular feature is supported."
  },
  {
    "title": "Configuration Objects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/configuration_objects",
    "html": "Overview\n\nConfiguration objects define how ARKit sets up and runs your augmented reality session. Although ARWorldTrackingConfiguration provides the widest range of features in a rear-camera experience, each feature consumes device energy and compute cycles. So to maximize device uptime and performance, enable additional options sparingly.\n\nIf another AR configuration fulfills your requirements with a more concise feature set, use that configuration instead. For example, use ARBodyTrackingConfiguration instead of a world-tracking configuration for 3D motion-capture if you don’t need user face-tracking, collaboration, or scene reconstruction.\n\nSelect frame features\n\nSome configurations support subfeatures that relate to a session's frame. Enable these features by setting the following flags in the configuration's frameSemantics:\n\nbodyDetection\n\nEnables 2D human body tracking.\n\npersonSegmentation\n\nEnables people occlusion.\n\npersonSegmentationWithDepth\n\nEnables people occlusion based on whether the people in the camera feed are closer to the camera than the virtual content.\n\nUse supportsFrameSemantics(_:) to verify whether the iOS device supports the ARConfiguration.FrameSemantics you desire before setting frameSemantics.\n\nImportant\n\nTo maximize device responsiveness, refrain from turning on people occlusion for single-user experiences when you don’t expect people in the scene.\n\nSwitch configurations at runtime\n\nTo toggle features like plane detection, frame semantics, and environment texturing, you switch your configuration at runtime by calling run(with:) on your existing session. Where possible, ARKit maintains all the information collected during the session under the prior configuration, such as information about the physical environment and anchors.\n\nTip\n\nYou can gracefully downgrade the AR experience in the event of low-power or thermal events. For example, you could temporarily switch from a world-tracking configuration to a position-tracking configuration (ARPositionalTrackingConfiguration) if your app can function at a basic level in that limited capacity until the device cools down.\n\nIf your session switches between face- and world-tracking configurations, the session doesn't maintain state.\n\nEnable high-quality video and custom capture settings\n\nIn iOS 16, you can enable a 4K and high dynamic range (HDR) video format. In addition, you can customize your session's video settings through the underlying AV capture device.\n\nTo determine whether your session supports 4K, call recommendedVideoFormatFor4KResolution.\n\nguard let hiResFormat = ARWorldTrackingConfiguration.recommendedVideoFormatFor4KResolution else {\n   print(\"4K video format not supported.\"); return }\n\n\nThen, create a configuration with the format. You can also indicate the intent to enable HDR by setting videoHDRAllowed to true.\n\nvar config = ARWorldTrackingConfiguration()\nconfig.videoFormat = hiResFormat\nconfig.videoHDRAllowed = true\nsession.run(config)\n\n\nIf the device supports a configurable capture session, the configurableCaptureDeviceForPrimaryCamera provides the underlying capture device that you can adjust as needed.\n\nif let device = ARWorldTrackingConfiguration.configurableCaptureDeviceForPrimaryCamera {\n   do { try device.lockForConfiguration()\n      // Configure capture settings here.\n      device.unlockForConfiguration()\n   } catch { /* Error handling. */ }\n}\n\n\nCapture high-resolution still frames\n\nIn iOS 16, you can enable high-resolution frame capture by calling recommendedVideoFormatForHighResolutionFrameCapturing on your configuration. If the device supports high-resolution stills, the function returns a video format you can use to start a session:\n\nguard let hiResFormat = type(of: config).recommendedVideoFormatForHighResolutionFrameCapturing else {\n    fatalError(\"The device doesn't support high-resolution stills.\") }\nconfig.videoFormat = hiResFormat\narSession.run(config)\n\n\nDuring the session, capture a high-resolution still frame at any time by calling captureHighResolutionFrame(completion:):\n\narSession.captureHighResolutionFrame { (frame, error) in\n    if let frame = frame {\n        saveHiResFrame(frame)\n    } else { /* Error handling. */ }\n\n\nTopics\nCommon Configuration Details\nclass ARConfiguration\nThe base object that contains information about how to configure an augmented reality session.\nSpatial Tracking\nUnderstanding World Tracking\nDiscover features and best practices for building rear-camera AR experiences.\nclass ARWorldTrackingConfiguration\nA configuration that tracks the position of a device in relation to objects in the environment.\nclass ARGeoTrackingConfiguration\nA configuration that tracks locations with GPS, map data, and a device's compass.\nclass AROrientationTrackingConfiguration\nA configuration that tracks only the device’s orientation using the rear-facing camera.\nclass ARPositionalTrackingConfiguration\nA configuration that tracks only the device’s position in 3D space.\nBody and Face Tracking\nclass ARBodyTrackingConfiguration\nA configuration that tracks human body poses, planar surfaces, and images using the rear-facing camera.\nclass ARFaceTrackingConfiguration\nA configuration that tracks facial movement and expressions using the front camera.\nImage Recognition\nclass ARImageTrackingConfiguration\nA configuration that tracks known images using the rear-facing camera.\nObject Detection\nclass ARObjectScanningConfiguration\nA configuration that recognizes objects and collects high-fidelity data about specific objects using the rear-facing camera.\nSee Also\nSetup\nChoosing Which Camera Feed to Augment\nAdd visual effects to the user's environment in an AR experience through the front or rear camera.\nManaging Session Life Cycle and Tracking Quality\nKeep the user informed on the current session state and recover from interruptions.\nDisplaying an AR Experience with Metal\nControl rendering of your app's virtual content on top of a camera feed.\nclass ARSession\nThe object that manages the major tasks associated with every AR experience, such as motion tracking, camera passthrough, and image analysis."
  },
  {
    "title": "supportsFrameSemantics(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/3089122-supportsframesemantics",
    "html": "Parameters\nframeSemantics\n\nThe frame semantics for which to check device support.\n\nReturn Value\n\nA boolean value that indicates whether the device supports the argument frame semantics.\n\nDiscussion\n\nCall this function before attempting to enable a frame semantic on your app's configuration. For example, if you call supportsFrameSemantic(.sceneDepth) on ARWorldTrackingConfiguration, the function returns true on devices that support the LiDAR scanner's depth buffer.\n\nWarning\n\nDo not call this function on the superclass, ARConfiguration. Only configuration subclasses support frame semantics, such as those listed in Choose your session's configuration.\n\nSee Also\nEnabling frame features\nvar frameSemantics: ARConfiguration.FrameSemantics\nThe set of active semantics on the frame.\nstruct ARConfiguration.FrameSemantics\nTypes of optional frame features you can enable in your app."
  },
  {
    "title": "frameSemantics | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/3089121-framesemantics",
    "html": "Discussion\n\nYou can choose whether ARKit reports information about a particular per-frame metric, or semantic. Before enabling a frame sementic, call supportsFrameSemantics(_:) to ensure device support.\n\nEnable 2D Body Detection\n\nTo get information about the 2D location of a person that ARKit recognizes in a frame, you enable the bodyDetection frame semantic.\n\nif let config = mySession.configuration as? ARBodyTrackingConfiguration {\n    config.frameSemantics.insert(.bodyDetection)\n    // Run the configuration to effect a frame semantics change.\n    mySession.run(config)\n}\n\n\n\n\nEnable People Occlusion\n\nPeople occlusion is a feature that enables people in the camera feed to cover your app’s virtual content.\n\nTo indicate that a person should overlap your app's virtual content when the person is closer to the camera than the virtual content, add the personSegmentationWithDepth option to your configuration's frame semantics.\n\nif let config = mySession.configuration as? ARWorldTrackingConfiguration {\n    config.frameSemantics.insert(.personSegmentationWithDepth)\n    // Run the configuration to effect a frame semantics change.\n    mySession.run(config)\n}\n\n\n\n\nTo indicate that a person should overlap your app's virtual content regardless of the person's depth in the scene, use the personSegmentation frame semantic instead. This option is particularly appropriate for green-screen scenarios.\n\nStandard renderers (ARView, and ARSCNView) implement people occlusion for you. See Occluding Virtual Content with People for a sample app that demonstrates people occlusion in RealityKit.\n\nIf you implement your own renderer, use segmentationBuffer and estimatedDepthData to implement people occlusion yourself. ARMatteGenerator helps you by providing masks. For a sample app that demonstrates matte generator and people occlusion, see Effecting People Occlusion in Custom Renderers.\n\nIf you enable Scene Reconstruction, ARKit adjusts the mesh according to any people ARKit may detect in the camera feed. ARKit removes any part of the scene mesh that overlaps with people, as defined by the with- or without-depth frame semantics. For more information about scene reconstruction, see Visualizing and Interacting with a Reconstructed Scene.\n\nSee Also\nEnabling frame features\nstruct ARConfiguration.FrameSemantics\nTypes of optional frame features you can enable in your app.\nclass func supportsFrameSemantics(ARConfiguration.FrameSemantics) -> Bool\nChecks whether a particular feature is supported."
  },
  {
    "title": "Managing Session Life Cycle and Tracking Quality | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/managing_session_life_cycle_and_tracking_quality",
    "html": "Overview\n\nWorld-tracking AR sessions use a technique called visual-inertial odometry. This process combines motion sensor data with computer vision analysis of camera imagery to track the device's position and orientation in real-world space, also known as pose, which is expressed in the ARCamera transform property. For best results, world tracking needs consistent sensor data and camera imagery with visual complexity or recognizable features.\n\nWhen you start a session, it takes some time for ARKit to gather enough data to precisely model device pose. During a session, the conditions that affect world-tracking quality can change. Use ARSessionObserver delegate methods and ARCamera properties to follow these changes.\n\nBasic Lifecycle of an AR Session\n\nThe figure below shows the changes in tracking state when you start running an AR session.\n\nImmediately after you run a new session, the tracking state for provided frames is ARCamera.TrackingState.notAvailable, indicating that ARKit has not yet gathered enough information to estimate the device’s pose.\n\nA few frames later, the tracking state changes to ARCamera.TrackingState.limited(_:), indicating that a device pose is available but its accuracy is uncertain. A limited state always includes a reason for reduced tracking quality; in this case, the session is still ARCamera.TrackingState.Reason.initializing.\n\nAfter a short time, the tracking state changes to ARCamera.TrackingState.normal, indicating that the device pose is accurate and all ARKit features are available.\n\nProvide Feedback for Tracking Quality Changes\n\nThe figure below shows changes in tracking state that can occur due to user interaction or changes in the environment.\n\nWhen tracking quality is ARCamera.TrackingState.limited(_:), features that depend on ARKit mapping the user's local environment are not available:\n\nPlane detection does not add or update plane anchors\n\nHit-testing methods provide no results\n\nA session can enter a ARCamera.TrackingState.limited(_:) tracking state at any time, based on changes in the user's local environment or the user moving the device. For example, if the user points the device at a blank wall, or the lights in the room go out, tracking quality may be reduced due to ARCamera.TrackingState.Reason.insufficientFeatures.\n\nUse the associated ARCamera.TrackingState.Reason value to provide feedback that guides the user to resolving the situation so that the tracking state can return to ARCamera.TrackingState.normal.\n\nRecover from Session Interruptions\n\nARKit can't track device pose without a running ARSession. By default, if your session is interrupted (for example, by switching to another app), any virtual content in that session is likely out of place relative to the real-world environment.\n\nYou can use relocalization to try to recover from an interruption. If you return true from the sessionShouldAttemptRelocalization(_:) method, ARKit attempts to reconcile its knowledge of the user's environment from before the interruption with current camera and sensor data. During this process, the tracking state is ARCamera.TrackingState.limited(_:) (with ARCamera.TrackingState.Reason.relocalizing as the reason). If successful, the tracking state returns to ARCamera.TrackingState.normal after a short time.\n\nFor relocalization to succeed, the device must be returned to a position and orientation near where it was when the session was interrupted. If these conditions never occur (for example, if the device has moved to an entirely different environment), the session remains in the ARCamera.TrackingState.Reason.relocalizing state indefinitely.\n\nImportant\n\nWhen your app is in the ARCamera.TrackingState.Reason.relocalizing state, offer the user a way to reset the session (with run(_:options:) and resetTracking) in case relocalization never succeeds.\n\nCreate a Persistent AR Experience\n\nIn iOS 12.0 and later, the ARWorldMap class stores the information that ARKit uses to resume a session. By saving a world map to a file, you can use the same relocalization process either to recover from a brief interruption or to resume from an earlier session, even if your app has relaunched. World maps include anchors, so you can also replace virtual content to match an earlier session.\n\nImportant\n\nThe reliability of using ARWorldMap to resume a session strongly depends on the real-world environment. For example, it's easy to successfully relocalize to a map recorded indoors under consistent artificial lighting, or to a map captured only moments beforehand. Success is less likely when lighting conditions or features of the local environment have changed over time.\n\nTo allow the user to come back to the same AR session after leaving your app, you might save the world map explicitly upon a user action, or automatically in applicationDidEnterBackground(_:). Save the world map only if your AR session has state worth saving—for example, if the user has placed virtual objects whose positions you want to remember, and the session is in the ARFrame.WorldMappingStatus.mapped state (or has been in that state at least once during the session).\n\nTo relocalize to a saved world map, use the initialWorldMap property when running a session. Like when resuming from an interruption, the session starts in the ARCamera.TrackingState.limited(_:) (ARCamera.TrackingState.Reason.relocalizing) tracking state. If ARKit can reconcile the world map with the current environment, the tracking state becomes ARCamera.TrackingState.normal after a short time, indicating that the session matches the recorded world map.\n\nTip\n\nFor relocalization to succeed, the device needs to visit areas of the local environment that it passed through before creating the map—you might assist the user with this task by saving a screenshot with the world map and displaying it as a placement guide when attempting to relocalize.\n\nA session resumed from a world map includes all anchors saved in that world map. If you use the name property to identify virtual objects you've placed anchors for, you can refer to the anchors in the resumed session to recreate that virtual content. To ensure that such content is placed correctly, display it only after the session's tracking state changes to ARCamera.TrackingState.normal.\n\nIf ARKit cannot reconcile the recorded world map with the current environment (for example, if the device is in an entirely different place from where the world map was recorded), the session remains in the ARCamera.TrackingState.Reason.relocalizing state indefinitely. Provide users with a way to restart the session in case they can't resume it. To give up on world map relocalization, call run(_:options:) on the session again, with the resetTracking option and a configuration whose initialWorldMap is nil.\n\nSee Also\nSetup\nChoosing Which Camera Feed to Augment\nAdd visual effects to the user's environment in an AR experience through the front or rear camera.\nDisplaying an AR Experience with Metal\nControl rendering of your app's virtual content on top of a camera feed.\nclass ARSession\nThe object that manages the major tasks associated with every AR experience, such as motion tracking, camera passthrough, and image analysis.\nConfiguration Objects\nConfigure your augmented reality session to detect and track specific types of content.\nRelated Documentation\nTracking and altering images\nCreate images from rectangular shapes found in the user’s environment, and augment their appearance."
  },
  {
    "title": "Choosing Which Camera Feed to Augment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios/choosing_which_camera_feed_to_augment",
    "html": "Overview\n\niOS devices come equipped with two cameras, and for each ARKit session you need to choose which camera's feed to augment. ARKit 3 and later provide simultaneous anchors from both cameras (see Combining User Face-Tracking and World Tracking), but you still must choose one camera feed to show to the user at a time.\n\nAugmented Reality with the Rear Camera\n\nThe most common kinds of AR experience display a view from the device's rear camera, augmented by other visual content, giving the user a new way to see and interact with the world around them.\n\nARWorldTrackingConfiguration provides this kind of experience: ARKit tracks the real-world the user inhabits, and matches it with a coordinate space for you to place virtual content. World tracking also offers features to make AR experiences more immersive, like the ability to recognize objects and images in the user's environment and respond to real-world lighting conditions.\n\nAugmented Reality with the Front Camera\n\nFor iOS devices that have a TrueDepth camera, ARFaceTrackingConfiguration enables you to augment the front-camera feed, while providing you with real-time tracking for the pose and expression of faces. With that information, that you might, for example, choose to overlay realistic virtual masks. Or, you might omit the camera view and use facial expression data to animate virtual characters, as in the Animoji app for iMessage.\n\nSee Also\nSetup\nManaging Session Life Cycle and Tracking Quality\nKeep the user informed on the current session state and recover from interruptions.\nDisplaying an AR Experience with Metal\nControl rendering of your app's virtual content on top of a camera feed.\nclass ARSession\nThe object that manages the major tasks associated with every AR experience, such as motion tracking, camera passthrough, and image analysis.\nConfiguration Objects\nConfigure your augmented reality session to detect and track specific types of content."
  },
  {
    "title": "SceneReconstructionProvider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/scenereconstructionprovider",
    "html": "Topics\nCreating a scene reconstruction provider\ninit(modes: [SceneReconstructionProvider.Mode])\nCreates a provider that reconstructs the person’s surroundings.\nlet modes: [SceneReconstructionProvider.Mode]\nThe modes of scene reconstruction this provider supplies.\nenum SceneReconstructionProvider.Mode\nThe additional kinds of information you can request about a person’s surroundings.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports scene reconstruction providers.\nObserving scene reconstruction\nvar anchorUpdates: AnchorUpdateSequence<MeshAnchor>\nAn asynchronous sequence of updates to scene meshes that the scene reconstruction provider detects.\nvar state: DataProviderState\nA value that indicates whether the scene reconstruction provider is currently supplying anchor updates.\nInspecting a scene reconstruction provider\nvar description: String\nA textual description of a scene reconstruction provider.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations needed to run scene reconstruction.\nRelationships\nConforms To\nDataProvider\nSendable\nSee Also\nScene reconstruction\nIncorporating real-world surroundings in an immersive experience\nCreate an immersive experience by making your app’s content respond to the local shape of the world.\nstruct MeshAnchor\nA volume of space that contains a mesh of a person’s surroundings."
  },
  {
    "title": "WorldTrackingProvider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/worldtrackingprovider",
    "html": "Topics\nTracking objects\ninit()\nCreates a world tracking provider.\nvar anchorUpdates: AnchorUpdateSequence<WorldAnchor>\nA sequence of updates to anchors this provider tracks.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to track world anchors.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports world tracking providers.\nfunc addAnchor(WorldAnchor)\nAdds a world anchor you supply to the set of currently tracked anchors.\nstruct WorldTrackingProvider.Error\nAn error that can occur during a world-tracking session.\nStopping object tracking\nfunc removeAnchor(WorldAnchor)\nRemoves a world anchor from a world tracking provider.\nfunc removeAnchor(forID: UUID)\nRemoves a world anchor from a world tracking provider based on its ID.\nPredicting device pose\nfunc queryDeviceAnchor(atTimestamp: TimeInterval) -> DeviceAnchor?\nThe predicted pose of the current device at a given time.\nInspecting a world tracking provider\nvar state: DataProviderState\nThe current status of data coming from this provider.\nvar description: String\nA textual description of a world tracking provider.\nRelationships\nConforms To\nDataProvider\nSendable\nSee Also\nWorld tracking\nTracking specific points in world space\nRetrieve the position and orientation of anchors your app stores in ARKit.\nstruct WorldAnchor\nA fixed location in a person’s surroundings.\nstruct DeviceAnchor\nThe position and orientation of Apple Vision Pro."
  },
  {
    "title": "ImageTrackingProvider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/imagetrackingprovider",
    "html": "Topics\nCreating an image tracking provider\ninit(referenceImages: [ReferenceImage])\nCreates an image tracking provider that tracks the reference images you supply.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports image tracking providers.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to track images.\nTracking images\nvar anchorUpdates: AnchorUpdateSequence<ImageAnchor>\nA sequence of updates that provide information about images tracked by this provider.\nInspecting an image tracking provider\nvar state: DataProviderState\nThe current status of data coming from this provider.\nvar description: String\nA textual description of an image tracking provider.\nRelationships\nConforms To\nDataProvider\nSendable\nSee Also\nImage tracking\nTracking preregistered images in 3D space\nPlace content based on the current position of a known image in a person’s surroundings.\nstruct ImageAnchor\nA 2D image’s position in a person’s surroundings.\nstruct ReferenceImage\nA 2D image the system uses as a reference to find the same image in a person’s surroundings."
  },
  {
    "title": "anchorUpdates | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handtrackingprovider/4180435-anchorupdates",
    "html": "See Also\nObserving hand anchor data\nvar latestAnchors: (leftHand: HandAnchor?, rightHand: HandAnchor?)\nThe most recent hand anchors for each hand."
  },
  {
    "title": "HandTrackingProvider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/handtrackingprovider",
    "html": "Topics\nCreating a hand tracking provider\ninit()\nCreates a hand tracking provider.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports hand tracking providers.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to track hands.\nObserving hand anchor data\nvar anchorUpdates: AnchorUpdateSequence<HandAnchor>\nA sequence of updates for all hands that this provider tracks.\nvar latestAnchors: (leftHand: HandAnchor?, rightHand: HandAnchor?)\nThe most recent hand anchors for each hand.\nInspecting a hand tracking provider\nvar state: DataProviderState\nThe current status of data coming from this provider.\nvar description: String\nA textual description of a hand tracking provider.\nRelationships\nConforms To\nDataProvider\nSendable\nSee Also\nHand tracking\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nstruct HandAnchor\nA hand’s position in a person’s surroundings.\nstruct HandSkeleton\nA collection of joints in a hand."
  },
  {
    "title": "PlaneDetectionProvider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/planedetectionprovider",
    "html": "Topics\nDetecting planes\ninit(alignments: [PlaneAnchor.Alignment])\nCreates a plane detection provider for the types of planes you want to detect.\nvar anchorUpdates: AnchorUpdateSequence<PlaneAnchor>\nA sequence of updates to planes this provider detects.\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe types of authorizations required to detect planes.\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports plane detection providers.\nInspecting a plane detection provider\nvar alignments: [PlaneAnchor.Alignment]\nThe plane alignments that you configured this provider to detect.\nvar state: DataProviderState\nThe current status of data coming from this provider.\nvar description: String\nA textual description of this provider.\nRelationships\nConforms To\nDataProvider\nSendable\nSee Also\nPlane detection\nPlacing content on detected planes\nDetect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.\nstruct PlaneAnchor\nAn anchor that represents horizontal and vertical planes."
  },
  {
    "title": "isSupported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/dataprovider/4218762-issupported",
    "html": "Required\n\nDiscussion\n\nFor example, data providers are not supported in Simulator.\n\nSee Also\nInspecting a data provider type\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe kinds of authorization you need to use a particular data provider type.\n\nRequired"
  },
  {
    "title": "requiredAuthorizations | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/dataprovider/4131683-requiredauthorizations",
    "html": "Required\n\nSee Also\nInspecting a data provider type\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports a particular provider type.\n\nRequired"
  },
  {
    "title": "ARSession.CollaborationData | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/collaborationdata",
    "html": "Overview\n\nTo create a multiuser AR experience, you enable collaboration on a world tracking session. ARKit regularly outputs ARSession.CollaborationData that users share with each other, which enables everyone to view the same virtual content from their own perspective. For more information, see isCollaborationEnabled.\n\nTopics\nObserving Priority\nvar priority: ARSession.CollaborationData.Priority\nA property that gives you a hint about how to send a given data instance over the network.\nenum ARSession.CollaborationData.Priority\nOptions that help you choose the appropriate network protocol or settings for a given data instance.\nRelationships\nInherits From\nNSObject\nConforms To\nNSSecureCoding\nSee Also\nManaging collaboration\nfunc update(with: ARSession.CollaborationData)\nUpdates your session with information about the physical environment that is collected by another user."
  },
  {
    "title": "ARSessionProviding | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsessionproviding",
    "html": "Overview\n\nAs an example usage, ARCoachingOverlayView exposes sessionProvider to access your app's current session.\n\nTopics\nProviding a Session\nvar session: ARSession\nA contract to declare an AR session.\n\nRequired\n\nRelationships\nInherits From\nNSObjectProtocol\nConforming Types\nARSCNView\nARSKView\nARView"
  },
  {
    "title": "update(with:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/3214029-update",
    "html": "Discussion\n\nCall this function to update a session when the app receives collaboration data from other users that are participating in a multiuser AR experience. Your app receives this data when multiple users scan different parts of an environment and share that information with your app over the network. For more information, see isCollaborationEnabled.\n\nCollaboration is supported for world tracking configurations only.\n\nSee Also\nManaging collaboration\nclass ARSession.CollaborationData\nAn object that holds information that a user has collected about the physical environment."
  },
  {
    "title": "state | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/dataprovider/4278285-state",
    "html": "Required"
  },
  {
    "title": "ARTrackable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/artrackable",
    "html": "Overview\n\nThis protocol is adopted by ARKit classes, such as the ARFaceAnchor class, that represent moving objects in a scene.\n\nARKit automatically manages representations of such objects in an active AR session, ensuring that changes in the real-world object's position and orientation (the transform property for anchors) are reflected in corresponding ARKit objects. The isTracked property indicates whether the current transform is valid with respect to movement of the real-world object.\n\nTrackable anchor classes affect other ARKit behaviors:\n\nThe getCurrentWorldMap(completionHandler:) method automatically includes only non-trackable anchors in the ARWorldMap it creates. (After you create a world map, you can add other anchors to it if you choose.)\n\nARSCNView and ARSKView automatically hide the nodes for anchors whose isTracked property is false.\n\nWorld-tracking sessions use non-trackable anchors to optimize tracking quality in the area around each anchor. Trackable anchors do not affect world tracking.\n\nTopics\nMonitoring Tracking State\nvar isTracked: Bool\nA Boolean value that indicates whether the object’s transform is valid.\n\nRequired\n\nRelationships\nInherits From\nNSObjectProtocol\nConforming Types\nARAppClipCodeAnchor\nARBodyAnchor\nARFaceAnchor\nARGeoAnchor\nARImageAnchor\nSee Also\nCommon Types\nclass ARAnchor\nAn object that specifies the position and orientation of an item in the physical environment.\nprotocol ARAnchorCopying\nSupport for custom anchor subclasses."
  },
  {
    "title": "run(_:options:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession/2875735-run",
    "html": "Parameters\nconfiguration\n\nAn object that defines motion and scene tracking behaviors for the session.\n\noptions\n\nOptions affecting how existing session state (if any) transitions to the new configuration.\n\nIf the session is running for the first time, this parameter has no effect.\n\nDiscussion\n\nThe session tracks device motion, captures and processes scene imagery from the device camera, and coordinates with your delegate object or ARSCNView or ARSKView view only when running.\n\nCalling this method on a session that has already started transitions immediately to the new session configuration. The options parameter determines how existing session state transitions to the new configuration. By default, the session resumes device position tracking from the last known state and keeps any anchors already included in the session (those you've added manually with add(anchor:), as well as those added automatically by ARKit features such as plane detection or face tracking).\n\nThis method returns immediately when called, but the session continues to run.\n\nSee Also\nConfiguring and running a session\nvar identifier: UUID\nA unique identifier of the running session.\nstruct ARSession.RunOptions\nOptions for transitioning an AR session's current state when you change its configuration.\nvar configuration: ARConfiguration?\nAn object that defines motion and scene tracking behaviors for the session.\nfunc pause()\nPauses processing in the session."
  },
  {
    "title": "ARFaceTrackingConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration",
    "html": "Overview\n\nA face-tracking configuration detects faces within 3 meters of the device’s front camera. When ARKit detects a face, it creates an ARFaceAnchor object that provides information about a person's facial position, orientation, topology, and expressions.\n\nFace tracking supports devices with Apple Neural Engine in iOS 14 and iPadOS 14 and requires a device with a TrueDepth camera on iOS 13 and iPadOS 13 and earlier. To determine whether the device supports face tracking, call isSupported on ARFaceTrackingConfiguration before attempting to use this configuration.\n\nWhen you enable the isLightEstimationEnabled setting, a face-tracking configuration estimates directional and environmental lighting (an ARDirectionalLightEstimate object) by referring to the detected face as a light probe.\n\nNote\n\nBecause face tracking provides your app with personal facial information, your app must include a privacy policy describing to users how you intend to use face tracking and face data. For details, see the Apple Developer Program License Agreement.\n\nTopics\nCreating a Configuration\ninit()\nCreates a new face-tracking configuration.\nEnabling World Tracking\nclass var supportsWorldTracking: Bool\nA Boolean value that indicates whether the iOS device supports tracking the user's facial features in a world-tracking session.\nvar isWorldTrackingEnabled: Bool\nA Boolean value that instructs a session to provide the app with user face data during a world-tracking session.\nTracking Multiple Faces\nvar maximumNumberOfTrackedFaces: Int\nThe number of faces to track during the session.\nclass var supportedNumberOfTrackedFaces: Int\nThe maximum number of faces that the framework can track.\nRelationships\nInherits From\nARConfiguration\nSee Also\nBody and Face Tracking\nclass ARBodyTrackingConfiguration\nA configuration that tracks human body poses, planar surfaces, and images using the rear-facing camera."
  },
  {
    "title": "ARConfiguration | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration",
    "html": "Overview\n\nARConfiguration defines a base class for the different options you can configure in your AR experience.\n\nAll AR configurations establish a correspondence between the real world that the device inhabits and the virtual 3D-coordinate space, where you model content. When your app mixes virtual content with a live-camera image, the user experiences the illusion that your virtual content is part of the real world.\n\nTo acquire the live-camera imagery, ARKit manages a camera-capture pipeline for you. Depending on the configuration you choose, it determines the cameras that capture imagery, and which camera feed the app displays.\n\nAR apps recognize real-world regions of interest. At runtime, ARKit generates an ARAnchor for a real-world object it recognizes, which allows an app to refer to its details, such as size and physical location. The configuration you choose determines the kinds of real-world objects ARKit recognizes and makes available to your app.\n\nDon't allocate ARConfiguration yourself; instead, instantiate one of its subclasses.\n\nFor more information about the camera-capture pipeline, see Choosing Which Camera Feed to Augment.\n\nTopics\nVerifying device support\nclass var isSupported: Bool\nA Boolean value indicating whether the current device supports this session configuration class.\nEnabling frame features\nvar frameSemantics: ARConfiguration.FrameSemantics\nThe set of active semantics on the frame.\nstruct ARConfiguration.FrameSemantics\nTypes of optional frame features you can enable in your app.\nclass func supportsFrameSemantics(ARConfiguration.FrameSemantics) -> Bool\nChecks whether a particular feature is supported.\nConfiguring the AR session\nvar isLightEstimationEnabled: Bool\nA Boolean value specifying whether ARKit analyzes scene lighting in captured camera images.\nvar worldAlignment: ARConfiguration.WorldAlignment\nA value specifying how the session maps real-world device motion into a 3D scene coordinate system.\nenum ARConfiguration.WorldAlignment\nOptions for how ARKit constructs a scene coordinate system based on real-world device motion.\nManaging video capture options\nvar videoFormat: ARConfiguration.VideoFormat\nA camera type, resolution, and frame rate for an AR session.\nclass var supportedVideoFormats: [ARConfiguration.VideoFormat]\nThe set of video capture formats available on the current device.\nclass ARConfiguration.VideoFormat\nA video size and frame rate specification for use with an AR session.\nvar videoHDRAllowed: Bool\nEnables high dynamic range (HDR) for the session's camera feed.\nclass var configurableCaptureDeviceForPrimaryCamera: AVCaptureDevice?\nAn object that enables you to alter the appearance of a frame's captured image.\nclass var recommendedVideoFormatFor4KResolution: ARConfiguration.VideoFormat?\nProvides a 4K video format if the device and configuration support it.\nclass var recommendedVideoFormatForHighResolutionFrameCapturing: ARConfiguration.VideoFormat?\nReturns a video format that the framework recommends for high-resolution-still-image capture.\nRecording Audio\nvar providesAudioData: Bool\nA Boolean value that specifies whether to capture audio during the AR session.\nReconstructing the Scene\nstruct ARConfiguration.SceneReconstruction\nOptions that enable ARKit to detect the shape of the physical environment.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying"
  },
  {
    "title": "ARKitSession | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkitsession",
    "html": "Overview\n\nSessions in ARKit require either implicit or explicit authorization. To explicitly ask for permission for a particular kind of data and choose when a person is prompted for that permission, call requestAuthorization(for:) before run(_:).\n\nThe following shows a session that starts by requesting implicit authorization to use world sensing:\n\nlet planeData = PlaneDetectionProvider(alignments: [.horizontal, .vertical])\n\n\nTask {\n    do {\n        try await self.session.run([planeData])\n        // Update app based on the planeData.anchorUpdates async sequence.\n    } catch {\n        print(\"ARKitSession error:\", error)\n    }\n}\n\n\nBecause a PlaneDetectionProvider instance’s required authorizations include ARKitSession.AuthorizationType.worldSensing, the system asks someone using your app to permit world sensing before ARKit supplies any of that kind of data.\n\nNote\n\nARKit stops sessions when they’re deinitialized; keep a reference to a session instance for as long as the session needs to run.\n\nTopics\nStarting and stopping a session\ninit()\nCreates a new session.\nfunc run([DataProvider])\nRuns a session with the data providers you supply.\nfunc stop()\nStops all data providers running in this session.\nstruct ARKitSession.Error\nAn error that might occur when running data providers on an ARKit session.\nGetting authorization\nfunc requestAuthorization(for: [ARKitSession.AuthorizationType]) -> [ARKitSession.AuthorizationType : ARKitSession.AuthorizationStatus]\nRequests authorization from the user to use the specified kinds of ARKit data.\nenum ARKitSession.AuthorizationType\nThe authorization types you can request from ARKit.\nfunc queryAuthorization(for: [ARKitSession.AuthorizationType]) -> [ARKitSession.AuthorizationType : ARKitSession.AuthorizationStatus]\nChecks whether the current session is authorized for particular authorization types without requesting authorization.\nenum ARKitSession.AuthorizationStatus\nThe authorization states for a type of ARKit data.\nObserving a session\nvar events: ARKitSession.Events\nAn asynchronous sequence of events that provide updates to the current authorization status of the session.\nstruct ARKitSession.Events\nAn asynchronous sequence of session events.\nenum ARKitSession.Event\nThe kinds of events that can occur in a session.\nvar description: String\nA string that describes this session.\nRelationships\nConforms To\nCustomStringConvertible\nSendable\nSee Also\nvisionOS\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nprotocol DataProvider\nA source of live data from ARKit.\nprotocol Anchor\nThe identity, location, and orientation of an object in world space.\nARKit in visionOS\nCreate immersive augmented reality experiences.\nARKit in visionOS C API\nIntegrate ARKit with low-level libraries and functionality."
  },
  {
    "title": "OS_ar_skeleton_joint | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_skeleton_joint",
    "html": "Relationships\nInherits From\nNSObjectProtocol"
  },
  {
    "title": "OS_ar_hand_skeleton | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/os_ar_hand_skeleton",
    "html": "Relationships\nInherits From\nNSObjectProtocol"
  },
  {
    "title": "ARKit in iOS | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_ios",
    "html": "Topics\nEssentials\nVerifying Device Support and User Permission\nCheck whether your app can use ARKit and respect user privacy at runtime.\nUSDZ schemas for AR\nAdd augmented reality functionality to your 3D content using USDZ schemas.\nSetup\nChoosing Which Camera Feed to Augment\nAdd visual effects to the user's environment in an AR experience through the front or rear camera.\nManaging Session Life Cycle and Tracking Quality\nKeep the user informed on the current session state and recover from interruptions.\nDisplaying an AR Experience with Metal\nControl rendering of your app's virtual content on top of a camera feed.\nclass ARSession\nThe object that manages the major tasks associated with every AR experience, such as motion tracking, camera passthrough, and image analysis.\nConfiguration Objects\nConfigure your augmented reality session to detect and track specific types of content.\nViews\nclass ARView\nA view that enables you to display an AR experience with RealityKit.\nclass ARSCNView\nA view that blends virtual 3D content from SceneKit into your augmented reality experience.\nclass ARSKView\nA view that blends virtual 2D content from SpriteKit into the 3D space of an augmented reality experience.\nclass ARCoachingOverlayView\nA view that displays standardized onboarding instructions to direct users toward a specific goal.\nVirtual Content\nContent Anchors\nIdentify items in the physical environment, including planar surfaces, images, physical objects, body positions, and faces.\nEnvironmental Analysis\nAnalyze the video from the cameras and the accompanying data, and use ray-casting and depth-map information to determine the location of items.\nCamera, Lighting, and Effects\nDetermine the camera position and lighting for the current session, and apply effects, such as occlusion, to elements of the environment.\nData Management\nObtain detailed information about skeletal and face geometry, and saved world data.\nCreating USD files for Apple devices\nGenerate 3D assets that render as expected.\nAR Quick Look\nAdd an AR experience to your app or website, or customize your content’s appearance in Quick Look.\nPreviewing a Model with AR Quick Look\nDisplay a model or scene that the user can move, scale, and share with others.\nAdding Visual Effects in AR Quick Look and RealityKit\nBalance the appearance and performance of your AR experiences with modeling strategies.\nAdding an Apple Pay Button or a Custom Action in AR Quick Look\nProvide a banner that users can tap to make a purchase or perform a custom action in an AR experience.\nclass ARQuickLookPreviewItem\nAn object for customizing the AR Quick Look experience.\nUSDZ schemas for AR\nAdd augmented reality functionality to your 3D content using USDZ schemas.\nSpecifying a lighting environment in AR Quick Look\nAdd metadata to your USDZ file to specify its lighting characteristics.\nShared Experiences\nCommunicate with other devices to create a shared AR experience.\nStreaming an AR Experience\nControl an AR experience remotely by transferring sensor and user input over the network.\nCreating a Collaborative Session\nEnable nearby devices to share an AR experience by using a peer-to-peer multiuser strategy.\nCreating a Multiuser AR Experience\nEnable nearby devices to share an AR experience by using a host-guest multiuser strategy.\nSwiftShot: Creating a Game for Augmented Reality\nSee how Apple built the featured demo for WWDC18, and get tips for making your own multiplayer games using ARKit, SceneKit, and Swift.\nclass ARParticipantAnchor\nAn anchor for another user in multiuser augmented reality experiences.\nclass ARSession.CollaborationData\nAn object that holds information that a user has collected about the physical environment.\nAudio\nCreating an Immersive AR Experience with Audio\nUse sound effects and environmental sound layers to create an engaging AR experience.\nErrors\nstruct ARError\nAn error reported by ARKit.\nenum ARError.Code\nCodes that identify errors in ARKit.\nSee Also\niOS\nVerifying Device Support and User Permission\nCheck whether your app can use ARKit and respect user privacy at runtime.\nclass ARSession\nThe object that manages the major tasks associated with every AR experience, such as motion tracking, camera passthrough, and image analysis.\nclass ARAnchor\nAn object that specifies the position and orientation of an item in the physical environment."
  },
  {
    "title": "ARSession | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arsession",
    "html": "Overview\n\nAn ARSession object coordinates the major processes that ARKit performs on your behalf to create an augmented reality experience. These processes include reading data from the device's motion sensing hardware, controlling the device's built-in camera, and performing image analysis on captured camera images. The session synthesizes all of these results to establish a correspondence between the real-world space the device inhabits and a virtual space where you model AR content.\n\nCreate a Session\n\nEvery AR experience requires an ARSession. If you implement a custom renderer, you instantiate the session yourself.\n\nlet session = ARSession()\nsession.delegate = self\n\n\nIf you use one of the standard renderers (like ARView, ARSCNView, or ARSKView), the renderer creates a session object for you. When you want to interact with your app's session, you access it on your app's renderer.\n\nlet session = myView.session\n\n\nRun a Session\n\nRunning a session requires a configuration. Subclasses of ARConfiguration determine how ARKit tracks a device's position and motion relative to the real world, and thus it determines the kinds of AR experiences you create. For example, ARWorldTrackingConfiguration enables you to augment the user's view of the world around them though the device's back camera.\n\nTopics\nConfiguring and running a session\nfunc run(ARConfiguration, options: ARSession.RunOptions)\nStarts AR processing for the session with the specified configuration and options.\nvar identifier: UUID\nA unique identifier of the running session.\nstruct ARSession.RunOptions\nOptions for transitioning an AR session's current state when you change its configuration.\nvar configuration: ARConfiguration?\nAn object that defines motion and scene tracking behaviors for the session.\nfunc pause()\nPauses processing in the session.\nResponding to events\nvar delegate: ARSessionDelegate?\nAn object you provide to receive captured video images and tracking information, or to respond to changes in session status.\nvar delegateQueue: dispatch_queue_t?\nThe dispatch queue through which the session calls your delegate methods.\nprotocol ARSessionDelegate\nMethods you can implement to receive captured video frame images and tracking state from an AR session.\nprotocol ARSessionObserver\nMethods you can implement to respond to changes in the state of an AR session.\nManaging anchors\nfunc add(anchor: ARAnchor)\nAdds the specified anchor to be tracked by the session.\nfunc remove(anchor: ARAnchor)\nRemoves the specified anchor from tracking by the session.\nSaving or sharing state\nfunc getCurrentWorldMap(completionHandler: (ARWorldMap?, Error?) -> Void)\nReturns an object encapsulating the world-tracking session's space-mapping state and set of anchors.\nRecording and Replaying AR Session Data\nRecord an AR session in Reality Composer and replay it in your ARKit app.\nScanning 3D objects\nfunc createReferenceObject(transform: simd_float4x4, center: simd_float3, extent: simd_float3, completionHandler: (ARReferenceObject?, Error?) -> Void)\nCreates a reference object (for 3D object detection) from the specified region of the session’s world space.\nUpdating the world origin\nfunc setWorldOrigin(relativeTransform: simd_float4x4)\nChanges the basis for the AR world coordinate space using the specified transform.\nFinding real-world surfaces\nCast a ray from a point on the screen to find intersections with real-world surfaces.\nfunc raycast(ARRaycastQuery) -> [ARRaycastResult]\nChecks once for intersections between a ray and real-world surfaces.\nfunc trackedRaycast(ARRaycastQuery, updateHandler: ([ARRaycastResult]) -> Void) -> ARTrackedRaycast?\nRepeats a ray-cast query over time to notify you of updated surfaces in the physical environment.\nConverting local coordinates to geographic coordinates\nfunc getGeoLocation(forPoint: simd_float3, completionHandler: (CLLocationCoordinate2D, CLLocationDistance, Error?) -> Void)\nConverts a position in the framework’s local coordinate system to latitude, longitude and altitude.\nAccessing the camera frame\nvar currentFrame: ARFrame?\nThe most recent still frame captured by the active camera feed, including ARKit's interpretation of it.\nclass ARFrame\nA video image captured as part of a session with position-tracking information.\nfunc captureHighResolutionFrame(completion: (ARFrame?, Error?) -> Void)\nRequests a frame outside of the normal frequency that contains a high-resolution captured image.\nManaging collaboration\nfunc update(with: ARSession.CollaborationData)\nUpdates your session with information about the physical environment that is collected by another user.\nclass ARSession.CollaborationData\nAn object that holds information that a user has collected about the physical environment.\nProviding a session\nprotocol ARSessionProviding\nAn object that provides a session.\nRelationships\nInherits From\nNSObject\nSee Also\niOS\nVerifying Device Support and User Permission\nCheck whether your app can use ARKit and respect user privacy at runtime.\nclass ARAnchor\nAn object that specifies the position and orientation of an item in the physical environment.\nARKit in iOS\nIntegrate iOS device camera and motion features to produce augmented reality experiences in your app or game."
  },
  {
    "title": "ARAnchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/aranchor",
    "html": "Overview\n\nTo track the static positions and orientations of real or virtual objects relative to the camera, create anchor objects and use the add(anchor:) method to add them to your AR session.\n\nTip\n\nAdding an anchor to the session helps ARKit to optimize world-tracking accuracy in the area around that anchor, so that virtual objects appear to stay in place relative to the real world. If a virtual object moves, remove the corresponding anchor from the old position and add one at the new position.\n\nSome ARKit features automatically add special anchors to a session. World-tracking sessions can add ARPlaneAnchor, ARObjectAnchor, and ARImageAnchor objects if you enable the corresponding features; face-tracking sessions add ARFaceAnchor objects.\n\nSubclassing Notes\n\nIn addition to creating your own ARAnchor instances to track the real-world positions of your virtual content, you can also subclass ARAnchor to associate custom data with anchors you create. Ensure that your anchor classes behave correctly when ARKit updates frames or saves and loads anchors in an ARWorldMap:\n\nAnchor subclasses must fullfill the requirements of the ARAnchorCopying protocol. ARKit calls init(anchor:) (on a background thread) to copy instances of your anchor class from each ARFrame to the next. Your implementation of this initializer should copy the values of any custom properties your subclass adds.\n\nAnchor subclasses must also adopt the NSSecureCoding protocol. Override encode(with:) and init(coder:) to save and restore the values your subclass' custom properties when ARKit saves and loads them in a world map.\n\nAnchors are considered equal based on their identifier property.\n\nOnly anchors that do not adopt ARTrackable are included when you save a world map.\n\nTopics\nCreating Anchors\ninit(transform: simd_float4x4)\nCreates a new anchor object with the specified transform.\ninit(name: String, transform: simd_float4x4)\nCreates a new anchor object with the specified transform and a descriptive name.\nvar name: String?\nA descriptive name for the anchor.\nTracking Anchors\nvar identifier: UUID\nA unique identifier for the anchor.\nvar sessionIdentifier: UUID?\nThe unique identifier of the session that owns this anchor.\nvar transform: simd_float4x4\nA matrix encoding the position, orientation, and scale of the anchor relative to the world coordinate space of the AR session the anchor is placed in.\nRelationships\nInherits From\nNSObject\nConforms To\nARAnchorCopying\nNSSecureCoding\nSee Also\niOS\nVerifying Device Support and User Permission\nCheck whether your app can use ARKit and respect user privacy at runtime.\nclass ARSession\nThe object that manages the major tasks associated with every AR experience, such as motion tracking, camera passthrough, and image analysis.\nARKit in iOS\nIntegrate iOS device camera and motion features to produce augmented reality experiences in your app or game."
  },
  {
    "title": "ARKit in visionOS | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_visionos",
    "html": "Overview\n\nARKit in visionOS offers a new set of sensing capabilities. You adopt these capabilities individually in your app, using data providers to deliver updates asynchronously. The available capabilities include:\n\nPlane detection. Detect surfaces in a person’s surroundings and use them to anchor content.\n\nWorld tracking. Determine the position and orientation of Apple Vision Pro relative to its surroundings and add world anchors to place content.\n\nHand tracking. Use the person’s hand and finger positions as input for custom gestures and interactivity.\n\nScene reconstruction. Build a mesh of the person’s physical surroundings and incorporate it into your immersive spaces to support interactions.\n\nImage tracking. Look for known images in the person’s surroundings and use them as anchor points for custom content.\n\nTopics\nSetup\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nclass ARKitSession\nThe main entry point for receiving data from ARKit.\nprotocol DataProvider\nA source of live data from ARKit.\nenum DataProviderState\nThe possible states of a data provider.\nprotocol Anchor\nThe identity, location, and orientation of an object in world space.\nprotocol TrackableAnchor\nAn anchor that can gain and lose its tracking state over the course of a session.\nPlane detection\nPlacing content on detected planes\nDetect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.\nclass PlaneDetectionProvider\nA source of live data about planes in a person’s surroundings.\nstruct PlaneAnchor\nAn anchor that represents horizontal and vertical planes.\nWorld tracking\nTracking specific points in world space\nRetrieve the position and orientation of anchors your app stores in ARKit.\nclass WorldTrackingProvider\nA source of live data about the device pose and anchors in a person’s surroundings.\nstruct WorldAnchor\nA fixed location in a person’s surroundings.\nstruct DeviceAnchor\nThe position and orientation of Apple Vision Pro.\nHand tracking\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nclass HandTrackingProvider\nA source of live data about the position of a person’s hands and hand joints.\nstruct HandAnchor\nA hand’s position in a person’s surroundings.\nstruct HandSkeleton\nA collection of joints in a hand.\nScene reconstruction\nIncorporating real-world surroundings in an immersive experience\nCreate an immersive experience by making your app’s content respond to the local shape of the world.\nclass SceneReconstructionProvider\nA source of live data about the shape of a person’s surroundings.\nstruct MeshAnchor\nA volume of space that contains a mesh of a person’s surroundings.\nImage tracking\nTracking preregistered images in 3D space\nPlace content based on the current position of a known image in a person’s surroundings.\nclass ImageTrackingProvider\nA source of live data about a 2D image’s position in a person’s surroundings.\nstruct ImageAnchor\nA 2D image’s position in a person’s surroundings.\nstruct ReferenceImage\nA 2D image the system uses as a reference to find the same image in a person’s surroundings.\nGeometry\nstruct GeometryElement\nA container for vertex indices of lines or triangles.\nstruct GeometrySource\nA container for geometrical vector data.\nSee Also\nvisionOS\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nclass ARKitSession\nThe main entry point for receiving data from ARKit.\nprotocol DataProvider\nA source of live data from ARKit.\nprotocol Anchor\nThe identity, location, and orientation of an object in world space.\nARKit in visionOS C API\nIntegrate ARKit with low-level libraries and functionality."
  },
  {
    "title": "ARKit in visionOS C API | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arkit_in_visionos_c_api",
    "html": "Overview\n\nARKit in visionOS includes a full C API for compatibility with C and Objective-C apps and frameworks.\n\nTopics\nObjective-C compatibility\nObjective-C compatibility\nSee Also\nvisionOS\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nclass ARKitSession\nThe main entry point for receiving data from ARKit.\nprotocol DataProvider\nA source of live data from ARKit.\nprotocol Anchor\nThe identity, location, and orientation of an object in world space.\nARKit in visionOS\nCreate immersive augmented reality experiences."
  },
  {
    "title": "Anchor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/anchor",
    "html": "Topics\nInspecting an anchor\nvar id: UUID\nA unique identifier that distinguishes this anchor from all other anchors.\n\nRequired\n\nvar originFromAnchorTransform: simd_float4x4\nThe position and orientation of this anchor in world space.\n\nRequired\n\nTracking anchors over time\nstruct AnchorUpdate\nInformation about the event that updated an anchor.\nstruct AnchorUpdateSequence\nAn asynchronous sequence of updates to anchors.\nRelationships\nInherits From\nCustomStringConvertible\nIdentifiable\nSendable\nInherited By\nTrackableAnchor\nConforming Types\nMeshAnchor\nPlaneAnchor\nSee Also\nvisionOS\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nclass ARKitSession\nThe main entry point for receiving data from ARKit.\nprotocol DataProvider\nA source of live data from ARKit.\nARKit in visionOS\nCreate immersive augmented reality experiences.\nARKit in visionOS C API\nIntegrate ARKit with low-level libraries and functionality."
  },
  {
    "title": "isSupported | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/arconfiguration/2923553-issupported",
    "html": "Discussion\n\nDifferent types of AR experiences (which you configure using concrete ARConfiguration subclasses) can have different hardware requirements.\n\nBefore attempting to create an AR configuration, verify that the user’s device supports the configuration you plan to use by checking the isSupported property of the corresponding configuration class. If this property’s value is false, the current device does not support the requested configuration.\n\nImportant\n\nAll ARKit configurations require an iOS device with an A9 or later processor. If your app otherwise supports other devices and offers augmented reality as a secondary feature, use this property to determine whether to offer AR-based features to the user.\n\nIf your app requires ARKit for its core functionality, use the arkit key in the UIRequiredDeviceCapabilities section of your app's Info.plist to make your app available only on devices that support ARKit."
  },
  {
    "title": "DataProvider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/dataprovider",
    "html": "Overview\n\nMost providers supply an asynchronous sequence of updated anchors for the provider’s data type. For example, a HandTrackingProvider instance’s anchorUpdates property gives updates over time for hand anchors.\n\nTopics\nInspecting a data provider\nvar state: DataProviderState\nThe current status of data coming from this provider.\n\nRequired\n\nInspecting a data provider type\nstatic var requiredAuthorizations: [ARKitSession.AuthorizationType]\nThe kinds of authorization you need to use a particular data provider type.\n\nRequired\n\nstatic var isSupported: Bool\nA Boolean value that indicates whether the current runtime environment supports a particular provider type.\n\nRequired\n\nRelationships\nInherits From\nCustomStringConvertible\nSendable\nConforming Types\nHandTrackingProvider\nImageTrackingProvider\nPlaneDetectionProvider\nSceneReconstructionProvider\nWorldTrackingProvider\nSee Also\nvisionOS\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nclass ARKitSession\nThe main entry point for receiving data from ARKit.\nprotocol Anchor\nThe identity, location, and orientation of an object in world space.\nARKit in visionOS\nCreate immersive augmented reality experiences.\nARKit in visionOS C API\nIntegrate ARKit with low-level libraries and functionality."
  },
  {
    "title": "Verifying Device Support and User Permission | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/arkit/verifying_device_support_and_user_permission",
    "html": "Overview\n\nARKit requires iOS 11.0 or later and an iOS device with an A9 or later processor. Some ARKit features require later iOS versions or specific devices. ARKit also uses a device camera, so you need to configure iOS privacy controls so the user can permit camera access for your app.\n\nHow to handle device compatibility support depends on how your app uses ARKit:\n\nIf the basic functionality of your app requires AR (using the back camera): Add the arkit key in the UIRequiredDeviceCapabilities section of your app's Info.plist file. Using this key makes your app available only to ARKit-compatible devices.\n\nIf augmented reality is a secondary feature of your app: Check for whether the current device supports the AR configuration you want to use by testing the isSupported property of the appropriate ARConfiguration subclass.\n\nIf your app uses face-tracking AR: Face tracking requires the front-facing TrueDepth camera on iPhone X. Your app remains available on other devices, so you must test the ARFaceTrackingConfiguration.isSupported property to determine face-tracking support on the current device.\n\nTip\n\nCheck the isSupported property before offering AR features in your app's UI, so that users on unsupported devices aren't disappointed by trying to access those features.\n\nHandle User Consent and Privacy\n\nFor your app to use ARKit, the user must explicitly grant your app permission for camera access. ARKit automatically asks the user for permission the first time your app runs an AR session.\n\niOS requires your app to provide a static message to be displayed when the system asks for camera or microphone permission. Your app's Info.plist file must include the NSCameraUsageDescription key. For that key, provide text that explains why your app needs camera access so that the user can feel confident granting permission to your app.\n\nNote\n\nIf you create a new ARKit app using the Xcode template, a default camera usage description is provided for you.\n\nIf your app uses ARFaceTrackingConfiguration, ARKit provides your app with personal facial information. If you use ARKit face tracking features, your app must include a privacy policy describing to users how you intend to use face tracking and face data. For details, see the Apple Developer Program License Agreement.\n\nSee Also\niOS\nclass ARSession\nThe object that manages the major tasks associated with every AR experience, such as motion tracking, camera passthrough, and image analysis.\nclass ARAnchor\nAn object that specifies the position and orientation of an item in the physical environment.\nARKit in iOS\nIntegrate iOS device camera and motion features to produce augmented reality experiences in your app or game."
  },
  {
    "title": "ARKit | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/ARKit",
    "html": "Overview\n\nAugmented reality (AR) describes user experiences that add 2D or 3D elements to the live view from a device’s sensors in a way that makes those elements appear to inhabit the real world. ARKit combines device motion tracking, world tracking, scene understanding, and display conveniences to simplify building an AR experience.\n\nTopics\nvisionOS\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nclass ARKitSession\nThe main entry point for receiving data from ARKit.\nprotocol DataProvider\nA source of live data from ARKit.\nprotocol Anchor\nThe identity, location, and orientation of an object in world space.\nARKit in visionOS\nCreate immersive augmented reality experiences.\nARKit in visionOS C API\nIntegrate ARKit with low-level libraries and functionality.\niOS\nVerifying Device Support and User Permission\nCheck whether your app can use ARKit and respect user privacy at runtime.\nclass ARSession\nThe object that manages the major tasks associated with every AR experience, such as motion tracking, camera passthrough, and image analysis.\nclass ARAnchor\nAn object that specifies the position and orientation of an item in the physical environment.\nARKit in iOS\nIntegrate iOS device camera and motion features to produce augmented reality experiences in your app or game.\nProtocols\nprotocol OS_ar_hand_skeleton\nBeta\nprotocol OS_ar_skeleton_joint\nBeta"
  }
]
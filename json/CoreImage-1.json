[
  {
    "title": "startTaskToRender:toDestination:error:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2875429-starttasktorender",
    "html": "Parameters\nimage\n\nCIImage to prepare to render.\n\ndestination\n\nThe CIRenderDestination to which to render.\n\nerror\n\nPointer to an error should the render task creation fail.\n\nReturn Value\n\nThe asynchronous CIRenderTask to render the image to the specified destination.\n\nSee Also\nCustomizing Render Destination\n- prepareRender:fromRect:toDestination:atPoint:error:\nAn optional call to warm up a CIContext so that subsequent calls to render with the same arguments run more efficiently.\n- startTaskToClear:error:\nFills the entire destination with black or clear depending on its alphaMode.\n- startTaskToRender:fromRect:toDestination:atPoint:error:\nRenders a portion of an image to a point in the destination."
  },
  {
    "title": "startTaskToRender:fromRect:toDestination:atPoint:error:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2875448-starttasktorender",
    "html": "Parameters\nimage\n\nA CIImage to render.\n\nfromRect\n\nThe part of the image to render, as if cropped.\n\ndestination\n\nA CIRenderDestination into which to render the image.\n\natPoint\n\nAn origin point in the destination at which to place the image.\n\nerror\n\nOn output, a pointer to an error object if the render task fails.\n\nReturn Value\n\nAn asynchronous CIRenderTask to render the image to the specified destination.\n\nDiscussion\n\nThis method crops the image to the specified rectangle and renders the result at the indicated origin point. If the image’s extent property and fromRect argument values are infinite, this call renders the image’s (0, 0) point starting from the origin atPoint.\n\nYou must use an MTLTexture-backed CIContext to support an MTLTexture-backed CIRenderDestination. Similarly, you must use GLContext-backed CIContext to support a GLTexture-backed CIRenderDestination.\n\nThis call returns as soon as it enqueues all work required to render the image on the context’s device. In many situations, after issuing a render, you may need to wait for it to complete. In these cases, use the returned CIRenderTask as follows:\n\nCIRenderTask* task = [context startTaskToRender:image fromRect:fromRect toDestination:renderDestination atPoint:point error:&error];\n\n\nCIRenderInfo* info = [task waitUntilCompletedAndReturnError:&error];\n\n\nSee Also\nCustomizing Render Destination\n- prepareRender:fromRect:toDestination:atPoint:error:\nAn optional call to warm up a CIContext so that subsequent calls to render with the same arguments run more efficiently.\n- startTaskToClear:error:\nFills the entire destination with black or clear depending on its alphaMode.\n- startTaskToRender:toDestination:error:\nRenders an image to a destination so that point (0, 0) of the image maps to point (0, 0) of the destination."
  },
  {
    "title": "startTaskToClear:error:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2875450-starttasktoclear",
    "html": "Parameters\ndestination\n\nThe CIRenderDestination to clear.\n\nerror\n\nPointer to an error object should the task fail.\n\nReturn Value\n\nThe asynchronous CIRenderTask for clearing the destination.\n\nDiscussion\n\nIf the destination's alphaMode is CIRenderDestinationAlphaNone, this command fills the entire destination with black (0, 0, 0, 1).\n\nIf the destination's alphaMode is CIRenderDestinationAlphaPremultiplied or CIRenderDestinationAlphaUnpremultiplied, this command fills the entire destination with clear (0, 0, 0, 0).\n\nSee Also\nCustomizing Render Destination\n- prepareRender:fromRect:toDestination:atPoint:error:\nAn optional call to warm up a CIContext so that subsequent calls to render with the same arguments run more efficiently.\n- startTaskToRender:fromRect:toDestination:atPoint:error:\nRenders a portion of an image to a point in the destination.\n- startTaskToRender:toDestination:error:\nRenders an image to a destination so that point (0, 0) of the image maps to point (0, 0) of the destination."
  },
  {
    "title": "workingFormat",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption/1437788-workingformat",
    "html": "Discussion\n\nThe value for this key is an NSNumber object containing a CIFormat value. The default working format is RGBA8 for CPU rendering and RGBAf for GPU rendering. For greater color precision, GPU rendering also supports the RGBAh format, but this format requires twice as much memory and can be used only with color management enabled."
  },
  {
    "title": "priorityRequestLow",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption/1620424-priorityrequestlow",
    "html": "Discussion\n\nThe value for this key is an NSNumber object containing a Boolean value. If this value is true, use of the Core Image context from a background thread takes lower priority than GPU usage from the main thread, allowing your app to perform Core Image rendering without disturbing the frame rate of UI animations."
  },
  {
    "title": "outputPremultiplied",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption/1642216-outputpremultiplied",
    "html": "Discussion\n\nThe value for this key is an NSNumber object containing a Boolean value. The default value of true produces premultiplied output."
  },
  {
    "title": "writePNGRepresentationOfImage:toURL:format:colorSpace:options:error:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2866197-writepngrepresentationofimage",
    "html": "Parameters\nimage\n\nThe image object to render.\n\nurl\n\nThe file URL at which to write the output PNG file.\n\nformat\n\nThe pixel format for the output image.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the kCGColorSpaceModelRGB or kCGColorSpaceModelMonochrome model and must be compatible with the specified pixel format.\n\noptions\n\nA dictionary with additional options for export.\n\nerrorPtr\n\nOn input, a pointer to an error object. If an error occurs, this pointer is set to an object containing error information.\n\nReturn Value\n\nIf YES, file export succeeded. If NO, examine the errorPtr parameter for possible failure reasons.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the imageByClampingToExtent method.\n\nSee Also\nRendering Images for Data or File Export\n- TIFFRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in TIFF format.\n- JPEGRepresentationOfImage:colorSpace:options:\nRenders the image and exports the resulting image data in JPEG format.\n- PNGRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in PNG format.\n- HEIFRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in HEIF format.\n- writeTIFFRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in TIFF format.\n- writeJPEGRepresentationOfImage:toURL:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in JPEG format.\n- writeHEIFRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in HEIF format.\nkCIImageRepresentationAVDepthData\noptions dictionary key for image export methods to represent data as AVDepthData.\nkCIImageRepresentationDepthImage\noptions dictionary key for image export methods to output depth data.\nkCIImageRepresentationDisparityImage\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "writeHEIFRepresentationOfImage:toURL:format:colorSpace:options:error:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2902266-writeheifrepresentationofimage",
    "html": "Parameters\nimage\n\nThe image object to render.\n\nurl\n\nThe file URL at which to write the output HEIF file.\n\nformat\n\nThe pixel format for the output image.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the kCGColorSpaceModelRGB or kCGColorSpaceModelMonochrome model and must be compatible with the specified pixel format.\n\noptions\n\nA dictionary with additional options for export.\n\nerrorPtr\n\nOn input, a pointer to an error object. If an error occurs, this pointer is set to an object containing error information.\n\nReturn Value\n\nIf YES, file export succeeded. If NO, examine the errorPtr parameter for possible failure reasons.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the imageByClampingToExtent method.\n\nSee Also\nRendering Images for Data or File Export\n- TIFFRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in TIFF format.\n- JPEGRepresentationOfImage:colorSpace:options:\nRenders the image and exports the resulting image data in JPEG format.\n- PNGRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in PNG format.\n- HEIFRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in HEIF format.\n- writeTIFFRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in TIFF format.\n- writeJPEGRepresentationOfImage:toURL:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in JPEG format.\n- writePNGRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in PNG format.\nkCIImageRepresentationAVDepthData\noptions dictionary key for image export methods to represent data as AVDepthData.\nkCIImageRepresentationDepthImage\noptions dictionary key for image export methods to output depth data.\nkCIImageRepresentationDisparityImage\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "avDepthData",
    "url": "https://developer.apple.com/documentation/coreimage/ciimagerepresentationoption/2902265-avdepthdata",
    "html": "See Also\nRendering Images for Data or File Export\nfunc tiffRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in TIFF format.\nfunc jpegRepresentation(of: CIImage, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in JPEG format.\nfunc pngRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in PNG format.\nfunc heifRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in HEIF format.\nfunc writeTIFFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in TIFF format.\nfunc writeJPEGRepresentation(of: CIImage, to: URL, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in JPEG format.\nfunc writePNGRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in PNG format.\nfunc writeHEIFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in HEIF format.\nstatic let depthImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output depth data.\nstatic let disparityImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "writeJPEGRepresentationOfImage:toURL:colorSpace:options:error:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1642218-writejpegrepresentationofimage",
    "html": "Parameters\nimage\n\nThe image object to render.\n\nurl\n\nThe file URL at which to write the output JPEG file.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the kCGColorSpaceModelRGB or kCGColorSpaceModelMonochrome model and must be compatible with the specified pixel format.\n\noptions\n\nA dictionary with additional options for export. Use the kCGImageDestinationLossyCompressionQuality key to specify JPEG compression level.\n\nerrorPtr\n\nOn input, a pointer to an error object. If an error occurs, this pointer is set to an object containing error information.\n\nReturn Value\n\nIf YES, file export succeeded. If NO, examine the errorPtr parameter for possible failure reasons.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the imageByClampingToExtent method.\n\nSee Also\nRendering Images for Data or File Export\n- TIFFRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in TIFF format.\n- JPEGRepresentationOfImage:colorSpace:options:\nRenders the image and exports the resulting image data in JPEG format.\n- PNGRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in PNG format.\n- HEIFRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in HEIF format.\n- writeTIFFRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in TIFF format.\n- writePNGRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in PNG format.\n- writeHEIFRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in HEIF format.\nkCIImageRepresentationAVDepthData\noptions dictionary key for image export methods to represent data as AVDepthData.\nkCIImageRepresentationDepthImage\noptions dictionary key for image export methods to output depth data.\nkCIImageRepresentationDisparityImage\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "writeTIFFRepresentationOfImage:toURL:format:colorSpace:options:error:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1642213-writetiffrepresentationofimage",
    "html": "Parameters\nimage\n\nThe image object to render.\n\nurl\n\nThe file URL at which to write the output TIFF file.\n\nformat\n\nThe pixel format for the output image.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the kCGColorSpaceModelRGB or kCGColorSpaceModelMonochrome model and must be compatible with the specified pixel format.\n\noptions\n\nA dictionary with additional options for export.\n\nerrorPtr\n\nOn input, a pointer to an error object. If an error occurs, this pointer is set to an object containing error information.\n\nReturn Value\n\nIf YES, file export succeeded. If NO, examine the errorPtr parameter for possible failure reasons.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the imageByClampingToExtent method.\n\nSee Also\nRendering Images for Data or File Export\n- TIFFRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in TIFF format.\n- JPEGRepresentationOfImage:colorSpace:options:\nRenders the image and exports the resulting image data in JPEG format.\n- PNGRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in PNG format.\n- HEIFRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in HEIF format.\n- writeJPEGRepresentationOfImage:toURL:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in JPEG format.\n- writePNGRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in PNG format.\n- writeHEIFRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in HEIF format.\nkCIImageRepresentationAVDepthData\noptions dictionary key for image export methods to represent data as AVDepthData.\nkCIImageRepresentationDepthImage\noptions dictionary key for image export methods to output depth data.\nkCIImageRepresentationDisparityImage\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "PNGRepresentationOfImage:format:colorSpace:options:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2866196-pngrepresentationofimage",
    "html": "Parameters\nimage\n\nThe image object to render.\n\nformat\n\nThe pixel format for the output image.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the kCGColorSpaceModelRGB or kCGColorSpaceModelMonochrome model and must be compatible with the specified pixel format.\n\noptions\n\nNo options keys are supported at this time.\n\nReturn Value\n\nA data representation of the rendered image in PNG format, or nil if the image could not be rendered.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the imageByClampingToExtent method.\n\nSee Also\nRendering Images for Data or File Export\n- TIFFRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in TIFF format.\n- JPEGRepresentationOfImage:colorSpace:options:\nRenders the image and exports the resulting image data in JPEG format.\n- HEIFRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in HEIF format.\n- writeTIFFRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in TIFF format.\n- writeJPEGRepresentationOfImage:toURL:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in JPEG format.\n- writePNGRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in PNG format.\n- writeHEIFRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in HEIF format.\nkCIImageRepresentationAVDepthData\noptions dictionary key for image export methods to represent data as AVDepthData.\nkCIImageRepresentationDepthImage\noptions dictionary key for image export methods to output depth data.\nkCIImageRepresentationDisparityImage\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "HEIFRepresentationOfImage:format:colorSpace:options:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2902269-heifrepresentationofimage",
    "html": "Parameters\nimage\n\nThe image object to render.\n\nformat\n\nThe pixel format for the output image.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the kCGColorSpaceModelRGB or kCGColorSpaceModelMonochrome model and must be compatible with the specified pixel format.\n\noptions\n\nA dictionary with additional options for export. Supported keys include kCGImageDestinationLossyCompressionQuality, kCIImageRepresentationAVDepthData, kCIImageRepresentationDepthImage, and kCIImageRepresentationDisparityImage.\n\nReturn Value\n\nA data representation of the rendered image in HEIF format, or nil if the image could not be rendered.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the imageByClampingToExtent method.\n\nSee Also\nRendering Images for Data or File Export\n- TIFFRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in TIFF format.\n- JPEGRepresentationOfImage:colorSpace:options:\nRenders the image and exports the resulting image data in JPEG format.\n- PNGRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in PNG format.\n- writeTIFFRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in TIFF format.\n- writeJPEGRepresentationOfImage:toURL:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in JPEG format.\n- writePNGRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in PNG format.\n- writeHEIFRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in HEIF format.\nkCIImageRepresentationAVDepthData\noptions dictionary key for image export methods to represent data as AVDepthData.\nkCIImageRepresentationDepthImage\noptions dictionary key for image export methods to output depth data.\nkCIImageRepresentationDisparityImage\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "depthImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciimagerepresentationoption/2902268-depthimage",
    "html": "See Also\nRendering Images for Data or File Export\nfunc tiffRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in TIFF format.\nfunc jpegRepresentation(of: CIImage, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in JPEG format.\nfunc pngRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in PNG format.\nfunc heifRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in HEIF format.\nfunc writeTIFFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in TIFF format.\nfunc writeJPEGRepresentation(of: CIImage, to: URL, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in JPEG format.\nfunc writePNGRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in PNG format.\nfunc writeHEIFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in HEIF format.\nstatic let avDepthData: CIImageRepresentationOption\noptions dictionary key for image export methods to represent data as AVDepthData.\nstatic let disparityImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "JPEGRepresentationOfImage:colorSpace:options:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1642214-jpegrepresentationofimage",
    "html": "Parameters\nimage\n\nThe image object to render.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the kCGColorSpaceModelRGB or kCGColorSpaceModelMonochrome model and must be compatible with the specified pixel format.\n\noptions\n\nA dictionary with additional options for export. Use the kCGImageDestinationLossyCompressionQuality key to specify JPEG compression level. Other supported keys include kCIImageRepresentationAVDepthData, kCIImageRepresentationDepthImage, and kCIImageRepresentationDisparityImage.\n\nReturn Value\n\nA data representation of the rendered image in JPEG format, or nil if the image could not be rendered.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the imageByClampingToExtent method.\n\nSee Also\nRendering Images for Data or File Export\n- TIFFRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in TIFF format.\n- PNGRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in PNG format.\n- HEIFRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in HEIF format.\n- writeTIFFRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in TIFF format.\n- writeJPEGRepresentationOfImage:toURL:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in JPEG format.\n- writePNGRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in PNG format.\n- writeHEIFRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in HEIF format.\nkCIImageRepresentationAVDepthData\noptions dictionary key for image export methods to represent data as AVDepthData.\nkCIImageRepresentationDepthImage\noptions dictionary key for image export methods to output depth data.\nkCIImageRepresentationDisparityImage\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "TIFFRepresentationOfImage:format:colorSpace:options:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1642220-tiffrepresentationofimage",
    "html": "Parameters\nimage\n\nThe image object to render.\n\nformat\n\nThe pixel format for the output image.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the kCGColorSpaceModelRGB or kCGColorSpaceModelMonochrome model and must be compatible with the specified pixel format.\n\noptions\n\nA dictionary with additional options for export.\n\nReturn Value\n\nA data representation of the rendered image in TIFF format, or nil if the image could not be rendered.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the imageByClampingToExtent method.\n\nSee Also\nRendering Images for Data or File Export\n- JPEGRepresentationOfImage:colorSpace:options:\nRenders the image and exports the resulting image data in JPEG format.\n- PNGRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in PNG format.\n- HEIFRepresentationOfImage:format:colorSpace:options:\nRenders the image and exports the resulting image data in HEIF format.\n- writeTIFFRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in TIFF format.\n- writeJPEGRepresentationOfImage:toURL:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in JPEG format.\n- writePNGRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in PNG format.\n- writeHEIFRepresentationOfImage:toURL:format:colorSpace:options:error:\nRenders the image and exports the resulting image data as a file in HEIF format.\nkCIImageRepresentationAVDepthData\noptions dictionary key for image export methods to represent data as AVDepthData.\nkCIImageRepresentationDepthImage\noptions dictionary key for image export methods to output depth data.\nkCIImageRepresentationDisparityImage\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "disparityImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciimagerepresentationoption/2902267-disparityimage",
    "html": "See Also\nRendering Images for Data or File Export\nfunc tiffRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in TIFF format.\nfunc jpegRepresentation(of: CIImage, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in JPEG format.\nfunc pngRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in PNG format.\nfunc heifRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in HEIF format.\nfunc writeTIFFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in TIFF format.\nfunc writeJPEGRepresentation(of: CIImage, to: URL, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in JPEG format.\nfunc writePNGRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in PNG format.\nfunc writeHEIFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in HEIF format.\nstatic let avDepthData: CIImageRepresentationOption\noptions dictionary key for image export methods to represent data as AVDepthData.\nstatic let depthImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output depth data."
  },
  {
    "title": "highQualityDownsample",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption/1437699-highqualitydownsample",
    "html": "Discussion\n\nThe value for this key is an NSNumber object containing a Boolean value. A value of true (the default in macOS) results in higher image quality and lower performance. In iOS and tvOS, the default value is false."
  },
  {
    "title": "drawImage:inRect:fromRect:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437786-drawimage",
    "html": "Parameters\nim\n\nA Core Image image object.\n\ndest\n\nThe rectangle in the context destination to draw into. The image is scaled to fill the destination rectangle.\n\nsrc\n\nThe subregion of the image that you want to draw into the context, with the origin and target size defined by the dest parameter. This rectangle is always in pixel dimensions.\n\nDiscussion\n\nIn iOS, this method draws the CIImage object into a renderbuffer for the OpenGL ES context. Use this method only if the CIContext object is created with contextWithEAGLContext: and if you are rendering to a CAEAGLayer. This method is asynchronous for apps linked against the iOS 6 or later SDK.\n\nIn macOS, you need to be aware of whether the CIContext object is created with a CGContextRef or a CGLContext object. If you create the CIContext object with a CGContextRef, the dimensions of the destination rectangle are in points. If you create the CIContext object with a CGLContext object, the dimensions are in pixels.\n\nSee Also\nDrawing Images\n- drawImage:atPoint:fromRect:\nRenders a region of an image to a point in the context destination."
  },
  {
    "title": "createCGLayerWithSize:info:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1438267-createcglayerwithsize",
    "html": "Parameters\nsize\n\nThe size, in default user space units, of the layer relative to the graphics context.\n\nd\n\nA dictionary, which is passed to CGLayerCreateWithContext as the auxiliaryInfo parameter. Pass NULL because this parameter is reserved for future use.\n\nReturn Value\n\nA CGLayer object.\n\nDiscussion\n\nAfter calling this method, Core Image draws content into the CGLayer object. Core Image creates a CGLayer object by calling the Quartz 2D function CGLayerCreateWithContext, whose prototype is:\n\nCGLayerRef CGLayerCreateWithContext (\n   CGContextRef context,\n   CGSize size,\n   CFDictionaryRef auxiliaryInfo\n);\n\n\nCore Image passes the CIContext object as the context parameter, the size as the size parameter, and the dictionary as the auxiliaryInfo parameter. For more information on CGLayer objects, see Quartz 2D Programming Guide and CGLayer.\n\nSee Also\nRendering Images\n- createCGImage:fromRect:\nCreates a Quartz 2D image from a region of a Core Image image object.\n- createCGImage:fromRect:format:colorSpace:\nCreates a Quartz 2D image from a region of a Core Image image object.\n- createCGImage:fromRect:format:colorSpace:deferred:\nCreates a Quartz 2D image from a region of a Core Image image object with deferred rendering.\n- render:toBitmap:rowBytes:bounds:format:colorSpace:\nRenders to the given bitmap.\n- render:toCVPixelBuffer:\nRenders an image into a pixel buffer.\n- render:toCVPixelBuffer:bounds:colorSpace:\nRenders a region of an image into a pixel buffer.\n- render:toIOSurface:bounds:colorSpace:\nRenders a region of an image into an IOSurface object.\n- render:toMTLTexture:commandBuffer:bounds:colorSpace:\nRenders a region of an image to a Metal texture.\nRelated Documentation\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options."
  },
  {
    "title": "contextForOfflineGPUAtIndex:colorSpace:options:sharedContext:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437758-contextforofflinegpuatindex",
    "html": "Parameters\nindex\n\nThe index of the offline GPU with which to create the context; a number between zero and the value returned by the offlineGPUCount method.\n\ncolorSpace\n\nA color space object encapsulating color space information that is used to specify how color values are interpreted.\n\noptions\n\nA dictionary that contains options for creating a CIContext object. You can pass any of the keys defined in Context Options along with the appropriate value.\n\nsharedContext\n\nA CGL context with which to share OpenGL resources, obtained by calling the CGL function CGLCreateContext. Pass NULL to use a context that does not share OpenGL resources.\n\nReturn Value\n\nA Core Image context.\n\nDiscussion\n\nGPU devices that are not currently being used to drive a display can be used for Core Image rendering. Use the offlineGPUCount method to determine whether any such GPUs are available.\n\nTo create a Metal-based Core Image context using an offline GPU, use the MTLCopyAllDevices function to list Metal devices, then choose a device without a display to pass to the contextWithMTLDevice: method.\n\nSee Also\nCreating a Context for GPU-Based Rendering with OpenGL\n+ contextWithCGLContext:pixelFormat:colorSpace:options:\nCreates a Core Image context from a CGL context, using the specified options, color space, and pixel format object.\nDeprecated\n+ contextWithCGLContext:pixelFormat:options:\nCreates a Core Image context from a CGL context, using the specified options and pixel format object.\nDeprecated\n+ contextWithEAGLContext:\nCreates a Core Image context from an EAGL context.\nDeprecated\n+ contextWithEAGLContext:options:\nCreates a Core Image context from an EAGL context using the specified options.\nDeprecated\n+ contextForOfflineGPUAtIndex:\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display.\nDeprecated"
  },
  {
    "title": "outputColorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption/1438052-outputcolorspace",
    "html": "Discussion\n\nBy default, Core Image uses the GenericRGB color space, which leaves color matching to the system. You can specify a different output color space by providing a Quartz 2D CGColorSpace object. (See Quartz 2D Programming Guide for information on creating and using CGColorSpace objects.)\n\nTo request that Core Image perform no color management, specify the NSNull object as the value for this key. Use this option for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables)."
  },
  {
    "title": "workingColorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption/1437728-workingcolorspace",
    "html": "Discussion\n\nBy default, Core Image assumes that processing nodes are 128 bits-per-pixel, linear light, premultiplied RGBA floating-point values that use the GenericRGB color space. You can specify a different working color space by providing a Quartz 2D CGColorSpace object (CGColorSpace). Note that the working color space must be RGB based. If you have YUV data as input (or other data that is not RGB based), you can use ColorSync functions to convert to the working color space. (See Quartz 2D Programming Guide for information on creating and using CGColorSpace objects.)\n\nTo request that Core Image perform no color management, specify the NSNull object as the value for this key. Use this option for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables)."
  },
  {
    "title": "contextWithMTLDevice:options:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437711-contextwithmtldevice",
    "html": "Parameters\ndevice\n\nThe Metal device object to use for rendering.\n\noptions\n\nA dictionary that contains options for creating a CIContext object. You can pass any of the keys defined in Context Options along with the appropriate value.\n\nReturn Value\n\nA Core Image context.\n\nDiscussion\n\nUse this method to choose a specific Metal device for rendering when a system contains multiple Metal devices. To create a Metal-based context using the system’s default Metal device, use the initWithOptions: method.\n\nSee Also\nCreating a Context for GPU-Based Rendering with Metal\n+ contextWithMTLDevice:\nCreates a Core Image context using the specified Metal device."
  },
  {
    "title": "drawImage:atPoint:fromRect:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1473521-drawimage",
    "html": "Deprecated\n\nInstead use drawImage:inRect:fromRect:.\n\nParameters\nim\n\nA Core Image image object.\n\np\n\nThe point in the context destination to draw to.\n\nsrc\n\nThe region of the image to draw.\n\nDiscussion\n\nThis method because it is ambiguous as to the units of the dimensions and won’t work as expected in a high-resolution environment which is why you should use drawImage:inRect:fromRect: instead.\n\nOn iOS platforms, this method draws the image onto a render buffer for the OpenGL ES context. Use this method only if the CIContext object is created with contextWithEAGLContext:, and hence, you are rendering to a CAEAGLLayer.\n\nSee Also\nDrawing Images\n- drawImage:inRect:fromRect:\nRenders a region of an image to a rectangle in the context destination."
  },
  {
    "title": "contextWithEAGLContext:options:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1620362-contextwitheaglcontext",
    "html": "Parameters\neaglContext\n\nThe EAGL context to render to.\n\noptions\n\nA dictionary that contains options for creating a CIContext object. You can pass any of the keys defined in Context Options along with the appropriate value.\n\nReturn Value\n\nA Core Image context that targets OpenGL ES.\n\nDiscussion\n\nThe OpenGL ES context must support OpenGL ES 2.0. All drawing performed using the methods listed in Drawing Images is rendered directly into the context.\n\nSee Also\nCreating a Context for GPU-Based Rendering with OpenGL\n+ contextWithCGLContext:pixelFormat:colorSpace:options:\nCreates a Core Image context from a CGL context, using the specified options, color space, and pixel format object.\nDeprecated\n+ contextWithCGLContext:pixelFormat:options:\nCreates a Core Image context from a CGL context, using the specified options and pixel format object.\nDeprecated\n+ contextWithEAGLContext:\nCreates a Core Image context from an EAGL context.\nDeprecated\n+ contextForOfflineGPUAtIndex:\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display.\nDeprecated\n+ contextForOfflineGPUAtIndex:colorSpace:options:sharedContext:\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display, with the specified options.\nDeprecated"
  },
  {
    "title": "contextForOfflineGPUAtIndex:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437772-contextforofflinegpuatindex",
    "html": "Parameters\nindex\n\nThe index of the offline GPU with which to create the context; a number between zero and the value returned by the offlineGPUCount method.\n\nReturn Value\n\nA Core Image context.\n\nDiscussion\n\nGPU devices that are not currently being used to drive a display can be used for Core Image rendering. Use the offlineGPUCount method to determine whether any such GPUs are available.\n\nTo create a Metal-based Core Image context using an offline GPU, use the MTLCopyAllDevices function to list Metal devices, then choose a device without a display to pass to the contextWithMTLDevice: method.\n\nSee Also\nCreating a Context for GPU-Based Rendering with OpenGL\n+ contextWithCGLContext:pixelFormat:colorSpace:options:\nCreates a Core Image context from a CGL context, using the specified options, color space, and pixel format object.\nDeprecated\n+ contextWithCGLContext:pixelFormat:options:\nCreates a Core Image context from a CGL context, using the specified options and pixel format object.\nDeprecated\n+ contextWithEAGLContext:\nCreates a Core Image context from an EAGL context.\nDeprecated\n+ contextWithEAGLContext:options:\nCreates a Core Image context from an EAGL context using the specified options.\nDeprecated\n+ contextForOfflineGPUAtIndex:colorSpace:options:sharedContext:\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display, with the specified options.\nDeprecated"
  },
  {
    "title": "initWithOptions:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1438261-initwithoptions",
    "html": "Parameters\noptions\n\nA dictionary containing options for the context. For applicable keys and values, see Context Options.\n\nReturn Value\n\nAn initialized Core Image context.\n\nDiscussion\n\nIf you create a context without specifying a rendering destination, Core Image automatically chooses and internally manages a rendering destination based on the current device’s capabilities and your settings in the options dictionary. You cannot use a context without an explicit destination for the methods listed in Drawing Images. Instead, use the methods listed in Rendering Images.\n\nThe options dictionary defines behaviors for the context, such as color space and rendering quality. For example, to create a CPU-based context, use the kCIContextUseSoftwareRenderer key.\n\nSee Also\nCreating a Context Without Specifying a Destination\n+ context\nCreates a context without a specific rendering destination, using default options.\n- init\nInitializes a context without a specific rendering destination, using default options."
  },
  {
    "title": "contextWithEAGLContext:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1620419-contextwitheaglcontext",
    "html": "Parameters\neaglContext\n\nThe EAGL context to render to.\n\nReturn Value\n\nA Core Image context that targets OpenGL ES.\n\nDiscussion\n\nThe OpenGL ES context must support OpenGL ES 2.0. All drawing performed using the methods listed in Drawing Images is rendered directly into the context.\n\nSee Also\nCreating a Context for GPU-Based Rendering with OpenGL\n+ contextWithCGLContext:pixelFormat:colorSpace:options:\nCreates a Core Image context from a CGL context, using the specified options, color space, and pixel format object.\nDeprecated\n+ contextWithCGLContext:pixelFormat:options:\nCreates a Core Image context from a CGL context, using the specified options and pixel format object.\nDeprecated\n+ contextWithEAGLContext:options:\nCreates a Core Image context from an EAGL context using the specified options.\nDeprecated\n+ contextForOfflineGPUAtIndex:\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display.\nDeprecated\n+ contextForOfflineGPUAtIndex:colorSpace:options:sharedContext:\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display, with the specified options.\nDeprecated"
  },
  {
    "title": "contextWithCGLContext:pixelFormat:options:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1473525-contextwithcglcontext",
    "html": "Deprecated\n\nInstead use contextWithCGLContext:pixelFormat:colorSpace:options:.\n\nParameters\nctx\n\nA CGL context (CGLContextObj object) obtain by calling the CGL function CGLCreateContext.\n\npf\n\nA CGL pixel format object (CGLPixelFormatObj object) created by calling the CGL function CGLChoosePixelFormat. This argument must be the same pixel format object used to create the CGL context. The pixel format object must be valid for the lifetime of the Core Image context. Don’t release the pixel format object until after you release the Core Image context.\n\noptions\n\nA dictionary that contains color space information. You can provide the keys kCIContextOutputColorSpace or kCIContextWorkingColorSpace along with a CGColorSpaceRef object for each color space.\n\nSee Also\nCreating a Context for GPU-Based Rendering with OpenGL\n+ contextWithCGLContext:pixelFormat:colorSpace:options:\nCreates a Core Image context from a CGL context, using the specified options, color space, and pixel format object.\nDeprecated\n+ contextWithEAGLContext:\nCreates a Core Image context from an EAGL context.\nDeprecated\n+ contextWithEAGLContext:options:\nCreates a Core Image context from an EAGL context using the specified options.\nDeprecated\n+ contextForOfflineGPUAtIndex:\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display.\nDeprecated\n+ contextForOfflineGPUAtIndex:colorSpace:options:sharedContext:\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display, with the specified options.\nDeprecated\nRelated Documentation\n+ contextWithCGContext:options:\nCreates a Core Image context from a Quartz context, using the specified options."
  },
  {
    "title": "contextWithCGLContext:pixelFormat:colorSpace:options:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1438137-contextwithcglcontext",
    "html": "Parameters\nctx\n\nA CGL context obtained by calling the CGL function CGLCreateContext.\n\npf\n\nA CGL pixel format object either obtained from the system or created by calling a CGL function such as CGLChoosePixelFormat. This parameter must be the same pixel format object used to create the CGL context. The pixel format object must be valid for the lifetime of the Core Image context. Don’t release the pixel format object until after you release the Core Image context.\n\nspace\n\nA color space object encapsulating color space information that is used to specify how color values are interpreted.\n\noptions\n\nA dictionary that contains options for creating a CIContext object. You can pass any of the keys defined in Context Options along with the appropriate value.\n\nDiscussion\n\nAfter calling this method, Core Image draws content into the surface (drawable object) attached to the CGL context. A CGL context is a macOS OpenGL context. For more information, see OpenGL Programming Guide for Mac.\n\nWhen you create a CIContext object using a CGL context, all OpenGL states set for the CGL context affect rendering to that context. That means that coordinate and viewport transformations set on the CGL context, as well as the vertex color, affect drawing to that context.\n\nFor best results, follow these guidelines when you use Core Image to render into an OpenGL context:\n\nEnsure that a single unit in the coordinate space of the OpenGL context represents a single pixel in the output device.\n\nThe Core Image coordinate space has the origin in the bottom-left corner of the screen. You should configure the OpenGL context in the same way.\n\nThe OpenGL context blending state is respected by Core Image. If the image you want to render contains translucent pixels, it’s best to enable blending using a blend function with the parameters GL_ONE, GL_ONE_MINUS_SRC_ALPHA, as shown in the following code example.\n\nCore Image manages its own internal OpenGL context that shares resources with the OpenGL context you specify. To enable resource sharing, use the following code:\n\nconst NSOpenGLPixelFormatAttribute attr[] = {\n        NSOpenGLPFAAccelerated,\n        NSOpenGLPFANoRecovery,\n        NSOpenGLPFAColorSize, 32,\n        0\n    };\nNSOpenGLPixelFormat *pf = [[NSOpenGLPixelFormat alloc] initWithAttributes:(void *)&attr];\nCIContext *myCIContext = [CIContext contextWithCGLContext: CGLGetCurrentContext()\n                                pixelFormat: [pf CGLPixelFormatObj]\n                                colorSpace: CGColorSpaceCreateDeviceRGB()\n                                options: nil];\n\nSee Also\nCreating a Context for GPU-Based Rendering with OpenGL\n+ contextWithCGLContext:pixelFormat:options:\nCreates a Core Image context from a CGL context, using the specified options and pixel format object.\nDeprecated\n+ contextWithEAGLContext:\nCreates a Core Image context from an EAGL context.\nDeprecated\n+ contextWithEAGLContext:options:\nCreates a Core Image context from an EAGL context using the specified options.\nDeprecated\n+ contextForOfflineGPUAtIndex:\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display.\nDeprecated\n+ contextForOfflineGPUAtIndex:colorSpace:options:sharedContext:\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display, with the specified options.\nDeprecated\nRelated Documentation\n+ contextWithCGContext:options:\nCreates a Core Image context from a Quartz context, using the specified options."
  },
  {
    "title": "contextWithCGContext:options:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437864-contextwithcgcontext",
    "html": "Parameters\nctx\n\nA Quartz graphics context (CGContextRef object) either obtained from the system or created using a Quartz function such as CGBitmapContextCreate. See Quartz 2D Programming Guide for information on creating Quartz graphics contexts.\n\ndict\n\nA dictionary that contains color space information. You can pass any of the keys defined in Context Options along with the appropriate value.\n\nDiscussion\n\nAfter calling this method, Core Image draws content to the specified Quartz graphics context.\n\nWhen you create a CIContext object using a Quartz graphics context, any transformations that are already set on the Quartz graphics context affect drawing to that context.\n\nNote\n\nTo obtain a Core Image context for the current AppKit drawing context in macOS, use the NSGraphicsContext CIContext property."
  },
  {
    "title": "context",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1642219-context",
    "html": "Return Value\n\nA new Core Image context.\n\nDiscussion\n\nIf you create a context without specifying a rendering destination, Core Image automatically chooses and internally manages a rendering destination based on the current device’s capabilities. You cannot use a context without an explicit destination for the methods listed in Drawing Images. Instead, use the methods listed in Rendering Images.\n\nTo specify additional options for the context, use the initWithOptions: method instead.\n\nSee Also\nCreating a Context Without Specifying a Destination\n- init\nInitializes a context without a specific rendering destination, using default options.\n+ initWithOptions:\nInitializes a context without a specific rendering destination, using the specified options.\nRelated Documentation\nImage Unit Tutorial\nCore Image Programming Guide"
  },
  {
    "title": "textureTarget",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/1437613-texturetarget",
    "html": "Discussion\n\nThe value for this key must be an NSNumber object containing a supported OpenGL texture target constant, either GL_TEXTURE_2D or GL_TEXTURE_RECTANGLE_ARB. You may only use this key when initializing an image using the init(texture:size:flipped:options:) method.\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "textureFormat",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/1437934-textureformat",
    "html": "Discussion\n\nThe value for this key must be an NSNumber object containing a Core Image pixel format constant. (See Pixel Formats.) You may only use this key when initializing an image using the init(texture:size:flipped:options:) method.\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "initWithRect:",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltershape/1437921-initwithrect",
    "html": "Parameters\nr\n\nA rectangle. Core Image uses the rectangle specified by integer parts of the values in the CGRect data structure.\n\nReturn Value\n\nAn initialized CIFilterShape object, or nil if the method fails.\n\nSee Also\nRelated Documentation\n+ shapeWithRect:\nCreates a filter shape object and initializes it with a rectangle."
  },
  {
    "title": "nearestSampling",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/2867426-nearestsampling",
    "html": "Discussion\n\nThe value for this key is an NSNumber containing a Boolean value specifying whether the image should be sampled using nearest neighbor behavior. An unspecified value defaults to linear sampling.\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "auxiliaryDepth",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/2890795-auxiliarydepth",
    "html": "Discussion\n\nThe value of this key is an NSNumber containing a Boolean true or false. If the value is true, then calls to imageWithContentsOfURL:options: and imageWithData:options: will return the auxiliary image as a half-float monochrome image instead of the primary image, or nil if no auxiliary image exists.\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "colorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcubesmixedwithmask/3228143-colorspace",
    "html": "Required"
  },
  {
    "title": "applyOrientationProperty",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/2915369-applyorientationproperty",
    "html": "Discussion\n\nImages can contain metadata that reveals the orientation at capture time. You can load this metadata into CIImage with imageWithContentsOfURL: or init(data:) when the captured image contains orientation metadata. Use any of the initWith:options: methods if the properties (NSDictionary of metadata properties) option is also provided.\n\nIf the value of this key is true, then calls to imageWithContentsOfURL:options: and imageWithData:options: will return the image transformed according to its orientation metadata.\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "cacheIntermediates",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption/1642217-cacheintermediates",
    "html": "Discussion\n\nThe value for this key is an NSNumber object containing a Boolean value. If this value is false, the context empties such buffers during and after renders. The default value is true."
  },
  {
    "title": "auxiliaryDisparity",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/2890794-auxiliarydisparity",
    "html": "Discussion\n\nThe value of this key is an NSNumber containing a Boolean true or false. If the value is true, then calls to imageWithContentsOfURL:options: and imageWithData:options: will return the auxiliary image as a half-float monochrome image instead of the primary image, or nil if no auxiliary image exists.\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "properties",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/1437679-properties",
    "html": "Discussion\n\nTo ensure that an image has no metadata properties, set the value of this key to [NSNull null].\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "vectorWithX:Y:",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1564091-vectorwithx",
    "html": "Parameters\nx\n\nThe value for the first position in the vector.\n\ny\n\nThe value for the second position in the vector.\n\nReturn Value\n\nA vector initialized with the specified values.\n\nSee Also\nCreating a Vector\n+ vectorWithValues:count:\nCreates and returns a vector that is initialized with the specified values.\n+ vectorWithX:\nCreates and returns a vector that is initialized with one value.\n+ vectorWithX:Y:Z:\nCreates and returns a vector that is initialized with three values.\n+ vectorWithX:Y:Z:W:\nCreates and returns a vector that is initialized with four values.\n+ vectorWithString:\nCreates and returns a vector that is initialized with values provided in a string representation.\n+ vectorWithCGAffineTransform:\nCreates and returns a vector that is initialized with values provided by a CGAffineTransform structure.\n+ vectorWithCGPoint:\nCreates and returns a vector that is initialized with values provided by a CGPoint structure.\n+ vectorWithCGRect:\nCreates and returns a vector that is initialized with values provided by a CGRect structure."
  },
  {
    "title": "kCIImageAuxiliaryPortraitEffectsMatte",
    "url": "https://developer.apple.com/documentation/coreimage/kciimageauxiliaryportraiteffectsmatte",
    "html": "Discussion\n\nThe value of this key is an NSNumber containing a Boolean YES or NO.\n\nSee Also\nImage Dictionary Keys\nkCIImageColorSpace\nThe key for a color space.\nkCIImageProperties\nThe key for image metadata properties.\nkCIImageApplyOrientationProperty\nThe key for transforming an image according to orientation metadata.\nkCIImageTextureTarget\nThe key for an OpenGL texture target.\nDeprecated\nkCIImageTextureFormat\nThe key for an OpenGL texture format.\nDeprecated\nkCIImageNearestSampling\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nkCIImageAuxiliaryDepth\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nkCIImageAuxiliaryDisparity\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cispotlight/3228745-inputimage",
    "html": "Required"
  },
  {
    "title": "lightPosition",
    "url": "https://developer.apple.com/documentation/coreimage/cispotlight/3228747-lightposition",
    "html": "Required"
  },
  {
    "title": "lightPointsAt",
    "url": "https://developer.apple.com/documentation/coreimage/cispotlight/3228746-lightpointsat",
    "html": "Required"
  },
  {
    "title": "imageAccumulatorWithExtent:format:colorSpace:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageaccumulator/1427712-imageaccumulatorwithextent",
    "html": "Parameters\nextent\n\nA rectangle that specifies the x-value of the rectangle origin, the y-value of the rectangle origin, and the width and height.\n\nformat\n\nThe format and size of each pixel. You must supply a pixel format constant, such as kCIFormatARGB8 (32 bit-per-pixel, fixed-point pixel format) or kCIFormatRGBAf (128 bit-per-pixel, floating-point pixel format). See CIImage for more information about pixel format constants.\n\ncolorSpace\n\nA CGColorSpaceRef object describing the color space for the image accumulator.\n\nReturn Value\n\nThe image accumulator object.\n\nSee Also\nCreating an Image Accumulator\n+ imageAccumulatorWithExtent:format:\nCreates an image accumulator with the specified extent and pixel format.\nRelated Documentation\n- initWithExtent:format:\nInitializes an image accumulator with the specified extent and pixel format."
  },
  {
    "title": "initWithExtent:format:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageaccumulator/1427718-initwithextent",
    "html": "Parameters\nextent\n\nA rectangle that specifies the x-value of the rectangle origin, the y-value of the rectangle origin, and the width and height.\n\nformat\n\nThe format and size of each pixel. You must supply a pixel format constant, such askCIFormatARGB8 (32 bit-per-pixel, fixed-point pixel format) or kCIFormatRGBAf (128 bit-per-pixel, floating-point pixel format). See CIImage for more information about pixel format constants.\n\nReturn Value\n\nThe initialized image accumulator object.\n\nSee Also\nInitializing an Image Accumulator\n- initWithExtent:format:colorSpace:\nInitializes an image accumulator with the specified extent, pixel format, and color space.\nRelated Documentation\n+ imageAccumulatorWithExtent:format:\nCreates an image accumulator with the specified extent and pixel format."
  },
  {
    "title": "vectorWithCGRect:",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1564085-vectorwithcgrect",
    "html": "Parameters\nr\n\nA rect.\n\nReturn Value\n\nA vector initialized with the specified values.\n\nDiscussion\n\nThe CGRect structure’s X, Y, height and width values are stored in the vector’s X, Y, Z and W properties.\n\nSee Also\nCreating a Vector\n+ vectorWithValues:count:\nCreates and returns a vector that is initialized with the specified values.\n+ vectorWithX:\nCreates and returns a vector that is initialized with one value.\n+ vectorWithX:Y:\nCreates and returns a vector that is initialized with two values.\n+ vectorWithX:Y:Z:\nCreates and returns a vector that is initialized with three values.\n+ vectorWithX:Y:Z:W:\nCreates and returns a vector that is initialized with four values.\n+ vectorWithString:\nCreates and returns a vector that is initialized with values provided in a string representation.\n+ vectorWithCGAffineTransform:\nCreates and returns a vector that is initialized with values provided by a CGAffineTransform structure.\n+ vectorWithCGPoint:\nCreates and returns a vector that is initialized with values provided by a CGPoint structure."
  },
  {
    "title": "initWithExtent:format:colorSpace:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageaccumulator/1427710-initwithextent",
    "html": "Parameters\nextent\n\nA rectangle that specifies the x-value of the rectangle origin, the y-value of the rectangle origin, and the width and height.\n\nformat\n\nThe format and size of each pixel. You must supply a pixel format constant, such askCIFormatARGB8 (32 bit-per-pixel, fixed-point pixel format) or kCIFormatRGBAf (128 bit-per-pixel, floating-point pixel format). See CIImage for more information about pixel format constants.\n\ncolorSpace\n\nA CGColorSpaceRef object describing the color space for the image accumulator.\n\nReturn Value\n\nThe initialized image accumulator object.\n\nSee Also\nInitializing an Image Accumulator\n- initWithExtent:format:\nInitializes an image accumulator with the specified extent and pixel format.\nRelated Documentation\n+ imageAccumulatorWithExtent:format:\nCreates an image accumulator with the specified extent and pixel format."
  },
  {
    "title": "vectorWithCGPoint:",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1564086-vectorwithcgpoint",
    "html": "Parameters\np\n\nA point.\n\nReturn Value\n\nA vector initialized with the specified values.\n\nDiscussion\n\nThe CGPoint structure’s X and Y values are stored in the vector’s X and Y properties.\n\nSee Also\nCreating a Vector\n+ vectorWithValues:count:\nCreates and returns a vector that is initialized with the specified values.\n+ vectorWithX:\nCreates and returns a vector that is initialized with one value.\n+ vectorWithX:Y:\nCreates and returns a vector that is initialized with two values.\n+ vectorWithX:Y:Z:\nCreates and returns a vector that is initialized with three values.\n+ vectorWithX:Y:Z:W:\nCreates and returns a vector that is initialized with four values.\n+ vectorWithString:\nCreates and returns a vector that is initialized with values provided in a string representation.\n+ vectorWithCGAffineTransform:\nCreates and returns a vector that is initialized with values provided by a CGAffineTransform structure.\n+ vectorWithCGRect:\nCreates and returns a vector that is initialized with values provided by a CGRect structure."
  },
  {
    "title": "point3",
    "url": "https://developer.apple.com/documentation/coreimage/citonecurve/3228796-point3",
    "html": "Required"
  },
  {
    "title": "point4",
    "url": "https://developer.apple.com/documentation/coreimage/citonecurve/3228797-point4",
    "html": "Required"
  },
  {
    "title": "point2",
    "url": "https://developer.apple.com/documentation/coreimage/citonecurve/3228795-point2",
    "html": "Required"
  },
  {
    "title": "vectorWithCGAffineTransform:",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1564090-vectorwithcgaffinetransform",
    "html": "Parameters\nt\n\nA transform.\n\nReturn Value\n\nA vector initialized with the specified values.\n\nDiscussion\n\nThe six values that comprise the affine transform fill the first six positions of the resulting CIVector object.\n\nSee Also\nCreating a Vector\n+ vectorWithValues:count:\nCreates and returns a vector that is initialized with the specified values.\n+ vectorWithX:\nCreates and returns a vector that is initialized with one value.\n+ vectorWithX:Y:\nCreates and returns a vector that is initialized with two values.\n+ vectorWithX:Y:Z:\nCreates and returns a vector that is initialized with three values.\n+ vectorWithX:Y:Z:W:\nCreates and returns a vector that is initialized with four values.\n+ vectorWithString:\nCreates and returns a vector that is initialized with values provided in a string representation.\n+ vectorWithCGPoint:\nCreates and returns a vector that is initialized with values provided by a CGPoint structure.\n+ vectorWithCGRect:\nCreates and returns a vector that is initialized with values provided by a CGRect structure."
  },
  {
    "title": "vectorWithX:Y:Z:W:",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1564087-vectorwithx",
    "html": "Parameters\nx\n\nThe value for the first position in the vector.\n\ny\n\nThe value for the second position in the vector.\n\nz\n\nThe value for the third position in the vector.\n\nw\n\nThe value for the fourth position in the vector.\n\nReturn Value\n\nA vector initialized with the specified values.\n\nSee Also\nCreating a Vector\n+ vectorWithValues:count:\nCreates and returns a vector that is initialized with the specified values.\n+ vectorWithX:\nCreates and returns a vector that is initialized with one value.\n+ vectorWithX:Y:\nCreates and returns a vector that is initialized with two values.\n+ vectorWithX:Y:Z:\nCreates and returns a vector that is initialized with three values.\n+ vectorWithString:\nCreates and returns a vector that is initialized with values provided in a string representation.\n+ vectorWithCGAffineTransform:\nCreates and returns a vector that is initialized with values provided by a CGAffineTransform structure.\n+ vectorWithCGPoint:\nCreates and returns a vector that is initialized with values provided by a CGPoint structure.\n+ vectorWithCGRect:\nCreates and returns a vector that is initialized with values provided by a CGRect structure."
  },
  {
    "title": "point1",
    "url": "https://developer.apple.com/documentation/coreimage/citonecurve/3228794-point1",
    "html": "Required"
  },
  {
    "title": "point0",
    "url": "https://developer.apple.com/documentation/coreimage/citonecurve/3228793-point0",
    "html": "Required"
  },
  {
    "title": "vectorWithX:Y:Z:",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1564089-vectorwithx",
    "html": "Parameters\nx\n\nThe value for the first position in the vector.\n\ny\n\nThe value for the second position in the vector.\n\nz\n\nThe value for the third position in the vector.\n\nReturn Value\n\nA vector initialized with the specified values.\n\nSee Also\nCreating a Vector\n+ vectorWithValues:count:\nCreates and returns a vector that is initialized with the specified values.\n+ vectorWithX:\nCreates and returns a vector that is initialized with one value.\n+ vectorWithX:Y:\nCreates and returns a vector that is initialized with two values.\n+ vectorWithX:Y:Z:W:\nCreates and returns a vector that is initialized with four values.\n+ vectorWithString:\nCreates and returns a vector that is initialized with values provided in a string representation.\n+ vectorWithCGAffineTransform:\nCreates and returns a vector that is initialized with values provided by a CGAffineTransform structure.\n+ vectorWithCGPoint:\nCreates and returns a vector that is initialized with values provided by a CGPoint structure.\n+ vectorWithCGRect:\nCreates and returns a vector that is initialized with values provided by a CGRect structure."
  },
  {
    "title": "vectorWithValues:count:",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1564088-vectorwithvalues",
    "html": "Parameters\nvalues\n\nThe values to initialize the vector with.\n\ncount\n\nThe number of values in the vector.\n\nReturn Value\n\nA vector initialized with the provided values.\n\nSee Also\nCreating a Vector\n+ vectorWithX:\nCreates and returns a vector that is initialized with one value.\n+ vectorWithX:Y:\nCreates and returns a vector that is initialized with two values.\n+ vectorWithX:Y:Z:\nCreates and returns a vector that is initialized with three values.\n+ vectorWithX:Y:Z:W:\nCreates and returns a vector that is initialized with four values.\n+ vectorWithString:\nCreates and returns a vector that is initialized with values provided in a string representation.\n+ vectorWithCGAffineTransform:\nCreates and returns a vector that is initialized with values provided by a CGAffineTransform structure.\n+ vectorWithCGPoint:\nCreates and returns a vector that is initialized with values provided by a CGPoint structure.\n+ vectorWithCGRect:\nCreates and returns a vector that is initialized with values provided by a CGRect structure.\nRelated Documentation\nCore Image Programming Guide"
  },
  {
    "title": "vectorWithX:",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1564092-vectorwithx",
    "html": "Parameters\nx\n\nThe value to initialize the vector with.\n\nReturn Value\n\nA vector initialized with the specified value.\n\nSee Also\nCreating a Vector\n+ vectorWithValues:count:\nCreates and returns a vector that is initialized with the specified values.\n+ vectorWithX:Y:\nCreates and returns a vector that is initialized with two values.\n+ vectorWithX:Y:Z:\nCreates and returns a vector that is initialized with three values.\n+ vectorWithX:Y:Z:W:\nCreates and returns a vector that is initialized with four values.\n+ vectorWithString:\nCreates and returns a vector that is initialized with values provided in a string representation.\n+ vectorWithCGAffineTransform:\nCreates and returns a vector that is initialized with values provided by a CGAffineTransform structure.\n+ vectorWithCGPoint:\nCreates and returns a vector that is initialized with values provided by a CGPoint structure.\n+ vectorWithCGRect:\nCreates and returns a vector that is initialized with values provided by a CGRect structure."
  },
  {
    "title": "contextWithMTLDevice:",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437609-contextwithmtldevice",
    "html": "Parameters\ndevice\n\nThe Metal device object to use for rendering.\n\nReturn Value\n\nA Core Image context.\n\nDiscussion\n\nUse this method to choose a specific Metal device for rendering when a system contains multiple Metal devices. To create a Metal-based context using the system’s default Metal device, use the initWithOptions: method.\n\nSee Also\nCreating a Context for GPU-Based Rendering with Metal\n+ contextWithMTLDevice:options:\nCreates a Core Image context using the specified Metal device and options."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/citonecurve/3228792-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciareareductionfilter/3547102-inputimage",
    "html": "Required"
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/ciareareductionfilter/3547101-extent",
    "html": "Required"
  },
  {
    "title": "providerTileSize",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/1437829-providertilesize",
    "html": "See Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cimaskedvariableblur/3228553-radius",
    "html": "Required"
  },
  {
    "title": "maxWidth",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417barcodegenerator/3228611-maxwidth",
    "html": "Required"
  },
  {
    "title": "bottomLeft",
    "url": "https://developer.apple.com/documentation/coreimage/cifourcoordinategeometryfilter/3338731-bottomleft",
    "html": "Required"
  },
  {
    "title": "topLeft",
    "url": "https://developer.apple.com/documentation/coreimage/cifourcoordinategeometryfilter/3338734-topleft",
    "html": "Required"
  },
  {
    "title": "topRight",
    "url": "https://developer.apple.com/documentation/coreimage/cifourcoordinategeometryfilter/3338735-topright",
    "html": "Required"
  },
  {
    "title": "providerUserInfo",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/1437989-provideruserinfo",
    "html": "See Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cifourcoordinategeometryfilter/3338733-inputimage",
    "html": "Required"
  },
  {
    "title": "bottomRight",
    "url": "https://developer.apple.com/documentation/coreimage/cifourcoordinategeometryfilter/3338732-bottomright",
    "html": "Required"
  },
  {
    "title": "initWithMTLTexture:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437890-initwithmtltexture",
    "html": "Parameters\ntexture\n\nThe Metal texture from which to use image data.\n\noptions\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nThe initialized image object, or nil if the image could not be initialized.\n\nDiscussion\n\nTo render the image using Metal, use this image with a Metal-based CIContext object created with the contextWithMTLDevice: method, and call the render:toMTLTexture:commandBuffer:bounds:colorSpace: method to create an output image in another Metal texture object.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "initWithCVPixelBuffer:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438209-initwithcvpixelbuffer",
    "html": "Parameters\nbuffer\n\nA CVPixelBuffer object.\n\ndict\n\nA dictionary that contains options for creating an image object. (See Image Dictionary Keys.) The pixel format is supplied by the CVPixelBuffer object.\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcube/3228136-inputimage",
    "html": "Required"
  },
  {
    "title": "extrapolate",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcube/4013843-extrapolate",
    "html": "Required"
  },
  {
    "title": "blueCoefficients",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcrosspolynomial/3228129-bluecoefficients",
    "html": "Required"
  },
  {
    "title": "greenCoefficients",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcrosspolynomial/3228130-greencoefficients",
    "html": "Required"
  },
  {
    "title": "redCoefficients",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcrosspolynomial/3228132-redcoefficients",
    "html": "Required"
  },
  {
    "title": "cube0Data",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcubesmixedwithmask/3228144-cube0data",
    "html": "Required\n\nDiscussion\n\nThis filter maps color values in the input image to new color values using a three-dimensional color lookup table. For each RGBA pixel in the input image, the filter uses the R, G, and B component values as indices to identify a location in the table; the RGBA value at that location becomes the RGBA value of the output pixel.\n\nUse the cubeData parameter to provide data formatted for use as a color lookup table, and the inputCubeDimension parameter to specify the size of the table. This data should be an array of texel values in 32-bit floating-point RGBA linear premultiplied format. The inputCubeDimension parameter identifies the size of the cube by specifying the length of one side, so the size of the array should be cubeDimension cubed times the size of a single texel value. In the color table, the R component varies fastest, followed by G, then B."
  },
  {
    "title": "cubeDimension",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcubesmixedwithmask/3228146-cubedimension",
    "html": "Required"
  },
  {
    "title": "colorWithString:",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1438059-colorwithstring",
    "html": "Parameters\nrepresentation\n\nA string that is in one of the formats returned by the stringRepresentation method. For example, the string:\n\n@\"0.5 0.7 0.3 1.0\"\n\nindicates an RGB color whose components are 50% red, 70% green, 30% blue, and 100% opaque (alpha value of 1.0). The string representation always has four components—red, green, blue, and alpha. The default value for the alpha component is 1.0.\n\nReturn Value\n\nA Core Image color object that represents an RGB color in the color space specified by the Quartz 2D constant kCGColorSpaceGenericRGB.\n\nSee Also\nCreating Color Objects\n+ colorWithCGColor:\nCreates a color object from a Quartz color.\n+ colorWithRed:green:blue:\nCreates a color object using the specified RGB color component values\n+ colorWithRed:green:blue:alpha:\nCreates a color object using the specified RGBA color component values.\n+ colorWithRed:green:blue:colorSpace:\nInitializes a Core Image color object with the specified red, green, and blue component values as measured in the specified color space.\n+ colorWithRed:green:blue:alpha:colorSpace:\nCreates a Core Image color object with the specified red, green, blue, and alpha component values as measured in the specified color space."
  },
  {
    "title": "maskImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcubesmixedwithmask/3228148-maskimage",
    "html": "Required"
  },
  {
    "title": "cube1Data",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcubesmixedwithmask/3228145-cube1data",
    "html": "Required\n\nDiscussion\n\nThis filter maps color values in the input image to new color values using a three-dimensional color lookup table. For each RGBA pixel in the input image, the filter uses the R, G, and B component values as indices to identify a location in the table; the RGBA value at that location becomes the RGBA value of the output pixel.\n\nUse the cubeData parameter to provide data formatted for use as a color lookup table, and the inputCubeDimension parameter to specify the size of the table. This data should be an array of texel values in 32-bit floating-point RGBA linear premultiplied format. The inputCubeDimension parameter identifies the size of the cube by specifying the length of one side, so the size of the array should be cubeDimension cubed times the size of a single texel value. In the color table, the R component varies fastest, followed by G, then B."
  },
  {
    "title": "colorWithRed:green:blue:",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1437941-colorwithred",
    "html": "Parameters\nr\n\nThe value of the red component.\n\ng\n\nThe value of the green component.\n\nb\n\nThe value of the blue component.\n\nReturn Value\n\nA Core Image color object that represents an RGB color in the color space specified by the Quartz 2D constant kCGColorSpaceGenericRGB.\n\nSee Also\nCreating Color Objects\n+ colorWithCGColor:\nCreates a color object from a Quartz color.\n+ colorWithRed:green:blue:alpha:\nCreates a color object using the specified RGBA color component values.\n+ colorWithString:\nCreates a color object using the RGBA color component values specified by a string.\n+ colorWithRed:green:blue:colorSpace:\nInitializes a Core Image color object with the specified red, green, and blue component values as measured in the specified color space.\n+ colorWithRed:green:blue:alpha:colorSpace:\nCreates a Core Image color object with the specified red, green, blue, and alpha component values as measured in the specified color space."
  },
  {
    "title": "colorWithRed:green:blue:alpha:colorSpace:",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1643575-colorwithred",
    "html": "Parameters\nr\n\nThe unpremultiplied red component of the color.\n\ng\n\nThe unpremultiplied green component of the color.\n\nb\n\nThe unpremultiplied blue component of the color.\n\na\n\nThe alpha (opacity) component of the color.\n\ncolorSpace\n\nThe color space in which to create the new color. This color space must conform to the kCGColorSpaceModelRGB color space model.\n\nReturn Value\n\nThe resulting Core Image color.\n\nSee Also\nCreating Color Objects\n+ colorWithCGColor:\nCreates a color object from a Quartz color.\n+ colorWithRed:green:blue:\nCreates a color object using the specified RGB color component values\n+ colorWithRed:green:blue:alpha:\nCreates a color object using the specified RGBA color component values.\n+ colorWithString:\nCreates a color object using the RGBA color component values specified by a string.\n+ colorWithRed:green:blue:colorSpace:\nInitializes a Core Image color object with the specified red, green, and blue component values as measured in the specified color space."
  },
  {
    "title": "cubeData",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcubewithcolorspace/3228139-cubedata",
    "html": "Required\n\nDiscussion\n\nThis filter maps color values in the input image to new color values using a three-dimensional color lookup table. For each RGBA pixel in the input image, the filter uses the R, G, and B component values as indices to identify a location in the table; the RGBA value at that location becomes the RGBA value of the output pixel.\n\nUse the cubeData parameter to provide data formatted for use as a color lookup table, and the inputCubeDimension parameter to specify the size of the table. This data should be an array of texel values in 32-bit floating-point RGBA linear premultiplied format. The inputCubeDimension parameter identifies the size of the cube by specifying the length of one side, so the size of the array should be cubeDimension cubed times the size of a single texel value. In the color table, the R component varies fastest, followed by G, then B."
  },
  {
    "title": "colorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcubewithcolorspace/3228138-colorspace",
    "html": "Required"
  },
  {
    "title": "colorWithRed:green:blue:colorSpace:",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1643579-colorwithred",
    "html": "Parameters\nr\n\nThe unpremultiplied red component of the color.\n\ng\n\nThe unpremultiplied green component of the color.\n\nb\n\nThe unpremultiplied blue component of the color.\n\ncolorSpace\n\nThe color space in which to create the new color. This color space must conform to the kCGColorSpaceModelRGB color space model.\n\nReturn Value\n\nThe resulting Core Image color.\n\nSee Also\nCreating Color Objects\n+ colorWithCGColor:\nCreates a color object from a Quartz color.\n+ colorWithRed:green:blue:\nCreates a color object using the specified RGB color component values\n+ colorWithRed:green:blue:alpha:\nCreates a color object using the specified RGBA color component values.\n+ colorWithString:\nCreates a color object using the RGBA color component values specified by a string.\n+ colorWithRed:green:blue:alpha:colorSpace:\nCreates a Core Image color object with the specified red, green, blue, and alpha component values as measured in the specified color space."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcubesmixedwithmask/3228147-inputimage",
    "html": "Required"
  },
  {
    "title": "extrapolate",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcubesmixedwithmask/3994650-extrapolate",
    "html": "Required"
  },
  {
    "title": "cubeDimension",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcubewithcolorspace/3228140-cubedimension",
    "html": "Required"
  },
  {
    "title": "detailSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801624-detailsupported",
    "html": "Discussion\n\nIf this value is YES, you can adjust the amount of detail enhancement to apply to the image by setting detailAmount.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nsupportedCameraModels\nAn array containing the names of all supported camera models.\nsupportedDecoderVersions\nAn array of all supported decoder versions for the given image type.\ncolorNoiseReductionSupported\nA Boolean that indicates if the current image supports color noise reduction adjustments.\ncontrastSupported\nA Boolean that indicates if the current image supports contrast adjustments.\nlensCorrectionSupported\nA Boolean that indicates if you can enable lens correction for the current image.\nlocalToneMapSupported\nA Boolean that indicates if the current image supports local tone curve adjustments.\nluminanceNoiseReductionSupported\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nmoireReductionSupported\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nsharpnessSupported\nA Boolean that indicates if the current image supports sharpness adjustments.\nnativeSize\nThe full native size of the unscaled image."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcubewithcolorspace/3228141-inputimage",
    "html": "Required"
  },
  {
    "title": "luminanceNoiseReductionSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801638-luminancenoisereductionsupported",
    "html": "Discussion\n\nIf this value is YES, you can adjust the amount of luminance noise reduction to apply to the image by setting luminanceNoiseReductionAmount.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nsupportedCameraModels\nAn array containing the names of all supported camera models.\nsupportedDecoderVersions\nAn array of all supported decoder versions for the given image type.\ncolorNoiseReductionSupported\nA Boolean that indicates if the current image supports color noise reduction adjustments.\ncontrastSupported\nA Boolean that indicates if the current image supports contrast adjustments.\ndetailSupported\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nlensCorrectionSupported\nA Boolean that indicates if you can enable lens correction for the current image.\nlocalToneMapSupported\nA Boolean that indicates if the current image supports local tone curve adjustments.\nmoireReductionSupported\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nsharpnessSupported\nA Boolean that indicates if the current image supports sharpness adjustments.\nnativeSize\nThe full native size of the unscaled image."
  },
  {
    "title": "lensCorrectionSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801633-lenscorrectionsupported",
    "html": "Discussion\n\nIf this value is YES, you can enable lens correction on the image by setting lensCorrectionEnabled to YES.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nsupportedCameraModels\nAn array containing the names of all supported camera models.\nsupportedDecoderVersions\nAn array of all supported decoder versions for the given image type.\ncolorNoiseReductionSupported\nA Boolean that indicates if the current image supports color noise reduction adjustments.\ncontrastSupported\nA Boolean that indicates if the current image supports contrast adjustments.\ndetailSupported\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nlocalToneMapSupported\nA Boolean that indicates if the current image supports local tone curve adjustments.\nluminanceNoiseReductionSupported\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nmoireReductionSupported\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nsharpnessSupported\nA Boolean that indicates if the current image supports sharpness adjustments.\nnativeSize\nThe full native size of the unscaled image."
  },
  {
    "title": "colorNoiseReductionSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801619-colornoisereductionsupported",
    "html": "Discussion\n\nIf this value is YES, you can adjust the amount of color noise reduction to apply to the image by setting colorNoiseReductionAmount.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nsupportedCameraModels\nAn array containing the names of all supported camera models.\nsupportedDecoderVersions\nAn array of all supported decoder versions for the given image type.\ncontrastSupported\nA Boolean that indicates if the current image supports contrast adjustments.\ndetailSupported\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nlensCorrectionSupported\nA Boolean that indicates if you can enable lens correction for the current image.\nlocalToneMapSupported\nA Boolean that indicates if the current image supports local tone curve adjustments.\nluminanceNoiseReductionSupported\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nmoireReductionSupported\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nsharpnessSupported\nA Boolean that indicates if the current image supports sharpness adjustments.\nnativeSize\nThe full native size of the unscaled image."
  },
  {
    "title": "contrastSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801621-contrastsupported",
    "html": "Discussion\n\nIf this value is YES, you can adjust the amount of contrast to apply to the image by setting contrastAmount.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nsupportedCameraModels\nAn array containing the names of all supported camera models.\nsupportedDecoderVersions\nAn array of all supported decoder versions for the given image type.\ncolorNoiseReductionSupported\nA Boolean that indicates if the current image supports color noise reduction adjustments.\ndetailSupported\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nlensCorrectionSupported\nA Boolean that indicates if you can enable lens correction for the current image.\nlocalToneMapSupported\nA Boolean that indicates if the current image supports local tone curve adjustments.\nluminanceNoiseReductionSupported\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nmoireReductionSupported\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nsharpnessSupported\nA Boolean that indicates if the current image supports sharpness adjustments.\nnativeSize\nThe full native size of the unscaled image."
  },
  {
    "title": "localToneMapSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801636-localtonemapsupported",
    "html": "Discussion\n\nIf this value is YES, you can adjust the amount of local tone curve to apply to the image by setting localToneMapAmount.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nsupportedCameraModels\nAn array containing the names of all supported camera models.\nsupportedDecoderVersions\nAn array of all supported decoder versions for the given image type.\ncolorNoiseReductionSupported\nA Boolean that indicates if the current image supports color noise reduction adjustments.\ncontrastSupported\nA Boolean that indicates if the current image supports contrast adjustments.\ndetailSupported\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nlensCorrectionSupported\nA Boolean that indicates if you can enable lens correction for the current image.\nluminanceNoiseReductionSupported\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nmoireReductionSupported\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nsharpnessSupported\nA Boolean that indicates if the current image supports sharpness adjustments.\nnativeSize\nThe full native size of the unscaled image."
  },
  {
    "title": "extrapolate",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcubewithcolorspace/3994649-extrapolate",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologygradient/3228574-inputimage",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cimotionblur/3228591-angle",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologygradient/3228575-radius",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cimotionblur/3228592-inputimage",
    "html": "Required"
  },
  {
    "title": "colorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/1438131-colorspace",
    "html": "Discussion\n\nFor more information on this data type see CGColorSpace. Typically you use this option when you need to load an elevation, mask, normal vector, or RAW sensor data directly from a file without color correcting it. This constant specifies to override Core Image, which, by default, assumes that data is in GenericRGB.\n\nThe value you supply for this dictionary key must be a CGColorSpace data type. If a value for this key isn’t supplied, the image’s colorSpace dictionary are populated automatically by calling CGImageSourceCopyPropertiesAtIndex(_:_:_:). To request that Core Image perform no color management, specify the NSNull object as the value for this key. Use this option for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables).\n\nSee Also\nImage Dictionary Keys\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "color",
    "url": "https://developer.apple.com/documentation/coreimage/cispotlight/3228743-color",
    "html": "Required"
  },
  {
    "title": "brightness",
    "url": "https://developer.apple.com/documentation/coreimage/cispotlight/3228742-brightness",
    "html": "Required"
  },
  {
    "title": "concentration",
    "url": "https://developer.apple.com/documentation/coreimage/cispotlight/3228744-concentration",
    "html": "Required\n\nDiscussion\n\nThe smaller the value, the more tightly focused the light beam."
  },
  {
    "title": "striationContrast",
    "url": "https://developer.apple.com/documentation/coreimage/cilenticularhalogenerator/3228525-striationcontrast",
    "html": "Required"
  },
  {
    "title": "moireReductionSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801640-moirereductionsupported",
    "html": "Discussion\n\nIf this value is YES, you can adjust the amount of moire artifact reduction to apply to the image by setting moireReductionAmount.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nsupportedCameraModels\nAn array containing the names of all supported camera models.\nsupportedDecoderVersions\nAn array of all supported decoder versions for the given image type.\ncolorNoiseReductionSupported\nA Boolean that indicates if the current image supports color noise reduction adjustments.\ncontrastSupported\nA Boolean that indicates if the current image supports contrast adjustments.\ndetailSupported\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nlensCorrectionSupported\nA Boolean that indicates if you can enable lens correction for the current image.\nlocalToneMapSupported\nA Boolean that indicates if the current image supports local tone curve adjustments.\nluminanceNoiseReductionSupported\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nsharpnessSupported\nA Boolean that indicates if the current image supports sharpness adjustments.\nnativeSize\nThe full native size of the unscaled image."
  },
  {
    "title": "sharpnessSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801658-sharpnesssupported",
    "html": "Discussion\n\nIf this value is YES, you can adjust the amount of sharpness to apply to the image by setting sharpnessAmount.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nsupportedCameraModels\nAn array containing the names of all supported camera models.\nsupportedDecoderVersions\nAn array of all supported decoder versions for the given image type.\ncolorNoiseReductionSupported\nA Boolean that indicates if the current image supports color noise reduction adjustments.\ncontrastSupported\nA Boolean that indicates if the current image supports contrast adjustments.\ndetailSupported\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nlensCorrectionSupported\nA Boolean that indicates if you can enable lens correction for the current image.\nlocalToneMapSupported\nA Boolean that indicates if the current image supports local tone curve adjustments.\nluminanceNoiseReductionSupported\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nmoireReductionSupported\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nnativeSize\nThe full native size of the unscaled image."
  },
  {
    "title": "yaw",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectiverotate/3325541-yaw",
    "html": "Required"
  },
  {
    "title": "height",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologyrectangleminimum/3228587-height",
    "html": "Required\n\nDiscussion\n\nThe value is rounded to the nearest odd integer."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologyrectangleminimum/3228588-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciboxblur/3228094-inputimage",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/ciboxblur/3228095-radius",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cigaussianblur/3228465-radius",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cigaussianblur/3228464-inputimage",
    "html": "Required"
  },
  {
    "title": "scale",
    "url": "https://developer.apple.com/documentation/coreimage/cibicubicscaletransform/3228078-scale",
    "html": "Required\n\nDiscussion\n\nValues less than 1.0 scale down the images. Values greater than 1.0 scale up the image."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cibicubicscaletransform/3228075-inputimage",
    "html": "Required"
  },
  {
    "title": "parameterC",
    "url": "https://developer.apple.com/documentation/coreimage/cibicubicscaletransform/3228077-parameterc",
    "html": "Required"
  },
  {
    "title": "aspectRatio",
    "url": "https://developer.apple.com/documentation/coreimage/cibicubicscaletransform/3228074-aspectratio",
    "html": "Required"
  },
  {
    "title": "parameterB",
    "url": "https://developer.apple.com/documentation/coreimage/cibicubicscaletransform/3228076-parameterb",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciedgepreserveupsample/3228237-inputimage",
    "html": "Required"
  },
  {
    "title": "lumaSigma",
    "url": "https://developer.apple.com/documentation/coreimage/ciedgepreserveupsample/3228238-lumasigma",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cipointillize/3228679-inputimage",
    "html": "Required"
  },
  {
    "title": "smallImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciedgepreserveupsample/3228239-smallimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cimaskedvariableblur/3228551-inputimage",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cistraighten/3228758-angle",
    "html": "Required"
  },
  {
    "title": "mask",
    "url": "https://developer.apple.com/documentation/coreimage/cimaskedvariableblur/3228552-mask",
    "html": "Required"
  },
  {
    "title": "spatialSigma",
    "url": "https://developer.apple.com/documentation/coreimage/ciedgepreserveupsample/3228240-spatialsigma",
    "html": "Required"
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectivetransformwithextent/3228667-extent",
    "html": "Required"
  },
  {
    "title": "compactStyle",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417barcodegenerator/3228606-compactstyle",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cistraighten/3228759-inputimage",
    "html": "Required"
  },
  {
    "title": "ringAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cibokehblur/3228090-ringamount",
    "html": "Required"
  },
  {
    "title": "initWithImage:options:",
    "url": "https://developer.apple.com/documentation/coreimage/cisampler/1437963-initwithimage",
    "html": "Parameters\nim\n\nThe image to initialize the sampler with.\n\ndict\n\nA dictionary that contains options specified as key-value pairs. See Sampler Option Keys.\n\nSee Also\nInitializing a Sampler\n- initWithImage:\nInitializes a sampler with an image object.\n- initWithImage:keysAndValues:\nInitializes the sampler with an image object using options specified as key-value pairs."
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cidiscblur/3228215-radius",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cibokehblur/3228088-inputimage",
    "html": "Required"
  },
  {
    "title": "alwaysSpecifyCompaction",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417barcodegenerator/3228605-alwaysspecifycompaction",
    "html": "Required\n\nDiscussion\n\nSet to nil for automatic."
  },
  {
    "title": "compactionMode",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417barcodegenerator/3228607-compactionmode",
    "html": "Required"
  },
  {
    "title": "correctionLevel",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417barcodegenerator/3228608-correctionlevel",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cidiscblur/3228214-inputimage",
    "html": "Required"
  },
  {
    "title": "maxHeight",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417barcodegenerator/3228610-maxheight",
    "html": "Required"
  },
  {
    "title": "initWithIOSurface:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438181-initwithiosurface",
    "html": "Parameters\nsurface\n\nAn IOSurface object.\n\noptions\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nAn image object initialized with the data from the IOSurface.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "minHeight",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417barcodegenerator/3228613-minheight",
    "html": "Required"
  },
  {
    "title": "dataColumns",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417barcodegenerator/3228609-datacolumns",
    "html": "Required"
  },
  {
    "title": "message",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417barcodegenerator/3228612-message",
    "html": "Required"
  },
  {
    "title": "minWidth",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417barcodegenerator/3228614-minwidth",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cieightfoldreflectedtile/3228250-inputimage",
    "html": "Required"
  },
  {
    "title": "initWithIOSurface:plane:format:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437670-initwithiosurface",
    "html": "Parameters\nsurface\n\nAn IOSurface object.\n\nplane\n\nThe index of the data plane in the IOSurface object containing bitmap data for initializing the image.\n\nformat\n\nA pixel format constant. See Pixel Formats.\n\noptions\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nAn image object initialized with the data from the IOSurface.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface."
  },
  {
    "title": "initWithIOSurface:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438030-initwithiosurface",
    "html": "Parameters\nsurface\n\nAn IOSurface object.\n\nReturn Value\n\nAn image object initialized with the data from the IOSurface object.\n\nDiscussion\n\nAn IOSurface object is a framebuffer object that is suitable for sharing across process boundaries. You can use it to allow your app to move complex image decompression and drawing logic into a separate process for the purpose of increasing security.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "cubeData",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcube/3228134-cubedata",
    "html": "Required\n\nDiscussion\n\nThis filter maps color values in the input image to new color values using a three-dimensional color lookup table. For each RGBA pixel in the input image, the filter uses the R, G, and B component values as indices to identify a location in the table; the RGBA value at that location becomes the RGBA value of the output pixel.\n\nUse the cubeData parameter to provide data formatted for use as a color lookup table, and the inputCubeDimension parameter to specify the size of the table. This data should be an array of texel values in 32-bit floating-point RGBA linear premultiplied format. The inputCubeDimension parameter identifies the size of the cube by specifying the length of one side, so the size of the array should be cubeDimension cubed times the size of a single texel value. In the color table, the R component varies fastest, followed by G, then B."
  },
  {
    "title": "cubeDimension",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcube/3228135-cubedimension",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcrosspolynomial/3228131-inputimage",
    "html": "Required"
  },
  {
    "title": "initWithContentsOfURL:",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1437742-initwithcontentsofurl",
    "html": "Parameters\naURL\n\nThe location of a filter generator file.\n\nReturn Value\n\nThe initialized CIFilterGenerator object. Returns nil if the file can’t be read.\n\nSee Also\nRelated Documentation\n+ filterGeneratorWithContentsOfURL:\nCreates and returns a filter generator object and initializes it with the contents of a filter generator file.\n+ filterGenerator\nCreates and returns an empty filter generator object."
  },
  {
    "title": "connectObject:withKey:toObject:withKey:",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1438159-connectobject",
    "html": "Parameters\nsourceObject\n\nA CIFilter object, a CIImage object, or the path (an NSString or NSURL object) to an image.\n\nsourceKey\n\nThe key that specifies the source object. For example, if the source is the output image of a filter, pass the outputImage key. Pass nil if the source object is used directly.\n\ntargetObject\n\nThe object to which the source object links.\n\ntargetKey\n\nThe key that specifies the target for the source. For example, if you are connecting the source to the input image of a CIFilter object, you would pass the inputImage key.\n\nSee Also\nConnecting and Disconnecting Objects\n- disconnectObject:withKey:toObject:withKey:\nRemoves the connection between two objects in the filter chain."
  },
  {
    "title": "writeToURL:atomically:",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1438179-writetourl",
    "html": "Parameters\naURL\n\nA location for the file generator file.\n\nflag\n\nPass true to specify that Core Image should create an interim file to avoid overwriting an existing file.\n\nReturn Value\n\nReturns true if the the object is successfully archived to the file.\n\nDiscussion\n\nUse this method to save your filter chain to a file for later use."
  },
  {
    "title": "thresholdHigh",
    "url": "https://developer.apple.com/documentation/coreimage/cicannyedgedetector/4190861-thresholdhigh",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciglidereflectedtile/3228474-inputimage",
    "html": "Required"
  },
  {
    "title": "rows",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417barcodegenerator/3228616-rows",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/ciglidereflectedtile/3228473-center",
    "html": "Required"
  },
  {
    "title": "preferredAspectRatio",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417barcodegenerator/3228615-preferredaspectratio",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/ciglidereflectedtile/3228472-angle",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologymaximum/3228577-inputimage",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologymaximum/3228578-radius",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologyminimum/3228581-radius",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologyminimum/3228580-inputimage",
    "html": "Required"
  },
  {
    "title": "striationStrength",
    "url": "https://developer.apple.com/documentation/coreimage/cilenticularhalogenerator/3228526-striationstrength",
    "html": "Required"
  },
  {
    "title": "haloWidth",
    "url": "https://developer.apple.com/documentation/coreimage/cilenticularhalogenerator/3228524-halowidth",
    "html": "Required"
  },
  {
    "title": "time",
    "url": "https://developer.apple.com/documentation/coreimage/cilenticularhalogenerator/3228527-time",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/citriangletile/3228810-center",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/citriangletile/3228809-angle",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/citriangletile/3228812-width",
    "html": "Required"
  },
  {
    "title": "decay",
    "url": "https://developer.apple.com/documentation/coreimage/citrianglekaleidoscope/3228803-decay",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cisixfoldrotatedtile/3228718-angle",
    "html": "Required"
  },
  {
    "title": "draftModeEnabled",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801625-draftmodeenabled",
    "html": "Discussion\n\nSetting this value to YES can improve image decoding speed with minimal loss of quality. The default value is NO.\n\nSee Also\nConfiguring a filter\nbaselineExposure\nA value that indicates the baseline exposure to apply to the image.\nboostAmount\nA value that indicates the amount of global tone curve to apply to the image.\nboostShadowAmount\nA value that indicates the amount to boost the shadow areas of the image.\ncolorNoiseReductionAmount\nA value that indicates the amount of chroma noise reduction to apply to the image.\ncontrastAmount\nA value that indicates the amount of local contrast to apply to the edges of the image.\ndecoderVersion\nA value that indicates the decoder version to use.\ndetailAmount\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nexposure\nA value that indicates the amount of exposure to apply to the image.\nextendedDynamicRangeAmount\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\ngamutMappingEnabled\nA Boolean that indicates whether to enable gamut mapping.\nlensCorrectionEnabled\nA Boolean that indicates whether to enable lens correction.\nlinearSpaceFilter\nAn optional filter you can apply to the RAW image while it’s in linear space.\nlocalToneMapAmount\nA value that indicates the amount of local tone curve to apply to the image.\nluminanceNoiseReductionAmount\nA value that indicates the amount of luminance noise reduction to apply to the image.\nmoireReductionAmount\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nneutralChromaticity\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nneutralLocation\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nneutralTemperature\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nneutralTint\nA value that indicates the amount of white balance based on tint values to apply to the image.\norientation\nA value that indicates the orientation of the image.\nportraitEffectsMatte\nAn optional auxiliary image that represents the portrait effects matte of the image.\npreviewImage\nAn optional auxiliary image that represents a preview of the original image.\nproperties\nA dictionary that contains properties of the image source.\nscaleFactor\nA value that indicates the desired scale factor to draw the output image.\nsemanticSegmentationGlassesMatte\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nsemanticSegmentationHairMatte\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nsemanticSegmentationSkinMatte\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nsemanticSegmentationSkyMatte\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nsemanticSegmentationTeethMatte\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nshadowBias\nA value that indicates the amount to subtract from the shadows in the image.\nsharpnessAmount\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "lensCorrectionEnabled",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801632-lenscorrectionenabled",
    "html": "Discussion\n\nThe default value varies by image.\n\nNote\n\nThe lensCorrectionSupported property is NO if the current image doesn’t support this adjustment.\n\nSee Also\nConfiguring a filter\nbaselineExposure\nA value that indicates the baseline exposure to apply to the image.\nboostAmount\nA value that indicates the amount of global tone curve to apply to the image.\nboostShadowAmount\nA value that indicates the amount to boost the shadow areas of the image.\ncolorNoiseReductionAmount\nA value that indicates the amount of chroma noise reduction to apply to the image.\ncontrastAmount\nA value that indicates the amount of local contrast to apply to the edges of the image.\ndecoderVersion\nA value that indicates the decoder version to use.\ndetailAmount\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nexposure\nA value that indicates the amount of exposure to apply to the image.\nextendedDynamicRangeAmount\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\ndraftModeEnabled\nA Boolean that indicates whether to enable draft mode.\ngamutMappingEnabled\nA Boolean that indicates whether to enable gamut mapping.\nlinearSpaceFilter\nAn optional filter you can apply to the RAW image while it’s in linear space.\nlocalToneMapAmount\nA value that indicates the amount of local tone curve to apply to the image.\nluminanceNoiseReductionAmount\nA value that indicates the amount of luminance noise reduction to apply to the image.\nmoireReductionAmount\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nneutralChromaticity\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nneutralLocation\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nneutralTemperature\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nneutralTint\nA value that indicates the amount of white balance based on tint values to apply to the image.\norientation\nA value that indicates the orientation of the image.\nportraitEffectsMatte\nAn optional auxiliary image that represents the portrait effects matte of the image.\npreviewImage\nAn optional auxiliary image that represents a preview of the original image.\nproperties\nA dictionary that contains properties of the image source.\nscaleFactor\nA value that indicates the desired scale factor to draw the output image.\nsemanticSegmentationGlassesMatte\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nsemanticSegmentationHairMatte\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nsemanticSegmentationSkinMatte\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nsemanticSegmentationSkyMatte\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nsemanticSegmentationTeethMatte\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nshadowBias\nA value that indicates the amount to subtract from the shadows in the image.\nsharpnessAmount\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "gamutMappingEnabled",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801631-gamutmappingenabled",
    "html": "Discussion\n\nThe default value is YES.\n\nSee Also\nConfiguring a filter\nbaselineExposure\nA value that indicates the baseline exposure to apply to the image.\nboostAmount\nA value that indicates the amount of global tone curve to apply to the image.\nboostShadowAmount\nA value that indicates the amount to boost the shadow areas of the image.\ncolorNoiseReductionAmount\nA value that indicates the amount of chroma noise reduction to apply to the image.\ncontrastAmount\nA value that indicates the amount of local contrast to apply to the edges of the image.\ndecoderVersion\nA value that indicates the decoder version to use.\ndetailAmount\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nexposure\nA value that indicates the amount of exposure to apply to the image.\nextendedDynamicRangeAmount\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\ndraftModeEnabled\nA Boolean that indicates whether to enable draft mode.\nlensCorrectionEnabled\nA Boolean that indicates whether to enable lens correction.\nlinearSpaceFilter\nAn optional filter you can apply to the RAW image while it’s in linear space.\nlocalToneMapAmount\nA value that indicates the amount of local tone curve to apply to the image.\nluminanceNoiseReductionAmount\nA value that indicates the amount of luminance noise reduction to apply to the image.\nmoireReductionAmount\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nneutralChromaticity\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nneutralLocation\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nneutralTemperature\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nneutralTint\nA value that indicates the amount of white balance based on tint values to apply to the image.\norientation\nA value that indicates the orientation of the image.\nportraitEffectsMatte\nAn optional auxiliary image that represents the portrait effects matte of the image.\npreviewImage\nAn optional auxiliary image that represents a preview of the original image.\nproperties\nA dictionary that contains properties of the image source.\nscaleFactor\nA value that indicates the desired scale factor to draw the output image.\nsemanticSegmentationGlassesMatte\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nsemanticSegmentationHairMatte\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nsemanticSegmentationSkinMatte\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nsemanticSegmentationSkyMatte\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nsemanticSegmentationTeethMatte\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nshadowBias\nA value that indicates the amount to subtract from the shadows in the image.\nsharpnessAmount\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cisixfoldrotatedtile/3228719-center",
    "html": "Required"
  },
  {
    "title": "roll",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectiverotate/3325540-roll",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cisixfoldrotatedtile/3228720-inputimage",
    "html": "Required"
  },
  {
    "title": "focalLength",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectiverotate/3325537-focallength",
    "html": "Required"
  },
  {
    "title": "pitch",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectiverotate/3325539-pitch",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectiverotate/3325538-inputimage",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologyrectangleminimum/3228589-width",
    "html": "Required\n\nDiscussion\n\nThe value is rounded to the nearest odd integer."
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cizoomblur/3228842-center",
    "html": "Required"
  },
  {
    "title": "scale",
    "url": "https://developer.apple.com/documentation/coreimage/cilanczosscaletransform/3228518-scale",
    "html": "Required\n\nDiscussion\n\nValues less than 1.0 scale down the images. Values greater than 1.0 scale up the image."
  },
  {
    "title": "amount",
    "url": "https://developer.apple.com/documentation/coreimage/cizoomblur/3228841-amount",
    "html": "Required"
  },
  {
    "title": "focalLength",
    "url": "https://developer.apple.com/documentation/coreimage/cikeystonecorrectionvertical/3325532-focallength",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cinoisereduction/3228595-inputimage",
    "html": "Required"
  },
  {
    "title": "noiseLevel",
    "url": "https://developer.apple.com/documentation/coreimage/cinoisereduction/3228596-noiselevel",
    "html": "Required\n\nDiscussion\n\nThe larger the value, the more noise reduction."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cilanczosscaletransform/3228517-inputimage",
    "html": "Required"
  },
  {
    "title": "sharpness",
    "url": "https://developer.apple.com/documentation/coreimage/cinoisereduction/3228597-sharpness",
    "html": "Required\n\nDiscussion\n\nThe larger the value, the sharper the result."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cizoomblur/3228843-inputimage",
    "html": "Required"
  },
  {
    "title": "crop",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectivecorrection/3228648-crop",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cimedian/3228557-inputimage",
    "html": "Required"
  },
  {
    "title": "aspectRatio",
    "url": "https://developer.apple.com/documentation/coreimage/cilanczosscaletransform/3228516-aspectratio",
    "html": "Required"
  },
  {
    "title": "focalLength",
    "url": "https://developer.apple.com/documentation/coreimage/cikeystonecorrectioncombined/3325518-focallength",
    "html": "Required"
  },
  {
    "title": "focalLength",
    "url": "https://developer.apple.com/documentation/coreimage/cikeystonecorrectionhorizontal/3325525-focallength",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cipointillize/3228678-center",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cipointillize/3228680-radius",
    "html": "Required"
  },
  {
    "title": "samplerWithImage:options:",
    "url": "https://developer.apple.com/documentation/coreimage/cisampler/1555076-samplerwithimage",
    "html": "Parameters\nim\n\nThe image that you want the sampler to reference.\n\ndict\n\nA dictionary that contains options specified as key-value pairs. See Sampler Option Keys.\n\nReturn Value\n\nA sampler that references the image specified by the im argument and uses the options specified in the dictionary.\n\nSee Also\nCreating a Sampler\n+ samplerWithImage:\nCreates and returns a sampler that references an image.\n+ samplerWithImage:keysAndValues:\nCreates and returns a sampler that references an image using options specified as key-value pairs."
  },
  {
    "title": "initWithImage:",
    "url": "https://developer.apple.com/documentation/coreimage/cisampler/1438117-initwithimage",
    "html": "Parameters\nim\n\nThe image object to initialize the sampler with.\n\nSee Also\nInitializing a Sampler\n- initWithImage:keysAndValues:\nInitializes the sampler with an image object using options specified as key-value pairs.\n- initWithImage:options:\nInitializes the sampler with an image object using options specified in a dictionary."
  },
  {
    "title": "ringSize",
    "url": "https://developer.apple.com/documentation/coreimage/cibokehblur/3228091-ringsize",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cibokehblur/3228089-radius",
    "html": "Required"
  },
  {
    "title": "softness",
    "url": "https://developer.apple.com/documentation/coreimage/cibokehblur/3228092-softness",
    "html": "Required"
  },
  {
    "title": "samplerWithImage:keysAndValues:",
    "url": "https://developer.apple.com/documentation/coreimage/cisampler/1555078-samplerwithimage",
    "html": "Parameters\nim\n\nThe image that you want the sampler to reference.\n\nkey0\n\nA list of key-value pairs that represent options. Each key needs to be followed by that appropriate value. You can supply one or more key-value pairs. Use nil to specify the end of the key-value options. See Sampler Option Keys.\n\nReturn Value\n\nA sampler that references the image specified by the im argument and uses the specified options.\n\nSee Also\nCreating a Sampler\n+ samplerWithImage:\nCreates and returns a sampler that references an image.\n+ samplerWithImage:options:\nCreates and returns a sampler that references an image using options specified in a dictionary."
  },
  {
    "title": "samplerWithImage:",
    "url": "https://developer.apple.com/documentation/coreimage/cisampler/1555075-samplerwithimage",
    "html": "Parameters\nim\n\nThe image that you want the sampler to reference.\n\nReturn Value\n\nA sampler object that references the image specified by the im argument.\n\nSee Also\nCreating a Sampler\n+ samplerWithImage:keysAndValues:\nCreates and returns a sampler that references an image using options specified as key-value pairs.\n+ samplerWithImage:options:\nCreates and returns a sampler that references an image using options specified in a dictionary.\nRelated Documentation\nCore Image Programming Guide"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cieightfoldreflectedtile/3228248-angle",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cieightfoldreflectedtile/3228251-width",
    "html": "Required"
  },
  {
    "title": "scale",
    "url": "https://developer.apple.com/documentation/coreimage/cidisplacementdistortion/3600129-scale",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cidisplacementdistortion/3600128-inputimage",
    "html": "Required"
  },
  {
    "title": "gaussianSigma",
    "url": "https://developer.apple.com/documentation/coreimage/cicannyedgedetector/4190857-gaussiansigma",
    "html": "Required"
  },
  {
    "title": "displacementImage",
    "url": "https://developer.apple.com/documentation/coreimage/cidisplacementdistortion/3600127-displacementimage",
    "html": "Required"
  },
  {
    "title": "hysteresisPasses",
    "url": "https://developer.apple.com/documentation/coreimage/cicannyedgedetector/4190858-hysteresispasses",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicannyedgedetector/4190859-inputimage",
    "html": "Required"
  },
  {
    "title": "perceptual",
    "url": "https://developer.apple.com/documentation/coreimage/cicannyedgedetector/4190860-perceptual",
    "html": "Required"
  },
  {
    "title": "thresholdLow",
    "url": "https://developer.apple.com/documentation/coreimage/cicannyedgedetector/4190862-thresholdlow",
    "html": "Required"
  },
  {
    "title": "headIndex",
    "url": "https://developer.apple.com/documentation/coreimage/cicoremlmodel/3228195-headindex",
    "html": "Required"
  },
  {
    "title": "model",
    "url": "https://developer.apple.com/documentation/coreimage/cicoremlmodel/3228197-model",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicoremlmodel/3228196-inputimage",
    "html": "Required"
  },
  {
    "title": "softmaxNormalization",
    "url": "https://developer.apple.com/documentation/coreimage/cicoremlmodel/3228198-softmaxnormalization",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendwithmask/3228081-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicomiceffect/3228180-inputimage",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cistripesgenerator/3228761-center",
    "html": "Required"
  },
  {
    "title": "backgroundImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendwithmask/3228080-backgroundimage",
    "html": "Required"
  },
  {
    "title": "striationContrast",
    "url": "https://developer.apple.com/documentation/coreimage/cisunbeamsgenerator/3228770-striationcontrast",
    "html": "Required"
  },
  {
    "title": "color",
    "url": "https://developer.apple.com/documentation/coreimage/cisunbeamsgenerator/3228768-color",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cisunbeamsgenerator/3228767-center",
    "html": "Required"
  },
  {
    "title": "striationStrength",
    "url": "https://developer.apple.com/documentation/coreimage/cisunbeamsgenerator/3228771-striationstrength",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cibumpdistortion/3600107-center",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cibumpdistortion/3600108-inputimage",
    "html": "Required"
  },
  {
    "title": "maxStriationRadius",
    "url": "https://developer.apple.com/documentation/coreimage/cisunbeamsgenerator/3228769-maxstriationradius",
    "html": "Required"
  },
  {
    "title": "sunRadius",
    "url": "https://developer.apple.com/documentation/coreimage/cisunbeamsgenerator/3228772-sunradius",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/citoruslensdistortion/3600199-radius",
    "html": "Required"
  },
  {
    "title": "time",
    "url": "https://developer.apple.com/documentation/coreimage/cisunbeamsgenerator/3228773-time",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/citoruslensdistortion/3600198-inputimage",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/citoruslensdistortion/3600201-width",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/citoruslensdistortion/3600197-center",
    "html": "Required"
  },
  {
    "title": "refraction",
    "url": "https://developer.apple.com/documentation/coreimage/citoruslensdistortion/3600200-refraction",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cigloom/3228477-inputimage",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cipixellate/3228674-center",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cicrystallize/3228200-center",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicrystallize/3228201-inputimage",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cicrystallize/3228202-radius",
    "html": "Required"
  },
  {
    "title": "shadowDensity",
    "url": "https://developer.apple.com/documentation/coreimage/cidisintegratewithmasktransition/3228218-shadowdensity",
    "html": "Required"
  },
  {
    "title": "shadowOffset",
    "url": "https://developer.apple.com/documentation/coreimage/cidisintegratewithmasktransition/3228219-shadowoffset",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cisharpenluminance/3228710-radius",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/ciglidereflectedtile/3228475-width",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cicopymachinetransition/3228193-width",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cicopymachinetransition/3228189-angle",
    "html": "Required"
  },
  {
    "title": "haloRadius",
    "url": "https://developer.apple.com/documentation/coreimage/cilenticularhalogenerator/3228523-haloradius",
    "html": "Required"
  },
  {
    "title": "shapeWithRect:",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltershape/1562074-shapewithrect",
    "html": "Parameters\nr\n\nA rectangle. The filter shape object will contain the smallest integral rectangle specified by this argument.\n\nSee Also\nRelated Documentation\n- initWithRect:\nInitializes a filter shape object with a rectangle.\nCore Image Programming Guide"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cimotionblur/3228593-radius",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologyrectanglemaximum/3228585-width",
    "html": "Required\n\nDiscussion\n\nThe value is rounded to the nearest odd integer."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologyrectanglemaximum/3228584-inputimage",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cilenticularhalogenerator/3228520-center",
    "html": "Required"
  },
  {
    "title": "haloOverlap",
    "url": "https://developer.apple.com/documentation/coreimage/cilenticularhalogenerator/3228522-halooverlap",
    "html": "Required\n\nDiscussion\n\nA value of 0 means the halo colors don't overlap. A value of 1 means the halo colors fully overlap, creating a white halo."
  },
  {
    "title": "color",
    "url": "https://developer.apple.com/documentation/coreimage/cilenticularhalogenerator/3228521-color",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/citriangletile/3228811-inputimage",
    "html": "Required"
  },
  {
    "title": "height",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologyrectanglemaximum/3228583-height",
    "html": "Required\n\nDiscussion\n\nThe value is rounded to the nearest odd integer."
  },
  {
    "title": "size",
    "url": "https://developer.apple.com/documentation/coreimage/citrianglekaleidoscope/3228807-size",
    "html": "Required"
  },
  {
    "title": "rotation",
    "url": "https://developer.apple.com/documentation/coreimage/citrianglekaleidoscope/3228806-rotation",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/citrianglekaleidoscope/3228804-inputimage",
    "html": "Required"
  },
  {
    "title": "point",
    "url": "https://developer.apple.com/documentation/coreimage/citrianglekaleidoscope/3228805-point",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldreflectedtile/3228446-center",
    "html": "Required"
  },
  {
    "title": "maxStriationRadius",
    "url": "https://developer.apple.com/documentation/coreimage/ciflashtransition/3228440-maxstriationradius",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/citwelvefoldreflectedtile/3228817-width",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/citwelvefoldreflectedtile/3228816-inputimage",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/citwelvefoldreflectedtile/3228815-center",
    "html": "Required"
  },
  {
    "title": "crossAngle",
    "url": "https://developer.apple.com/documentation/coreimage/cistarshinegenerator/3228751-crossangle",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/ciunsharpmask/3228821-radius",
    "html": "Required"
  },
  {
    "title": "epsilon",
    "url": "https://developer.apple.com/documentation/coreimage/cistarshinegenerator/3228755-epsilon",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cistarshinegenerator/3228756-radius",
    "html": "Required"
  },
  {
    "title": "bottomRight",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectivetile/3228654-bottomright",
    "html": "Required"
  },
  {
    "title": "bottomLeft",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectivetile/3228653-bottomleft",
    "html": "Required"
  },
  {
    "title": "topLeft",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectivetile/3228656-topleft",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cikaleidoscope/3228508-angle",
    "html": "Required"
  },
  {
    "title": "topRight",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectivetile/3228657-topright",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cikaleidoscope/3228509-center",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cisixfoldreflectedtile/3228716-width",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cioptile/3228600-center",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cioptile/3228599-angle",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cioptile/3228601-inputimage",
    "html": "Required"
  },
  {
    "title": "highLimit",
    "url": "https://developer.apple.com/documentation/coreimage/cihistogramdisplay/3547126-highlimit",
    "html": "Required"
  },
  {
    "title": "message",
    "url": "https://developer.apple.com/documentation/coreimage/cicode128barcodegenerator/3228117-message",
    "html": "Required"
  },
  {
    "title": "quietSpace",
    "url": "https://developer.apple.com/documentation/coreimage/cicode128barcodegenerator/3228118-quietspace",
    "html": "Required"
  },
  {
    "title": "barcodeHeight",
    "url": "https://developer.apple.com/documentation/coreimage/cicode128barcodegenerator/3228116-barcodeheight",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cioptile/3228603-width",
    "html": "Required"
  },
  {
    "title": "scale",
    "url": "https://developer.apple.com/documentation/coreimage/cioptile/3228602-scale",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cihistogramdisplay/3547127-inputimage",
    "html": "Required"
  },
  {
    "title": "lowLimit",
    "url": "https://developer.apple.com/documentation/coreimage/cihistogramdisplay/3547128-lowlimit",
    "html": "Required"
  },
  {
    "title": "qualityLevel",
    "url": "https://developer.apple.com/documentation/coreimage/cipersonsegmentation/3784637-qualitylevel",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cisixfoldrotatedtile/3228721-width",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cisobelgradients/4190866-inputimage",
    "html": "Required"
  },
  {
    "title": "height",
    "url": "https://developer.apple.com/documentation/coreimage/cihistogramdisplay/3547125-height",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cipersonsegmentation/3750392-inputimage",
    "html": "Required"
  },
  {
    "title": "sigma",
    "url": "https://developer.apple.com/documentation/coreimage/ciblurredrectanglegenerator/4273902-sigma",
    "html": "Required"
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/ciblurredrectanglegenerator/4273901-extent",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cicircularwrap/3600125-radius",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cicircularwrap/3600122-angle",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicircularwrap/3600124-inputimage",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cieightfoldreflectedtile/3228249-center",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cicircularwrap/3600123-center",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cibumpdistortion/3600109-radius",
    "html": "Required"
  },
  {
    "title": "maskImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendwithmask/3228082-maskimage",
    "html": "Required\n\nDiscussion\n\nWhen a mask value is 0.0, the result is the background. When the mask value is 1.0, the result is the image."
  },
  {
    "title": "color1",
    "url": "https://developer.apple.com/documentation/coreimage/cistripesgenerator/3228763-color1",
    "html": "Required"
  },
  {
    "title": "color0",
    "url": "https://developer.apple.com/documentation/coreimage/cistripesgenerator/3228762-color0",
    "html": "Required"
  },
  {
    "title": "sharpness",
    "url": "https://developer.apple.com/documentation/coreimage/cistripesgenerator/3228764-sharpness",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cistripesgenerator/3228765-width",
    "html": "Required"
  },
  {
    "title": "scale",
    "url": "https://developer.apple.com/documentation/coreimage/cibumpdistortion/3600110-scale",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cihighlightshadowadjust/3228496-radius",
    "html": "Required"
  },
  {
    "title": "intensity",
    "url": "https://developer.apple.com/documentation/coreimage/cigloom/3228478-intensity",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cicirclesplashdistortion/3600118-center",
    "html": "Required"
  },
  {
    "title": "intensity",
    "url": "https://developer.apple.com/documentation/coreimage/ciedges/3228246-intensity",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/ciedgework/3228243-radius",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciedges/3228245-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cipixellate/3228675-inputimage",
    "html": "Required"
  },
  {
    "title": "scale",
    "url": "https://developer.apple.com/documentation/coreimage/cipixellate/3228676-scale",
    "html": "Required"
  },
  {
    "title": "maskImage",
    "url": "https://developer.apple.com/documentation/coreimage/cidisintegratewithmasktransition/3228217-maskimage",
    "html": "Required"
  },
  {
    "title": "scale",
    "url": "https://developer.apple.com/documentation/coreimage/cishadedmaterial/3228706-scale",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cishadedmaterial/3228705-inputimage",
    "html": "Required"
  },
  {
    "title": "shadingImage",
    "url": "https://developer.apple.com/documentation/coreimage/cishadedmaterial/3228707-shadingimage",
    "html": "Required\n\nDiscussion\n\nThe resulting image has greater heights with lighter shades, and lesser heights (lower areas) with darker shades."
  },
  {
    "title": "acuteAngle",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldreflectedtile/3228444-acuteangle",
    "html": "Required\n\nDiscussion\n\nSmall values create thin diamond tiles, and higher values create fatter reflected tiles."
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/ciflashtransition/3228436-center",
    "html": "Required"
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/ciflashtransition/3228438-extent",
    "html": "Required"
  },
  {
    "title": "color",
    "url": "https://developer.apple.com/documentation/coreimage/ciflashtransition/3228437-color",
    "html": "Required"
  },
  {
    "title": "fadeThreshold",
    "url": "https://developer.apple.com/documentation/coreimage/ciflashtransition/3228439-fadethreshold",
    "html": "Required\n\nDiscussion\n\nThe higher the value, the more flash time and the less fade time."
  },
  {
    "title": "striationContrast",
    "url": "https://developer.apple.com/documentation/coreimage/ciflashtransition/3228441-striationcontrast",
    "html": "Required"
  },
  {
    "title": "striationStrength",
    "url": "https://developer.apple.com/documentation/coreimage/ciflashtransition/3228442-striationstrength",
    "html": "Required"
  },
  {
    "title": "intensity",
    "url": "https://developer.apple.com/documentation/coreimage/ciunsharpmask/3228820-intensity",
    "html": "Required"
  },
  {
    "title": "crossWidth",
    "url": "https://developer.apple.com/documentation/coreimage/cistarshinegenerator/3228754-crosswidth",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/citwelvefoldreflectedtile/3228814-angle",
    "html": "Required"
  },
  {
    "title": "color",
    "url": "https://developer.apple.com/documentation/coreimage/cistarshinegenerator/3228750-color",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cistarshinegenerator/3228749-center",
    "html": "Required"
  },
  {
    "title": "crossScale",
    "url": "https://developer.apple.com/documentation/coreimage/cistarshinegenerator/3228753-crossscale",
    "html": "Required"
  },
  {
    "title": "crossOpacity",
    "url": "https://developer.apple.com/documentation/coreimage/cistarshinegenerator/3228752-crossopacity",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cisixfoldreflectedtile/3228715-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectivetile/3228655-inputimage",
    "html": "Required"
  },
  {
    "title": "closeness1",
    "url": "https://developer.apple.com/documentation/coreimage/cispotcolor/3228731-closeness1",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cisixfoldreflectedtile/3228713-angle",
    "html": "Required"
  },
  {
    "title": "count",
    "url": "https://developer.apple.com/documentation/coreimage/cikaleidoscope/3228510-count",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cikaleidoscope/3228511-inputimage",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cisixfoldreflectedtile/3228714-center",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciunsharpmask/3228819-inputimage",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cihexagonalpixellate/3228490-center",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cispotcolor/3228737-inputimage",
    "html": "Required"
  },
  {
    "title": "color",
    "url": "https://developer.apple.com/documentation/coreimage/ciblurredrectanglegenerator/4273900-color",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cicirclesplashdistortion/3600120-radius",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicirclesplashdistortion/3600119-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cilineoverlay/3228533-inputimage",
    "html": "Required"
  },
  {
    "title": "count",
    "url": "https://developer.apple.com/documentation/coreimage/cikmeans/3547130-count",
    "html": "Required"
  },
  {
    "title": "perceptual",
    "url": "https://developer.apple.com/documentation/coreimage/cikmeans/3547133-perceptual",
    "html": "Required"
  },
  {
    "title": "inputMeans",
    "url": "https://developer.apple.com/documentation/coreimage/cikmeans/3547131-inputmeans",
    "html": "Required"
  },
  {
    "title": "passes",
    "url": "https://developer.apple.com/documentation/coreimage/cikmeans/3547132-passes",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cidepthoffield/3228204-inputimage",
    "html": "Required"
  },
  {
    "title": "point0",
    "url": "https://developer.apple.com/documentation/coreimage/cidepthoffield/3228205-point0",
    "html": "Required"
  },
  {
    "title": "correctionLevel",
    "url": "https://developer.apple.com/documentation/coreimage/ciazteccodegenerator/3228064-correctionlevel",
    "html": "Required"
  },
  {
    "title": "layers",
    "url": "https://developer.apple.com/documentation/coreimage/ciazteccodegenerator/3228065-layers",
    "html": "Required\n\nDiscussion\n\nSet to nil for automatic."
  },
  {
    "title": "message",
    "url": "https://developer.apple.com/documentation/coreimage/ciazteccodegenerator/3228066-message",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldrotatedtile/3228450-angle",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciaffinetile/3228057-inputimage",
    "html": "Required"
  },
  {
    "title": "transform",
    "url": "https://developer.apple.com/documentation/coreimage/ciaffinetile/3228058-transform",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldrotatedtile/3228451-center",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldrotatedtile/3228453-width",
    "html": "Required"
  },
  {
    "title": "acuteAngle",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldtranslatedtile/3228455-acuteangle",
    "html": "Required\n\nDiscussion\n\nSmall values create thin diamond tiles, and higher values create fatter translated tiles."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldrotatedtile/3228452-inputimage",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldtranslatedtile/3228457-center",
    "html": "Required"
  },
  {
    "title": "highlightAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cihighlightshadowadjust/3228494-highlightamount",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciaffineclamp/3228054-inputimage",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldtranslatedtile/3228456-angle",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldtranslatedtile/3228459-width",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldtranslatedtile/3228458-inputimage",
    "html": "Required"
  },
  {
    "title": "color",
    "url": "https://developer.apple.com/documentation/coreimage/ciroundedrectanglestrokegenerator/4126863-color",
    "html": "Required"
  },
  {
    "title": "transform",
    "url": "https://developer.apple.com/documentation/coreimage/ciaffineclamp/3228055-transform",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cicheckerboardgenerator/3228109-width",
    "html": "Required"
  },
  {
    "title": "sharpness",
    "url": "https://developer.apple.com/documentation/coreimage/cicheckerboardgenerator/3228108-sharpness",
    "html": "Required\n\nDiscussion\n\nThe smaller the value, the more blurry the pattern. Values range from 0.0 to 1.0."
  },
  {
    "title": "color1",
    "url": "https://developer.apple.com/documentation/coreimage/cicheckerboardgenerator/3228107-color1",
    "html": "Required"
  },
  {
    "title": "color0",
    "url": "https://developer.apple.com/documentation/coreimage/cicheckerboardgenerator/3228106-color0",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cicheckerboardgenerator/3228105-center",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cihighlightshadowadjust/3228495-inputimage",
    "html": "Required"
  },
  {
    "title": "shadowAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cihighlightshadowadjust/3228497-shadowamount",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciedgework/3228242-inputimage",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cigloom/3228479-radius",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cisaliencymap/3228700-inputimage",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/ciheightfieldfrommask/3228488-radius",
    "html": "Required\n\nDiscussion\n\nLarger values make the transition smoother and more pronounced. Smaller values make the transition approximate a fillet radius."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciheightfieldfrommask/3228487-inputimage",
    "html": "Required\n\nDiscussion\n\nThe white values of the input image define those pixels that are inside the height field while the black values define those pixels that are outside. The field varies smoothly and continuously inside the mask, reaching the value 0 at the edge of the mask."
  },
  {
    "title": "color",
    "url": "https://developer.apple.com/documentation/coreimage/cicopymachinetransition/3228190-color",
    "html": "Required"
  },
  {
    "title": "sharpness",
    "url": "https://developer.apple.com/documentation/coreimage/cisharpenluminance/3228711-sharpness",
    "html": "Required"
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/cicopymachinetransition/3228191-extent",
    "html": "Required"
  },
  {
    "title": "opacity",
    "url": "https://developer.apple.com/documentation/coreimage/cicopymachinetransition/3228192-opacity",
    "html": "Required\n\nDiscussion\n\nA value of 0.0 is transparent. A value of 1.0 is opaque."
  },
  {
    "title": "shadowRadius",
    "url": "https://developer.apple.com/documentation/coreimage/cidisintegratewithmasktransition/3228220-shadowradius",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cisharpenluminance/3228709-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldreflectedtile/3228447-inputimage",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldreflectedtile/3228445-angle",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldreflectedtile/3228448-width",
    "html": "Required"
  },
  {
    "title": "drawAtPoint:fromRect:operation:fraction:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1534432-drawatpoint",
    "html": "Parameters\npoint\n\nThe location in the current coordinate system at which to draw the image.\n\nsrcRect\n\nThe source rectangle specifying the portion of the image you want to draw. The coordinates of this rectangle must be specified using the image's own coordinate system.\n\nop\n\nThe compositing operation to use when drawing the image. For details, see NSCompositingOperation.\n\ndelta\n\nThe opacity of the image, specified as a value from 0.0 to 1.0. Specifying a value of 0.0 draws the image as fully transparent while a value of 1.0 draws the image as fully opaque. Values greater than 1.0 are interpreted as 1.0.\n\nDiscussion\n\nThe image content is drawn at its current resolution and is not scaled unless the CTM of the current coordinate system itself contains a scaling factor. The image is otherwise positioned and oriented using the current coordinate system.\n\nSee Also\nDrawing Images\n- drawInRect:fromRect:operation:fraction:\nDraws all or part of the image in the specified rectangle in the current coordinate system"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/citwirldistortion/3600205-inputimage",
    "html": "Required"
  },
  {
    "title": "scale",
    "url": "https://developer.apple.com/documentation/coreimage/cibumpdistortionlinear/3600116-scale",
    "html": "Required"
  },
  {
    "title": "padding",
    "url": "https://developer.apple.com/documentation/coreimage/citextimagegenerator/4013844-padding",
    "html": "Required"
  },
  {
    "title": "maximumStop",
    "url": "https://developer.apple.com/documentation/coreimage/ciarealogarithmichistogram/3987918-maximumstop",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/citemperatureandtint/3228781-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciholedistortion/3600168-inputimage",
    "html": "Required"
  },
  {
    "title": "neutral",
    "url": "https://developer.apple.com/documentation/coreimage/citemperatureandtint/3228782-neutral",
    "html": "Required"
  },
  {
    "title": "centerColor1",
    "url": "https://developer.apple.com/documentation/coreimage/cispotcolor/3228728-centercolor1",
    "html": "Required"
  },
  {
    "title": "centerColor2",
    "url": "https://developer.apple.com/documentation/coreimage/cispotcolor/3228729-centercolor2",
    "html": "Required"
  },
  {
    "title": "centerColor3",
    "url": "https://developer.apple.com/documentation/coreimage/cispotcolor/3228730-centercolor3",
    "html": "Required"
  },
  {
    "title": "closeness2",
    "url": "https://developer.apple.com/documentation/coreimage/cispotcolor/3228732-closeness2",
    "html": "Required"
  },
  {
    "title": "closeness3",
    "url": "https://developer.apple.com/documentation/coreimage/cispotcolor/3228733-closeness3",
    "html": "Required"
  },
  {
    "title": "contrast1",
    "url": "https://developer.apple.com/documentation/coreimage/cispotcolor/3228734-contrast1",
    "html": "Required"
  },
  {
    "title": "contrast2",
    "url": "https://developer.apple.com/documentation/coreimage/cispotcolor/3228735-contrast2",
    "html": "Required"
  },
  {
    "title": "contrast3",
    "url": "https://developer.apple.com/documentation/coreimage/cispotcolor/3228736-contrast3",
    "html": "Required"
  },
  {
    "title": "replacementColor1",
    "url": "https://developer.apple.com/documentation/coreimage/cispotcolor/3228738-replacementcolor1",
    "html": "Required"
  },
  {
    "title": "replacementColor2",
    "url": "https://developer.apple.com/documentation/coreimage/cispotcolor/3228739-replacementcolor2",
    "html": "Required"
  },
  {
    "title": "threshold",
    "url": "https://developer.apple.com/documentation/coreimage/cilineoverlay/3228534-threshold",
    "html": "Required\n\nDiscussion\n\nLarger values thin out the edges."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicompositeoperation/3228183-inputimage",
    "html": "Required"
  },
  {
    "title": "backgroundImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicompositeoperation/3228182-backgroundimage",
    "html": "Required"
  },
  {
    "title": "numberOfFolds",
    "url": "https://developer.apple.com/documentation/coreimage/ciaccordionfoldtransition/3228052-numberoffolds",
    "html": "Required"
  },
  {
    "title": "foldShadowAmount",
    "url": "https://developer.apple.com/documentation/coreimage/ciaccordionfoldtransition/3228051-foldshadowamount",
    "html": "Required"
  },
  {
    "title": "initWithImageProvider:size::format:colorSpace:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437868-initwithimageprovider",
    "html": "Parameters\np\n\nA data provider that implements the CIImageProvider informal protocol. Core Image maintains a strong reference to this object until the image is deallocated.\n\nwidth\n\nThe width of the image data.\n\nheight\n\nThe height of the image data.\n\nf\n\nA pixel format constant. See Pixel Formats.\n\ncs\n\nThe color space of the image. If this value is nil, the image is not color matched. Pass nil for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables).\n\ndict\n\nA dictionary that specifies image-creation options, either kCIImageProviderTileSize or kCIImageProviderUserInfo. See CIImageProvider for more information on these options.\n\nReturn Value\n\nThe initialized image object.\n\nDiscussion\n\nCore Image does not populate the image until it needs the data.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider."
  },
  {
    "title": "imageByCroppingToRect:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437833-imagebycroppingtorect",
    "html": "Parameters\nrect\n\nThe rectangle, in image coordinates, to which to crop the image.\n\nReturn Value\n\nAn image object cropped to the specified rectangle.\n\nFigure 1 Cropping an image\n\nDiscussion\n\nDue to Core Image's coordinate system mismatch with UIKit, this filtering approach may yield unexpected results when displayed in a UIImageView with contentMode. Be sure to back it with a CGImage so that it handles contentMode properly.\n\nListing 1 Backing a with a to preserve\nCIContext* context = [CIContext context];\nCGImageRef cgCroppedImage = [context createCGImage:ciCroppedImage fromRect:ciCroppedImage.extent];\nUIImage* croppedImage = [UIImage imageWithCGImage:cgCroppedImage];\nCGImageRelease(cgCroppedImage);\n\n\nIf you are displaying or processing your image primarily as a CGImageRef or UIImage, with no additional Core Image application, consider cropping in Core Graphics using the CGImageCreateWithImageInRect function to save processing overhead from conversion of images to CIImage. It makes most sense to use imageByCroppingToRect: when you already have CIImage in your pipeline.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "imageByApplyingFilter:withInputParameters:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437589-imagebyapplyingfilter",
    "html": "Parameters\nfilterName\n\nThe name of the filter to apply, as used when creating a CIFilter instance with the filterWithName: method.\n\nparams\n\nA dictionary whose key-value pairs are set as input values to the filter. Each key is a constant that specifies the name of an input parameter for the filter, and the corresponding value is the value for that parameter. See Core Image Filter Reference for built-in filters and their allowed parameters.\n\nReturn Value\n\nAn image object representing the result of applying the filter.\n\nDiscussion\n\nCalling this method is equivalent to the following sequence of steps:\n\nCreating a CIFilter instance\n\nSetting the original image as the filter’s inputImage parameter\n\nSetting the remaining filter parameters from the params dictionary\n\nRetrieving the outputImage object from the filter\n\nImportant\n\nThis method, though convenient, is inefficient if used multiple times in succession. Achieve better performance by chaining filters without asking for the outputs of individual filters.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "imageByApplyingTransform:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438203-imagebyapplyingtransform",
    "html": "Parameters\nmatrix\n\nAn affine transform.\n\nReturn Value\n\nThe transformed image object.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "imageByApplyingFilter:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2915368-imagebyapplyingfilter",
    "html": "Discussion\n\nA convenience method for applying a single filter to the method receiver and returning the output image. Identical to imageByApplyingFilter:withInputParameters: with default parameters.\n\nImportant\n\nThis method, though convenient, is inefficient if used multiple times in succession. Achieve better performance by chaining filters without asking for the outputs of individual filters.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "imageByApplyingOrientation:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438223-imagebyapplyingorientation",
    "html": "Parameters\norientation\n\nAn integer specifying an image orientation according to the EXIF specification. For details, see kCGImagePropertyOrientation.\n\nReturn Value\n\nAn image object representing the result of rotating or mirroring the image to the target orientation.\n\nDiscussion\n\nThis method determines and then applies the transformation needed to reorient the image to the specified orientation. If you plan to also apply other transformations, you can retrieve the transformation this method would use by calling the imageTransformForOrientation: method.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "imageByClampingToRect:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645893-imagebyclampingtorect",
    "html": "Parameters\nrect\n\nThe rectangle, in image coordinates, to which to crop the image.\n\nReturn Value\n\nAn image object representing the result of the clamp operation.\n\nDiscussion\n\nCalling this method is equivalent to cropping the image (with the imageByCroppingToRect: method or the CICrop filter), then using the imageByClampingToExtent method (or the CIAffineClamp filter), which creates an image of infinite extent by repeating pixel colors from the edges of the cropped image.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "imageByCompositingOverImage:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437837-imagebycompositingoverimage",
    "html": "Parameters\ndest\n\nAn image to serve as the destination of the compositing operation.\n\nReturn Value\n\nAn image object representing the result of the compositing operation.\n\nDiscussion\n\nCalling this method is equivalent to using the CISourceOverCompositing filter. To use other compositing operations and blending modes, create a CIFilter object using one of the built-in filters from the CICategoryCompositeOperation category. For details, see Core Image Filter Reference.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "bottomHeight",
    "url": "https://developer.apple.com/documentation/coreimage/ciaccordionfoldtransition/3228050-bottomheight",
    "html": "Required"
  },
  {
    "title": "imageByClampingToExtent",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437628-imagebyclampingtoextent",
    "html": "Return Value\n\nAn image object representing the result of the clamp operation.\n\nDiscussion\n\nCalling this method is equivalent to using the CIAffineClamp filter, which creates an image of infinite extent by repeating pixel colors from the edges of the original image.\n\nThis operation can be useful when using the image as input to other filters. When an image has finite extent, Core Image treats the area outside the extent as if it were filled with empty (black, zero alpha) pixels. If you apply a filter that samples from outside the image’s extent, those empty pixels affect the result of the filter.\n\nFor example, applying the CIGaussianBlur filter to an image softens the edges of the blurred image, because the opaque pixels at the edges of the image blur into the transparent pixels outside the image’s extent. Applying a clamp effect before the blur filter avoids edge softening by making the original image opaque in all directions. (However, the blurred image will also have infinite extent. Use the imageByCroppingToRect: method to return to the original image’s dimensions while retaining hard edges.)\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "imageByColorMatchingWorkingSpaceToColorSpace:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645898-imagebycolormatchingworkingspace",
    "html": "Parameters\ncolorSpace\n\nThe color space to be converted to. This color space must conform to the kCGColorSpaceModelRGB color space model.\n\nReturn Value\n\nAn image object representing the result of the color matching operation, or nil if the color spaces to be converted are not compatible.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "imageByColorMatchingColorSpaceToWorkingSpace:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645896-imagebycolormatchingcolorspaceto",
    "html": "Parameters\ncolorSpace\n\nThe color space to be converted from. This color space must conform to the kCGColorSpaceModelRGB color space model.\n\nReturn Value\n\nAn image object representing the result of the color matching operation, or nil if the color spaces to be converted are not compatible.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "imageByPremultiplyingAlpha",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645894-imagebypremultiplyingalpha",
    "html": "Return Value\n\nAn image object representing the result of the operation.\n\nDiscussion\n\nPremultiplied alpha speeds up the rendering of images, so Core Image filters require that input image data be premultiplied. If you have an image without premultiplied alpha that you want to feed into a filter, use this method before applying the filter.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "imageByApplyingGaussianBlurWithSigma:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645897-imagebyapplyinggaussianblurwiths",
    "html": "Parameters\nsigma\n\nThe radius, in pixels, of the blur effect to apply.\n\nReturn Value\n\nAn image object representing the result of the operation.\n\nDiscussion\n\nCalling this method is equivalent to using the CIGaussianBlur filter with the specified radius. To use other blur effects, create a CIFilter object using one of the built-in filters from the CICategoryBlur category. For details, see Core Image Filter Reference.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "imageByUnpremultiplyingAlpha",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645892-imagebyunpremultiplyingalpha",
    "html": "Return Value\n\nAn image object representing the result of the operation.\n\nDiscussion\n\nPremultiplied alpha speeds up the rendering of images, but some custom filter routines can be expressed more efficiently with non-premultiplied RGB values. Use this method if you need to apply such a filter to an image that has premultiplied alpha.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cidepthoffield/3228207-radius",
    "html": "Required"
  },
  {
    "title": "unsharpMaskIntensity",
    "url": "https://developer.apple.com/documentation/coreimage/cidepthoffield/3228209-unsharpmaskintensity",
    "html": "Required"
  },
  {
    "title": "saturation",
    "url": "https://developer.apple.com/documentation/coreimage/cidepthoffield/3228208-saturation",
    "html": "Required"
  },
  {
    "title": "unsharpMaskRadius",
    "url": "https://developer.apple.com/documentation/coreimage/cidepthoffield/3228210-unsharpmaskradius",
    "html": "Required"
  },
  {
    "title": "point1",
    "url": "https://developer.apple.com/documentation/coreimage/cidepthoffield/3228206-point1",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cimeshgenerator/3228561-width",
    "html": "Required"
  },
  {
    "title": "mesh",
    "url": "https://developer.apple.com/documentation/coreimage/cimeshgenerator/3228560-mesh",
    "html": "Required\n\nDiscussion\n\nSpecify the mesh as an array of line segments. Each line segment is stored as a CIVector instance that describes the line as a start point and an end point."
  },
  {
    "title": "color",
    "url": "https://developer.apple.com/documentation/coreimage/cimeshgenerator/3228559-color",
    "html": "Required"
  },
  {
    "title": "correctionLevel",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodegenerator/3228682-correctionlevel",
    "html": "Required"
  },
  {
    "title": "barcodeDescriptor",
    "url": "https://developer.apple.com/documentation/coreimage/cibarcodegenerator/3228068-barcodedescriptor",
    "html": "Required\n\nSee Also\nRelated Documentation\nCIQRCodeDescriptor\nA concrete subclass of that represents a square QR code symbol.\nCIAztecCodeDescriptor\nA concrete subclass of that represents an Aztec code symbol.\nCIPDF417CodeDescriptor\nA concrete subclass of that represents a PDF 417 symbol.\nCIDataMatrixCodeDescriptor\nA concrete subclass of that represents a Data Matrix code symbol."
  },
  {
    "title": "message",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodegenerator/3228683-message",
    "html": "Required"
  },
  {
    "title": "compactStyle",
    "url": "https://developer.apple.com/documentation/coreimage/ciazteccodegenerator/3228063-compactstyle",
    "html": "Required\n\nDiscussion\n\nSet to nil for automatic."
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cibloom/3228086-radius",
    "html": "Required"
  },
  {
    "title": "intensity",
    "url": "https://developer.apple.com/documentation/coreimage/cibloom/3228085-intensity",
    "html": "Required\n\nDiscussion\n\nA value of 0.0 is no effect. A value of 1.0 is the maximum effect."
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/ciroundedrectanglestrokegenerator/4126864-extent",
    "html": "Required"
  },
  {
    "title": "cropAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cistretchcrop/3600193-cropamount",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/ciroundedrectanglestrokegenerator/4126865-radius",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cibloom/3228084-inputimage",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/ciroundedrectanglestrokegenerator/4126866-width",
    "html": "Required"
  },
  {
    "title": "centerStretchAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cistretchcrop/3600192-centerstretchamount",
    "html": "Required"
  },
  {
    "title": "size",
    "url": "https://developer.apple.com/documentation/coreimage/cistretchcrop/3600195-size",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cistretchcrop/3600194-inputimage",
    "html": "Required"
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/ciroundedrectanglegenerator/3338738-extent",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciexposureadjust/3228254-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cibumpdistortionlinear/3600114-inputimage",
    "html": "Required"
  },
  {
    "title": "initWithData:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437925-initwithdata",
    "html": "Parameters\ndata\n\nThe image data. The data you supply must be premultiplied.\n\nReturn Value\n\nThe initialized image object, or nil if the object could not be initialized.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data."
  },
  {
    "title": "imageBySettingProperties:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645895-imagebysettingproperties",
    "html": "Parameters\nproperties\n\nA dictionary of metadata properties to associate with the image.\n\nReturn Value\n\nAn image object with the specified properties.\n\nDiscussion\n\nWhen you create an image, Core Image sets an image’s properties dictionary to the metadata you specify (using the kCIImageProperties key in an options dictionary), or to the underlying image’s metadata (by calling the CGImageSourceCopyPropertiesAtIndex function). Use this method to override an image’s metadata properties with new values.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "imageByInsertingIntermediate:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2966522-imagebyinsertingintermediate",
    "html": "Parameters\ncache\n\nA Boolean value indicating whether to cache the intermediate.\n\nReturn Value\n\nThe image obtained from inserting the intermediate.\n\nDiscussion\n\nIntermediate buffers created through setting cache to YES have a higher priority than others.\n\nThis setting is independent of of CIContext's kCIContextCacheIntermediates option.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate."
  },
  {
    "title": "imageByInsertingIntermediate",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2966521-imagebyinsertingintermediate",
    "html": "Return Value\n\nThe image obtained from inserting the intermediate.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageBySettingAlphaOneInExtent:\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "initWithData:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438032-initwithdata",
    "html": "Parameters\ndata\n\nThe image data. The data you supply must be premultiplied.\n\nd\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nThe initialized image object, or nil if the object could not be initialized.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options."
  },
  {
    "title": "count",
    "url": "https://developer.apple.com/documentation/coreimage/ciareahistogram/3547092-count",
    "html": "Required"
  },
  {
    "title": "scale",
    "url": "https://developer.apple.com/documentation/coreimage/ciareahistogram/3547093-scale",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cininepartstretched/3600179-inputimage",
    "html": "Required"
  },
  {
    "title": "breakpoint1",
    "url": "https://developer.apple.com/documentation/coreimage/cininepartstretched/3600177-breakpoint1",
    "html": "Required"
  },
  {
    "title": "count",
    "url": "https://developer.apple.com/documentation/coreimage/ciarealogarithmichistogram/3987917-count",
    "html": "Required"
  },
  {
    "title": "minimumStop",
    "url": "https://developer.apple.com/documentation/coreimage/ciarealogarithmichistogram/3987919-minimumstop",
    "html": "Required"
  },
  {
    "title": "scale",
    "url": "https://developer.apple.com/documentation/coreimage/ciarealogarithmichistogram/3987920-scale",
    "html": "Required"
  },
  {
    "title": "power",
    "url": "https://developer.apple.com/documentation/coreimage/cigammaadjust/3228462-power",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cigammaadjust/3228461-inputimage",
    "html": "Required"
  },
  {
    "title": "breakpoint0",
    "url": "https://developer.apple.com/documentation/coreimage/cininepartstretched/3600176-breakpoint0",
    "html": "Required"
  },
  {
    "title": "growAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cininepartstretched/3600178-growamount",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/ciholedistortion/3600167-center",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/ciholedistortion/3600169-radius",
    "html": "Required"
  },
  {
    "title": "targetNeutral",
    "url": "https://developer.apple.com/documentation/coreimage/citemperatureandtint/3228783-targetneutral",
    "html": "Required"
  },
  {
    "title": "AVector",
    "url": "https://developer.apple.com/documentation/coreimage/cicolormatrix/3228160-avector",
    "html": "Required"
  },
  {
    "title": "BVector",
    "url": "https://developer.apple.com/documentation/coreimage/cicolormatrix/3228161-bvector",
    "html": "Required"
  },
  {
    "title": "RVector",
    "url": "https://developer.apple.com/documentation/coreimage/cicolormatrix/3228163-rvector",
    "html": "Required"
  },
  {
    "title": "GVector",
    "url": "https://developer.apple.com/documentation/coreimage/cicolormatrix/3228162-gvector",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cimix/3228567-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciparallelogramtile/3228643-inputimage",
    "html": "Required"
  },
  {
    "title": "backgroundImage",
    "url": "https://developer.apple.com/documentation/coreimage/cimix/3228566-backgroundimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cihexagonalpixellate/3228491-inputimage",
    "html": "Required"
  },
  {
    "title": "scale",
    "url": "https://developer.apple.com/documentation/coreimage/cihexagonalpixellate/3228492-scale",
    "html": "Required"
  },
  {
    "title": "replacementColor3",
    "url": "https://developer.apple.com/documentation/coreimage/cispotcolor/3228740-replacementcolor3",
    "html": "Required"
  },
  {
    "title": "acuteAngle",
    "url": "https://developer.apple.com/documentation/coreimage/ciparallelogramtile/3228640-acuteangle",
    "html": "Required\n\nDiscussion\n\nSmall values create thin diamond tiles, and higher values create fatter parallelogram tiles."
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/ciparallelogramtile/3228641-angle",
    "html": "Required"
  },
  {
    "title": "edgeIntensity",
    "url": "https://developer.apple.com/documentation/coreimage/cilineoverlay/3228532-edgeintensity",
    "html": "Required\n\nDiscussion\n\nHigher values find more edges, although typically, you’d use a low value (such as 1.0)."
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/ciparallelogramtile/3228644-width",
    "html": "Required"
  },
  {
    "title": "amount",
    "url": "https://developer.apple.com/documentation/coreimage/cimix/3228565-amount",
    "html": "Required"
  },
  {
    "title": "NRNoiseLevel",
    "url": "https://developer.apple.com/documentation/coreimage/cilineoverlay/3228529-nrnoiselevel",
    "html": "Required\n\nDiscussion\n\nIncreasing the noise level helps to clean up the traced edges of the image."
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/ciparallelogramtile/3228642-center",
    "html": "Required"
  },
  {
    "title": "NRSharpness",
    "url": "https://developer.apple.com/documentation/coreimage/cilineoverlay/3228530-nrsharpness",
    "html": "Required\n\nDiscussion\n\nThis improves the edge acquisition."
  },
  {
    "title": "contrast",
    "url": "https://developer.apple.com/documentation/coreimage/cilineoverlay/3228531-contrast",
    "html": "Required\n\nDiscussion\n\nHigher values produce higher contrast edges, that is, they’re less antialiased."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cipinchdistortion/3600188-inputimage",
    "html": "Required"
  },
  {
    "title": "point1",
    "url": "https://developer.apple.com/documentation/coreimage/ciglasslozenge/3600163-point1",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/ciglasslozenge/3600164-radius",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cipinchdistortion/3600187-center",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cinineparttiled/3600185-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage2",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorabsolutedifference/3547105-inputimage2",
    "html": "Required"
  },
  {
    "title": "point0",
    "url": "https://developer.apple.com/documentation/coreimage/ciglasslozenge/3600162-point0",
    "html": "Required"
  },
  {
    "title": "refraction",
    "url": "https://developer.apple.com/documentation/coreimage/ciglasslozenge/3600165-refraction",
    "html": "Required"
  },
  {
    "title": "strands",
    "url": "https://developer.apple.com/documentation/coreimage/cidroste/3600136-strands",
    "html": "Required"
  },
  {
    "title": "scale",
    "url": "https://developer.apple.com/documentation/coreimage/ciglassdistortion/3600158-scale",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/ciglassdistortion/3600156-center",
    "html": "Required"
  },
  {
    "title": "weights",
    "url": "https://developer.apple.com/documentation/coreimage/ciconvolution/3228187-weights",
    "html": "Required"
  },
  {
    "title": "periodicity",
    "url": "https://developer.apple.com/documentation/coreimage/cidroste/3600134-periodicity",
    "html": "Required"
  },
  {
    "title": "rotation",
    "url": "https://developer.apple.com/documentation/coreimage/cidroste/3600135-rotation",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/civibrance/3228824-inputimage",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/ciroundedrectanglegenerator/3338739-radius",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cilighttunnel/3600172-inputimage",
    "html": "Required"
  },
  {
    "title": "EV",
    "url": "https://developer.apple.com/documentation/coreimage/ciexposureadjust/3228253-ev",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cibumpdistortionlinear/3600112-angle",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cibumpdistortionlinear/3600113-center",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cidisparitytodepth/3228222-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cidepthtodisparity/3228212-inputimage",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cibumpdistortionlinear/3600115-radius",
    "html": "Required"
  },
  {
    "title": "initWithCVPixelBuffer:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438072-initwithcvpixelbuffer",
    "html": "Parameters\nbuffer\n\nA CVPixelBuffer object.\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object."
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/citwirldistortion/3600204-center",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/civignette/3228826-inputimage",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/citwirldistortion/3600203-angle",
    "html": "Required"
  },
  {
    "title": "drawInRect:fromRect:operation:fraction:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1534407-drawinrect",
    "html": "Parameters\ndstRect\n\nThe rectangle in which to draw the image.\n\nsrcRect\n\nThe source rectangle specifying the portion of the image you want to draw. The coordinates of this rectangle must be specified using the image's own coordinate system.\n\nop\n\nThe compositing operation to use when drawing the image. For details, see NSCompositingOperation.\n\ndelta\n\nThe opacity of the image, specified as a value from 0.0 to 1.0. Specifying a value of 0.0 draws the image as fully transparent while a value of 1.0 draws the image as fully opaque. Values greater than 1.0 are interpreted as 1.0.\n\nDiscussion\n\nIf the srcRect and dstRect rectangles have different sizes, the source portion of the image is scaled to fit the specified destination rectangle. The image is otherwise positioned and oriented using the current coordinate system.\n\nSee Also\nDrawing Images\n- drawAtPoint:fromRect:operation:fraction:\nDraws all or part of the image at the specified point in the current coordinate system."
  },
  {
    "title": "imageTransformForOrientation:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437930-imagetransformfororientation",
    "html": "Parameters\norientation\n\nAn integer specifying an image orientation according to the EXIF specification. For details, see kCGImagePropertyOrientation.\n\nReturn Value\n\nAn affine transform that will rotate or mirror the image to match the specified orientation when applied.\n\nDiscussion\n\nThis method determines the transformation needed to match the specified orientation, but does not apply that transformation to the image. To apply the transformation (possibly after concatenating it with other transformations), use the imageByApplyingTransform: method or the CIAffineTransform filter. To determine and apply the transformation in a single step, use the imageByApplyingOrientation: method.\n\nSee Also\nGetting Image Information\ndefinition\nReturns a filter shape object that represents the domain of definition of the image.\nextent\nA rectangle that specifies the extent of the image.\nproperties\nA dictionary containing metadata about the image.\nurl\nThe URL from which the image was loaded.\ncolorSpace\nThe color space of the image."
  },
  {
    "title": "autoAdjustmentFiltersWithOptions:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437792-autoadjustmentfilterswithoptions",
    "html": "Parameters\noptions\n\nYou can control which filters are returned by supplying one or more of the keys described in Autoadjustment Keys.\n\nThe options dictionary can also contain a CIDetectorImageOrientation key. Because some autoadjustment filters rely on face detection, you should specify an image orientation if you want to enable these filters for an image containing face whose orientation does not match that of the image.\n\nReturn Value\n\nAn array of CIFilter instances preconfigured for correcting deficiencies in the supplied image.\n\nSee Also\nGetting Autoadjustment Filters\n- autoAdjustmentFilters\nReturns all possible automatically selected and configured filters for adjusting the image.\nAutoadjustment Keys\nConstants used as keys in the options dictionary for the autoAdjustmentFiltersWithOptions: method."
  },
  {
    "title": "regionOfInterestForImage:inRect:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437994-regionofinterestforimage",
    "html": "Parameters\nim\n\nAnother image that is part of the filter chain that generates the image.\n\nr\n\nA rectangle in the image’s coordinate space.\n\nReturn Value\n\nA rectangle in the coordinate space of the input image (the im parameter).\n\nDiscussion\n\nThe region of interest is the rectangle containing pixel data in a source image (the im parameter) necessary to produce a corresponding rectangle in the output image. If the image is not the output of a filter (or of a chain or graph of several CIFilter objects), or the image in the im parameter is not an input to that filter, the rectangle returned is the same as that in the r parameter.\n\nFor example,\n\nIf the image is the output of a filter that doubles the size of its input image, the rectangle returned will be half the size of that in the r parameter. (Upscaling causes every pixel in the input image to correspond to multiple pixels in the output image.)\n\nIf the image is the output of a blur filter, the rectangle returned will be slightly larger than that in the r parameter. (In a blur filter, each pixel in the output image is produced using information from the corresponding pixel and those immediately surrounding it in the input image.)"
  },
  {
    "title": "fontName",
    "url": "https://developer.apple.com/documentation/coreimage/citextimagegenerator/3228785-fontname",
    "html": "Required"
  },
  {
    "title": "imageByApplyingCGOrientation:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2919727-imagebyapplyingcgorientation",
    "html": "Discussion\n\nReturns a new image representing the original image transformed for the given CGImagePropertyOrientation.\n\nSee Also\nWorking with Orientation\n- imageTransformForCGOrientation:\nThe affine transform for changing the image to the given orientation."
  },
  {
    "title": "imageTransformForCGOrientation:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2919726-imagetransformforcgorientation",
    "html": "Discussion\n\nReturns a CGAffineTransform for the CGImagePropertyOrientation value to apply to the image.\n\nSee Also\nWorking with Orientation\n- imageByApplyingCGOrientation:\nTransforms the original image by a given CGImagePropertyOrientation and returns the result."
  },
  {
    "title": "imageBySamplingLinear",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2867346-imagebysamplinglinear",
    "html": "Discussion\n\nReturns the image sampled using bilinear interpolation.\n\nSee Also\nSampling the Image\n- imageBySamplingNearest\nSamples the image using nearest-neighbor and returns the result."
  },
  {
    "title": "imageBySamplingNearest",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2867429-imagebysamplingnearest",
    "html": "Discussion\n\nReturns the image sampled using nearest-neighbor.\n\nSee Also\nSampling the Image\n- imageBySamplingLinear\nSamples the image using bilinear interpolation and returns the result."
  },
  {
    "title": "scaleFactor",
    "url": "https://developer.apple.com/documentation/coreimage/citextimagegenerator/3228787-scalefactor",
    "html": "Required"
  },
  {
    "title": "fontSize",
    "url": "https://developer.apple.com/documentation/coreimage/citextimagegenerator/3228786-fontsize",
    "html": "Required"
  },
  {
    "title": "text",
    "url": "https://developer.apple.com/documentation/coreimage/citextimagegenerator/3228788-text",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cilineartosrgbtonecurve/3228547-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cihueadjust/3228500-inputimage",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cihueadjust/3228499-angle",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorthresholdotsu/3584769-inputimage",
    "html": "Required"
  },
  {
    "title": "brightness",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcontrols/3228124-brightness",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcontrols/3228126-inputimage",
    "html": "Required"
  },
  {
    "title": "saturation",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcontrols/3228127-saturation",
    "html": "Required"
  },
  {
    "title": "contrast",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcontrols/3228125-contrast",
    "html": "Required"
  },
  {
    "title": "greenCoefficients",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorpolynomial/3228173-greencoefficients",
    "html": "Required"
  },
  {
    "title": "alphaCoefficients",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorpolynomial/3228171-alphacoefficients",
    "html": "Required"
  },
  {
    "title": "redCoefficients",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorpolynomial/3228175-redcoefficients",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorpolynomial/3228174-inputimage",
    "html": "Required"
  },
  {
    "title": "blueCoefficients",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorpolynomial/3228172-bluecoefficients",
    "html": "Required"
  },
  {
    "title": "CISpotLight",
    "url": "https://developer.apple.com/documentation/coreimage/cispotlight",
    "html": "Topics\nInstance Properties\nbrightness\nThe brightness of the spotlight.\n\nRequired\n\ncolor\nThe color of the spotlight.\n\nRequired\n\nconcentration\nThe size of the spotlight.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nlightPointsAt\nThe x and y position that the spotlight points at.\n\nRequired\n\nlightPosition\nThe x and y position of the spotlight.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "insetPoint1",
    "url": "https://developer.apple.com/documentation/coreimage/cidroste/3600133-insetpoint1",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorinvert/3228155-inputimage",
    "html": "Required"
  },
  {
    "title": "normalize",
    "url": "https://developer.apple.com/documentation/coreimage/ciconvertlab/4018376-normalize",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolormatrix/3228165-inputimage",
    "html": "Required"
  },
  {
    "title": "biasVector",
    "url": "https://developer.apple.com/documentation/coreimage/cicolormatrix/3228164-biasvector",
    "html": "Required"
  },
  {
    "title": "insetPoint0",
    "url": "https://developer.apple.com/documentation/coreimage/cidroste/3600132-insetpoint0",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cidroste/3600131-inputimage",
    "html": "Required"
  },
  {
    "title": "customAttributes",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228047-customattributes",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/civortexdistortion/3600208-angle",
    "html": "Required"
  },
  {
    "title": "breakpoint1",
    "url": "https://developer.apple.com/documentation/coreimage/cinineparttiled/3600182-breakpoint1",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorabsolutedifference/3547104-inputimage",
    "html": "Required"
  },
  {
    "title": "breakpoint0",
    "url": "https://developer.apple.com/documentation/coreimage/cinineparttiled/3600181-breakpoint0",
    "html": "Required"
  },
  {
    "title": "flipYTiles",
    "url": "https://developer.apple.com/documentation/coreimage/cinineparttiled/3600183-flipytiles",
    "html": "Required"
  },
  {
    "title": "growAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cinineparttiled/3600184-growamount",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciglasslozenge/3600161-inputimage",
    "html": "Required"
  },
  {
    "title": "text",
    "url": "https://developer.apple.com/documentation/coreimage/ciattributedtextimagegenerator/3228061-text",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorclamp/3228120-inputimage",
    "html": "Required"
  },
  {
    "title": "maxComponents",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorclamp/3228121-maxcomponents",
    "html": "Required"
  },
  {
    "title": "initWithTexture:size:flipped:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437880-initwithtexture",
    "html": "Parameters\nname\n\nAn OpenGL texture. Because CIImage objects are immutable, the texture must remain unchanged for the life of the image object. See the discussion for more information.\n\nsize\n\nThe dimensions of the texture.\n\nflag\n\nYES to have Core Image flip the coordinates of the texture vertically to convert between OpenGL and Core Image coordinate systems.\n\noptions\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nThe initialized image object.\n\nDiscussion\n\nWhen using a texture to create a CIImage object, the texture must be valid in the Core Image context (CIContext) that you draw the CIImage object into. This means that one of the following must be true:\n\nThe texture must be created using the CGLContext object that the CIContext is based on.\n\nThe context that the texture was created in must be shared with the CGLContext that the CIContextis based on.\n\nNote that textures do not have a retain and release mechanism. This means that your application must make sure that the texture exists for the life cycle of the image. When you no longer need the image, you can delete the texture.\n\nCore Image ignores the texture filtering and wrap modes (GL_TEXTURE_FILTER and GL_TEXTURE_WRAP) that you set through OpenGL. The filter and wrap modes are overridden by what the CISampler object specifies when you apply a filter to the CIImage object.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorthreshold/3547107-inputimage",
    "html": "Required"
  },
  {
    "title": "minComponents",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorclamp/3228122-mincomponents",
    "html": "Required"
  },
  {
    "title": "initWithTexture:size:flipped:colorSpace:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438015-initwithtexture",
    "html": "Parameters\nname\n\nAn OpenGL texture. Because CIImage objects are immutable, the texture must remain unchanged for the life of the image object. See the discussion for more information.\n\nsize\n\nThe dimensions of the texture.\n\nflag\n\nYES to have Core Image flip the coordinates of the texture vertically to convert between OpenGL and Core Image coordinate systems.\n\ncs\n\nThe color space that the image is defined in. This must be a Quartz color space (CGColorSpaceRef). If the colorSpace value is nil, the image is not color matched. Pass nil for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables).\n\nReturn Value\n\nThe initialized image object.\n\nDiscussion\n\nWhen you use a texture to create a CIImage object, the texture must be valid in the Core Image context (CIContext) that you draw the CIImage object into. This means that one of the following must be true:\n\nThe texture must be created using the CGLContext object that the CIContext is based on.\n\nThe context that the texture was created in must be shared with the CGLContext that the CIContextis based on.\n\nNote that textures do not have a retain and release mechanism. This means that your application must make sure that the texture exists for the life cycle of the image. When you no longer need the image, you can delete the texture.\n\nCore Image ignores the texture filtering and wrap modes (GL_TEXTURE_FILTER and GL_TEXTURE_WRAP) that you set through OpenGL. The filter and wrap modes are overridden by what the CISampler object specifies when you apply a filter to the CIImage object.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture."
  },
  {
    "title": "initWithCVImageBuffer:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437617-initwithcvimagebuffer",
    "html": "Parameters\nimageBuffer\n\nA CVImageBuffer object in a supported pixel format constant. For more information, see Core Video Programming Guide and Core Video.\n\ndict\n\nA dictionary that contains options for creating an image object. (See Image Dictionary Keys.) The pixel format is supplied by the CVImageBuffer object.)\n\nReturn Value\n\nThe initialized image object.\n\nDiscussion\n\nThe imageBuffer parameter must be in one of the following formats:\n\nkCVPixelFormatType_32ARGB\n\nkCVPixelFormatType_422YpCbCr8\n\nkCVPixelFormatType_32BGRA\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options."
  },
  {
    "title": "initWithCGLayer:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437687-initwithcglayer",
    "html": "Parameters\nlayer\n\nA CGLayer object. For more information see Quartz 2D Programming Guide and CGLayer.\n\nd\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options."
  },
  {
    "title": "initWithImage:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1624119-initwithimage",
    "html": "Parameters\nimage\n\nAn image containing the source data.\n\nReturn Value\n\nThe initialized image object, or nil if the object could not be initialized.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "threshold",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorthreshold/3547108-threshold",
    "html": "Required"
  },
  {
    "title": "colorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcurves/3228150-colorspace",
    "html": "Required"
  },
  {
    "title": "vectorWithString:",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1564093-vectorwithstring",
    "html": "Parameters\nrepresentation\n\nA string that is in one of the formats returned by the stringRepresentation method.\n\nDiscussion\n\nSome typical string representations for vectors are:\n\n@\"[1.0 0.5 0.3]\"\n\nwhich specifies a vec3 vector whose components are X = 1.0, Y = 0.5, and Z = 0.3\n\n@\"[10.0 23.0]\n\nwhich specifies a vec2 vector show components are X = 10.0 and Y = 23.0\n\nSee Also\nCreating a Vector\n+ vectorWithValues:count:\nCreates and returns a vector that is initialized with the specified values.\n+ vectorWithX:\nCreates and returns a vector that is initialized with one value.\n+ vectorWithX:Y:\nCreates and returns a vector that is initialized with two values.\n+ vectorWithX:Y:Z:\nCreates and returns a vector that is initialized with three values.\n+ vectorWithX:Y:Z:W:\nCreates and returns a vector that is initialized with four values.\n+ vectorWithCGAffineTransform:\nCreates and returns a vector that is initialized with values provided by a CGAffineTransform structure.\n+ vectorWithCGPoint:\nCreates and returns a vector that is initialized with values provided by a CGPoint structure.\n+ vectorWithCGRect:\nCreates and returns a vector that is initialized with values provided by a CGRect structure.\nRelated Documentation\nstringRepresentation\nThe string representation of the vector."
  },
  {
    "title": "initWithColor:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437947-initwithcolor",
    "html": "Parameters\ncolor\n\nA color object.\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color."
  },
  {
    "title": "imageWithMTLTexture:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1546999-imagewithmtltexture",
    "html": "Parameters\ntexture\n\nThe Metal texture from which to use image data.\n\noptions\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nAn image object initialized with the texture data.\n\nDiscussion\n\nTo also render using Metal, use this image with a Metal-based CIContext object created with the contextWithMTLDevice: method, and call the render:toMTLTexture:commandBuffer:bounds:colorSpace: method to create an output image in another Metal texture object.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture."
  },
  {
    "title": "imageWithIOSurface:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547024-imagewithiosurface",
    "html": "Parameters\nsurface\n\nAn IOSurface object.\n\nReturn Value\n\nAn image object initialized with the data from the IOSurface object.\n\nDiscussion\n\nAn IOSurface object is a framebuffer object that is suitable for sharing across process boundaries. You can use it to allow your app to move complex image decompression and drawing logic into a separate process for the purpose of increasing security.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface."
  },
  {
    "title": "emptyImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438023-emptyimage",
    "html": "Return Value\n\nAn image object.\n\nSee Also\nCreating an Image\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\nCore Image Programming Guide"
  },
  {
    "title": "imageWithCVPixelBuffer:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547003-imagewithcvpixelbuffer",
    "html": "Parameters\nbuffer\n\nA CVPixelBuffer object.\n\ndict\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nAn image object initialized with the contents of the image buffer object and set up with the specified options.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options."
  },
  {
    "title": "imageAccumulatorWithExtent:format:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageaccumulator/1427722-imageaccumulatorwithextent",
    "html": "Parameters\nextent\n\nA rectangle that specifies the x-value of the rectangle origin, the y-value of the rectangle origin, and the width and height.\n\nformat\n\nThe format and size of each pixel. You must supply a pixel format constant, such as kCIFormatARGB8 (32 bit-per-pixel, fixed-point pixel format) or kCIFormatRGBAf (128 bit-per-pixel, floating-point pixel format). See CIImage for more information about pixel format constants.\n\nReturn Value\n\nThe image accumulator object.\n\nSee Also\nCreating an Image Accumulator\n+ imageAccumulatorWithExtent:format:colorSpace:\nCreates an image accumulator with the specified extent, pixel format, and color space.\nRelated Documentation\nCore Image Programming Guide\n- initWithExtent:format:\nInitializes an image accumulator with the specified extent and pixel format."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cidotscreen/3228233-inputimage",
    "html": "Required"
  },
  {
    "title": "color",
    "url": "https://developer.apple.com/documentation/coreimage/ciwhitepointadjust/3228836-color",
    "html": "Required"
  },
  {
    "title": "zoom",
    "url": "https://developer.apple.com/documentation/coreimage/cidroste/3600137-zoom",
    "html": "Required"
  },
  {
    "title": "color",
    "url": "https://developer.apple.com/documentation/coreimage/ciroundedrectanglegenerator/3338737-color",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cilighttunnel/3600173-radius",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cisrgbtonecurvetolinear/3228698-inputimage",
    "html": "Required"
  },
  {
    "title": "rotation",
    "url": "https://developer.apple.com/documentation/coreimage/cilighttunnel/3600174-rotation",
    "html": "Required"
  },
  {
    "title": "amount",
    "url": "https://developer.apple.com/documentation/coreimage/civibrance/3228823-amount",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cilighttunnel/3600171-center",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciwhitepointadjust/3228837-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciglassdistortion/3600157-inputimage",
    "html": "Required"
  },
  {
    "title": "textureImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciglassdistortion/3600159-textureimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciconvolution/3228186-inputimage",
    "html": "Required"
  },
  {
    "title": "bias",
    "url": "https://developer.apple.com/documentation/coreimage/ciconvolution/3228185-bias",
    "html": "Required"
  },
  {
    "title": "paletteImage",
    "url": "https://developer.apple.com/documentation/coreimage/cipalettecentroid/3228633-paletteimage",
    "html": "Required"
  },
  {
    "title": "intensity",
    "url": "https://developer.apple.com/documentation/coreimage/civignette/3228827-intensity",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/civignette/3228828-radius",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cipagecurltransition/3228618-angle",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolormonochrome/3228168-inputimage",
    "html": "Required"
  },
  {
    "title": "sharpness",
    "url": "https://developer.apple.com/documentation/coreimage/cicircularscreen/3228113-sharpness",
    "html": "Required"
  },
  {
    "title": "softness",
    "url": "https://developer.apple.com/documentation/coreimage/cihuesaturationvaluegradient/3228505-softness",
    "html": "Required"
  },
  {
    "title": "color",
    "url": "https://developer.apple.com/documentation/coreimage/cicolormonochrome/3228167-color",
    "html": "Required"
  },
  {
    "title": "intensity",
    "url": "https://developer.apple.com/documentation/coreimage/cicolormonochrome/3228169-intensity",
    "html": "Required\n\nDiscussion\n\nA value of 1.0 creates a monochrome image using the supplied color. A value of 0.0 has no effect on the image."
  },
  {
    "title": "gradientImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolormap/3228157-gradientimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolormap/3228158-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cidither/3228225-inputimage",
    "html": "Required"
  },
  {
    "title": "intensity",
    "url": "https://developer.apple.com/documentation/coreimage/cidither/3228226-intensity",
    "html": "Required"
  },
  {
    "title": "levels",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorposterize/3228178-levels",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorposterize/3228177-inputimage",
    "html": "Required"
  },
  {
    "title": "backsideImage",
    "url": "https://developer.apple.com/documentation/coreimage/cipagecurlwithshadowtransition/3228625-backsideimage",
    "html": "Required"
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/cipagecurlwithshadowtransition/3228626-extent",
    "html": "Required"
  },
  {
    "title": "CIToneCurve",
    "url": "https://developer.apple.com/documentation/coreimage/citonecurve",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\npoint0\nA vector containing the position of the first point of the tone curve.\n\nRequired\n\npoint1\nA vector containing the position of the second point of the tone curve.\n\nRequired\n\npoint2\nA vector containing the position of the third point of the tone curve.\n\nRequired\n\npoint3\nA vector containing the position of the fourth point of the tone curve.\n\nRequired\n\npoint4\nA vector containing the position of the fifth point of the tone curve.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points."
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cipagecurlwithshadowtransition/3228627-radius",
    "html": "Required"
  },
  {
    "title": "shadowSize",
    "url": "https://developer.apple.com/documentation/coreimage/cipagecurlwithshadowtransition/3228630-shadowsize",
    "html": "Required"
  },
  {
    "title": "shadowAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cipagecurlwithshadowtransition/3228628-shadowamount",
    "html": "Required"
  },
  {
    "title": "shadowExtent",
    "url": "https://developer.apple.com/documentation/coreimage/cipagecurlwithshadowtransition/3228629-shadowextent",
    "html": "Required"
  },
  {
    "title": "CIAreaReductionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/ciareareductionfilter",
    "html": "Topics\nInstance Properties\nextent\n\nRequired\n\ninputImage\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nInherited By\nCIAreaAverage\nCIAreaHistogram\nCIAreaLogarithmicHistogram\nCIAreaMaximum\nCIAreaMaximumAlpha\nCIAreaMinMax\nCIAreaMinMaxRed\nCIAreaMinimum\nCIAreaMinimumAlpha\nCIColumnAverage\nCIKMeans\nCIRowAverage"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cipalettize/3228636-inputimage",
    "html": "Required"
  },
  {
    "title": "perceptual",
    "url": "https://developer.apple.com/documentation/coreimage/cipalettize/3228638-perceptual",
    "html": "Required"
  },
  {
    "title": "paletteImage",
    "url": "https://developer.apple.com/documentation/coreimage/cipalettize/3228637-paletteimage",
    "html": "Required"
  },
  {
    "title": "CIFourCoordinateGeometryFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifourcoordinategeometryfilter",
    "html": "Topics\nInstance Properties\nbottomLeft\n\nRequired\n\nbottomRight\n\nRequired\n\ninputImage\n\nRequired\n\ntopLeft\n\nRequired\n\ntopRight\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nInherited By\nCIKeystoneCorrectionCombined\nCIKeystoneCorrectionHorizontal\nCIKeystoneCorrectionVertical\nCIPerspectiveCorrection\nCIPerspectiveTransform\nCIPerspectiveTransformWithExtent"
  },
  {
    "title": "outputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228048-outputimage",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/citwirldistortion/3600206-radius",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/civortexdistortion/3600210-inputimage",
    "html": "Required"
  },
  {
    "title": "scale",
    "url": "https://developer.apple.com/documentation/coreimage/cipinchdistortion/3600190-scale",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/civortexdistortion/3600211-radius",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cipinchdistortion/3600189-radius",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/civortexdistortion/3600209-center",
    "html": "Required"
  },
  {
    "title": "scaleFactor",
    "url": "https://developer.apple.com/documentation/coreimage/ciattributedtextimagegenerator/3228060-scalefactor",
    "html": "Required"
  },
  {
    "title": "padding",
    "url": "https://developer.apple.com/documentation/coreimage/ciattributedtextimagegenerator/4013842-padding",
    "html": "Required"
  },
  {
    "title": "initWithCVImageBuffer:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438012-initwithcvimagebuffer",
    "html": "Parameters\nimageBuffer\n\nA CVImageBuffer object in a supported pixel format constant. For more information, see Core Video Programming Guide and Core Video.\n\nReturn Value\n\nThe initialized image object.\n\nDiscussion\n\nThe imageBuffer parameter must be in one of the following formats:\n\nkCVPixelFormatType_32ARGB\n\nkCVPixelFormatType_422YpCbCr8\n\nkCVPixelFormatType_32BGRA\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object."
  },
  {
    "title": "initWithContentsOfURL:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437867-initwithcontentsofurl",
    "html": "Parameters\nurl\n\nThe location of the image file to read.\n\nd\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nThe initialized image object, or nil if the object could not be initialized.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options."
  },
  {
    "title": "initWithCGImage:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437986-initwithcgimage",
    "html": "Parameters\nimage\n\nA Quartz 2D image (CGImageRef) object. For more information, see Quartz 2D Programming Guide and CGImage.\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image."
  },
  {
    "title": "initWithCGLayer:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438065-initwithcglayer",
    "html": "Parameters\nlayer\n\nA CGLayer object. For more information see Quartz 2D Programming Guide and CGLayer.\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object."
  },
  {
    "title": "initWithBitmapImageRep:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1535335-initwithbitmapimagerep",
    "html": "Parameters\nbitmapImageRep\n\nAn image representation object containing the bitmap data.\n\nReturn Value\n\nThe initialized image object, or nil if the object could not be initialized.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "initWithBitmapData:bytesPerRow:size:format:colorSpace:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437857-initwithbitmapdata",
    "html": "Parameters\nd\n\nThe bitmap data to use for the image. The data you supply must be premultiplied.\n\nbpr\n\nThe number of bytes per row.\n\nsize\n\nThe size of the image data.\n\nf\n\nA pixel format constant. See Pixel Formats.\n\nc\n\nThe color space that the image is defined in. It must be a Quartz 2D color space (CGColorSpaceRef). Pass nil for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables).\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data."
  },
  {
    "title": "initWithContentsOfURL:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437908-initwithcontentsofurl",
    "html": "Parameters\nurl\n\nThe location of the image file to read.\n\nReturn Value\n\nThe initialized image object, or nil if the object could not be initialized.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file."
  },
  {
    "title": "initWithImage:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1624098-initwithimage",
    "html": "Parameters\nimage\n\nAn image containing the source data.\n\noptions\n\nA dictionary that contains options for creating an image object. You can supply such options as a pixel format and a color space. See Image Dictionary Keys.\n\nReturn Value\n\nThe initialized image object, or nil if the object could not be initialized.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "initWithCGImage:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437764-initwithcgimage",
    "html": "Parameters\nimage\n\nA Quartz 2D image (CGImageRef) object. For more information, see Quartz 2D Programming Guide and CGImage.\n\noptions\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color.\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data.\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image.\n- initWithBitmapImageRep:\nInitializes an image object with the specified bitmap image representation.\n- initWithImage:\nInitializes an image object with the specified UIKit image object.\n- initWithImage:options:\nInitializes an image object with the specified UIKit image object, using the specified options.\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL.\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options.\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer.\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer.\n- initWithCVPixelBuffer:options:\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\n- initWithData:\nInitializes an image object with the supplied image data.\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options.\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options.\nkCIImageProviderTileSize\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nkCIImageProviderUserInfo\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\n- initWithMTLTexture:options:\nInitializes an image object with data supplied by a Metal texture.\n- initWithIOSurface:\nInitializes an image with the contents of an IOSurface.\n- initWithIOSurface:options:\nInitializes, using the specified options, an image with the contents of an IOSurface.\n- initWithIOSurface:plane:format:options:\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options."
  },
  {
    "title": "imageWithIOSurface:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547001-imagewithiosurface",
    "html": "Parameters\nsurface\n\nAn IOSurface object.\n\noptions\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nAn image object initialized with the data from the IOSurface.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface."
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cidotscreen/3228235-width",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cimasktoalpha/3228549-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cipalettecentroid/3228632-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciminimumcomponent/3228563-inputimage",
    "html": "Required"
  },
  {
    "title": "perceptual",
    "url": "https://developer.apple.com/documentation/coreimage/cipalettecentroid/3228634-perceptual",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cithermal/3228790-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cimaximumcomponent/3228555-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciphotoeffect/3228672-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cisepiatone/3228702-inputimage",
    "html": "Required"
  },
  {
    "title": "intensity",
    "url": "https://developer.apple.com/documentation/coreimage/cisepiatone/3228703-intensity",
    "html": "Required\n\nDiscussion\n\nA value of 1.0 creates a monochrome sepia image. A value of 0.0 has no effect on the image."
  },
  {
    "title": "extrapolate",
    "url": "https://developer.apple.com/documentation/coreimage/ciphotoeffect/4273904-extrapolate",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cihuesaturationvaluegradient/3228504-radius",
    "html": "Required"
  },
  {
    "title": "filterGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1525954-filtergenerator",
    "html": "Return Value\n\nA CIFilterGenerator object.\n\nDiscussion\n\nYou use the returned object to connect two or more CIFilter objects and input images. It is also valid to have only one CIFilter object in a filter generator.\n\nSee Also\nCreating Filter Generator Objects\n+ filterGeneratorWithContentsOfURL:\nCreates and returns a filter generator object and initializes it with the contents of a filter generator file.\nRelated Documentation\nCore Image Programming Guide\nCore Image Filter Reference"
  },
  {
    "title": "value",
    "url": "https://developer.apple.com/documentation/coreimage/cihuesaturationvaluegradient/3228506-value",
    "html": "Required"
  },
  {
    "title": "filterGeneratorWithContentsOfURL:",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1525950-filtergeneratorwithcontentsofurl",
    "html": "Parameters\naURL\n\nThe location of a filter generator file.\n\nReturn Value\n\nA CIFilterGenerator object; returns nil if the file can’t be read.\n\nSee Also\nCreating Filter Generator Objects\n+ filterGenerator\nCreates and returns an empty filter generator object."
  },
  {
    "title": "colorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/cihuesaturationvaluegradient/3228502-colorspace",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcurves/3228153-inputimage",
    "html": "Required"
  },
  {
    "title": "curvesDomain",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcurves/3228152-curvesdomain",
    "html": "Required"
  },
  {
    "title": "curvesData",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcurves/3228151-curvesdata",
    "html": "Required\n\nDiscussion\n\nCreate the curves data as an NSData object containing a sequence of single-precision RGB values. These values represent a lookup table that’s applied to the input image.\n\nCore Image unpremultiplies the image before applying the effect, and premultiplies the result after applying the effect."
  },
  {
    "title": "dither",
    "url": "https://developer.apple.com/documentation/coreimage/cihuesaturationvaluegradient/3228503-dither",
    "html": "Required"
  },
  {
    "title": "shadingImage",
    "url": "https://developer.apple.com/documentation/coreimage/cipagecurltransition/3228622-shadingimage",
    "html": "Required"
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/cipagecurltransition/3228620-extent",
    "html": "Required"
  },
  {
    "title": "backsideImage",
    "url": "https://developer.apple.com/documentation/coreimage/cipagecurltransition/3228619-backsideimage",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cipagecurltransition/3228621-radius",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cimodtransition/3228570-center",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cimodtransition/3228569-angle",
    "html": "Required"
  },
  {
    "title": "colorWithRed:green:blue:alpha:",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1502111-colorwithred",
    "html": "Parameters\nr\n\nThe value of the red component.\n\ng\n\nThe value of the green component.\n\nb\n\nThe value of the blue component.\n\na\n\nThe value of the alpha component.\n\nReturn Value\n\nA Core Image color object that represents an RGB color, in the color space specified by the Quartz 2D constant kCGColorSpaceGenericRGB, with an alpha value.\n\nSee Also\nCreating Color Objects\n+ colorWithCGColor:\nCreates a color object from a Quartz color.\n+ colorWithRed:green:blue:\nCreates a color object using the specified RGB color component values\n+ colorWithString:\nCreates a color object using the RGBA color component values specified by a string.\n+ colorWithRed:green:blue:colorSpace:\nInitializes a Core Image color object with the specified red, green, and blue component values as measured in the specified color space.\n+ colorWithRed:green:blue:alpha:colorSpace:\nCreates a Core Image color object with the specified red, green, blue, and alpha component values as measured in the specified color space."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciconvertlab/4018375-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cixray/3228839-inputimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cidocumentenhancer/3228229-inputimage",
    "html": "Required"
  },
  {
    "title": "image2",
    "url": "https://developer.apple.com/documentation/coreimage/cilabdeltae/3228513-image2",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cilabdeltae/3228514-inputimage",
    "html": "Required"
  },
  {
    "title": "amount",
    "url": "https://developer.apple.com/documentation/coreimage/cidocumentenhancer/3228228-amount",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cipagecurlwithshadowtransition/3228624-angle",
    "html": "Required"
  },
  {
    "title": "CIPerspectiveTransform",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectivetransform",
    "html": "Relationships\nInherits From\nCIFourCoordinateGeometryFilter\nSee Also\nRelated Documentation\n+ perspectiveTransformFilter\nAlters an image’s geometry to adjust the perspective."
  },
  {
    "title": "time",
    "url": "https://developer.apple.com/documentation/coreimage/citransitionfilter/3228801-time",
    "html": "Required\n\nDiscussion\n\nThis value drives the transition from start, at time 0, to end, at time 1."
  },
  {
    "title": "color1",
    "url": "https://developer.apple.com/documentation/coreimage/cismoothlineargradient/3228724-color1",
    "html": "Required"
  },
  {
    "title": "color0",
    "url": "https://developer.apple.com/documentation/coreimage/cismoothlineargradient/3228723-color0",
    "html": "Required"
  },
  {
    "title": "radius0",
    "url": "https://developer.apple.com/documentation/coreimage/ciradialgradient/3228688-radius0",
    "html": "Required"
  },
  {
    "title": "color1",
    "url": "https://developer.apple.com/documentation/coreimage/ciradialgradient/3228687-color1",
    "html": "Required"
  },
  {
    "title": "radius1",
    "url": "https://developer.apple.com/documentation/coreimage/ciradialgradient/3228689-radius1",
    "html": "Required"
  },
  {
    "title": "color0",
    "url": "https://developer.apple.com/documentation/coreimage/ciradialgradient/3228686-color0",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cilinescreen/3228540-width",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cihatchedscreen/3228481-angle",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cihatchedscreen/3228482-center",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cihatchedscreen/3228483-inputimage",
    "html": "Required"
  },
  {
    "title": "CIColorCube",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcube",
    "html": "Topics\nInstance Properties\ncubeData\nThe cube texture data to use as a color lookup table.\n\nRequired\n\ncubeDimension\nThe length, in texels, of each side of the cube texture.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nextrapolate\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table."
  },
  {
    "title": "CIColorCrossPolynomial",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcrosspolynomial",
    "html": "Topics\nInstance Properties\nblueCoefficients\nPolynomial coefficients for the blue channel.\n\nRequired\n\ngreenCoefficients\nPolynomial coefficients for the green channel.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nredCoefficients\nPolynomial coefficients for the red channel.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products."
  },
  {
    "title": "sharpness",
    "url": "https://developer.apple.com/documentation/coreimage/cihatchedscreen/3228484-sharpness",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cidotscreen/3228232-center",
    "html": "Required"
  },
  {
    "title": "sharpness",
    "url": "https://developer.apple.com/documentation/coreimage/cidotscreen/3228234-sharpness",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cicircularscreen/3228114-width",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicircularscreen/3228112-inputimage",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/civignetteeffect/3228834-radius",
    "html": "Required"
  },
  {
    "title": "falloff",
    "url": "https://developer.apple.com/documentation/coreimage/civignetteeffect/3228831-falloff",
    "html": "Required"
  },
  {
    "title": "intensity",
    "url": "https://developer.apple.com/documentation/coreimage/civignetteeffect/3228833-intensity",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/civignetteeffect/3228830-center",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/civignetteeffect/3228832-inputimage",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cimodtransition/3228572-radius",
    "html": "Required"
  },
  {
    "title": "point0",
    "url": "https://developer.apple.com/documentation/coreimage/cismoothlineargradient/3228725-point0",
    "html": "Required"
  },
  {
    "title": "compression",
    "url": "https://developer.apple.com/documentation/coreimage/cimodtransition/3228571-compression",
    "html": "Required\n\nDiscussion\n\nHoles in the center aren’t distorted as much as those at the edge of the image."
  },
  {
    "title": "point1",
    "url": "https://developer.apple.com/documentation/coreimage/cismoothlineargradient/3228726-point1",
    "html": "Required"
  },
  {
    "title": "colorWithCGColor:",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1502106-colorwithcgcolor",
    "html": "Parameters\nc\n\nA Quartz color (CGColorRef object) created using a Quartz color creation function such as CGColorCreate.\n\nReturn Value\n\nA Core Image color object that represents a Quartz color.\n\nDiscussion\n\nA CGColorRef object is the fundamental opaque data type used internally by Quartz to represent colors. For more information on Quartz 2D color and color spaces, see Quartz 2D Programming Guide.\n\nYou can pass a CGColorRef object that represents any color space, including CMYK. However, Core Image converts all color spaces to the Core Image working color space before it passes the color to the filter kernel. The Core Image working color space uses three color components plus alpha.\n\nSee Also\nCreating Color Objects\n+ colorWithRed:green:blue:\nCreates a color object using the specified RGB color component values\n+ colorWithRed:green:blue:alpha:\nCreates a color object using the specified RGBA color component values.\n+ colorWithString:\nCreates a color object using the RGBA color component values specified by a string.\n+ colorWithRed:green:blue:colorSpace:\nInitializes a Core Image color object with the specified red, green, and blue component values as measured in the specified color space.\n+ colorWithRed:green:blue:alpha:colorSpace:\nCreates a Core Image color object with the specified red, green, blue, and alpha component values as measured in the specified color space."
  },
  {
    "title": "LAh",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/2867344-lah",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "R16",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1437646-r16",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "CIColorCubesMixedWithMask",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcubesmixedwithmask",
    "html": "Topics\nInstance Properties\ncolorSpace\nThe working color space.\n\nRequired\n\ncube0Data\nThe cube texture data to use as a color lookup table.\n\nRequired\n\ncube1Data\nThe cube texture data to use as a color lookup table.\n\nRequired\n\ncubeDimension\nThe length, in texels, of each side of the cube texture.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nmaskImage\nA masking image.\n\nRequired\n\nextrapolate\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image."
  },
  {
    "title": "CIColorCubeWithColorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcubewithcolorspace",
    "html": "Topics\nInstance Properties\ncolorSpace\nThe working color space.\n\nRequired\n\ncubeData\nThe cube texture data to use as a color lookup table.\n\nRequired\n\ncubeDimension\nThe length, in texels, of each side of the cube texture.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nextrapolate\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space."
  },
  {
    "title": "filterWithImageData:identifierHint:",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801629-filterwithimagedata",
    "html": "Parameters\ndata\n\nThe image data.\n\nidentifierHint\n\nA string that identifies the image type.\n\nSee Also\nCreating a filter\n+ filterWithCVPixelBuffer:properties:\nCreates a RAW filter from the pixel buffer and its properties that you specify.\n+ filterWithImageURL:\nCreates a RAW filter from the image at the URL location that you specify."
  },
  {
    "title": "filterWithImageURL:",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801630-filterwithimageurl",
    "html": "Parameters\nurl\n\nThe URL location of the image.\n\nSee Also\nCreating a filter\n+ filterWithCVPixelBuffer:properties:\nCreates a RAW filter from the pixel buffer and its properties that you specify.\n+ filterWithImageData:identifierHint:\nCreates a RAW filter from the image data and type hint that you specify."
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/ciswipetransition/3228779-width",
    "html": "Required"
  },
  {
    "title": "color",
    "url": "https://developer.apple.com/documentation/coreimage/ciswipetransition/3228776-color",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/ciswipetransition/3228775-angle",
    "html": "Required"
  },
  {
    "title": "opacity",
    "url": "https://developer.apple.com/documentation/coreimage/ciswipetransition/3228778-opacity",
    "html": "Required"
  },
  {
    "title": "filterWithCVPixelBuffer:properties:",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801628-filterwithcvpixelbuffer",
    "html": "Parameters\nbuffer\n\nA Core Video pixel buffer.\n\nproperties\n\nA dictionary that defines the properties of the pixel buffer.\n\nSee Also\nCreating a filter\n+ filterWithImageData:identifierHint:\nCreates a RAW filter from the image data and type hint that you specify.\n+ filterWithImageURL:\nCreates a RAW filter from the image at the URL location that you specify."
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/ciswipetransition/3228777-extent",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cibarsswipetransition/3228072-width",
    "html": "Required"
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cibarsswipetransition/3228070-angle",
    "html": "Required"
  },
  {
    "title": "CIPerspectiveRotate",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectiverotate",
    "html": "Topics\nInstance Properties\nfocalLength\nThe 35mm equivalent focal length of the input image.\n\nRequired\n\ninputImage\nThe image to process.\n\nRequired\n\npitch\nThe pitch angle, in radians.\n\nRequired\n\nroll\nThe roll angle, in radians.\n\nRequired\n\nyaw\nThe yaw angle, in radians.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ perspectiveRotateFilter\nRotates an image in a 3D space."
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cicmykhalftone/3228097-angle",
    "html": "Required"
  },
  {
    "title": "CIMorphologyRectangleMinimum",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologyrectangleminimum",
    "html": "Topics\nInstance Properties\nheight\nThe height, in pixels, of the morphological structuring element.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nwidth\nThe width, in pixels, of the morphological structuring element.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ morphologyRectangleMinimumFilter\nBlurs a rectangular area by reducing contrasting pixels."
  },
  {
    "title": "CIKeystoneCorrectionVertical",
    "url": "https://developer.apple.com/documentation/coreimage/cikeystonecorrectionvertical",
    "html": "Topics\nInstance Properties\nfocalLength\nThe 35mm equivalent focal length of the input image.\n\nRequired\n\nRelationships\nInherits From\nCIFourCoordinateGeometryFilter\nSee Also\nRelated Documentation\n+ keystoneCorrectionVerticalFilter\nVertically adjusts an image to remove distortion."
  },
  {
    "title": "CILanczosScaleTransform",
    "url": "https://developer.apple.com/documentation/coreimage/cilanczosscaletransform",
    "html": "Topics\nInstance Properties\naspectRatio\nThe additional horizontal scaling factor to use on the image.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nscale\nThe scaling factor to use on the image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ lanczosScaleTransformFilter\nCreates a high-quality, scaled version of a source image."
  },
  {
    "title": "CIPerspectiveCorrection",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectivecorrection",
    "html": "Topics\nInstance Properties\ncrop\nA rectangle that specifies the extent of the corrected image.\n\nRequired\n\nRelationships\nInherits From\nCIFourCoordinateGeometryFilter\nSee Also\nRelated Documentation\n+ perspectiveCorrectionFilter\nTransforms an image’s perspective."
  },
  {
    "title": "CIKeystoneCorrectionHorizontal",
    "url": "https://developer.apple.com/documentation/coreimage/cikeystonecorrectionhorizontal",
    "html": "Topics\nInstance Properties\nfocalLength\nThe 35mm equivalent focal length of the input image.\n\nRequired\n\nRelationships\nInherits From\nCIFourCoordinateGeometryFilter\nSee Also\nRelated Documentation\n+ keystoneCorrectionHorizontalFilter\nHorizontally adjusts an image to remove distortion."
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cilinescreen/3228536-angle",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cilinescreen/3228537-center",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cilinescreen/3228538-inputimage",
    "html": "Required"
  },
  {
    "title": "sharpness",
    "url": "https://developer.apple.com/documentation/coreimage/cilinescreen/3228539-sharpness",
    "html": "Required"
  },
  {
    "title": "Lh",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/2867349-lh",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "Lf",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/2867381-lf",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/ciradialgradient/3228685-center",
    "html": "Required"
  },
  {
    "title": "radius",
    "url": "https://developer.apple.com/documentation/coreimage/cigaussiangradient/3228470-radius",
    "html": "Required"
  },
  {
    "title": "color0",
    "url": "https://developer.apple.com/documentation/coreimage/cigaussiangradient/3228468-color0",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cigaussiangradient/3228467-center",
    "html": "Required"
  },
  {
    "title": "color1",
    "url": "https://developer.apple.com/documentation/coreimage/cigaussiangradient/3228469-color1",
    "html": "Required"
  },
  {
    "title": "CIDetectorMinFeatureSize",
    "url": "https://developer.apple.com/documentation/coreimage/cidetectorminfeaturesize",
    "html": "Discussion\n\nThe value for this key is an NSNumber object ranging from 0.0 through 1.0 that represents a fraction of the minor dimension of the image."
  },
  {
    "title": "CIDetectorMaxFeatureCount",
    "url": "https://developer.apple.com/documentation/coreimage/cidetectormaxfeaturecount",
    "html": "Discussion\n\nThe default value is 1. Valid values fall between 1 and 256 inclusive."
  },
  {
    "title": "angle",
    "url": "https://developer.apple.com/documentation/coreimage/cidotscreen/3228231-angle",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cicircularscreen/3228111-center",
    "html": "Required"
  },
  {
    "title": "LA8",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/2867355-la8",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "LAf",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/2867390-laf",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "LA16",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/2867382-la16",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "L8",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/2867387-l8",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "L16",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/2867345-l16",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "Rf",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1437668-rf",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "Rh",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1438219-rh",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "RGf",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1438157-rgf",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "RGh",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1437729-rgh",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "RGBA16",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1437999-rgba16",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "CIMedian",
    "url": "https://developer.apple.com/documentation/coreimage/cimedian",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ medianFilter\nCalculates the median of an image to refine detail."
  },
  {
    "title": "RG8",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1438080-rg8",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "CIZoomBlur",
    "url": "https://developer.apple.com/documentation/coreimage/cizoomblur",
    "html": "Topics\nInstance Properties\namount\nThe zoom-in amount.\n\nRequired\n\ncenter\nThe center of the effect, as x and y coordinates.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ zoomBlurFilter\nCreates a zoom blur centered around a single point on the image."
  },
  {
    "title": "Ah",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1438161-ah",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "ABGR8",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1437726-abgr8",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "RG16",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1437648-rg16",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "CINoiseReduction",
    "url": "https://developer.apple.com/documentation/coreimage/cinoisereduction",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nnoiseLevel\nThe amount of noise reduction.\n\nRequired\n\nsharpness\nThe sharpness of the final image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ noiseReductionFilter\nReduces noise by sharpening the edges of objects."
  },
  {
    "title": "CIKeystoneCorrectionCombined",
    "url": "https://developer.apple.com/documentation/coreimage/cikeystonecorrectioncombined",
    "html": "Topics\nInstance Properties\nfocalLength\nThe 35mm equivalent focal length of the input image.\n\nRequired\n\nRelationships\nInherits From\nCIFourCoordinateGeometryFilter\nSee Also\nRelated Documentation\n+ keystoneCorrectionCombinedFilter\nAdjusts the image vertically and horizontally to remove distortion."
  },
  {
    "title": "CIBoxBlur",
    "url": "https://developer.apple.com/documentation/coreimage/ciboxblur",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nradius\nThe radius of the blur, in pixels.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ boxBlurFilter\nApplies a square-shaped blur to an area of an image."
  },
  {
    "title": "CIGaussianBlur",
    "url": "https://developer.apple.com/documentation/coreimage/cigaussianblur",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nradius\nThe radius of the blur, in pixels.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ gaussianBlurFilter\nBlurs an image with a Gaussian distribution pattern."
  },
  {
    "title": "CIBicubicScaleTransform",
    "url": "https://developer.apple.com/documentation/coreimage/cibicubicscaletransform",
    "html": "Topics\nInstance Properties\naspectRatio\nThe additional horizontal scaling factor to use on the image.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nparameterB\nThe value of B to use for the cubic resampling function.\n\nRequired\n\nparameterC\nThe value of C to use for the cubic resampling function.\n\nRequired\n\nscale\nThe scaling factor to use on the image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ bicubicScaleTransformFilter\nProduces a high-quality scaled version of an image."
  },
  {
    "title": "CIEdgePreserveUpsample",
    "url": "https://developer.apple.com/documentation/coreimage/ciedgepreserveupsample",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nlumaSigma\nA value that specifies the influence of the input image’s luma information on the upsampling operation.\n\nRequired\n\nsmallImage\nThe image that the filter upsamples.\n\nRequired\n\nspatialSigma\nA value that specifies the influence of the input image’s spatial information on the upsampling operation.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ edgePreserveUpsampleFilter\nCreates a high-quality upscaled image."
  },
  {
    "title": "CIDetectorAccuracy",
    "url": "https://developer.apple.com/documentation/coreimage/cidetectoraccuracy",
    "html": "Discussion\n\nThe value associated with the key should be one of the values found in Detector Accuracy Options."
  },
  {
    "title": "CIMaskedVariableBlur",
    "url": "https://developer.apple.com/documentation/coreimage/cimaskedvariableblur",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nmask\nA grayscale mask that defines the blur amount.\n\nRequired\n\nradius\nThe distance from the center of the effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ maskedVariableBlurFilter\nBlurs a specified portion of an image."
  },
  {
    "title": "CIPerspectiveTransformWithExtent",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectivetransformwithextent",
    "html": "Topics\nInstance Properties\nextent\nA rectangle that defines the extent of the effect.\n\nRequired\n\nRelationships\nInherits From\nCIFourCoordinateGeometryFilter\nSee Also\nRelated Documentation\n+ perspectiveTransformWithExtentFilter\nAlters an image’s geometry to adjust the perspective while applying constraints."
  },
  {
    "title": "targetImage",
    "url": "https://developer.apple.com/documentation/coreimage/citransitionfilter/3228800-targetimage",
    "html": "Required"
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/citransitionfilter/3228799-inputimage",
    "html": "Required"
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cicmykhalftone/3228103-width",
    "html": "Required"
  },
  {
    "title": "sharpness",
    "url": "https://developer.apple.com/documentation/coreimage/cicmykhalftone/3228101-sharpness",
    "html": "Required"
  },
  {
    "title": "barOffset",
    "url": "https://developer.apple.com/documentation/coreimage/cibarsswipetransition/3228071-baroffset",
    "html": "Required"
  },
  {
    "title": "underColorRemoval",
    "url": "https://developer.apple.com/documentation/coreimage/cicmykhalftone/3228102-undercolorremoval",
    "html": "Required"
  },
  {
    "title": "CIStraighten",
    "url": "https://developer.apple.com/documentation/coreimage/cistraighten",
    "html": "Topics\nInstance Properties\nangle\nThe rotation angle, in radians.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ straightenFilter\nRotates and crops an image."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cicmykhalftone/3228100-inputimage",
    "html": "Required"
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cicmykhalftone/3228098-center",
    "html": "Required"
  },
  {
    "title": "grayComponentReplacement",
    "url": "https://developer.apple.com/documentation/coreimage/cicmykhalftone/3228099-graycomponentreplacement",
    "html": "Required"
  },
  {
    "title": "CIDetectorNumberOfAngles",
    "url": "https://developer.apple.com/documentation/coreimage/cidetectornumberofangles",
    "html": "Discussion\n\nThe value for this key is an NSNumber object containing the number 1, 3, 5, 7, 9, or 11. At higher numbers of angles, face detection in video becomes more accurate, but at a higher computational cost."
  },
  {
    "title": "CIDetectorTypeFace",
    "url": "https://developer.apple.com/documentation/coreimage/cidetectortypeface",
    "html": "Discussion\n\nFor better accuracy and performance in face detection, use the CIDetectorImageOrientation key to specify the image orientation when using the features(in:options:) method."
  },
  {
    "title": "CIPointillize",
    "url": "https://developer.apple.com/documentation/coreimage/cipointillize",
    "html": "Topics\nInstance Properties\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nradius\nThe radius of the circles in the resulting pattern.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ pointillizeFilter\nApplies a pointillize effect to an image."
  },
  {
    "title": "initWithImage:keysAndValues:",
    "url": "https://developer.apple.com/documentation/coreimage/cisampler/1555077-initwithimage",
    "html": "Parameters\nim\n\nThe image object to initialize the sampler with.\n\nkey0\n\nA list of key-value pairs that represent options. Each key needs to be followed by that appropriate value. You can supply one or more key-value pairs. Use nil to specify the end of the key-value options. See Sampler Option Keys.\n\nSee Also\nInitializing a Sampler\n- initWithImage:\nInitializes a sampler with an image object.\n- initWithImage:options:\nInitializes the sampler with an image object using options specified in a dictionary."
  },
  {
    "title": "CIDetectorReturnSubFeatures",
    "url": "https://developer.apple.com/documentation/coreimage/cidetectorreturnsubfeatures",
    "html": "Discussion\n\nThe value of this key is an NSNumber object with a Boolean value. Use this option with the CIDetectorTypeText detector type to choose whether to detect only regions likely to contain text (false, the default) or to also identify sub-regions likely to contain individual characters of text (true)."
  },
  {
    "title": "CIBokehBlur",
    "url": "https://developer.apple.com/documentation/coreimage/cibokehblur",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nradius\nThe radius of the blur, in pixels.\n\nRequired\n\nringAmount\nThe amount of extra emphasis at the ring of the bokeh.\n\nRequired\n\nringSize\nThe radius of the extra emphasis at the ring of the bokeh.\n\nRequired\n\nsoftness\nThe softness of the bokeh effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ bokehBlurFilter\nApplies a bokeh effect to an image."
  },
  {
    "title": "BGRA8",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1438064-bgra8",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "CIDiscBlur",
    "url": "https://developer.apple.com/documentation/coreimage/cidiscblur",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nradius\nThe radius of the blur, in pixels.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ discBlurFilter\nApplies a circle-shaped blur to an area of an image."
  },
  {
    "title": "CIPDF417BarcodeGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417barcodegenerator",
    "html": "Topics\nInstance Properties\nalwaysSpecifyCompaction\nA Boolean value specifying whether to force compaction style.\n\nRequired\n\ncompactStyle\nA Boolean value specifying whether to force compact style Aztec code.\n\nRequired\n\ncompactionMode\nThe compaction mode of the generated barcode.\n\nRequired\n\ncorrectionLevel\nThe correction level ratio of the generated barcode.\n\nRequired\n\ndataColumns\nThe number of data columns in the generated barcode.\n\nRequired\n\nmaxHeight\nThe maximum height, in pixels, of the generated barcode.\n\nRequired\n\nmaxWidth\nThe maximum width, in pixels, of the generated barcode.\n\nRequired\n\nmessage\nThe message to encode in the PDF417 barcode.\n\nRequired\n\nminHeight\nThe minimum height, in pixels, of the generated barcode.\n\nRequired\n\nminWidth\nThe minimum width, in pixels, of the generated barcode.\n\nRequired\n\npreferredAspectRatio\nThe preferred aspect ratio of the generated barcode.\n\nRequired\n\nrows\nThe number of rows in the generated barcode.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode."
  },
  {
    "title": "CIGlideReflectedTile",
    "url": "https://developer.apple.com/documentation/coreimage/ciglidereflectedtile",
    "html": "Topics\nInstance Properties\nangle\nThe angle, in radians, of the tiled pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nwidth\nThe width of a tile.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image."
  },
  {
    "title": "intersect(with:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltershape/1437881-intersect",
    "html": "Parameters\ns2\n\nA filter shape object.\n\nReturn Value\n\nThe filter shape object that results from the intersection.\n\nSee Also\nModifying a Filter Shape\nfunc insetBy(x: Int32, y: Int32) -> CIFilterShape\nModifies a filter shape object so that it is inset by the specified x and y values.\nfunc intersect(with: CGRect) -> CIFilterShape\nCreates a filter shape that represents the intersection of the current filter shape and a rectangle.\nfunc transform(by: CGAffineTransform, interior: Bool) -> CIFilterShape\nCreates a filter shape that results from applying a transform to the current filter shape.\nfunc union(with: CIFilterShape) -> CIFilterShape\nCreates a filter shape that results from the union of the current filter shape and another filter shape object.\nfunc union(with: CGRect) -> CIFilterShape\nCreates a filter shape that results from the union of the current filter shape and a rectangle."
  },
  {
    "title": "insetBy(x:y:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltershape/1437987-insetby",
    "html": "Parameters\ndx\n\nA value that specifies an inset in the x direction.\n\ndy\n\nA value that specifies an inset in the y direction.\n\nSee Also\nModifying a Filter Shape\nfunc intersect(with: CIFilterShape) -> CIFilterShape\nCreates a filter shape object that represents the intersection of the current filter shape and the specified filter shape object.\nfunc intersect(with: CGRect) -> CIFilterShape\nCreates a filter shape that represents the intersection of the current filter shape and a rectangle.\nfunc transform(by: CGAffineTransform, interior: Bool) -> CIFilterShape\nCreates a filter shape that results from applying a transform to the current filter shape.\nfunc union(with: CIFilterShape) -> CIFilterShape\nCreates a filter shape that results from the union of the current filter shape and another filter shape object.\nfunc union(with: CGRect) -> CIFilterShape\nCreates a filter shape that results from the union of the current filter shape and a rectangle."
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltershape/1438022-extent",
    "html": "Discussion\n\nExtent is a rectangle that describes the filter shape in the working coordinate space with a fixed area."
  },
  {
    "title": "init(rect:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltershape/1437921-init",
    "html": "Parameters\nr\n\nA rectangle. Core Image uses the rectangle specified by integer parts of the values in the CGRect data structure.\n\nReturn Value\n\nAn initialized CIFilterShape object, or nil if the method fails.\n\nSee Also\nRelated Documentation\n+ shapeWithRect:\nCreates a filter shape object and initializes it with a rectangle."
  },
  {
    "title": "CIMorphologyGradient",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologygradient",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nradius\nThe radius of the circular morphological structuring element.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ morphologyGradientFilter\nDetects and highlights edges of objects."
  },
  {
    "title": "CIMorphologyMaximum",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologymaximum",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nradius\nThe radius of the circular morphological structuring element.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ morphologyMaximumFilter\nBlurs a circular area by enlarging contrasting pixels."
  },
  {
    "title": "CIMorphologyMinimum",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologyminimum",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nradius\nThe radius of the circular morphological structuring element.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ morphologyMinimumFilter\nBlurs a circular area by reducing contrasting pixels."
  },
  {
    "title": "CIMotionBlur",
    "url": "https://developer.apple.com/documentation/coreimage/cimotionblur",
    "html": "Topics\nInstance Properties\nangle\nThe angle of the motion, in radians, that determines which direction the blur smears.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nradius\nThe radius of the blur, in pixels.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ motionBlurFilter\nCreates motion blur on an image."
  },
  {
    "title": "CIMorphologyRectangleMaximum",
    "url": "https://developer.apple.com/documentation/coreimage/cimorphologyrectanglemaximum",
    "html": "Topics\nInstance Properties\nheight\nThe height, in pixels, of the morphological structuring element.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nwidth\nThe width, in pixels, of the morphological structuring element.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ morphologyRectangleMaximumFilter\nBlurs a rectangular area by enlarging contrasting pixels."
  },
  {
    "title": "A8",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1438141-a8",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "ARGB8",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1437883-argb8",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "Af",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1438259-af",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "A16",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1438171-a16",
    "html": "See Also\nImage Formats\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "CILenticularHaloGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/cilenticularhalogenerator",
    "html": "Topics\nInstance Properties\ncenter\nThe x and y position to use as the center of the halo.\n\nRequired\n\ncolor\nThe color of the halo.\n\nRequired\n\nhaloOverlap\nThe separation of colors in the halo.\n\nRequired\n\nhaloRadius\nThe radius of the halo.\n\nRequired\n\nhaloWidth\nThe width of the halo, from its inner radius to its outer radius.\n\nRequired\n\nstriationContrast\nThe contrast of the halo colors.\n\nRequired\n\nstriationStrength\nThe intensity of the halo colors.\n\nRequired\n\ntime\nThe current time of the effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image."
  },
  {
    "title": "CITriangleTile",
    "url": "https://developer.apple.com/documentation/coreimage/citriangletile",
    "html": "Topics\nInstance Properties\nangle\nThe angle, in radians, of the tiled pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nwidth\nThe width of a tile.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ triangleTileFilter\nTiles a triangular area of an image."
  },
  {
    "title": "CITriangleKaleidoscope",
    "url": "https://developer.apple.com/documentation/coreimage/citrianglekaleidoscope",
    "html": "Topics\nInstance Properties\ndecay\nA value that determines how fast the color fades from the center triangle.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\npoint\nThe x and y position to use as the center of the triangular area in the input image.\n\nRequired\n\nrotation\nThe rotation angle of the triangle.\n\nRequired\n\nsize\nThe size, in pixels, of the triangle.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result."
  },
  {
    "title": "CISixfoldRotatedTile",
    "url": "https://developer.apple.com/documentation/coreimage/cisixfoldrotatedtile",
    "html": "Topics\nInstance Properties\nangle\nThe angle, in radians, of the tiled pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nwidth\nThe width of a tile.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees."
  },
  {
    "title": "CISobelGradients",
    "url": "https://developer.apple.com/documentation/coreimage/cisobelgradients",
    "html": "Topics\nInstance Properties\ninputImage\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIPersonSegmentation",
    "url": "https://developer.apple.com/documentation/coreimage/cipersonsegmentation",
    "html": "Topics\nInstance Properties\ninputImage\n\nRequired\n\nqualityLevel\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIEightfoldReflectedTile",
    "url": "https://developer.apple.com/documentation/coreimage/cieightfoldreflectedtile",
    "html": "Topics\nInstance Properties\nangle\nThe angle, in radians, of the tiled pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nwidth\nThe width of a tile.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern."
  },
  {
    "title": "CIDisplacementDistortion",
    "url": "https://developer.apple.com/documentation/coreimage/cidisplacementdistortion",
    "html": "Topics\nInstance Properties\ndisplacementImage\n\nRequired\n\ninputImage\n\nRequired\n\nscale\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CICannyEdgeDetector",
    "url": "https://developer.apple.com/documentation/coreimage/cicannyedgedetector",
    "html": "Topics\nInstance Properties\ngaussianSigma\n\nRequired\n\nhysteresisPasses\n\nRequired\n\ninputImage\n\nRequired\n\nperceptual\n\nRequired\n\nthresholdHigh\n\nRequired\n\nthresholdLow\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CICoreMLModel",
    "url": "https://developer.apple.com/documentation/coreimage/cicoremlmodel",
    "html": "Topics\nInstance Properties\nheadIndex\nA number that specifies which output of a multihead Core ML model applies the effect on the image.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nmodel\nThe Core ML model used to apply the effect on the image.\n\nRequired\n\nsoftmaxNormalization\nA Boolean value that specifies whether to apply Softmax normalization to the output of the model.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ coreMLModelFilter\nFilters an image with a Core ML model."
  },
  {
    "title": "CIComicEffect",
    "url": "https://developer.apple.com/documentation/coreimage/cicomiceffect",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ comicEffectFilter\nCreates an image with a comic book effect."
  },
  {
    "title": "CIBlendWithMask",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendwithmask",
    "html": "Topics\nInstance Properties\nbackgroundImage\nThe image to use as a background image.\n\nRequired\n\ninputImage\nThe image to use as a foreground image.\n\nRequired\n\nmaskImage\nA grayscale mask that defines the blend.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ blendWithMaskFilter\nBlends two images by using a mask image."
  },
  {
    "title": "CIStripesGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/cistripesgenerator",
    "html": "Topics\nInstance Properties\ncenter\nThe x and y position to use as the center of the stripe pattern.\n\nRequired\n\ncolor0\nA color to use for the odd stripes.\n\nRequired\n\ncolor1\nA color to use for the even stripes.\n\nRequired\n\nsharpness\nThe sharpness of the stripe pattern.\n\nRequired\n\nwidth\nThe width of a stripe.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image"
  },
  {
    "title": "CIBumpDistortion",
    "url": "https://developer.apple.com/documentation/coreimage/cibumpdistortion",
    "html": "Topics\nInstance Properties\ncenter\n\nRequired\n\ninputImage\n\nRequired\n\nradius\n\nRequired\n\nscale\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CISunbeamsGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/cisunbeamsgenerator",
    "html": "Topics\nInstance Properties\ncenter\nThe x and y position to use as the center of the sunbeam pattern.\n\nRequired\n\ncolor\nThe color of the sun.\n\nRequired\n\nmaxStriationRadius\nThe radius of the sunbeams.\n\nRequired\n\nstriationContrast\nThe contrast of the sunbeams.\n\nRequired\n\nstriationStrength\nThe intensity of the sunbeams.\n\nRequired\n\nsunRadius\nThe radius of the sun.\n\nRequired\n\ntime\nThe duration of the effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun."
  },
  {
    "title": "CITorusLensDistortion",
    "url": "https://developer.apple.com/documentation/coreimage/citoruslensdistortion",
    "html": "Topics\nInstance Properties\ncenter\n\nRequired\n\ninputImage\n\nRequired\n\nradius\n\nRequired\n\nrefraction\n\nRequired\n\nwidth\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CITwelvefoldReflectedTile",
    "url": "https://developer.apple.com/documentation/coreimage/citwelvefoldreflectedtile",
    "html": "Topics\nInstance Properties\nangle\nThe angle, in radians, of the tiled pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nwidth\nThe width of a tile.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "CIStarShineGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/cistarshinegenerator",
    "html": "Topics\nInstance Properties\ncenter\nThe x and y position to use as the center of the star.\n\nRequired\n\ncolor\nThe color to use for the outer shell of the circular star.\n\nRequired\n\ncrossAngle\nThe angle of the cross pattern.\n\nRequired\n\ncrossOpacity\nThe opacity of the cross pattern.\n\nRequired\n\ncrossScale\nThe size of the cross pattern.\n\nRequired\n\ncrossWidth\nThe width of the cross pattern.\n\nRequired\n\nepsilon\nThe length of the cross spikes.\n\nRequired\n\nradius\nThe radius of the star.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ starShineGeneratorFilter\nGenerates a star-shine image."
  },
  {
    "title": "CIPerspectiveTile",
    "url": "https://developer.apple.com/documentation/coreimage/ciperspectivetile",
    "html": "Topics\nInstance Properties\nbottomLeft\nThe bottom-left coordinate of a tile.\n\nRequired\n\nbottomRight\nThe bottom-right coordinate of a tile.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\ntopLeft\nThe top-left coordinate of a tile.\n\nRequired\n\ntopRight\nThe top-right coordinate of a tile.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image."
  },
  {
    "title": "CIKaleidoscope",
    "url": "https://developer.apple.com/documentation/coreimage/cikaleidoscope",
    "html": "Topics\nInstance Properties\nangle\nThe angle of the reflection.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ncount\nThe number of reflections in the pattern.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image."
  },
  {
    "title": "CISixfoldReflectedTile",
    "url": "https://developer.apple.com/documentation/coreimage/cisixfoldreflectedtile",
    "html": "Topics\nInstance Properties\nangle\nThe angle, in radians, of the tiled pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nwidth\nThe width of a tile.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry."
  },
  {
    "title": "CICode128BarcodeGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/cicode128barcodegenerator",
    "html": "Topics\nInstance Properties\nbarcodeHeight\nThe height, in pixels, of the generated barcode.\n\nRequired\n\nmessage\nThe message to encode in the Code 128 barcode.\n\nRequired\n\nquietSpace\nThe number of empty white pixels that should surround the barcode.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode."
  },
  {
    "title": "CIOpTile",
    "url": "https://developer.apple.com/documentation/coreimage/cioptile",
    "html": "Topics\nInstance Properties\nangle\nThe angle of a tile.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nscale\nA value that determines the number of tiles in the effect.\n\nRequired\n\nwidth\nThe width of a tile.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions."
  },
  {
    "title": "CIHistogramDisplay",
    "url": "https://developer.apple.com/documentation/coreimage/cihistogramdisplay",
    "html": "Topics\nInstance Properties\nheight\n\nRequired\n\nhighLimit\n\nRequired\n\ninputImage\n\nRequired\n\nlowLimit\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIBlurredRectangleGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/ciblurredrectanglegenerator",
    "html": "Topics\nInstance Properties\ncolor\n\nRequired\n\nextent\n\nRequired\n\nsigma\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CICircularWrap",
    "url": "https://developer.apple.com/documentation/coreimage/cicircularwrap",
    "html": "Topics\nInstance Properties\nangle\n\nRequired\n\ncenter\n\nRequired\n\ninputImage\n\nRequired\n\nradius\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CICircleSplashDistortion",
    "url": "https://developer.apple.com/documentation/coreimage/cicirclesplashdistortion",
    "html": "Topics\nInstance Properties\ncenter\n\nRequired\n\ninputImage\n\nRequired\n\nradius\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIGloom",
    "url": "https://developer.apple.com/documentation/coreimage/cigloom",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nintensity\nThe intensity of the effect.\n\nRequired\n\nradius\nThe radius, in pixels, of the effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter."
  },
  {
    "title": "CIAreaMinimum",
    "url": "https://developer.apple.com/documentation/coreimage/ciareaminimum",
    "html": "Relationships\nInherits From\nCIAreaReductionFilter"
  },
  {
    "title": "CISaliencyMap",
    "url": "https://developer.apple.com/documentation/coreimage/cisaliencymap",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ saliencyMapFilter\nCreates a saliency map from an image."
  },
  {
    "title": "CISharpenLuminance",
    "url": "https://developer.apple.com/documentation/coreimage/cisharpenluminance",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nradius\nThe distance from the center of the effect.\n\nRequired\n\nsharpness\nThe amount of sharpening to apply.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ sharpenLuminanceFilter\nApplies a sharpening effect to an image."
  },
  {
    "title": "spotLightFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228414-spotlightfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the spotlight filter to an image. The effect applies a directional spotlight effect to an image while creating a transparent area not highlighted by the spotlight.\n\nThe spotlight filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nlightPointsAt\n\nA CIVector with the x and y positions that the spotlight points at.\n\nbrightness\n\nA float representing the brightness of the spotlight as an NSNumber.\n\nlightPosition\n\nA CIVector containing the x and y position of the spotlight.\n\nconcentration\n\nA float representing the size of the spotlight in pixels as an NSNumber.\n\ncolor\n\nA CIColor representing the spotlight color.\n\nThe following code creates a filter that results in only the bottom left of the image becoming visible while the rest of the image gradually becomes transparent:\n\nfunc spotlight(inputImage: CIImage) -> CIImage {\n    let spotlightFilter = CIFilter.spotLight()\n    spotlightFilter.inputImage = inputImage\n    spotlightFilter.lightPointsAt = CIVector(x: 100, y: 100)\n    spotlightFilter.brightness = 10\n    spotlightFilter.lightPosition = CIVector(x: 100, y: 100)\n    spotlightFilter.concentration = 20\n    return spotlightFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors."
  },
  {
    "title": "CIHeightFieldFromMask",
    "url": "https://developer.apple.com/documentation/coreimage/ciheightfieldfrommask",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nradius\nThe length of the height-field transition.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image."
  },
  {
    "title": "CIEdgeWork",
    "url": "https://developer.apple.com/documentation/coreimage/ciedgework",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nradius\nThe thickness of the edges.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print."
  },
  {
    "title": "CIEdges",
    "url": "https://developer.apple.com/documentation/coreimage/ciedges",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nintensity\nThe intensity of the edges.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ edgesFilter\nHilghlights edges of objects found within an image."
  },
  {
    "title": "expandToHDR",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/4210206-expandtohdr",
    "html": "See Also\nRelated Documentation\nApplying Apple HDR effect to your photos\nYou can decode and apply Apple’s HDR gain map to your own images."
  },
  {
    "title": "CIPixellate",
    "url": "https://developer.apple.com/documentation/coreimage/cipixellate",
    "html": "Topics\nInstance Properties\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nscale\nA value that determines the size of the squares.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect."
  },
  {
    "title": "CIDisintegrateWithMaskTransition",
    "url": "https://developer.apple.com/documentation/coreimage/cidisintegratewithmasktransition",
    "html": "Topics\nInstance Properties\nmaskImage\nAn image that defines the shape to use when disintegrating from the source to the target image.\n\nRequired\n\nshadowDensity\nThe density of the shadow the mask creates.\n\nRequired\n\nshadowOffset\nThe offset of the shadow the mask creates.\n\nRequired\n\nshadowRadius\nThe radius of the shadow the mask creates.\n\nRequired\n\nRelationships\nInherits From\nCITransitionFilter\nSee Also\nRelated Documentation\n+ disintegrateWithMaskTransitionFilter\nTransitions between two images using a mask image."
  },
  {
    "title": "CICopyMachineTransition",
    "url": "https://developer.apple.com/documentation/coreimage/cicopymachinetransition",
    "html": "Topics\nInstance Properties\nangle\nThe angle of the copier light.\n\nRequired\n\ncolor\nThe color of the copier light.\n\nRequired\n\nextent\nA rectangle that defines the extent of the effect.\n\nRequired\n\nopacity\nThe opacity of the copier light.\n\nRequired\n\nwidth\nThe width of the copier light.\n\nRequired\n\nRelationships\nInherits From\nCITransitionFilter\nSee Also\nRelated Documentation\n+ copyMachineTransitionFilter\nSimulates the effect of a copy machine scanner light to transiton between two images."
  },
  {
    "title": "R8",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1437695-r8",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "intersect(with:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltershape/1437806-intersect",
    "html": "Parameters\nrect\n\nA rectangle. Core Image uses the rectangle specified by integer parts of the width and height.\n\nReturn Value\n\nThe filter shape that results from the intersection\n\nSee Also\nModifying a Filter Shape\nfunc insetBy(x: Int32, y: Int32) -> CIFilterShape\nModifies a filter shape object so that it is inset by the specified x and y values.\nfunc intersect(with: CIFilterShape) -> CIFilterShape\nCreates a filter shape object that represents the intersection of the current filter shape and the specified filter shape object.\nfunc transform(by: CGAffineTransform, interior: Bool) -> CIFilterShape\nCreates a filter shape that results from applying a transform to the current filter shape.\nfunc union(with: CIFilterShape) -> CIFilterShape\nCreates a filter shape that results from the union of the current filter shape and another filter shape object.\nfunc union(with: CGRect) -> CIFilterShape\nCreates a filter shape that results from the union of the current filter shape and a rectangle."
  },
  {
    "title": "union(with:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltershape/1438227-union",
    "html": "Parameters\ns2\n\nA filter shape object.\n\nReturn Value\n\nThe filter shape object that results from the union.\n\nSee Also\nModifying a Filter Shape\nfunc insetBy(x: Int32, y: Int32) -> CIFilterShape\nModifies a filter shape object so that it is inset by the specified x and y values.\nfunc intersect(with: CIFilterShape) -> CIFilterShape\nCreates a filter shape object that represents the intersection of the current filter shape and the specified filter shape object.\nfunc intersect(with: CGRect) -> CIFilterShape\nCreates a filter shape that represents the intersection of the current filter shape and a rectangle.\nfunc transform(by: CGAffineTransform, interior: Bool) -> CIFilterShape\nCreates a filter shape that results from applying a transform to the current filter shape.\nfunc union(with: CGRect) -> CIFilterShape\nCreates a filter shape that results from the union of the current filter shape and a rectangle."
  },
  {
    "title": "union(with:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltershape/1437601-union",
    "html": "Parameters\nrect\n\nA rectangle. Core Image uses the rectangle specified by integer parts of the width and height.\n\nSee Also\nModifying a Filter Shape\nfunc insetBy(x: Int32, y: Int32) -> CIFilterShape\nModifies a filter shape object so that it is inset by the specified x and y values.\nfunc intersect(with: CIFilterShape) -> CIFilterShape\nCreates a filter shape object that represents the intersection of the current filter shape and the specified filter shape object.\nfunc intersect(with: CGRect) -> CIFilterShape\nCreates a filter shape that represents the intersection of the current filter shape and a rectangle.\nfunc transform(by: CGAffineTransform, interior: Bool) -> CIFilterShape\nCreates a filter shape that results from applying a transform to the current filter shape.\nfunc union(with: CIFilterShape) -> CIFilterShape\nCreates a filter shape that results from the union of the current filter shape and another filter shape object."
  },
  {
    "title": "transform(by:interior:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltershape/1437808-transform",
    "html": "Parameters\nm\n\nA transform.\n\nflag\n\nfalse specifies that the new filter shape object can contain all the pixels in the transformed shape (and possibly some that are outside the transformed shape). true specifies that the new filter shape object can contain a subset of the pixels in the transformed shape (but none of those outside the transformed shape).\n\nReturn Value\n\nThe transformed filter shape object.\n\nSee Also\nModifying a Filter Shape\nfunc insetBy(x: Int32, y: Int32) -> CIFilterShape\nModifies a filter shape object so that it is inset by the specified x and y values.\nfunc intersect(with: CIFilterShape) -> CIFilterShape\nCreates a filter shape object that represents the intersection of the current filter shape and the specified filter shape object.\nfunc intersect(with: CGRect) -> CIFilterShape\nCreates a filter shape that represents the intersection of the current filter shape and a rectangle.\nfunc union(with: CIFilterShape) -> CIFilterShape\nCreates a filter shape that results from the union of the current filter shape and another filter shape object.\nfunc union(with: CGRect) -> CIFilterShape\nCreates a filter shape that results from the union of the current filter shape and a rectangle."
  },
  {
    "title": "CICrystallize",
    "url": "https://developer.apple.com/documentation/coreimage/cicrystallize",
    "html": "Topics\nInstance Properties\ncenter\nThe center of the effect as x and y coordinates.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nradius\nThe radius, in pixels, of the effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons."
  },
  {
    "title": "CIShadedMaterial",
    "url": "https://developer.apple.com/documentation/coreimage/cishadedmaterial",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nscale\nThe scale of the effect.\n\nRequired\n\nshadingImage\nThe image to use as the height field.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image."
  },
  {
    "title": "CIFourfoldReflectedTile",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldreflectedtile",
    "html": "Topics\nInstance Properties\nacuteAngle\nThe primary angle for the repeating reflected tile.\n\nRequired\n\nangle\nThe angle, in radians, of the tiled pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nwidth\nThe width of a tile.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\nCIFourfoldReflectedTile\nThe properties you use to configure a fourfold reflected tile filter."
  },
  {
    "title": "CIFlashTransition",
    "url": "https://developer.apple.com/documentation/coreimage/ciflashtransition",
    "html": "Topics\nInstance Properties\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ncolor\nThe color of the light rays emanating from the flash.\n\nRequired\n\nextent\nThe extent of the flash.\n\nRequired\n\nfadeThreshold\nThe amount of fade between the flash and the target image.\n\nRequired\n\nmaxStriationRadius\nThe radius of the light rays emanating from the flash.\n\nRequired\n\nstriationContrast\nThe contrast of the light rays emanating from the flash.\n\nRequired\n\nstriationStrength\nThe strength of the light rays emanating from the flash.\n\nRequired\n\nRelationships\nInherits From\nCITransitionFilter\nSee Also\nRelated Documentation\n+ flashTransitionFilter\nCreates a flash of light to transition between two images."
  },
  {
    "title": "CIDissolveTransition",
    "url": "https://developer.apple.com/documentation/coreimage/cidissolvetransition",
    "html": "Relationships\nInherits From\nCITransitionFilter\nSee Also\nRelated Documentation\n+ dissolveTransitionFilter\nTransitions between two images with a fade effect."
  },
  {
    "title": "CIUnsharpMask",
    "url": "https://developer.apple.com/documentation/coreimage/ciunsharpmask",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nintensity\nThe intensity of the effect.\n\nRequired\n\nradius\nThe radius of the unsharp mask effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ unsharpMaskFilter\nIncreases an image’s contrast between two colors."
  },
  {
    "title": "CIKMeans",
    "url": "https://developer.apple.com/documentation/coreimage/cikmeans",
    "html": "Topics\nInstance Properties\ncount\n\nRequired\n\ninputMeans\n\nRequired\n\npasses\n\nRequired\n\nperceptual\n\nRequired\n\nRelationships\nInherits From\nCIAreaReductionFilter"
  },
  {
    "title": "vividLightBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3801605-vividlightblendmodefilter",
    "html": "Return Value\n\nThe blended image as a CIImage.\n\nDiscussion\n\nThe vivid-light blend mode combines the color-dodge and color-burn blend modes (rescaled so that neutral colors become middle gray). If the input Images values are lighter than middle gray, the filter uses dodge; for darker values, the filter uses burn.\n\ninputImage\n\nA CIImage containing the input image.\n\nbackgroundImage\n\nA CIImage containing the background image.\n\nThe following code sample applies the vivid-light blend mode filter to two images:\n\nfunc vividLightBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let filter = CIFilter.vividLightBlendMode()\n    filter.inputImage = inputImage\n    filter.backgroundImage = backgroundImage\n    return filter.outputImage!\n}\n"
  },
  {
    "title": "CIDepthOfField",
    "url": "https://developer.apple.com/documentation/coreimage/cidepthoffield",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\npoint0\nThe first point in the focused region of the output image.\n\nRequired\n\npoint1\nThe second point in the focused region of the output image.\n\nRequired\n\nradius\nThe distance from the center of the effect.\n\nRequired\n\nsaturation\nThe amount to adjust the saturation by.\n\nRequired\n\nunsharpMaskIntensity\nThe intensity of the unsharp mask effect applied to the in-focus area.\n\nRequired\n\nunsharpMaskRadius\nThe radius of the unsharp mask effect applied to the in-focus area.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ depthOfFieldFilter\nSimulates a depth of field effect."
  },
  {
    "title": "CIMeshGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/cimeshgenerator",
    "html": "Topics\nInstance Properties\ncolor\nThe color of the rendered mesh.\n\nRequired\n\nmesh\nAn array that describes the mesh to render.\n\nRequired\n\nwidth\nThe width of the effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments."
  },
  {
    "title": "CIQRCodeGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodegenerator",
    "html": "Topics\nInstance Properties\ncorrectionLevel\nThe QR code correction level: L, M, Q, or H.\n\nRequired\n\nmessage\nThe message to encode in the QR code.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ QRCodeGenerator\nGenerates a quick response (QR) code image."
  },
  {
    "title": "CIBarcodeGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/cibarcodegenerator",
    "html": "Topics\nInstance Properties\nbarcodeDescriptor\nThe barcode descriptor to generate an image for.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor."
  },
  {
    "title": "CIAztecCodeGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/ciazteccodegenerator",
    "html": "Topics\nInstance Properties\ncompactStyle\nA Boolean that specifies whether to force a compact style Aztec code.\n\nRequired\n\ncorrectionLevel\nThe Aztec error correction, a value from 5 to 95.\n\nRequired\n\nlayers\nThe number of Aztec layers, a value from 1 to 32.\n\nRequired\n\nmessage\nThe message to encode in the Aztec barcode.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode."
  },
  {
    "title": "CIAffineTile",
    "url": "https://developer.apple.com/documentation/coreimage/ciaffinetile",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\ntransform\nThe transform to apply to the image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ affineTileFilter\nPerforms a transform on the image and tiles the result."
  },
  {
    "title": "CIFourfoldRotatedTile",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldrotatedtile",
    "html": "Topics\nInstance Properties\nangle\nThe angle, in radians, of the tiled pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nwidth\nThe width of a tile.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees."
  },
  {
    "title": "CIFourfoldTranslatedTile",
    "url": "https://developer.apple.com/documentation/coreimage/cifourfoldtranslatedtile",
    "html": "Topics\nInstance Properties\nacuteAngle\nThe primary angle for the repeating translated tile.\n\nRequired\n\nangle\nThe angle, in radians, of the tiled pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nwidth\nThe width of a tile.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations."
  },
  {
    "title": "CIAffineClamp",
    "url": "https://developer.apple.com/documentation/coreimage/ciaffineclamp",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\ntransform\nThe transform to apply to the image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity."
  },
  {
    "title": "CIBloom",
    "url": "https://developer.apple.com/documentation/coreimage/cibloom",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nintensity\nThe intensity of the effect.\n\nRequired\n\nradius\nThe radius, in pixels, of the effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect."
  },
  {
    "title": "CIStretchCrop",
    "url": "https://developer.apple.com/documentation/coreimage/cistretchcrop",
    "html": "Topics\nInstance Properties\ncenterStretchAmount\n\nRequired\n\ncropAmount\n\nRequired\n\ninputImage\n\nRequired\n\nsize\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIRoundedRectangleStrokeGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/ciroundedrectanglestrokegenerator",
    "html": "Topics\nInstance Properties\ncolor\n\nRequired\n\nextent\n\nRequired\n\nradius\n\nRequired\n\nwidth\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CICheckerboardGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/cicheckerboardgenerator",
    "html": "Topics\nInstance Properties\ncenter\nThe center of the effect as x and y coordinates.\n\nRequired\n\ncolor0\nA color to use for the first set of squares.\n\nRequired\n\ncolor1\nA color to use for the second set of squares.\n\nRequired\n\nsharpness\nThe sharpness of the edges in the pattern.\n\nRequired\n\nwidth\nThe width of the squares in the pattern.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image."
  },
  {
    "title": "CIHighlightShadowAdjust",
    "url": "https://developer.apple.com/documentation/coreimage/cihighlightshadowadjust",
    "html": "Topics\nInstance Properties\nhighlightAmount\nThe amount of adjustment to the highlights in the image.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nradius\nThe shadow highlight radius.\n\nRequired\n\nshadowAmount\nThe amount of adjustment to the shadows in the image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows."
  },
  {
    "title": "CISpotColor",
    "url": "https://developer.apple.com/documentation/coreimage/cispotcolor",
    "html": "Topics\nInstance Properties\ncenterColor1\nThe center value of the first color range to replace.\n\nRequired\n\ncenterColor2\nThe center value of the second color range to replace.\n\nRequired\n\ncenterColor3\nThe center value of the third color range to replace.\n\nRequired\n\ncloseness1\nA value that indicates how closely the first color must match before it’s replaced.\n\nRequired\n\ncloseness2\nA value that indicates how closely the second color must match before it’s replaced.\n\nRequired\n\ncloseness3\nA value that indicates how closely the third color must match before it’s replaced.\n\nRequired\n\ncontrast1\nThe contrast of the first replacement color.\n\nRequired\n\ncontrast2\nThe contrast of the second replacement color.\n\nRequired\n\ncontrast3\nThe contrast of the third replacement color.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nreplacementColor1\nA replacement color for the first color range.\n\nRequired\n\nreplacementColor2\nA replacement color for the second color range.\n\nRequired\n\nreplacementColor3\nA replacement color for the third color range.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ spotColorFilter\nReplaces colors of an image with specifed colors."
  },
  {
    "title": "CIHexagonalPixellate",
    "url": "https://developer.apple.com/documentation/coreimage/cihexagonalpixellate",
    "html": "Topics\nInstance Properties\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nscale\nThe size of the hexagons.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons."
  },
  {
    "title": "CIColumnAverage",
    "url": "https://developer.apple.com/documentation/coreimage/cicolumnaverage",
    "html": "Relationships\nInherits From\nCIAreaReductionFilter"
  },
  {
    "title": "CIParallelogramTile",
    "url": "https://developer.apple.com/documentation/coreimage/ciparallelogramtile",
    "html": "Topics\nInstance Properties\nacuteAngle\nThe primary angle for the repeating parallelogram tile.\n\nRequired\n\nangle\nThe angle, in radians, of the tiled pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nwidth\nThe width of a tile.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result."
  },
  {
    "title": "CIMix",
    "url": "https://developer.apple.com/documentation/coreimage/cimix",
    "html": "Topics\nInstance Properties\namount\nThe amount of the effect.\n\nRequired\n\nbackgroundImage\nThe image to use as a background image.\n\nRequired\n\ninputImage\nThe image to use as a foreground image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ mixFilter\nBlends two images together."
  },
  {
    "title": "CILineOverlay",
    "url": "https://developer.apple.com/documentation/coreimage/cilineoverlay",
    "html": "Topics\nInstance Properties\nNRNoiseLevel\nThe noise level of the image, used with camera data, that's removed before tracing the edges of the image.\n\nRequired\n\nNRSharpness\nThe amount of sharpening done when removing noise in the image before tracing the edges of the image.\n\nRequired\n\ncontrast\nThe amount of antialiasing to use on the edges produced by this filter.\n\nRequired\n\nedgeIntensity\nThe accentuation factor of the Sobel gradient information when tracing the edges of the image.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nthreshold\nA value that determines edge visibility.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects."
  },
  {
    "title": "CICompositeOperation",
    "url": "https://developer.apple.com/documentation/coreimage/cicompositeoperation",
    "html": "Topics\nInstance Properties\nbackgroundImage\nThe image to use as a background image.\n\nRequired\n\ninputImage\nThe image to use as a foreground image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\nComposite Operations\nComposite images by using a range of blend modes and compositing operators."
  },
  {
    "title": "CIAreaMinimumAlpha",
    "url": "https://developer.apple.com/documentation/coreimage/ciareaminimumalpha",
    "html": "Relationships\nInherits From\nCIAreaReductionFilter"
  },
  {
    "title": "CIRowAverage",
    "url": "https://developer.apple.com/documentation/coreimage/cirowaverage",
    "html": "Relationships\nInherits From\nCIAreaReductionFilter"
  },
  {
    "title": "CIAreaMinMax",
    "url": "https://developer.apple.com/documentation/coreimage/ciareaminmax",
    "html": "Relationships\nInherits From\nCIAreaReductionFilter"
  },
  {
    "title": "CIAccordionFoldTransition",
    "url": "https://developer.apple.com/documentation/coreimage/ciaccordionfoldtransition",
    "html": "Topics\nInstance Properties\nbottomHeight\nThe height of the accordion-fold part of the transition.\n\nRequired\n\nfoldShadowAmount\nA value that specifies the intensity of the shadow in the transtion.\n\nRequired\n\nnumberOfFolds\nThe number of folds used in the transition.\n\nRequired\n\nRelationships\nInherits From\nCITransitionFilter\nSee Also\nRelated Documentation\n+ accordionFoldTransitionFilter\nTransitions by folding and crossfading an image to reveal the target image."
  },
  {
    "title": "CIAreaMinMaxRed",
    "url": "https://developer.apple.com/documentation/coreimage/ciareaminmaxred",
    "html": "Relationships\nInherits From\nCIAreaReductionFilter"
  },
  {
    "title": "imageWithImageProvider:size::format:colorSpace:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1579115-imagewithimageprovider",
    "html": "Parameters\np\n\nA data provider that implements the CIImageProvider informal protocol. Core Image maintains a strong reference to this object until the image is deallocated.\n\nwidth\n\nThe width of the image.\n\nheight\n\nThe height of the image.\n\nf\n\nA pixel format constant. See Pixel Formats.\n\ncs\n\nThe color space that the image is defined in. If the this value is nil, the image is not color matched. Pass nil for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables).\n\ndict\n\nA dictionary that specifies image-creation options, either kCIImageProviderTileSize or kCIImageProviderUserInfo. See CIImageProvider for more information on these options.\n\nReturn Value\n\nAn image object initialized with the data from the data provider. Core Image does not populate the image object until the object needs the data.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithImageProvider:size::format:colorSpace:options:\nInitializes an image object with data provided by an image provider, using the specified options."
  },
  {
    "title": "imageBySettingAlphaOneInExtent:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645891-imagebysettingalphaoneinextent",
    "html": "Parameters\nextent\n\nThe rectangular area in the image to be set to full opacity.\n\nReturn Value\n\nAn image object representing the result of the operation.\n\nSee Also\nCreating an Image by Modifying an Existing Image\n- imageByApplyingFilter:withInputParameters:\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\n- imageByApplyingFilter:\nApplies the filter to an image and returns the output.\n- imageByApplyingTransform:\nReturns a new image that represents the original image after applying an affine transform.\n- imageByCroppingToRect:\nReturns a new image with a cropped portion of the original image.\n- imageByApplyingOrientation:\nReturns a new image created by transforming the original image to the specified EXIF orientation.\n- imageByClampingToExtent\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\n- imageByClampingToRect:\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\n- imageByCompositingOverImage:\nReturns a new image created by compositing the original image over the specified destination image.\n- imageByColorMatchingColorSpaceToWorkingSpace:\nReturns a new image created by color matching from the specified color space to the context’s working color space.\n- imageByColorMatchingWorkingSpaceToColorSpace:\nReturns a new image created by color matching from the context’s working color space to the specified color space.\n- imageByPremultiplyingAlpha\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\n- imageByUnpremultiplyingAlpha\nReturns a new image created by dividing the image’s RGB values by its alpha values.\n- imageByApplyingGaussianBlurWithSigma:\nReturns a new image created by applying a Gaussian Blur filter to the image.\n- imageBySettingProperties:\nReturns a new image created by adding the specified metadata properties to the image.\n- imageByInsertingIntermediate\nReturns a new image created by inserting an intermediate.\n- imageByInsertingIntermediate:\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "CITextImageGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/citextimagegenerator",
    "html": "Topics\nInstance Properties\nfontName\nThe name of the font to use for the generated text.\n\nRequired\n\nfontSize\nThe size of the font to use for the generated text.\n\nRequired\n\nscaleFactor\nThe scale of the font to use for the generated text.\n\nRequired\n\ntext\nThe text to render.\n\nRequired\n\npadding\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "CIAreaHistogram",
    "url": "https://developer.apple.com/documentation/coreimage/ciareahistogram",
    "html": "Topics\nInstance Properties\ncount\n\nRequired\n\nscale\n\nRequired\n\nRelationships\nInherits From\nCIAreaReductionFilter"
  },
  {
    "title": "CIAreaAverage",
    "url": "https://developer.apple.com/documentation/coreimage/ciareaaverage",
    "html": "Relationships\nInherits From\nCIAreaReductionFilter"
  },
  {
    "title": "imageWithData:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547016-imagewithdata",
    "html": "Parameters\ndata\n\nA pointer to the image data. The data must be premultiplied.\n\nd\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nAn image object initialized with the supplied data and set up with the specified options.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithData:options:\nInitializes an image object with the supplied image data, using the specified options."
  },
  {
    "title": "imageWithCVPixelBuffer:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547005-imagewithcvpixelbuffer",
    "html": "Parameters\nbuffer\n\nA CVPixelBuffer object.\n\nReturn Value\n\nAn image object initialized with the contents of the image buffer object.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithCVPixelBuffer:\nInitializes an image object from the contents of a Core Video pixel buffer."
  },
  {
    "title": "CILinearToSRGBToneCurve",
    "url": "https://developer.apple.com/documentation/coreimage/cilineartosrgbtonecurve",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity."
  },
  {
    "title": "CIAreaLogarithmicHistogram",
    "url": "https://developer.apple.com/documentation/coreimage/ciarealogarithmichistogram",
    "html": "Topics\nInstance Properties\ncount\n\nRequired\n\nmaximumStop\n\nRequired\n\nminimumStop\n\nRequired\n\nscale\n\nRequired\n\nRelationships\nInherits From\nCIAreaReductionFilter"
  },
  {
    "title": "CIAreaMaximumAlpha",
    "url": "https://developer.apple.com/documentation/coreimage/ciareamaximumalpha",
    "html": "Relationships\nInherits From\nCIAreaReductionFilter"
  },
  {
    "title": "CIAreaMaximum",
    "url": "https://developer.apple.com/documentation/coreimage/ciareamaximum",
    "html": "Relationships\nInherits From\nCIAreaReductionFilter"
  },
  {
    "title": "CIHueAdjust",
    "url": "https://developer.apple.com/documentation/coreimage/cihueadjust",
    "html": "Topics\nInstance Properties\nangle\nAn angle, in radians, to use to correct the hue of an image.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ hueAdjustFilter\nModifies an image’s hue."
  },
  {
    "title": "CIGammaAdjust",
    "url": "https://developer.apple.com/documentation/coreimage/cigammaadjust",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\npower\nA gamma value to use to correct image brightness.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ gammaAdjustFilter\nAlters an image’s transition between black and white."
  },
  {
    "title": "CIColorControls",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcontrols",
    "html": "Topics\nInstance Properties\nbrightness\nThe amount of brightness to apply.\n\nRequired\n\ncontrast\nThe amount of contrast to apply.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nsaturation\nThe amount of saturation to apply.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors."
  },
  {
    "title": "CIColorThresholdOtsu",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorthresholdotsu",
    "html": "Topics\nInstance Properties\ninputImage\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIColorPolynomial",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorpolynomial",
    "html": "Topics\nInstance Properties\nalphaCoefficients\nPolynomial coefficients for the alpha channel.\n\nRequired\n\nblueCoefficients\nPolynomial coefficients for the blue channel.\n\nRequired\n\ngreenCoefficients\nPolynomial coefficients for the green channel.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nredCoefficients\nPolynomial coefficients for the red channel.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ colorPolynomialFilter\nAlters an image’s colors."
  },
  {
    "title": "CINinePartStretched",
    "url": "https://developer.apple.com/documentation/coreimage/cininepartstretched",
    "html": "Topics\nInstance Properties\nbreakpoint0\n\nRequired\n\nbreakpoint1\n\nRequired\n\ngrowAmount\n\nRequired\n\ninputImage\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIHoleDistortion",
    "url": "https://developer.apple.com/documentation/coreimage/ciholedistortion",
    "html": "Topics\nInstance Properties\ncenter\n\nRequired\n\ninputImage\n\nRequired\n\nradius\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CITemperatureAndTint",
    "url": "https://developer.apple.com/documentation/coreimage/citemperatureandtint",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nneutral\nA vector containing the source white point defined by color temperature and tint.\n\nRequired\n\ntargetNeutral\nA vector containing the desired white point defined by color temperature and tint.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint."
  },
  {
    "title": "CIColorMatrix",
    "url": "https://developer.apple.com/documentation/coreimage/cicolormatrix",
    "html": "Topics\nInstance Properties\nAVector\nThe amount of alpha to multiply the source color values by.\n\nRequired\n\nBVector\nThe amount of blue to multiply the source color values by.\n\nRequired\n\nGVector\nThe amount of green to multiply the source color values by.\n\nRequired\n\nRVector\nThe amount of red to multiply the source color values by.\n\nRequired\n\nbiasVector\nA vector that’s added to each color component.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided."
  },
  {
    "title": "CIDroste",
    "url": "https://developer.apple.com/documentation/coreimage/cidroste",
    "html": "Topics\nInstance Properties\ninputImage\n\nRequired\n\ninsetPoint0\n\nRequired\n\ninsetPoint1\n\nRequired\n\nperiodicity\n\nRequired\n\nrotation\n\nRequired\n\nstrands\n\nRequired\n\nzoom\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIGlassDistortion",
    "url": "https://developer.apple.com/documentation/coreimage/ciglassdistortion",
    "html": "Topics\nInstance Properties\ncenter\n\nRequired\n\ninputImage\n\nRequired\n\nscale\n\nRequired\n\ntextureImage\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIVibrance",
    "url": "https://developer.apple.com/documentation/coreimage/civibrance",
    "html": "Topics\nInstance Properties\namount\nThe amount to adjust the saturation by.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ vibranceFilter\nAdjusts an image’s vibrancy."
  },
  {
    "title": "CIWhitePointAdjust",
    "url": "https://developer.apple.com/documentation/coreimage/ciwhitepointadjust",
    "html": "Topics\nInstance Properties\ncolor\nA color to use as the white point.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "CIConvolution",
    "url": "https://developer.apple.com/documentation/coreimage/ciconvolution",
    "html": "Topics\nInstance Properties\nbias\nA value that’s added to each output pixel.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nweights\nThe convolution kernel.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ convolution3X3Filter\nApplies a convolution 3 x 3 filter to the RGBA components of an image.\n+ convolution5X5Filter\nApplies a convolution 5 x 5 filter to the RGBA components image.\n+ convolution7X7Filter\nApplies a convolution 7 x 7 filter to the RGBA color components of an image.\n+ convolution9HorizontalFilter\nApplies a convolution-9 horizontal filter to the RGBA components of an image.\n+ convolution9VerticalFilter\nApplies a convolution-9 vertical filter to the RGBA components of an image."
  },
  {
    "title": "CISRGBToneCurveToLinear",
    "url": "https://developer.apple.com/documentation/coreimage/cisrgbtonecurvetolinear",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear."
  },
  {
    "title": "CILightTunnel",
    "url": "https://developer.apple.com/documentation/coreimage/cilighttunnel",
    "html": "Topics\nInstance Properties\ncenter\n\nRequired\n\ninputImage\n\nRequired\n\nradius\n\nRequired\n\nrotation\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIRoundedRectangleGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/ciroundedrectanglegenerator",
    "html": "Topics\nInstance Properties\ncolor\nThe color of the rounded rectangle.\n\nRequired\n\nextent\nA rectangle that defines the extent of the effect.\n\nRequired\n\nradius\nThe distance from the center of the effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image."
  },
  {
    "title": "CIDepthToDisparity",
    "url": "https://developer.apple.com/documentation/coreimage/cidepthtodisparity",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data."
  },
  {
    "title": "CIRandomGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/cirandomgenerator",
    "html": "Relationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ randomGeneratorFilter\nGenerates a random filter image."
  },
  {
    "title": "CIExposureAdjust",
    "url": "https://developer.apple.com/documentation/coreimage/ciexposureadjust",
    "html": "Topics\nInstance Properties\nEV\nThe amount to adjust the exposure of the image by.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ exposureAdjustFilter\nAdjusts an image’s exposure."
  },
  {
    "title": "CIDisparityToDepth",
    "url": "https://developer.apple.com/documentation/coreimage/cidisparitytodepth",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data."
  },
  {
    "title": "CIBumpDistortionLinear",
    "url": "https://developer.apple.com/documentation/coreimage/cibumpdistortionlinear",
    "html": "Topics\nInstance Properties\nangle\n\nRequired\n\ncenter\n\nRequired\n\ninputImage\n\nRequired\n\nradius\n\nRequired\n\nscale\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "imageWithCGLayer:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547022-imagewithcglayer",
    "html": "Parameters\nlayer\n\nA CGLayer object. For more information see Quartz 2D Programming Guide and CGLayer.\n\nReturn Value\n\nAn image object initialized with the contents of the layer object.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithCGLayer:\nInitializes an image object from the contents supplied by a CGLayer object."
  },
  {
    "title": "imageWithData:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547029-imagewithdata",
    "html": "Parameters\ndata\n\nThe data object that holds the contents of an image file (such as TIFF, GIF, JPG, or whatever else the system supports). The image data must be premultiplied.\n\nReturn Value\n\nAn image object initialized with the supplied data, or nil if the method cannot create an image representation from the contents of the supplied data object.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithData:\nInitializes an image object with the supplied image data."
  },
  {
    "title": "redEye",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageautoadjustmentoption/1437988-redeye",
    "html": "Discussion\n\nThe value associated with this key is a CFBoolean value. Supply false to indicate not to return a red eye filter. If you don’t specify this option, Core Image assumes its value is true."
  },
  {
    "title": "features",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageautoadjustmentoption/1438029-features",
    "html": "Discussion\n\nThe associated value is an array of CIFeature objects. If you don’t supply an array, the Core Image searches for features using the CIDetector class."
  },
  {
    "title": "crop",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageautoadjustmentoption/1438229-crop",
    "html": "Discussion\n\nThe value associated with this key is a CFBoolean value. If true, the returned filters include an operation that crops the image around the features specified with the features option (or any features detected in the image, if that option is not present). Supply false to indicate not to return a crop filter. If you don’t specify this option, Core Image assumes its value is false."
  },
  {
    "title": "enhance",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageautoadjustmentoption/1437819-enhance",
    "html": "Discussion\n\nThe value associated with this key is a CFBoolean value. Supply false to indicate not to return enhancement filters. If you don’t specify this option, Core Image assumes its value is true."
  },
  {
    "title": "CITwirlDistortion",
    "url": "https://developer.apple.com/documentation/coreimage/citwirldistortion",
    "html": "Topics\nInstance Properties\nangle\n\nRequired\n\ncenter\n\nRequired\n\ninputImage\n\nRequired\n\nradius\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIVortexDistortion",
    "url": "https://developer.apple.com/documentation/coreimage/civortexdistortion",
    "html": "Topics\nInstance Properties\nangle\n\nRequired\n\ncenter\n\nRequired\n\ninputImage\n\nRequired\n\nradius\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIPinchDistortion",
    "url": "https://developer.apple.com/documentation/coreimage/cipinchdistortion",
    "html": "Topics\nInstance Properties\ncenter\n\nRequired\n\ninputImage\n\nRequired\n\nradius\n\nRequired\n\nscale\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIColorAbsoluteDifference",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorabsolutedifference",
    "html": "Topics\nInstance Properties\ninputImage\n\nRequired\n\ninputImage2\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CINinePartTiled",
    "url": "https://developer.apple.com/documentation/coreimage/cinineparttiled",
    "html": "Topics\nInstance Properties\nbreakpoint0\n\nRequired\n\nbreakpoint1\n\nRequired\n\nflipYTiles\n\nRequired\n\ngrowAmount\n\nRequired\n\ninputImage\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIGlassLozenge",
    "url": "https://developer.apple.com/documentation/coreimage/ciglasslozenge",
    "html": "Topics\nInstance Properties\ninputImage\n\nRequired\n\npoint0\n\nRequired\n\npoint1\n\nRequired\n\nradius\n\nRequired\n\nrefraction\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIAttributedTextImageGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/ciattributedtextimagegenerator",
    "html": "Topics\nInstance Properties\nscaleFactor\nThe scale at which to render the text.\n\nRequired\n\ntext\nThe text to render.\n\nRequired\n\npadding\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image."
  },
  {
    "title": "CIColorClamp",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorclamp",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nmaxComponents\nA vector containing the higher clamping values.\n\nRequired\n\nminComponents\nA vector containing the lower clamping values.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ colorClampFilter\nAlters the colors in an image based on color components."
  },
  {
    "title": "CIColorThreshold",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorthreshold",
    "html": "Topics\nInstance Properties\ninputImage\n\nRequired\n\nthreshold\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "z",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1437627-z",
    "html": "See Also\nGetting Values From a Vector\nfunc value(at: Int) -> CGFloat\nReturns a value from a specific position in the vector.\nvar count: Int\nThe number of items in the vector.\nvar x: CGFloat\nThe value located in the first position in the vector.\nvar y: CGFloat\nThe value located in the second position in the vector.\nvar w: CGFloat\nThe value located in the fourth position in the vector.\nvar stringRepresentation: String\nThe string representation of the vector.\nvar cgAffineTransformValue: CGAffineTransform\nThe values in the vector represented as an affine transform.\nvar cgPointValue: CGPoint\nThe values in the vector as a Core Graphics point structure.\nvar cgRectValue: CGRect\nThe values in the vector as a Core Graphics rectangle structure."
  },
  {
    "title": "y",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1437843-y",
    "html": "See Also\nGetting Values From a Vector\nfunc value(at: Int) -> CGFloat\nReturns a value from a specific position in the vector.\nvar count: Int\nThe number of items in the vector.\nvar x: CGFloat\nThe value located in the first position in the vector.\nvar z: CGFloat\nThe value located in the third position in the vector.\nvar w: CGFloat\nThe value located in the fourth position in the vector.\nvar stringRepresentation: String\nThe string representation of the vector.\nvar cgAffineTransformValue: CGAffineTransform\nThe values in the vector represented as an affine transform.\nvar cgPointValue: CGPoint\nThe values in the vector as a Core Graphics point structure.\nvar cgRectValue: CGRect\nThe values in the vector as a Core Graphics rectangle structure."
  },
  {
    "title": "filterArrayFromSerializedXMP:inputImageExtent:error:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438237-filterarrayfromserializedxmp",
    "html": "Parameters\nxmpData\n\nThe XMP data created previously by calling serializedXMPFromFilters:inputImageExtent:.\n\nextent\n\nThe extent of the image from which the XMP data was extracted.\n\noutError\n\nThe address of an NSError object for receiving errors, otherwise nil.\n\nSee Also\nSerializing and Deserializing Filters\n+ serializedXMPFromFilters:inputImageExtent:\nSerializes filter parameters into XMP form that is suitable for embedding in an image.\nDeprecated"
  },
  {
    "title": "stringRepresentation",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1437752-stringrepresentation",
    "html": "Discussion\n\nSome example string representations of vectors :\n\n@\"[1.0 0.5 0.3]\" — a vec3 vector whose components are X = 1.0, Y = 0.5, and Z = 0.3\n\n@\"[10.0 23.0] — a vec2 vector whose components are X = 10.0 and Y = 23.0\n\nTo create a CIVector object from a string representation, use the vectorWithString: method.\n\nSee Also\nGetting Values From a Vector\nfunc value(at: Int) -> CGFloat\nReturns a value from a specific position in the vector.\nvar count: Int\nThe number of items in the vector.\nvar x: CGFloat\nThe value located in the first position in the vector.\nvar y: CGFloat\nThe value located in the second position in the vector.\nvar z: CGFloat\nThe value located in the third position in the vector.\nvar w: CGFloat\nThe value located in the fourth position in the vector.\nvar cgAffineTransformValue: CGAffineTransform\nThe values in the vector represented as an affine transform.\nvar cgPointValue: CGPoint\nThe values in the vector as a Core Graphics point structure.\nvar cgRectValue: CGRect\nThe values in the vector as a Core Graphics rectangle structure."
  },
  {
    "title": "cgRectValue",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1438108-cgrectvalue",
    "html": "Discussion\n\nReading this property creates a CGRect structure whose origin is the x and y values in the vector and whose size is the z and w values in the vector.\n\nSee Also\nGetting Values From a Vector\nfunc value(at: Int) -> CGFloat\nReturns a value from a specific position in the vector.\nvar count: Int\nThe number of items in the vector.\nvar x: CGFloat\nThe value located in the first position in the vector.\nvar y: CGFloat\nThe value located in the second position in the vector.\nvar z: CGFloat\nThe value located in the third position in the vector.\nvar w: CGFloat\nThe value located in the fourth position in the vector.\nvar stringRepresentation: String\nThe string representation of the vector.\nvar cgAffineTransformValue: CGAffineTransform\nThe values in the vector represented as an affine transform.\nvar cgPointValue: CGPoint\nThe values in the vector as a Core Graphics point structure."
  },
  {
    "title": "cgAffineTransformValue",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1438249-cgaffinetransformvalue",
    "html": "Discussion\n\nReading this property creates a CGAffineTransform structure from the first six values in the vector.\n\nSee Also\nGetting Values From a Vector\nfunc value(at: Int) -> CGFloat\nReturns a value from a specific position in the vector.\nvar count: Int\nThe number of items in the vector.\nvar x: CGFloat\nThe value located in the first position in the vector.\nvar y: CGFloat\nThe value located in the second position in the vector.\nvar z: CGFloat\nThe value located in the third position in the vector.\nvar w: CGFloat\nThe value located in the fourth position in the vector.\nvar stringRepresentation: String\nThe string representation of the vector.\nvar cgPointValue: CGPoint\nThe values in the vector as a Core Graphics point structure.\nvar cgRectValue: CGRect\nThe values in the vector as a Core Graphics rectangle structure."
  },
  {
    "title": "enableVendorLensCorrection",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilteroption/1437715-enablevendorlenscorrection",
    "html": "Discussion\n\nThe value for this key is a NSNumber object containing a Boolean value. If this value is true, or if this option is not specified but the image contains metadata for lens distortion parameters, Core Image corrects for lens distortion."
  },
  {
    "title": "luminanceNoiseReductionAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilteroption/1437945-luminancenoisereductionamount",
    "html": "Discussion\n\nThe value for this key is a NSNumber object containing a value between 0.0 and 1.0."
  },
  {
    "title": "colorNoiseReductionAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilteroption/1437640-colornoisereductionamount",
    "html": "Discussion\n\nThe value for this key is a NSNumber object containing a value between 0.0 and 1.0."
  },
  {
    "title": "noiseReductionContrastAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilteroption/1437681-noisereductioncontrastamount",
    "html": "Discussion\n\nThe value for this key is a NSNumber object containing a value between 0.0 and 1.0."
  },
  {
    "title": "noiseReductionSharpnessAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilteroption/1438009-noisereductionsharpnessamount",
    "html": "Discussion\n\nThe value for this key is a NSNumber object containing a value between 0.0 and 1.0."
  },
  {
    "title": "noiseReductionDetailAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilteroption/1437943-noisereductiondetailamount",
    "html": "Discussion\n\nThe value for this key is a NSNumber object containing a value between 0.0 and 1.0."
  },
  {
    "title": "serializedXMPFromFilters:inputImageExtent:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438006-serializedxmpfromfilters",
    "html": "Parameters\nfilters\n\nThe array of filters to serialize. See Discussion for the filters that can be serialized.\n\nextent\n\nThe extent of the input image to the filter.\n\nDiscussion\n\nAt this time the only filters classes that can be serialized using this method are, CIAffineTransform, CICrop, and the filters returned by the CIImage methods autoAdjustmentFilters and autoAdjustmentFiltersWithOptions:. The parameters of other filter classes will not be serialized.\n\nSee Also\nSerializing and Deserializing Filters\n+ filterArrayFromSerializedXMP:inputImageExtent:error:\nReturns an array of filter objects de-serialized from XMP data.\nDeprecated"
  },
  {
    "title": "imageWithCGImage:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547021-imagewithcgimage",
    "html": "Parameters\nimage\n\nA Quartz 2D image (CGImageRef) object. For more information, see Quartz 2D Programming Guide and CGImage.\n\nd\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nAn image object initialized with the contents of the Quartz 2D image and the specified options.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithCGImage:options:\nInitializes an image object with a Quartz 2D image, using the specified options."
  },
  {
    "title": "level",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageautoadjustmentoption/1438040-level",
    "html": "Discussion\n\nThe value associated with this key is a CFBoolean value. If true, Core Image analyzes the image to determine whether it would benefit from rotation—for example, a landscape photo in which the horizon is not horizontal—and returns a filter to perform that rotation. Supply false to indicate not to return a rotation filter. If you don’t specify this option, Core Image assumes its value is false."
  },
  {
    "title": "cgPointValue",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1437672-cgpointvalue",
    "html": "Discussion\n\nReading this property creates a CGPoint structure from the first two values (x and y) in the vector.\n\nSee Also\nGetting Values From a Vector\nfunc value(at: Int) -> CGFloat\nReturns a value from a specific position in the vector.\nvar count: Int\nThe number of items in the vector.\nvar x: CGFloat\nThe value located in the first position in the vector.\nvar y: CGFloat\nThe value located in the second position in the vector.\nvar z: CGFloat\nThe value located in the third position in the vector.\nvar w: CGFloat\nThe value located in the fourth position in the vector.\nvar stringRepresentation: String\nThe string representation of the vector.\nvar cgAffineTransformValue: CGAffineTransform\nThe values in the vector represented as an affine transform.\nvar cgRectValue: CGRect\nThe values in the vector as a Core Graphics rectangle structure."
  },
  {
    "title": "w",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1438058-w",
    "html": "See Also\nGetting Values From a Vector\nfunc value(at: Int) -> CGFloat\nReturns a value from a specific position in the vector.\nvar count: Int\nThe number of items in the vector.\nvar x: CGFloat\nThe value located in the first position in the vector.\nvar y: CGFloat\nThe value located in the second position in the vector.\nvar z: CGFloat\nThe value located in the third position in the vector.\nvar stringRepresentation: String\nThe string representation of the vector.\nvar cgAffineTransformValue: CGAffineTransform\nThe values in the vector represented as an affine transform.\nvar cgPointValue: CGPoint\nThe values in the vector as a Core Graphics point structure.\nvar cgRectValue: CGRect\nThe values in the vector as a Core Graphics rectangle structure."
  },
  {
    "title": "viewForUIConfiguration:excludedKeys:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1427521-viewforuiconfiguration",
    "html": "Parameters\ninUIConfiguration\n\nA dictionary that contains values for the IKUISizeFlavor and kCIUIParameterSet keys. For allowed values for the IKUISizeFlavor key, see User Interface Options. For allowed values for the kCIUIParameterSet key, see User Interface Control Options.\n\ninKeys\n\nAn array of the input keys for which you do not want to provide a user interface. Pass nil if you want all input keys to be represented in the user interface.\n\nReturn Value\n\nAn IKFilterUIView object.\n\nDiscussion\n\nCalling this method to receive a view for a filter causes the CIFilter class to invoke the provideViewForUIConfiguration:excludedKeys: method. If you override provideViewForUIConfiguration:excludedKeys: the user interface is created by your filter subclass. Otherwise, Core Image automatically generates the user interface based on the filter keys and attributes.\n\nYour app can retrieve a view whose control sizes complement the size of user interface elements already used in the application. It is also possible to choose which filter input parameters appear in the view. Consumer applications, for example, may want to show a small, basic set of input parameters whereas professional applications may want to provide access to all input parameters.\n\nWhen you request a user interface for a parameter set, all keys for that set and below are included. For example, the advanced set consists of all parameters in the basic, intermediate and advanced sets. The development set should contain parameters that are either experimental or for debugging purposes. You should use them only during the development of filters and client applications, and not in a shipping product.\n\nThe controls in the view use bindings to set the values of the filter. See Cocoa Bindings Programming Topics if you are unfamiliar with bindings."
  },
  {
    "title": "photoEffectProcessFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228389-photoeffectprocessfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a preconfigured set of effects that imitate vintage photography film with emphasized cool colors.\n\nThe photo effect process filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds a lower contrast to the input image:\n\nfunc photoEffectProcess(inputImage: CIImage ) -> CIImage {\n    let photoEffect = CIFilter.photoEffectProcess()\n    photoEffect.inputImage = inputImage\n    return photoEffect.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "white",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1643571-white",
    "html": "Return Value\n\nThe color object in the sRGB colorspace.\n\nSee Also\nCreating a CIColor Object with Preset Components\nclass var black: CIColor\nReturns a color object whose RGB values are all 0.0 and whose alpha value is 1.0.\nclass var blue: CIColor\nReturns a color object whose RGB values are 0.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var clear: CIColor\nReturns a color object whose RGB and alpha values are all 0.0.\nclass var cyan: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 1.0 and whose alpha value is 1.0.\nclass var gray: CIColor\nReturns a color object whose RGB values are all 0.5 and whose alpha value is 1.0.\nclass var green: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 0.0 and whose alpha value is 1.0.\nclass var magenta: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var red: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 0.0 and whose alpha value is 1.0.\nclass var yellow: CIColor\nReturns a color object whose RGB values are 1.0, 1.0, and 0.0 and whose alpha value is 1.0."
  },
  {
    "title": "symbolVersion",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodedescriptor/2875193-symbolversion",
    "html": "Discussion\n\nISO/IEC 18004 defines versions from 1 to 40, where a higher symbol version indicates a larger data-carrying capacity\n\nSee Also\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload containing the data encoded in the QR code.\nvar maskPattern: UInt8\nThe QR code's mask pattern.\nvar errorCorrectionLevel: CIQRCodeDescriptor.ErrorCorrectionLevel\nThe QR code error correction level."
  },
  {
    "title": "maskPattern",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodedescriptor/2875191-maskpattern",
    "html": "See Also\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload containing the data encoded in the QR code.\nvar symbolVersion: Int\nThe version of the QR code.\nvar errorCorrectionLevel: CIQRCodeDescriptor.ErrorCorrectionLevel\nThe QR code error correction level."
  },
  {
    "title": "imageWithCVImageBuffer:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547028-imagewithcvimagebuffer",
    "html": "Parameters\nimageBuffer\n\nA CVImageBuffer object. For more information, see Core Video Programming Guide and Core Video.\n\ndict\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nAn image object initialized with the contents of the image buffer object and set up with the specified options.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithCVImageBuffer:options:\nInitializes an image object from the contents of a Core Video image buffer, using the specified options."
  },
  {
    "title": "init(extent:format:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageaccumulator/1427718-init",
    "html": "Parameters\nextent\n\nA rectangle that specifies the x-value of the rectangle origin, the y-value of the rectangle origin, and the width and height.\n\nformat\n\nThe format and size of each pixel. You must supply a pixel format constant, such askCIFormatARGB8 (32 bit-per-pixel, fixed-point pixel format) or kCIFormatRGBAf (128 bit-per-pixel, floating-point pixel format). See CIImage for more information about pixel format constants.\n\nReturn Value\n\nThe initialized image accumulator object.\n\nSee Also\nInitializing an Image Accumulator\ninit?(extent: CGRect, format: CIFormat, colorSpace: CGColorSpace)\nInitializes an image accumulator with the specified extent, pixel format, and color space.\nRelated Documentation\n+ imageAccumulatorWithExtent:format:\nCreates an image accumulator with the specified extent and pixel format."
  },
  {
    "title": "image()",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageaccumulator/1427704-image",
    "html": "Return Value\n\nThe image object that represents the current contents of the image accumulator.\n\nSee Also\nObtaining Data From an Image Accumulator\nvar extent: CGRect\nThe extent of the image associated with the image accumulator.\nvar format: CIFormat\nThe pixel format of the image accumulator."
  },
  {
    "title": "x",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1437738-x",
    "html": "See Also\nGetting Values From a Vector\nfunc value(at: Int) -> CGFloat\nReturns a value from a specific position in the vector.\nvar count: Int\nThe number of items in the vector.\nvar y: CGFloat\nThe value located in the second position in the vector.\nvar z: CGFloat\nThe value located in the third position in the vector.\nvar w: CGFloat\nThe value located in the fourth position in the vector.\nvar stringRepresentation: String\nThe string representation of the vector.\nvar cgAffineTransformValue: CGAffineTransform\nThe values in the vector represented as an affine transform.\nvar cgPointValue: CGPoint\nThe values in the vector as a Core Graphics point structure.\nvar cgRectValue: CGRect\nThe values in the vector as a Core Graphics rectangle structure."
  },
  {
    "title": "init(cgPoint:)",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1438133-init",
    "html": "Parameters\np\n\nA point.\n\nDiscussion\n\nThe CGPoint structure’s X and Y values are stored in the vector’s X and Y properties.\n\nSee Also\nInitializing a Vector\ninit(values: UnsafePointer<CGFloat>, count: Int)\nInitializes a vector with the provided values.\ninit(x: CGFloat)\nInitializes the first position of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat)\nInitializes the first two positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat)\nInitializes the first three positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat, w: CGFloat)\nInitializes four positions of a vector with the provided values.\ninit(string: String)\nInitializes a vector with values provided in a string representation.\ninit(cgAffineTransform: CGAffineTransform)\nInitializes a vector that is initialized with values provided by a CGAffineTransform structure.\ninit(cgRect: CGRect)\nInitializes a vector that is initialized with values provided by a CGRect structure."
  },
  {
    "title": "imageWithTexture:size:flipped:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547000-imagewithtexture",
    "html": "Parameters\nname\n\nAn OpenGL texture. Because CIImage objects are immutable, the texture must remain unchanged for the life of the image object. See the discussion for more information.\n\nsize\n\nThe dimensions of the texture.\n\nflag\n\nYES to have Core Image flip the coordinates of the texture vertically to convert between OpenGL and Core Image coordinate systems.\n\noptions\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nAn image object initialized with the texture data.\n\nDiscussion\n\nWhen using a texture to create a CIImage object, the texture must be valid in the Core Image context (CIContext) that you draw the CIImage object into. This means that one of the following must be true:\n\nThe texture must be created using the CGLContext object that the Core Image context is based on.\n\nThe context that the texture was created in must be shared with the CGLContext that the Core Image context is based on.\n\nNote that textures do not have a retain and release mechanism. This means that your application must make sure that the texture exists for the life cycle of the image. When you no longer need the image, you can delete the texture.\n\nCore Image ignores the texture filtering and wrap modes (GL_TEXTURE_FILTER and GL_TEXTURE_WRAP) that you set through OpenGL. The filter and wrap modes are overridden by what the CISampler object specifies when you apply a filter to the CIImage object.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithTexture:size:flipped:options:\nInitializes an image object with data supplied by an OpenGL texture."
  },
  {
    "title": "init(x:y:)",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1437865-init",
    "html": "Parameters\nx\n\nThe initialization value for the first position.\n\ny\n\nThe initialization value for the second position.\n\nSee Also\nInitializing a Vector\ninit(values: UnsafePointer<CGFloat>, count: Int)\nInitializes a vector with the provided values.\ninit(x: CGFloat)\nInitializes the first position of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat)\nInitializes the first three positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat, w: CGFloat)\nInitializes four positions of a vector with the provided values.\ninit(string: String)\nInitializes a vector with values provided in a string representation.\ninit(cgAffineTransform: CGAffineTransform)\nInitializes a vector that is initialized with values provided by a CGAffineTransform structure.\ninit(cgPoint: CGPoint)\nInitializes a vector that is initialized with values provided by a CGPoint structure.\ninit(cgRect: CGRect)\nInitializes a vector that is initialized with values provided by a CGRect structure."
  },
  {
    "title": "init(cgAffineTransform:)",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1438102-init",
    "html": "Parameters\nr\n\nA transform.\n\nDiscussion\n\nThe six values that comprise the affine transform fill the first six positions of the resulting CIVector object.\n\nSee Also\nInitializing a Vector\ninit(values: UnsafePointer<CGFloat>, count: Int)\nInitializes a vector with the provided values.\ninit(x: CGFloat)\nInitializes the first position of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat)\nInitializes the first two positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat)\nInitializes the first three positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat, w: CGFloat)\nInitializes four positions of a vector with the provided values.\ninit(string: String)\nInitializes a vector with values provided in a string representation.\ninit(cgPoint: CGPoint)\nInitializes a vector that is initialized with values provided by a CGPoint structure.\ninit(cgRect: CGRect)\nInitializes a vector that is initialized with values provided by a CGRect structure."
  },
  {
    "title": "init(x:y:z:w:)",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1438088-init",
    "html": "Parameters\nx\n\nThe initialization value for the first position.\n\ny\n\nThe initialization value for the second position.\n\nz\n\nThe initialization value for the third position.\n\nw\n\nThe initialization value for the fourth position.\n\nSee Also\nInitializing a Vector\ninit(values: UnsafePointer<CGFloat>, count: Int)\nInitializes a vector with the provided values.\ninit(x: CGFloat)\nInitializes the first position of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat)\nInitializes the first two positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat)\nInitializes the first three positions of a vector with the provided values.\ninit(string: String)\nInitializes a vector with values provided in a string representation.\ninit(cgAffineTransform: CGAffineTransform)\nInitializes a vector that is initialized with values provided by a CGAffineTransform structure.\ninit(cgPoint: CGPoint)\nInitializes a vector that is initialized with values provided by a CGPoint structure.\ninit(cgRect: CGRect)\nInitializes a vector that is initialized with values provided by a CGRect structure."
  },
  {
    "title": "init(x:y:z:)",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1438056-init",
    "html": "Parameters\nx\n\nThe initialization value for the first position.\n\ny\n\nThe initialization value for the second position.\n\nz\n\nThe initialization value for the third position.\n\nSee Also\nInitializing a Vector\ninit(values: UnsafePointer<CGFloat>, count: Int)\nInitializes a vector with the provided values.\ninit(x: CGFloat)\nInitializes the first position of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat)\nInitializes the first two positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat, w: CGFloat)\nInitializes four positions of a vector with the provided values.\ninit(string: String)\nInitializes a vector with values provided in a string representation.\ninit(cgAffineTransform: CGAffineTransform)\nInitializes a vector that is initialized with values provided by a CGAffineTransform structure.\ninit(cgPoint: CGPoint)\nInitializes a vector that is initialized with values provided by a CGPoint structure.\ninit(cgRect: CGRect)\nInitializes a vector that is initialized with values provided by a CGRect structure."
  },
  {
    "title": "init(x:)",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1437657-init",
    "html": "Parameters\nx\n\nThe initialization value.\n\nSee Also\nInitializing a Vector\ninit(values: UnsafePointer<CGFloat>, count: Int)\nInitializes a vector with the provided values.\ninit(x: CGFloat, y: CGFloat)\nInitializes the first two positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat)\nInitializes the first three positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat, w: CGFloat)\nInitializes four positions of a vector with the provided values.\ninit(string: String)\nInitializes a vector with values provided in a string representation.\ninit(cgAffineTransform: CGAffineTransform)\nInitializes a vector that is initialized with values provided by a CGAffineTransform structure.\ninit(cgPoint: CGPoint)\nInitializes a vector that is initialized with values provided by a CGPoint structure.\ninit(cgRect: CGRect)\nInitializes a vector that is initialized with values provided by a CGRect structure."
  },
  {
    "title": "init(string:)",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1437938-init",
    "html": "Parameters\nrepresentation\n\nA string that is in one of the formats returned by the stringRepresentation method.\n\nSee Also\nInitializing a Vector\ninit(values: UnsafePointer<CGFloat>, count: Int)\nInitializes a vector with the provided values.\ninit(x: CGFloat)\nInitializes the first position of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat)\nInitializes the first two positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat)\nInitializes the first three positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat, w: CGFloat)\nInitializes four positions of a vector with the provided values.\ninit(cgAffineTransform: CGAffineTransform)\nInitializes a vector that is initialized with values provided by a CGAffineTransform structure.\ninit(cgPoint: CGPoint)\nInitializes a vector that is initialized with values provided by a CGPoint structure.\ninit(cgRect: CGRect)\nInitializes a vector that is initialized with values provided by a CGRect structure.\nRelated Documentation\nvar stringRepresentation: String\nThe string representation of the vector."
  },
  {
    "title": "init(cgRect:)",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1437644-init",
    "html": "Parameters\nr\n\nA rect.\n\nDiscussion\n\nThe CGRect structure’s X, Y, height and width values are stored in the vector’s X, Y, Z and W properties.\n\nSee Also\nInitializing a Vector\ninit(values: UnsafePointer<CGFloat>, count: Int)\nInitializes a vector with the provided values.\ninit(x: CGFloat)\nInitializes the first position of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat)\nInitializes the first two positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat)\nInitializes the first three positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat, w: CGFloat)\nInitializes four positions of a vector with the provided values.\ninit(string: String)\nInitializes a vector with values provided in a string representation.\ninit(cgAffineTransform: CGAffineTransform)\nInitializes a vector that is initialized with values provided by a CGAffineTransform structure.\ninit(cgPoint: CGPoint)\nInitializes a vector that is initialized with values provided by a CGPoint structure."
  },
  {
    "title": "value(at:)",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1438207-value",
    "html": "Parameters\nindex\n\nThe position in the vector of the value that you want to retrieve.\n\nReturn Value\n\nThe value retrieved from the vector or 0 if the position is undefined.\n\nDiscussion\n\nThe numbering of elements in a vector begins with zero.\n\nSee Also\nGetting Values From a Vector\nvar count: Int\nThe number of items in the vector.\nvar x: CGFloat\nThe value located in the first position in the vector.\nvar y: CGFloat\nThe value located in the second position in the vector.\nvar z: CGFloat\nThe value located in the third position in the vector.\nvar w: CGFloat\nThe value located in the fourth position in the vector.\nvar stringRepresentation: String\nThe string representation of the vector.\nvar cgAffineTransformValue: CGAffineTransform\nThe values in the vector represented as an affine transform.\nvar cgPointValue: CGPoint\nThe values in the vector as a Core Graphics point structure.\nvar cgRectValue: CGRect\nThe values in the vector as a Core Graphics rectangle structure."
  },
  {
    "title": "init(values:count:)",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1437849-init",
    "html": "Parameters\nvalues\n\nThe values to initialize the vector with.\n\ncount\n\nThe number of values specified by the values argument.\n\nSee Also\nInitializing a Vector\ninit(x: CGFloat)\nInitializes the first position of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat)\nInitializes the first two positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat)\nInitializes the first three positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat, w: CGFloat)\nInitializes four positions of a vector with the provided values.\ninit(string: String)\nInitializes a vector with values provided in a string representation.\ninit(cgAffineTransform: CGAffineTransform)\nInitializes a vector that is initialized with values provided by a CGAffineTransform structure.\ninit(cgPoint: CGPoint)\nInitializes a vector that is initialized with values provided by a CGPoint structure.\ninit(cgRect: CGRect)\nInitializes a vector that is initialized with values provided by a CGRect structure."
  },
  {
    "title": "count",
    "url": "https://developer.apple.com/documentation/coreimage/civector/1438197-count",
    "html": "See Also\nGetting Values From a Vector\nfunc value(at: Int) -> CGFloat\nReturns a value from a specific position in the vector.\nvar x: CGFloat\nThe value located in the first position in the vector.\nvar y: CGFloat\nThe value located in the second position in the vector.\nvar z: CGFloat\nThe value located in the third position in the vector.\nvar w: CGFloat\nThe value located in the fourth position in the vector.\nvar stringRepresentation: String\nThe string representation of the vector.\nvar cgAffineTransformValue: CGAffineTransform\nThe values in the vector represented as an affine transform.\nvar cgPointValue: CGPoint\nThe values in the vector as a Core Graphics point structure.\nvar cgRectValue: CGRect\nThe values in the vector as a Core Graphics rectangle structure."
  },
  {
    "title": "imageWithCGImage:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547025-imagewithcgimage",
    "html": "Parameters\nimage\n\nA Quartz 2D image (CGImageRef) object. For more information, see Quartz 2D Programming Guide and CGImage.\n\nReturn Value\n\nAn image object initialized with the contents of the Quartz 2D image.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithCGImage:\nInitializes an image object with a Quartz 2D image."
  },
  {
    "title": "imageWithColor:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547012-imagewithcolor",
    "html": "Parameters\ncolor\n\nA color object.\n\nReturn Value\n\nThe image object initialized with the color represented by the CIColor object.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithColor:\nInitializes an image of infinite extent whose entire content is the specified color."
  },
  {
    "title": "imageWithBitmapData:bytesPerRow:size:format:colorSpace:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547023-imagewithbitmapdata",
    "html": "Parameters\nd\n\nThe bitmap data for the image. This data must be premultiplied.\n\nbpr\n\nThe number of bytes per row.\n\nsize\n\nThe dimensions of the image.\n\nf\n\nThe format and size of each pixel. You must supply a pixel format constant. See Pixel Formats.\n\ncs\n\nThe color space that the image is defined in. If this value is nil, the image is not color matched. Pass nil for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables).\n\nReturn Value\n\nAn image object.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithBitmapData:bytesPerRow:size:format:colorSpace:\nInitializes an image object with bitmap data."
  },
  {
    "title": "photoEffectChromeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228384-photoeffectchromefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a preconfigured set of effects that imitate vintage photography film with higher contrast.\n\nThe photo effect chrome filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in diminished color in the input image:\n\nfunc photoEffectChrome(inputImage: CIImage ) -> CIImage {\n    let photoEffect = CIFilter.photoEffectChrome()\n    photoEffect.inputImage = inputImage\n    return photoEffect.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "photoEffectTonalFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228390-photoeffecttonalfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a preconfigured set of effects that imitate black-and-white photography film without significantly altering contrast.\n\nThe photo effect tonal filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that produces a grayscale image:\n\nfunc photoEffectTonal(inputImage: CIImage ) -> CIImage {\n    let photoEffect = CIFilter.photoEffectTonal()\n    photoEffect.inputImage = inputImage\n    return photoEffect.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "imageWithTexture:size:flipped:colorSpace:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547006-imagewithtexture",
    "html": "Parameters\nname\n\nAn OpenGL texture. Because CIImage objects are immutable, the texture must remain unchanged for the life of the image object. See the discussion for more information.\n\nsize\n\nThe dimensions of the texture.\n\nflag\n\nYES to have Core Image flip the coordinates of the texture vertically to convert between OpenGL and Core Image coordinate systems.\n\ncs\n\nThe color space that the image is defined in. If the colorSpace value is nil, the image is not color matched. Pass nil for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables).\n\nReturn Value\n\nAn image object initialized with the texture data.\n\nDiscussion\n\nWhen using a texture to create a CIImage object, the texture must be valid in the Core Image context (CIContext) that you draw the CIImage object into. This means that one of the following must be true:\n\nThe texture must be created using the CGLContext object that the Core Image context is based on.\n\nThe context that the texture was created in must be shared with the CGLContext that the Core Image context is based on.\n\nNote that textures do not have a retain and release mechanism. This means that your application must make sure that the texture exists for the life cycle of the image. When you no longer need the image, you can delete the texture.\n\nCore Image ignores the texture filtering and wrap modes (GL_TEXTURE_FILTER and GL_TEXTURE_WRAP) that you set through OpenGL. The filter and wrap modes are overridden by what the CISampler object specifies when you apply a filter to the CIImage object.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithTexture:size:flipped:colorSpace:\nInitializes an image object with data supplied by an OpenGL texture."
  },
  {
    "title": "imageWithContentsOfURL:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1546997-imagewithcontentsofurl",
    "html": "Parameters\nurl\n\nThe location of the file.\n\nd\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nAn image object initialized with the contents of the file and set up with the specified options.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithContentsOfURL:options:\nInitializes an image object by reading an image from a URL, using the specified options."
  },
  {
    "title": "imageWithCVImageBuffer:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547007-imagewithcvimagebuffer",
    "html": "Parameters\nimageBuffer\n\nA CVImageBuffer object. For more information, see Core Video Programming Guide and Core Video.\n\nReturn Value\n\nAn image object initialized with the contents of the image buffer object.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithCVImageBuffer:\nInitializes an image object from the contents of a Core Video image buffer."
  },
  {
    "title": "imageWithContentsOfURL:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1547027-imagewithcontentsofurl",
    "html": "Parameters\nurl\n\nThe location of the file.\n\nReturn Value\n\nAn image object initialized with the contents of the file.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithContentsOfURL:\nInitializes an image object by reading an image from a URL."
  },
  {
    "title": "imageWithCGLayer:options:",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1546998-imagewithcglayer",
    "html": "Parameters\nlayer\n\nA CGLayer object. For more information see Quartz 2D Programming Guide and CGLayer.\n\nd\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nAn image object initialized with the contents of the layer object and set up with the specified options.\n\nSee Also\nCreating an Image\n+ emptyImage\nCreates and returns an empty image object.\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color.\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data.\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image.\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options.\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\nDeprecated\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file.\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options.\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object.\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options.\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object.\n+ imageWithCVPixelBuffer:options:\nCreates and returns an image object from the contents of CVPixelBuffer object, using the specified options.\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data.\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options.\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider.\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture.\nDeprecated\n+ imageWithMTLTexture:options:\nCreates and returns an image object with data supplied by a Metal texture.\n+ imageWithIOSurface:\nCreates and returns an image from the contents of an IOSurface.\n+ imageWithIOSurface:options:\nCreates, using the specified options, and returns an image from the contents of an IOSurface.\nRelated Documentation\n- initWithCGLayer:options:\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options."
  },
  {
    "title": "init(payload:symbolVersion:maskPattern:errorCorrectionLevel:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodedescriptor/2875180-init",
    "html": "Parameters\nerrorCorrectedPayload\n\nThe data to encode in the QR code.\n\nsymbolVersion\n\nThe symbol version, from 1 through 40.\n\nmaskPattern\n\nThe mask pattern to use in the QR code.\n\nerrorCorrectionLevel\n\nThe QR code's error correction level: L, M, Q, or H.\n\nReturn Value\n\nA QR code descriptor encoding the specified data at the specified error correction level.\n\nDiscussion\n\nThe CIBarcodeGenerator filter can recreate a QR code given the descriptor created using this method."
  },
  {
    "title": "errorCorrectedPayload",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodedescriptor/2875167-errorcorrectedpayload",
    "html": "See Also\nExamining a Descriptor\nvar symbolVersion: Int\nThe version of the QR code.\nvar maskPattern: UInt8\nThe QR code's mask pattern.\nvar errorCorrectionLevel: CIQRCodeDescriptor.ErrorCorrectionLevel\nThe QR code error correction level."
  },
  {
    "title": "errorCorrectionLevel",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodedescriptor/2875196-errorcorrectionlevel",
    "html": "Discussion\n\nThe possible error correction levels of CIQRCodeDescriptor.ErrorCorrectionLevel are enumerated as follows:\n\nCIQRCodeDescriptor.ErrorCorrectionLevel.levelL = 'L'\n\nCIQRCodeDescriptor.ErrorCorrectionLevel.levelM = 'M'\n\nCIQRCodeDescriptor.ErrorCorrectionLevel.levelQ = 'Q'\n\nCIQRCodeDescriptor.ErrorCorrectionLevel.levelH = 'H'\n\nSee Also\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload containing the data encoded in the QR code.\nvar symbolVersion: Int\nThe version of the QR code.\nvar maskPattern: UInt8\nThe QR code's mask pattern."
  },
  {
    "title": "CIQRCodeDescriptor.ErrorCorrectionLevel",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodedescriptor/errorcorrectionlevel",
    "html": "Topics\nEnumeration Cases\ncase levelL\nLow-percentage error correction: 20% of the symbol data is dedicated to error correction.\ncase levelM\nMedium-percentage error correction: 37% of the symbol data is dedicated to error correction.\ncase levelQ\nHigh-percentage error correction: 55% of the symbol data is dedicated to error correction.\ncase levelH\nVery-high-percentage error correction: 65% of the symbol data is dedicated to error correction.\nRelationships\nConforms To\nSendable"
  },
  {
    "title": "init(extent:format:colorSpace:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageaccumulator/1427710-init",
    "html": "Parameters\nextent\n\nA rectangle that specifies the x-value of the rectangle origin, the y-value of the rectangle origin, and the width and height.\n\nformat\n\nThe format and size of each pixel. You must supply a pixel format constant, such askCIFormatARGB8 (32 bit-per-pixel, fixed-point pixel format) or kCIFormatRGBAf (128 bit-per-pixel, floating-point pixel format). See CIImage for more information about pixel format constants.\n\ncolorSpace\n\nA CGColorSpace object describing the color space for the image accumulator.\n\nReturn Value\n\nThe initialized image accumulator object.\n\nSee Also\nInitializing an Image Accumulator\ninit?(extent: CGRect, format: CIFormat)\nInitializes an image accumulator with the specified extent and pixel format.\nRelated Documentation\n+ imageAccumulatorWithExtent:format:\nCreates an image accumulator with the specified extent and pixel format."
  },
  {
    "title": "setImage(_:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageaccumulator/1427702-setimage",
    "html": "Parameters\nim\n\nThe image object whose contents you want to assign to the image accumulator.\n\nSee Also\nSetting an Image\nfunc setImage(CIImage, dirtyRect: CGRect)\nUpdates an image accumulator with a subregion of an image object."
  },
  {
    "title": "setImage(_:dirtyRect:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageaccumulator/1427706-setimage",
    "html": "Parameters\nim\n\nThe image object whose contents you want to assign to the image accumulator.\n\nr\n\nA rectangle that defines the subregion of the image object that’s changed since the last time you updated the image accumulator. You must guarantee that the new contents differ from the old only within the region specified by the this argument.\n\nDiscussion\n\nFor additional details on using this method, see “Imaging Dynamical Systems” in Core Image Programming Guide.\n\nSee Also\nSetting an Image\nfunc setImage(CIImage)\nSets the contents of the image accumulator to the contents of the specified image object."
  },
  {
    "title": "format",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageaccumulator/1427716-format",
    "html": "Discussion\n\nFor applicable values, see Pixel Formats.\n\nSee Also\nObtaining Data From an Image Accumulator\nvar extent: CGRect\nThe extent of the image associated with the image accumulator.\nfunc image() -> CIImage\nReturns the current contents of the image accumulator."
  },
  {
    "title": "xRayFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228433-xrayfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a preconfigured set of effects that imitate a photo being scanned by an X-ray.\n\nThe X-ray filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds an X-ray effect to the input image:\n\nfunc xRay(inputImage: CIImage ) -> CIImage {\n    let xRayFilter = CIFilter.xRay()\n    xRayFilter.inputImage = inputImage\n    return xRayFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image."
  },
  {
    "title": "vignetteFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228431-vignettefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the vignette filter to an image. This is a preconfigured effect that reduces brightness of the image at the periphery.\n\nThe vignette filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nintensity\n\nA float representing the intensity of the vignette effect as an NSNumber.\n\nradius\n\nA float representing the radius of the effect as an NSNumber.\n\nThe following code creates a filter that darkens the edges of the input image:\n\nfunc vignette(inputImage: CIImage ) -> CIImage {\n    let vignetteFilter = CIFilter.vignette()\n    vignetteFilter.inputImage = inputImage\n    vignetteFilter.intensity = 4\n    vignetteFilter.radius = 10\n    return vignetteFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "moireAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilteroption/2880265-moireamount",
    "html": "Discussion\n\nThe range of valid moiré reduction is 0.0 to 1.0."
  },
  {
    "title": "thermalFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228423-thermalfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a preconfigured set of effects that make it appear like a thermal camera captured the image.\n\nThe thermal filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds a thermal effect to the input image:\n\nfunc thermal(inputImage: CIImage ) -> CIImage {\n    let thermalFilter = CIFilter.thermal()\n    thermalFilter.inputImage = inputImage\n    return thermalFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "photoEffectTransferFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228391-photoeffecttransferfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a preconfigured set of effects that imitate vintage photography film and emphasize warm colors.\n\nThe photo effect transfer filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds brightness to the input image:\n\nfunc photoEffectTransfer(inputImage: CIImage ) -> CIImage {\n    let photoEffect = CIFilter.photoEffectTransfer()\n    photoEffect.inputImage = inputImage\n    return photoEffect.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "vignetteEffectFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228430-vignetteeffectfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the vignette effect filter to an image. This effect reduces brightness of the image at the periphery of a specified region.\n\nThe vignette effect filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nintensity\n\nA float representing the intensity of the vignette effect as an NSNumber.\n\nradius\n\nA float representing the radius of the effect as an NSNumber.\n\nfalloff\n\nA float representing the fall off of brightness toward the edge of the image as an NSNumber.\n\ncenter\n\nA CGPoint representing the center of the image.\n\nThe following code creates a filter that darkens the edges of an area on the input image:\n\nfunc vignetteEffect(inputImage: CIImage ) -> CIImage {\n    let vignetteFilter = CIFilter.vignetteEffect()\n    vignetteFilter.inputImage = inputImage\n    vignetteFilter.intensity = 1\n    vignetteFilter.radius = 650\n    vignetteFilter.falloff = 0.5\n    vignetteFilter.center = CGPoint(x: 1024, y: 768)\n    return vignetteFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "contrastAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801620-contrastamount",
    "html": "Discussion\n\nThe value should be in the range of 0...1. The default value varies by image. A value of 0 indicates no contrast, and a value of 1 indicates maximum contrast.\n\nNote\n\nA value of false for isContrastSupported indicates that the current image doesn’t support this adjustment.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "CIColorCurves",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorcurves",
    "html": "Topics\nInstance Properties\ncolorSpace\nThe working color space.\n\nRequired\n\ncurvesData\nColor values that determine the color curves transform.\n\nRequired\n\ncurvesDomain\nA two-element vector that defines the minimum and maximum values of the curve data.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ colorCurvesFilter\nAdjusts an image’s color curves."
  },
  {
    "title": "disableGamutMap",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilteroption/2202264-disablegamutmap",
    "html": "Discussion\n\nA value of true disables gamut mapping. The default is false."
  },
  {
    "title": "layerCount",
    "url": "https://developer.apple.com/documentation/coreimage/ciazteccodedescriptor/2875174-layercount",
    "html": "See Also\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload containing the data encoded in the Aztec code.\nvar isCompact: Bool\nA Boolean value telling if the Aztec code is compact.\nvar dataCodewordCount: Int\nThe number of data codewords in the Aztec code."
  },
  {
    "title": "isMoireReductionSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801640-ismoirereductionsupported",
    "html": "Discussion\n\nIf this value is true, you can adjust the amount of moire artifact reduction to apply to the image by setting moireReductionAmount.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nclass var supportedCameraModels: [String]\nAn array containing the names of all supported camera models.\nvar supportedDecoderVersions: [CIRAWDecoderVersion]\nAn array of all supported decoder versions for the given image type.\nvar isColorNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports color noise reduction adjustments.\nvar isContrastSupported: Bool\nA Boolean that indicates if the current image supports contrast adjustments.\nvar isDetailSupported: Bool\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nvar isLensCorrectionSupported: Bool\nA Boolean that indicates if you can enable lens correction for the current image.\nvar isLocalToneMapSupported: Bool\nA Boolean that indicates if the current image supports local tone curve adjustments.\nvar isLuminanceNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nvar isSharpnessSupported: Bool\nA Boolean that indicates if the current image supports sharpness adjustments.\nvar nativeSize: CGSize\nThe full native size of the unscaled image."
  },
  {
    "title": "CIVignette",
    "url": "https://developer.apple.com/documentation/coreimage/civignette",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nintensity\nThe intensity of the effect.\n\nRequired\n\nradius\nThe distance from the center of the effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ vignetteFilter\nGradually darkens an image’s edges."
  },
  {
    "title": "photoEffectInstantFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228386-photoeffectinstantfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a preconfigured set of effects that imitate vintage photography film with desaturated colors.\n\nThe photo effect instant filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the input image becoming desaturated:\n\nfunc photoEffectInstant(inputImage: CIImage ) -> CIImage {\n    let photoEffect = CIFilter.photoEffectInstant()\n    photoEffect.inputImage = inputImage\n    return photoEffect.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "black",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1643578-black",
    "html": "Return Value\n\nThe color object in the sRGB colorspace.\n\nSee Also\nCreating a CIColor Object with Preset Components\nclass var blue: CIColor\nReturns a color object whose RGB values are 0.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var clear: CIColor\nReturns a color object whose RGB and alpha values are all 0.0.\nclass var cyan: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 1.0 and whose alpha value is 1.0.\nclass var gray: CIColor\nReturns a color object whose RGB values are all 0.5 and whose alpha value is 1.0.\nclass var green: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 0.0 and whose alpha value is 1.0.\nclass var magenta: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var red: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 0.0 and whose alpha value is 1.0.\nclass var white: CIColor\nReturns a color object whose RGB values are all 1.0 and whose alpha value is 1.0.\nclass var yellow: CIColor\nReturns a color object whose RGB values are 1.0, 1.0, and 0.0 and whose alpha value is 1.0."
  },
  {
    "title": "green",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1437607-green",
    "html": "See Also\nGetting Color Components\nvar colorSpace: CGColorSpace\nThe Quartz 2D color space associated with the color.\nvar components: UnsafePointer<CGFloat>\nThe color components of the color.\nvar numberOfComponents: Int\nReturns the number of color components in the color.\nvar red: CGFloat\nThe unpremultiplied red component of the color.\nvar blue: CGFloat\nThe unpremultiplied blue component of the color.\nvar alpha: CGFloat\nThe alpha value of the color.\nvar stringRepresentation: String\nA formatted string that specifies the components of the color."
  },
  {
    "title": "CIColorMonochrome",
    "url": "https://developer.apple.com/documentation/coreimage/cicolormonochrome",
    "html": "Topics\nInstance Properties\ncolor\nThe monochrome color to apply to the image.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nintensity\nThe intensity of the monochrome effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color."
  },
  {
    "title": "CIColorMap",
    "url": "https://developer.apple.com/documentation/coreimage/cicolormap",
    "html": "Topics\nInstance Properties\ngradientImage\nThe image data that transforms the source image values.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image."
  },
  {
    "title": "CIColorInvert",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorinvert",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ colorInvertFilter\nInverts an image’s colors."
  },
  {
    "title": "CIConvertLab",
    "url": "https://developer.apple.com/documentation/coreimage/ciconvertlab",
    "html": "Topics\nInstance Properties\ninputImage\n\nRequired\n\nnormalize\n\nRequired\n\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "CIDither",
    "url": "https://developer.apple.com/documentation/coreimage/cidither",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nintensity\nThe intensity of the effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ ditherFilter\nApplies randomized noise to produce a processed look."
  },
  {
    "title": "CIColorPosterize",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorposterize",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nlevels\nThe number of brightness levels to use for each color component.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ colorPosterizeFilter\nFlattens an image’s colors."
  },
  {
    "title": "CIDocumentEnhancer",
    "url": "https://developer.apple.com/documentation/coreimage/cidocumentenhancer",
    "html": "Topics\nInstance Properties\namount\nThe amount of enhancement.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast."
  },
  {
    "title": "CILabDeltaE",
    "url": "https://developer.apple.com/documentation/coreimage/cilabdeltae",
    "html": "Topics\nInstance Properties\nimage2\nThe second input image for comparison.\n\nRequired\n\ninputImage\nThe first input image for comparison.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ LabDeltaE\nCompares an image’s color values."
  },
  {
    "title": "color0",
    "url": "https://developer.apple.com/documentation/coreimage/cifalsecolor/3228256-color0",
    "html": "Required"
  },
  {
    "title": "CIXRay",
    "url": "https://developer.apple.com/documentation/coreimage/cixray",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "photoEffectFadeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228385-photoeffectfadefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a preconfigured set of effects that imitate vintage photography film with diminished color.\n\nThe photo effect fade filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in a desaturated image:\n\nfunc photoEffectFade(inputImage: CIImage ) -> CIImage {\n    let photoEffect = CIFilter.photoEffectFade()\n    photoEffect.inputImage = inputImage\n    return photoEffect.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "photoEffectMonoFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228387-photoeffectmonofilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a preconfigured set of effects that imitate black-and-white photography film with low contrast.\n\nThe photo effect mono filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that produces a black-and-white image:\n\nfunc photoEffectMono(inputImage: CIImage ) -> CIImage {\n    let photoEffect = CIFilter.photoEffectMono()\n    photoEffect.inputImage = inputImage\n    return photoEffect.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "green",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1643580-green",
    "html": "Return Value\n\nThe color object in the sRGB colorspace.\n\nSee Also\nCreating a CIColor Object with Preset Components\nclass var black: CIColor\nReturns a color object whose RGB values are all 0.0 and whose alpha value is 1.0.\nclass var blue: CIColor\nReturns a color object whose RGB values are 0.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var clear: CIColor\nReturns a color object whose RGB and alpha values are all 0.0.\nclass var cyan: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 1.0 and whose alpha value is 1.0.\nclass var gray: CIColor\nReturns a color object whose RGB values are all 0.5 and whose alpha value is 1.0.\nclass var magenta: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var red: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 0.0 and whose alpha value is 1.0.\nclass var white: CIColor\nReturns a color object whose RGB values are all 1.0 and whose alpha value is 1.0.\nclass var yellow: CIColor\nReturns a color object whose RGB values are 1.0, 1.0, and 0.0 and whose alpha value is 1.0."
  },
  {
    "title": "yellow",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1643582-yellow",
    "html": "Return Value\n\nThe color object in the sRGB colorspace.\n\nSee Also\nCreating a CIColor Object with Preset Components\nclass var black: CIColor\nReturns a color object whose RGB values are all 0.0 and whose alpha value is 1.0.\nclass var blue: CIColor\nReturns a color object whose RGB values are 0.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var clear: CIColor\nReturns a color object whose RGB and alpha values are all 0.0.\nclass var cyan: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 1.0 and whose alpha value is 1.0.\nclass var gray: CIColor\nReturns a color object whose RGB values are all 0.5 and whose alpha value is 1.0.\nclass var green: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 0.0 and whose alpha value is 1.0.\nclass var magenta: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var red: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 0.0 and whose alpha value is 1.0.\nclass var white: CIColor\nReturns a color object whose RGB values are all 1.0 and whose alpha value is 1.0."
  },
  {
    "title": "magenta",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1643574-magenta",
    "html": "Return Value\n\nThe color object in the sRGB colorspace.\n\nSee Also\nCreating a CIColor Object with Preset Components\nclass var black: CIColor\nReturns a color object whose RGB values are all 0.0 and whose alpha value is 1.0.\nclass var blue: CIColor\nReturns a color object whose RGB values are 0.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var clear: CIColor\nReturns a color object whose RGB and alpha values are all 0.0.\nclass var cyan: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 1.0 and whose alpha value is 1.0.\nclass var gray: CIColor\nReturns a color object whose RGB values are all 0.5 and whose alpha value is 1.0.\nclass var green: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 0.0 and whose alpha value is 1.0.\nclass var red: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 0.0 and whose alpha value is 1.0.\nclass var white: CIColor\nReturns a color object whose RGB values are all 1.0 and whose alpha value is 1.0.\nclass var yellow: CIColor\nReturns a color object whose RGB values are 1.0, 1.0, and 0.0 and whose alpha value is 1.0."
  },
  {
    "title": "red",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1643570-red",
    "html": "Return Value\n\nThe color object in the sRGB colorspace.\n\nSee Also\nCreating a CIColor Object with Preset Components\nclass var black: CIColor\nReturns a color object whose RGB values are all 0.0 and whose alpha value is 1.0.\nclass var blue: CIColor\nReturns a color object whose RGB values are 0.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var clear: CIColor\nReturns a color object whose RGB and alpha values are all 0.0.\nclass var cyan: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 1.0 and whose alpha value is 1.0.\nclass var gray: CIColor\nReturns a color object whose RGB values are all 0.5 and whose alpha value is 1.0.\nclass var green: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 0.0 and whose alpha value is 1.0.\nclass var magenta: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var white: CIColor\nReturns a color object whose RGB values are all 1.0 and whose alpha value is 1.0.\nclass var yellow: CIColor\nReturns a color object whose RGB values are 1.0, 1.0, and 0.0 and whose alpha value is 1.0."
  },
  {
    "title": "gray",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1643573-gray",
    "html": "Return Value\n\nThe color object in the sRGB colorspace.\n\nSee Also\nCreating a CIColor Object with Preset Components\nclass var black: CIColor\nReturns a color object whose RGB values are all 0.0 and whose alpha value is 1.0.\nclass var blue: CIColor\nReturns a color object whose RGB values are 0.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var clear: CIColor\nReturns a color object whose RGB and alpha values are all 0.0.\nclass var cyan: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 1.0 and whose alpha value is 1.0.\nclass var green: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 0.0 and whose alpha value is 1.0.\nclass var magenta: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var red: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 0.0 and whose alpha value is 1.0.\nclass var white: CIColor\nReturns a color object whose RGB values are all 1.0 and whose alpha value is 1.0.\nclass var yellow: CIColor\nReturns a color object whose RGB values are 1.0, 1.0, and 0.0 and whose alpha value is 1.0."
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageaccumulator/1427714-extent",
    "html": "Discussion\n\nExtent is a rectangle that specifies the size of the image associated with the image accumulator. This rectangle is the size of the complete region of the working coordinate space, and is a fixed area. It specifies the x-value of the rectangle origin, the y-value of the rectangle origin, and the width and height.\n\nSee Also\nObtaining Data From an Image Accumulator\nvar format: CIFormat\nThe pixel format of the image accumulator.\nfunc image() -> CIImage\nReturns the current contents of the image accumulator."
  },
  {
    "title": "exportedKeys",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1437955-exportedkeys",
    "html": "Return Value\n\nAn array of dictionaries that describe the exported key and target object. See kCIFilterGeneratorExportedKey, kCIFilterGeneratorExportedKeyTargetObject, and kCIFilterGeneratorExportedKey for keys used in the dictionary.\n\nDiscussion\n\nThis method returns the keys that you exported using the exportKey(_:from:withName:) method or that were exported before being written to the file from which you read the filter chain.\n\nSee Also\nManaging Exported Keys\nfunc exportKey(String, from: Any, withName: String?)\nExports an input or output key of an object in the filter chain.\nfunc removeExportedKey(String)\nRemoves a key that was previously exported.\nfunc setAttributes([AnyHashable : Any], forExportedKey: String)\nSets a dictionary of attributes for an exported key."
  },
  {
    "title": "classAttributes",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1437855-classattributes",
    "html": "Discussion\n\nFor more information about class attributes for a filter, see Core Image Programming Guide and the filter attributes key constants defined in CIFilter."
  },
  {
    "title": "errorCorrectedPayload",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417codedescriptor/2875204-errorcorrectedpayload",
    "html": "See Also\nExamining a Descriptor\nvar isCompact: Bool\nA boolean value telling if the PDF417 code is compact.\nvar rowCount: Int\nThe number of rows in the PDF417 code.\nvar columnCount: Int\nThe number of columns in the PDF417 code."
  },
  {
    "title": "columnCount",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417codedescriptor/2875171-columncount",
    "html": "See Also\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload containing the data encoded in the PDF417 code.\nvar isCompact: Bool\nA boolean value telling if the PDF417 code is compact.\nvar rowCount: Int\nThe number of rows in the PDF417 code."
  },
  {
    "title": "init(cvPixelBuffer:properties:)",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801628-init",
    "html": "Parameters\nbuffer\n\nA Core Video pixel buffer.\n\nproperties\n\nA dictionary that defines the properties of the pixel buffer.\n\nSee Also\nCreating a filter\ninit?(imageData: Data, identifierHint: String?)\nCreates a RAW filter from the image data and type hint that you specify.\ninit?(imageURL: URL)\nCreates a RAW filter from the image at the URL location that you specify."
  },
  {
    "title": "CIPageCurlWithShadowTransition",
    "url": "https://developer.apple.com/documentation/coreimage/cipagecurlwithshadowtransition",
    "html": "Topics\nInstance Properties\nangle\nThe angle of the curling page.\n\nRequired\n\nbacksideImage\nThe image that appears on the back of the source image as the page curls to reveal the target image.\n\nRequired\n\nextent\nThe extent of the effect.\n\nRequired\n\nradius\nThe radius of the curl.\n\nRequired\n\nshadowAmount\nThe strength of the shadow.\n\nRequired\n\nshadowExtent\nThe rectagular portion of input image that casts a shadow.\n\nRequired\n\nshadowSize\nThe maximum size, in pixels, of the shadow.\n\nRequired\n\nRelationships\nInherits From\nCITransitionFilter\nSee Also\nRelated Documentation\n+ pageCurlWithShadowTransitionFilter\nSimulates the curl of a page, revealing the target image with added shadow."
  },
  {
    "title": "blue",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1643569-blue",
    "html": "Return Value\n\nThe color object in the sRGB colorspace.\n\nSee Also\nCreating a CIColor Object with Preset Components\nclass var black: CIColor\nReturns a color object whose RGB values are all 0.0 and whose alpha value is 1.0.\nclass var clear: CIColor\nReturns a color object whose RGB and alpha values are all 0.0.\nclass var cyan: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 1.0 and whose alpha value is 1.0.\nclass var gray: CIColor\nReturns a color object whose RGB values are all 0.5 and whose alpha value is 1.0.\nclass var green: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 0.0 and whose alpha value is 1.0.\nclass var magenta: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var red: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 0.0 and whose alpha value is 1.0.\nclass var white: CIColor\nReturns a color object whose RGB values are all 1.0 and whose alpha value is 1.0.\nclass var yellow: CIColor\nReturns a color object whose RGB values are 1.0, 1.0, and 0.0 and whose alpha value is 1.0."
  },
  {
    "title": "clear",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1643577-clear",
    "html": "Return Value\n\nThe color object in the sRGB colorspace.\n\nSee Also\nCreating a CIColor Object with Preset Components\nclass var black: CIColor\nReturns a color object whose RGB values are all 0.0 and whose alpha value is 1.0.\nclass var blue: CIColor\nReturns a color object whose RGB values are 0.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var cyan: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 1.0 and whose alpha value is 1.0.\nclass var gray: CIColor\nReturns a color object whose RGB values are all 0.5 and whose alpha value is 1.0.\nclass var green: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 0.0 and whose alpha value is 1.0.\nclass var magenta: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var red: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 0.0 and whose alpha value is 1.0.\nclass var white: CIColor\nReturns a color object whose RGB values are all 1.0 and whose alpha value is 1.0.\nclass var yellow: CIColor\nReturns a color object whose RGB values are 1.0, 1.0, and 0.0 and whose alpha value is 1.0."
  },
  {
    "title": "stringRepresentation",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1437910-stringrepresentation",
    "html": "Discussion\n\nThe string representation always has four components—red, green, blue, and alpha. The default value for the alpha component is 1.0. For example, this string:\n\n@\"0.5 0.7 0.3 1.0\"\n\nindicates an RGB color whose components are 50% red, 70% green, 30% blue, and 100% opaque (alpha value of 1.0).\n\nSee Also\nGetting Color Components\nvar colorSpace: CGColorSpace\nThe Quartz 2D color space associated with the color.\nvar components: UnsafePointer<CGFloat>\nThe color components of the color.\nvar numberOfComponents: Int\nReturns the number of color components in the color.\nvar red: CGFloat\nThe unpremultiplied red component of the color.\nvar green: CGFloat\nThe unpremultiplied green component of the color.\nvar blue: CGFloat\nThe unpremultiplied blue component of the color.\nvar alpha: CGFloat\nThe alpha value of the color."
  },
  {
    "title": "alpha",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1437981-alpha",
    "html": "Discussion\n\nA color created without an explicit alpha value has an alpha of 1.0 by default.\n\nSee Also\nGetting Color Components\nvar colorSpace: CGColorSpace\nThe Quartz 2D color space associated with the color.\nvar components: UnsafePointer<CGFloat>\nThe color components of the color.\nvar numberOfComponents: Int\nReturns the number of color components in the color.\nvar red: CGFloat\nThe unpremultiplied red component of the color.\nvar green: CGFloat\nThe unpremultiplied green component of the color.\nvar blue: CGFloat\nThe unpremultiplied blue component of the color.\nvar stringRepresentation: String\nA formatted string that specifies the components of the color."
  },
  {
    "title": "blue",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1438033-blue",
    "html": "See Also\nGetting Color Components\nvar colorSpace: CGColorSpace\nThe Quartz 2D color space associated with the color.\nvar components: UnsafePointer<CGFloat>\nThe color components of the color.\nvar numberOfComponents: Int\nReturns the number of color components in the color.\nvar red: CGFloat\nThe unpremultiplied red component of the color.\nvar green: CGFloat\nThe unpremultiplied green component of the color.\nvar alpha: CGFloat\nThe alpha value of the color.\nvar stringRepresentation: String\nA formatted string that specifies the components of the color."
  },
  {
    "title": "numberOfComponents",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1438151-numberofcomponents",
    "html": "Discussion\n\nThis number includes the alpha component if the color contains one.\n\nSee Also\nGetting Color Components\nvar colorSpace: CGColorSpace\nThe Quartz 2D color space associated with the color.\nvar components: UnsafePointer<CGFloat>\nThe color components of the color.\nvar red: CGFloat\nThe unpremultiplied red component of the color.\nvar green: CGFloat\nThe unpremultiplied green component of the color.\nvar blue: CGFloat\nThe unpremultiplied blue component of the color.\nvar alpha: CGFloat\nThe alpha value of the color.\nvar stringRepresentation: String\nA formatted string that specifies the components of the color."
  },
  {
    "title": "components",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1437862-components",
    "html": "Discussion\n\nThis property’s value is an array of color components, specified as floating-point values in the range of 0.0 through 1.0. This array includes an alpha component if the color contains one.\n\nSee Also\nGetting Color Components\nvar colorSpace: CGColorSpace\nThe Quartz 2D color space associated with the color.\nvar numberOfComponents: Int\nReturns the number of color components in the color.\nvar red: CGFloat\nThe unpremultiplied red component of the color.\nvar green: CGFloat\nThe unpremultiplied green component of the color.\nvar blue: CGFloat\nThe unpremultiplied blue component of the color.\nvar alpha: CGFloat\nThe alpha value of the color.\nvar stringRepresentation: String\nA formatted string that specifies the components of the color."
  },
  {
    "title": "semanticSegmentationGlassesMatte",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801651-semanticsegmentationglassesmatte",
    "html": "Discussion\n\nThis matting image segments eyeglasses and sunglasses from all people in the visible field of view of the image.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "sharpnessAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801657-sharpnessamount",
    "html": "Discussion\n\nThe value should be in the range of 0...1. The default value varies by image. A value of 0 indicates no sharpness, and a value of 1 indicates maximum sharpness.\n\nNote\n\nThe isSharpnessSupported property is false if the current image doesn’t support this adjustment.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image."
  },
  {
    "title": "semanticSegmentationSkinMatte",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801653-semanticsegmentationskinmatte",
    "html": "Discussion\n\nThis matting image segments skin from all people in the visible field of view of the image.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "orientation",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801646-orientation",
    "html": "Discussion\n\nThe value should be in the range of 1...8 and follow the EXIF specification.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "supportedCameraModels",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801659-supportedcameramodels",
    "html": "See Also\nInspecting supported camera models, decoders, and filters\nvar supportedDecoderVersions: [CIRAWDecoderVersion]\nAn array of all supported decoder versions for the given image type.\nvar isColorNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports color noise reduction adjustments.\nvar isContrastSupported: Bool\nA Boolean that indicates if the current image supports contrast adjustments.\nvar isDetailSupported: Bool\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nvar isLensCorrectionSupported: Bool\nA Boolean that indicates if you can enable lens correction for the current image.\nvar isLocalToneMapSupported: Bool\nA Boolean that indicates if the current image supports local tone curve adjustments.\nvar isLuminanceNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nvar isMoireReductionSupported: Bool\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nvar isSharpnessSupported: Bool\nA Boolean that indicates if the current image supports sharpness adjustments.\nvar nativeSize: CGSize\nThe full native size of the unscaled image."
  },
  {
    "title": "isLuminanceNoiseReductionSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801638-isluminancenoisereductionsupport",
    "html": "Discussion\n\nIf this value is true, you can adjust the amount of luminance noise reduction to apply to the image by setting luminanceNoiseReductionAmount.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nclass var supportedCameraModels: [String]\nAn array containing the names of all supported camera models.\nvar supportedDecoderVersions: [CIRAWDecoderVersion]\nAn array of all supported decoder versions for the given image type.\nvar isColorNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports color noise reduction adjustments.\nvar isContrastSupported: Bool\nA Boolean that indicates if the current image supports contrast adjustments.\nvar isDetailSupported: Bool\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nvar isLensCorrectionSupported: Bool\nA Boolean that indicates if you can enable lens correction for the current image.\nvar isLocalToneMapSupported: Bool\nA Boolean that indicates if the current image supports local tone curve adjustments.\nvar isMoireReductionSupported: Bool\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nvar isSharpnessSupported: Bool\nA Boolean that indicates if the current image supports sharpness adjustments.\nvar nativeSize: CGSize\nThe full native size of the unscaled image."
  },
  {
    "title": "supportedDecoderVersions",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801660-supporteddecoderversions",
    "html": "Discussion\n\nThis array is sorted in reverse chronological order. All entries represent a valid version identifier set to decoderVersion.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nclass var supportedCameraModels: [String]\nAn array containing the names of all supported camera models.\nvar isColorNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports color noise reduction adjustments.\nvar isContrastSupported: Bool\nA Boolean that indicates if the current image supports contrast adjustments.\nvar isDetailSupported: Bool\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nvar isLensCorrectionSupported: Bool\nA Boolean that indicates if you can enable lens correction for the current image.\nvar isLocalToneMapSupported: Bool\nA Boolean that indicates if the current image supports local tone curve adjustments.\nvar isLuminanceNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nvar isMoireReductionSupported: Bool\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nvar isSharpnessSupported: Bool\nA Boolean that indicates if the current image supports sharpness adjustments.\nvar nativeSize: CGSize\nThe full native size of the unscaled image."
  },
  {
    "title": "isLocalToneMapSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801636-islocaltonemapsupported",
    "html": "Discussion\n\nIf this value is true, you can adjust the amount of local tone curve to apply to the image by setting localToneMapAmount.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nclass var supportedCameraModels: [String]\nAn array containing the names of all supported camera models.\nvar supportedDecoderVersions: [CIRAWDecoderVersion]\nAn array of all supported decoder versions for the given image type.\nvar isColorNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports color noise reduction adjustments.\nvar isContrastSupported: Bool\nA Boolean that indicates if the current image supports contrast adjustments.\nvar isDetailSupported: Bool\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nvar isLensCorrectionSupported: Bool\nA Boolean that indicates if you can enable lens correction for the current image.\nvar isLuminanceNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nvar isMoireReductionSupported: Bool\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nvar isSharpnessSupported: Bool\nA Boolean that indicates if the current image supports sharpness adjustments.\nvar nativeSize: CGSize\nThe full native size of the unscaled image."
  },
  {
    "title": "isColorNoiseReductionSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801619-iscolornoisereductionsupported",
    "html": "Discussion\n\nIf this value is true, you can adjust the amount of color noise reduction to apply to the image by setting colorNoiseReductionAmount.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nclass var supportedCameraModels: [String]\nAn array containing the names of all supported camera models.\nvar supportedDecoderVersions: [CIRAWDecoderVersion]\nAn array of all supported decoder versions for the given image type.\nvar isContrastSupported: Bool\nA Boolean that indicates if the current image supports contrast adjustments.\nvar isDetailSupported: Bool\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nvar isLensCorrectionSupported: Bool\nA Boolean that indicates if you can enable lens correction for the current image.\nvar isLocalToneMapSupported: Bool\nA Boolean that indicates if the current image supports local tone curve adjustments.\nvar isLuminanceNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nvar isMoireReductionSupported: Bool\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nvar isSharpnessSupported: Bool\nA Boolean that indicates if the current image supports sharpness adjustments.\nvar nativeSize: CGSize\nThe full native size of the unscaled image."
  },
  {
    "title": "isLensCorrectionSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801633-islenscorrectionsupported",
    "html": "Discussion\n\nIf this value is true, you can enable lens correction on the image by setting isLensCorrectionEnabled to true.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nclass var supportedCameraModels: [String]\nAn array containing the names of all supported camera models.\nvar supportedDecoderVersions: [CIRAWDecoderVersion]\nAn array of all supported decoder versions for the given image type.\nvar isColorNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports color noise reduction adjustments.\nvar isContrastSupported: Bool\nA Boolean that indicates if the current image supports contrast adjustments.\nvar isDetailSupported: Bool\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nvar isLocalToneMapSupported: Bool\nA Boolean that indicates if the current image supports local tone curve adjustments.\nvar isLuminanceNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nvar isMoireReductionSupported: Bool\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nvar isSharpnessSupported: Bool\nA Boolean that indicates if the current image supports sharpness adjustments.\nvar nativeSize: CGSize\nThe full native size of the unscaled image."
  },
  {
    "title": "isDetailSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801624-isdetailsupported",
    "html": "Discussion\n\nIf this value is true, you can adjust the amount of detail enhancement to apply to the image by setting detailAmount.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nclass var supportedCameraModels: [String]\nAn array containing the names of all supported camera models.\nvar supportedDecoderVersions: [CIRAWDecoderVersion]\nAn array of all supported decoder versions for the given image type.\nvar isColorNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports color noise reduction adjustments.\nvar isContrastSupported: Bool\nA Boolean that indicates if the current image supports contrast adjustments.\nvar isLensCorrectionSupported: Bool\nA Boolean that indicates if you can enable lens correction for the current image.\nvar isLocalToneMapSupported: Bool\nA Boolean that indicates if the current image supports local tone curve adjustments.\nvar isLuminanceNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nvar isMoireReductionSupported: Bool\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nvar isSharpnessSupported: Bool\nA Boolean that indicates if the current image supports sharpness adjustments.\nvar nativeSize: CGSize\nThe full native size of the unscaled image."
  },
  {
    "title": "RGBAf",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1437756-rgbaf",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "isContrastSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801621-iscontrastsupported",
    "html": "Discussion\n\nIf this value is true, you can adjust the amount of contrast to apply to the image by setting contrastAmount.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nclass var supportedCameraModels: [String]\nAn array containing the names of all supported camera models.\nvar supportedDecoderVersions: [CIRAWDecoderVersion]\nAn array of all supported decoder versions for the given image type.\nvar isColorNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports color noise reduction adjustments.\nvar isDetailSupported: Bool\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nvar isLensCorrectionSupported: Bool\nA Boolean that indicates if you can enable lens correction for the current image.\nvar isLocalToneMapSupported: Bool\nA Boolean that indicates if the current image supports local tone curve adjustments.\nvar isLuminanceNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nvar isMoireReductionSupported: Bool\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nvar isSharpnessSupported: Bool\nA Boolean that indicates if the current image supports sharpness adjustments.\nvar nativeSize: CGSize\nThe full native size of the unscaled image."
  },
  {
    "title": "colorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1437917-colorspace",
    "html": "See Also\nGetting Color Components\nvar components: UnsafePointer<CGFloat>\nThe color components of the color.\nvar numberOfComponents: Int\nReturns the number of color components in the color.\nvar red: CGFloat\nThe unpremultiplied red component of the color.\nvar green: CGFloat\nThe unpremultiplied green component of the color.\nvar blue: CGFloat\nThe unpremultiplied blue component of the color.\nvar alpha: CGFloat\nThe alpha value of the color.\nvar stringRepresentation: String\nA formatted string that specifies the components of the color."
  },
  {
    "title": "RGBA8",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1438063-rgba8",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "setName:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437997-setname",
    "html": "Discussion\n\nYou use a filter’s name to construct key paths to its attributes when the filter is attached to a Core Animation layer. For example, if a CALayer object has an attached CIFilter instance whose name is myExposureFilter, you can refer to attributes of the filter using a key path such as filters.myExposureFilter.inputEV. Layer animations may also access filter attributes via these key paths.\n\nCore Animation can animate this property on a layer.\n\nThe default value for this property is nil.\n\nSee Also\nGetting Filter Parameters and Attributes\nenabled\nA Boolean value that determines whether the filter is enabled. Animatable.\nattributes\nA dictionary of key-value pairs that describe the filter.\ninputKeys\nThe names of all input parameters to the filter.\noutputKeys\nThe names of all output parameters from the filter.\noutputImage\nReturns a CIImage object that encapsulates the operations configured in the filter."
  },
  {
    "title": "center",
    "url": "https://developer.apple.com/documentation/coreimage/cirippletransition/3228692-center",
    "html": "Required"
  },
  {
    "title": "scale",
    "url": "https://developer.apple.com/documentation/coreimage/cirippletransition/3228694-scale",
    "html": "Required"
  },
  {
    "title": "CIFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter-gce",
    "html": "Topics\nInstance Properties\noutputImage\nA CIImage object that encapsulates the operations configured in the filter.\n\nRequired\n\nType Methods\n+ customAttributes\nReturns a dictionary that contains key-value pairs that describe the filter.\n\nRequired\n\nRelationships\nInherited By\nCIAffineClamp\nCIAffineTile\nCIAreaReductionFilter\nCIAttributedTextImageGenerator\nCIAztecCodeGenerator\nCIBarcodeGenerator\nCIBicubicScaleTransform\nCIBlendWithMask\nCIBloom\nCIBlurredRectangleGenerator\nCIBokehBlur\nCIBoxBlur\nCIBumpDistortion\nCIBumpDistortionLinear\nCICMYKHalftone\nCICannyEdgeDetector\nCICheckerboardGenerator\nCICircleSplashDistortion\nCICircularScreen\nCICircularWrap\nCICode128BarcodeGenerator\nCIColorAbsoluteDifference\nCIColorClamp\nCIColorControls\nCIColorCrossPolynomial\nCIColorCube\nCIColorCubeWithColorSpace\nCIColorCubesMixedWithMask\nCIColorCurves\nCIColorInvert\nCIColorMap\nCIColorMatrix\nCIColorMonochrome\nCIColorPolynomial\nCIColorPosterize\nCIColorThreshold\nCIColorThresholdOtsu\nCIComicEffect\nCICompositeOperation\nCIConvertLab\nCIConvolution\nCICoreMLModel\nCICrystallize\nCIDepthOfField\nCIDepthToDisparity\nCIDiscBlur\nCIDisparityToDepth\nCIDisplacementDistortion\nCIDither\nCIDocumentEnhancer\nCIDotScreen\nCIDroste\nCIEdgePreserveUpsample\nCIEdgeWork\nCIEdges\nCIEightfoldReflectedTile\nCIExposureAdjust\nCIFalseColor\nCIFourCoordinateGeometryFilter\nCIFourfoldReflectedTile\nCIFourfoldRotatedTile\nCIFourfoldTranslatedTile\nCIGaborGradients\nCIGammaAdjust\nCIGaussianBlur\nCIGaussianGradient\nCIGlassDistortion\nCIGlassLozenge\nCIGlideReflectedTile\nCIGloom\nCIHatchedScreen\nCIHeightFieldFromMask\nCIHexagonalPixellate\nCIHighlightShadowAdjust\nCIHistogramDisplay\nCIHoleDistortion\nCIHueAdjust\nCIHueSaturationValueGradient\nCIKaleidoscope\nCILabDeltaE\nCILanczosScaleTransform\nCILenticularHaloGenerator\nCILightTunnel\nCILineOverlay\nCILineScreen\nCILinearGradient\nCILinearToSRGBToneCurve\nCIMaskToAlpha\nCIMaskedVariableBlur\nCIMaximumComponent\nCIMedian\nCIMeshGenerator\nCIMinimumComponent\nCIMix\nCIMorphologyGradient\nCIMorphologyMaximum\nCIMorphologyMinimum\nCIMorphologyRectangleMaximum\nCIMorphologyRectangleMinimum\nCIMotionBlur\nCINinePartStretched\nCINinePartTiled\nCINoiseReduction\nCIOpTile\nCIPDF417BarcodeGenerator\nCIPaletteCentroid\nCIPalettize\nCIParallelogramTile\nCIPersonSegmentation\nCIPerspectiveRotate\nCIPerspectiveTile\nCIPhotoEffect\nCIPinchDistortion\nCIPixellate\nCIPointillize\nCIQRCodeGenerator\nCIRadialGradient\nCIRandomGenerator\nCIRoundedRectangleGenerator\nCIRoundedRectangleStrokeGenerator\nCISRGBToneCurveToLinear\nCISaliencyMap\nCISepiaTone\nCIShadedMaterial\nCISharpenLuminance\nCISixfoldReflectedTile\nCISixfoldRotatedTile\nCISmoothLinearGradient\nCISobelGradients\nCISpotColor\nCISpotLight\nCIStarShineGenerator\nCIStraighten\nCIStretchCrop\nCIStripesGenerator\nCISunbeamsGenerator\nCITemperatureAndTint\nCITextImageGenerator\nCIThermal\nCIToneCurve\nCITorusLensDistortion\nCITransitionFilter\nCITriangleKaleidoscope\nCITriangleTile\nCITwelvefoldReflectedTile\nCITwirlDistortion\nCIUnsharpMask\nCIVibrance\nCIVignette\nCIVignetteEffect\nCIVortexDistortion\nCIWhitePointAdjust\nCIXRay\nCIZoomBlur"
  },
  {
    "title": "CIPalettize",
    "url": "https://developer.apple.com/documentation/coreimage/cipalettize",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\npaletteImage\nThe input color palette, obtained by using a k-means clustering filter.\n\nRequired\n\nperceptual\nA Boolean value that specifies whether the filter applies the color palette in a perceptual color space.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ palettizeFilter\nReplaces colors with colors from a palette image."
  },
  {
    "title": "CIRippleTransition",
    "url": "https://developer.apple.com/documentation/coreimage/cirippletransition",
    "html": "Topics\nInstance Properties\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\nextent\nA rectangle that defines the extent of the effect.\n\nRequired\n\nscale\nA value that determines whether the ripple starts as a bulge (a higher value) or a dimple (a lower value).\n\nRequired\n\nshadingImage\nAn image that looks like a shaded sphere enclosed in a square.\n\nRequired\n\nwidth\nThe width of the ripple.\n\nRequired\n\nRelationships\nInherits From\nCITransitionFilter\nSee Also\nRelated Documentation\n+ rippleTransitionFilter\nSimulates a ripple in a pond to transiton from one image to another."
  },
  {
    "title": "color1",
    "url": "https://developer.apple.com/documentation/coreimage/cifalsecolor/3228257-color1",
    "html": "Required"
  },
  {
    "title": "CIMaskToAlpha",
    "url": "https://developer.apple.com/documentation/coreimage/cimasktoalpha",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component."
  },
  {
    "title": "init(functionName:fromMetalLibraryData:)",
    "url": "https://developer.apple.com/documentation/coreimage/cikernel/2880194-init",
    "html": "Parameters\nfunctionName\n\nThe name of the function in the Metal shading language.\n\nfromMetalLibraryData\n\nA metallib file compiled with the Core Image Standard Library.\n\nDiscussion\n\nThis method allows you to use MSL as the shader language for a Core Image kernel. Since MSL based kernels are precompiled, initializing them is faster than their than Core Image Kernel Language (CIKL) counterparts and Xcode can provide error diagnostics during development rather than at runtime. MSL is a more modern language than CIKL, and you can write shader code that uses arrays, structs and matrices.\n\nMSL based kernels still support concatenation and tiling and can work in the same filter graph as traditional CIKL kernels.\n\nSpecifying Compiler and Linker Options\n\nTo use MSL as the shader language for a CIKernel, you must specify some options in Xcode under the Build Settings tab of your project's target. The first option you need to specify is an -fcikernel flag in the Other Metal Compiler Flags option. The second is to add a user-defined setting with a key called MTLLINKER_FLAGS with a value of -cikernel:\n\nFigure 1 Specifying Compiler and Linker Options in Xcode.\n\nCreating a General Kernel in Swift\n\nThe following code shows how you can create a general kernel based on a Metal function named myKernel.\n\nThe first step is to create a Data object that represents the the default Metal library. If you have built this in Xcode, it will be called default.metallib and can be loaded using the Bundle type's url method.\n\nUsing the representation of the Metal library and the function name myKernel, you initialize a CIKernel.\n\nguard\n    let url = Bundle.main.url(forResource: \"default\", withExtension: \"metallib\"),\n    let data = try? Data(contentsOf: url) else {\n    fatalError(\"Unable to get metallib\")\n}\n \nguard let generalKernel = try? CIKernel(functionName: \"myKernel\", fromMetalLibraryData: data) else {\n    fatalError(\"Unable to create CIKernel from myKernel\")\n}\n\n\nThe code to create general, color, warp and blend filters is identical.\n\nMetal Shading Language Extensions\n\nCore Image provides a set of language extensions to MSL in CIKernelMetalLib.h. These extensions include three new data types for working with images: sampler (for accessing the input image), sample_t (represents a single color sample from the input image), and destination (for accessing the output image). The extensions also include convenience functions such as color conversion and premultiply / unpremultiply.\n\nWhereas in CIKL, you typically called global functions when working with, for example, coordinates and samples, these functions are implemented as member functions on the new types.\n\nThe following table shows a summary of CIKL global functions and their equivalent MSL functions.\n\n\t\n\nCore Image Kernel Language\n\n\t\n\nMetal Shading Language\n\n\n\n\nGet destination coordinate\n\n\t\n\ndestCoord()\n\n\t\n\ndest.coord()\n\n\n\n\nTransform coordinate to sampler space\n\n\t\n\nsamplerTransform(src, p)\n\n\t\n\nsrc.transform(p)\n\n\n\n\nGet destination coordinate in sampler space\n\n\t\n\nsamplerCoord(src)\n\n\t\n\nsrc.coord()\n\n\n\n\nSample from source image\n\n\t\n\nsample(src, p)\n\n\t\n\nsrc.sample(p)\n\n\n\n\nGet extent of source image\n\n\t\n\nsamplerExtent(src)\n\n\t\n\nsrc.extent()\n\nSee Also\nCreating a Kernel Using Metal Shading Language\ninit(functionName: String, fromMetalLibraryData: Data, outputPixelFormat: CIFormat)\nCreates a single kernel object using a Metal Shading Language kernel function with optional pixel format."
  },
  {
    "title": "init(functionName:fromMetalLibraryData:outputPixelFormat:)",
    "url": "https://developer.apple.com/documentation/coreimage/cikernel/2880195-init",
    "html": "Parameters\nname\n\nThe name of the function in the Metal shading language.\n\ndata\n\nA metallib file compiled with the Core Image Standard Library.\n\nformat\n\nThe pixel format of the output kernel.\n\nDiscussion\n\nThis method allows you to use MSL as the shader language for a Core Image kernel. Since MSL based kernels are precompiled, initializing them is faster than their than Core Image Kernel Language (CIKL) counterparts and Xcode can provide error diagnostics during development rather than at runtime. MSL is a more modern language than CIKL, and you can write shader code that uses arrays, structs and matrices.\n\nMSL based kernels still support concatenation and tiling and can work in the same filter graph as traditional CIKL kernels.\n\nSee Also\nCreating a Kernel Using Metal Shading Language\ninit(functionName: String, fromMetalLibraryData: Data)\nCreates a single kernel object using a Metal Shading Language (MSL) kernel function."
  },
  {
    "title": "CIMinimumComponent",
    "url": "https://developer.apple.com/documentation/coreimage/ciminimumcomponent",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image."
  },
  {
    "title": "CIThermal",
    "url": "https://developer.apple.com/documentation/coreimage/cithermal",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera."
  },
  {
    "title": "CIMaximumComponent",
    "url": "https://developer.apple.com/documentation/coreimage/cimaximumcomponent",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image."
  },
  {
    "title": "CIPaletteCentroid",
    "url": "https://developer.apple.com/documentation/coreimage/cipalettecentroid",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\npaletteImage\nThe input color palette, obtained by using a k-means clustering filter.\n\nRequired\n\nperceptual\nA Boolean value that specifies whether the filter applies the color palette in a perceptual color space.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ paletteCentroidFilter\nCalculates the location of an image’s colors."
  },
  {
    "title": "CIPhotoEffect",
    "url": "https://developer.apple.com/documentation/coreimage/ciphotoeffect",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nextrapolate\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors."
  },
  {
    "title": "CISepiaTone",
    "url": "https://developer.apple.com/documentation/coreimage/cisepiatone",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\nintensity\nThe intensity of the sepia effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown."
  },
  {
    "title": "CIVignetteEffect",
    "url": "https://developer.apple.com/documentation/coreimage/civignetteeffect",
    "html": "Topics\nInstance Properties\ncenter\nThe center of the effect as x and y coordinates.\n\nRequired\n\nfalloff\nThe falloff of the effect.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nintensity\nThe intensity of the effect.\n\nRequired\n\nradius\nThe distance from the center of the effect.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ vignetteEffectFilter\nGradually darkens a specified area of an image."
  },
  {
    "title": "inputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cifalsecolor/3228258-inputimage",
    "html": "Required"
  },
  {
    "title": "setROISelector(_:)",
    "url": "https://developer.apple.com/documentation/coreimage/cikernel/1437691-setroiselector",
    "html": "Parameters\naMethod\n\nA selector name.\n\nDiscussion\n\nImportant\n\nThis method is not supported in iOS, and not recommended in macOS 10.11 and later. Supply and use a block-based ROI callback instead using the apply(extent:roiCallback:arguments:) method. The discussion below applies only to OS X v10.10 and earlier.\n\nWhen applying a filter kernel, the region of interest (ROI) is the area of source image pixels that must be processed to produce a given area of destination image pixels. For a more detailed definition, see The Region of Interest.\n\nThe aMethod argument must use the signature that is defined for the regionOf:destRect:userInfo: method, which is as follows:\n\n- (CGRect) regionOf:(int)samplerIndex destRect:(CGRect)r userInfo:obj;\n\nwhere:\n\nsamplerIndex defines the sampler to query\n\ndestRect is the extent of the region, in working space coordinates, to render.\n\nuserInfo is the object associated with the kCIApplyOptionUserInfo option when the kernel is applied to its arguments (with the apply(_:arguments:options:) method of a CIFilter object using the kernel). The userInfo is important because instance variables can’t be used by the defining class. Instance variables must be passed through the userInfo argument.\n\nThe regionOf:destRect:userInfo: method of the CIFilter object is called by the framework. This method returns the rectangle that contains the region of the sampler that the kernel needs to render the specified destination rectangle.\n\nA sample regionOf:destRect:userInfo: method might look as follows:\n\n- (CGRect)regionOf:(int)sampler destRect:(CGRect)r userInfo:params\n{\n  float scale = fabs ([params X]);\n  return CGRectInset (r, scale * -1.3333, scale * -1.3333);\n}\n\n\nIf your kernel does not need the image at index to produce output in the rectangle rect, your method should return CGRectNull.\n\nIn the filter code, you set the selector using the following:\n\n[kernel setROISelector:@selector(regionOf:destRect:userInfo:)]\n\nAlternatively, use the apply(extent:roiCallback:arguments:) method to directly apply a kernel to create an output image, specifying the ROI callback as a block or closure.\n\nSee Also\nRelated Documentation\nfunc apply(extent: CGRect, roiCallback: CIKernelROICallback, arguments: [Any]) -> CIImage?\nCreates a new image using the kernel and specified arguments.\nfunc apply(CIKernel, arguments: [Any]?, options: [String : Any]?) -> CIImage?\nProduces a object by applying arguments to a kernel function and using options to control how the kernel function is evaluated."
  },
  {
    "title": "boostShadowAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801617-boostshadowamount",
    "html": "Discussion\n\nUse this value to lighten details in shadows. The value should be in the range of 0...2. The default value is 1. A value less than 1 darkens the shadows, and a value greater than 1 lightens the shadows.\n\nNote\n\nSetting this value has no effect if the boostAmount is 0.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "point1",
    "url": "https://developer.apple.com/documentation/coreimage/cilineargradient/3228545-point1",
    "html": "Required"
  },
  {
    "title": "palettizeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228378-palettizefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the palette filter to an image. The effect uses the palette image that is K x 1 pixels in size containing a set of colors, replacing the image colors.\n\nThe palettize filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\npaletteImage\n\nAn image with the dimensions of N x 1 where N represents the colors to add to the image, with type CIImage.\n\nperceptual\n\nA Boolean value that specifies if the filter applies the color palette in a perceptual color space.\n\nThe following code creates a filter that replaces the colors of the input image with the specified colors found in the palette image:\n\nfunc palettize(inputImage: CIImage, paletteImage: CIImage) -> CIImage {\n    let palettizeFilter = CIFilter.palettize()\n    palettizeFilter.inputImage = inputImage\n    palettizeFilter.paletteImage = paletteImage\n    palettizeFilter.perceptual = true\n    return palettizeFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "baselineExposure",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801615-baselineexposure",
    "html": "Discussion\n\nThe default value varies with camera settings. A value of 0 indicates linear response.\n\nSee Also\nConfiguring a filter\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "point0",
    "url": "https://developer.apple.com/documentation/coreimage/cilineargradient/3228544-point0",
    "html": "Required"
  },
  {
    "title": "nativeSize",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801641-nativesize",
    "html": "Discussion\n\nThis value isn’t affected by orientation changes.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nclass var supportedCameraModels: [String]\nAn array containing the names of all supported camera models.\nvar supportedDecoderVersions: [CIRAWDecoderVersion]\nAn array of all supported decoder versions for the given image type.\nvar isColorNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports color noise reduction adjustments.\nvar isContrastSupported: Bool\nA Boolean that indicates if the current image supports contrast adjustments.\nvar isDetailSupported: Bool\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nvar isLensCorrectionSupported: Bool\nA Boolean that indicates if you can enable lens correction for the current image.\nvar isLocalToneMapSupported: Bool\nA Boolean that indicates if the current image supports local tone curve adjustments.\nvar isLuminanceNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nvar isMoireReductionSupported: Bool\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nvar isSharpnessSupported: Bool\nA Boolean that indicates if the current image supports sharpness adjustments."
  },
  {
    "title": "luminanceNoiseReductionAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801637-luminancenoisereductionamount",
    "html": "Discussion\n\nThe value should be in the range of 0...1. The default value varies by image. A value of 0 indicates no luminance noise reduction, and a value of 1 indicates maximum luminance noise reduction.\n\nNote\n\nThe isLuminanceNoiseReductionSupported property is false if the current image doesn’t support this adjustment.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "CIHueSaturationValueGradient",
    "url": "https://developer.apple.com/documentation/coreimage/cihuesaturationvaluegradient",
    "html": "Topics\nInstance Properties\ncolorSpace\nThe color space for the generated color wheel.\n\nRequired\n\ndither\nA Boolean value specifying whether the dither the generated output.\n\nRequired\n\nradius\nThe distance from the center of the effect.\n\nRequired\n\nsoftness\nThe softness of the generated color wheel.\n\nRequired\n\nvalue\nThe lightness of the hue-saturation gradient.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ hueSaturationValueGradientFilter\nGenerates a gradient representing a specified color space."
  },
  {
    "title": "neutralChromaticity",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801642-neutralchromaticity",
    "html": "Discussion\n\nSet this value to change the level of white balance to apply to the image, based on x,y chromaticity values in the range 0...1.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "moireReductionAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801639-moirereductionamount",
    "html": "Discussion\n\nThe value should be in the range of 0...1. The default value varies by image. A value of 0 indicates no moire reduction, and a value of 1 indicates maximum moire reduction.\n\nNote\n\nThe isMoireReductionSupported property is false if the current image doesn’t support this adjustment.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "neutralLocation",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801643-neutrallocation",
    "html": "Discussion\n\nSet this value based on x,y pixel coordinates in the image.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "init(imageData:identifierHint:)",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801629-init",
    "html": "Parameters\ndata\n\nThe image data.\n\nidentifierHint\n\nA string that identifies the image type.\n\nSee Also\nCreating a filter\ninit?(cvPixelBuffer: CVPixelBuffer, properties: [AnyHashable : Any])\nCreates a RAW filter from the pixel buffer and its properties that you specify.\ninit?(imageURL: URL)\nCreates a RAW filter from the image at the URL location that you specify."
  },
  {
    "title": "connect(_:withKey:to:withKey:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1438159-connect",
    "html": "Parameters\nsourceObject\n\nA CIFilter object, a CIImage object, or the path (an NSString or NSURL object) to an image.\n\nsourceKey\n\nThe key that specifies the source object. For example, if the source is the output image of a filter, pass the outputImage key. Pass nil if the source object is used directly.\n\ntargetObject\n\nThe object to which the source object links.\n\ntargetKey\n\nThe key that specifies the target for the source. For example, if you are connecting the source to the input image of a CIFilter object, you would pass the inputImage key.\n\nSee Also\nConnecting and Disconnecting Objects\nfunc disconnectObject(Any, withKey: String, to: Any, withKey: String)\nRemoves the connection between two objects in the filter chain."
  },
  {
    "title": "init(contentsOf:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1437742-init",
    "html": "Parameters\naURL\n\nThe location of a filter generator file.\n\nReturn Value\n\nThe initialized CIFilterGenerator object. Returns nil if the file can’t be read.\n\nSee Also\nRelated Documentation\n+ filterGeneratorWithContentsOfURL:\nCreates and returns a filter generator object and initializes it with the contents of a filter generator file.\n+ filterGenerator\nCreates and returns an empty filter generator object."
  },
  {
    "title": "disconnectObject(_:withKey:to:withKey:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1438075-disconnectobject",
    "html": "Parameters\nsourceObject\n\nA CIFilter object, a CIImage object, or the path (an NSString or NSURL object) to an image.\n\nsourceKey\n\nThe key that specifies the source object. Pass nil if the source object is used directly.\n\ntargetObject\n\nThe object from which you want to disconnect the source object.\n\ntargetKey\n\nThe key that specifies the target that the source object is currently connected to.\n\nSee Also\nConnecting and Disconnecting Objects\nfunc connect(Any, withKey: String?, to: Any, withKey: String)\nAdds an object to the filter chain."
  },
  {
    "title": "exportKey(_:from:withName:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1438155-exportkey",
    "html": "Parameters\nkey\n\nThe key to export from the target object (for example, inputImage).\n\ntargetObject\n\nThe object associated with the key (for example, the filter).\n\nexportedKeyName\n\nA unique name to use for the exported key. Pass nil to use the original key name.\n\nDiscussion\n\nWhen you create a CIFilter object from a CIFilterGenerator object, you might want the filter client to be able to set some of the parameters associated with the filter chain. You can make a parameter settable by exporting the key associated with the parameter. If the exported key represents an input parameter of the filter, the key is exported as an input key. If the key represents an output parameter, it is exported as an output key.\n\nSee Also\nManaging Exported Keys\nvar exportedKeys: [AnyHashable : Any]\nReturns an array of the exported keys.\nfunc removeExportedKey(String)\nRemoves a key that was previously exported.\nfunc setAttributes([AnyHashable : Any], forExportedKey: String)\nSets a dictionary of attributes for an exported key."
  },
  {
    "title": "init(payload:isCompact:rowCount:columnCount:)",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417codedescriptor/2875182-init",
    "html": "Parameters\nerrorCorrectedPayload\n\nThe data to encode in the PDF417 code.\n\nisCompact\n\nA Boolean specifying whether or not the code is compact.\n\nrowCount\n\nThe number of rows in the PDF417 code.\n\ncolumnCount\n\nThe number of columns in the PDF417 code.\n\nReturn Value\n\nA PDF417 code descriptor encoding the specified payload with the specified row and column counts.\n\nDiscussion\n\nThe CIBarcodeGenerator filter can recreate a PDF417 code given the descriptor created using this method."
  },
  {
    "title": "write(to:atomically:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1438179-write",
    "html": "Parameters\naURL\n\nA location for the file generator file.\n\nflag\n\nPass true to specify that Core Image should create an interim file to avoid overwriting an existing file.\n\nReturn Value\n\nReturns true if the the object is successfully archived to the file.\n\nDiscussion\n\nUse this method to save your filter chain to a file for later use."
  },
  {
    "title": "filter()",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1438044-filter",
    "html": "Return Value\n\nA CIFilter object.\n\nDiscussion\n\nThe topology of the filter chain is immutable, meaning that any changes you make to the filter chain are not reflected in the filter. The returned filter holds the export input and output keys."
  },
  {
    "title": "removeExportedKey(_:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1438191-removeexportedkey",
    "html": "Parameters\nexportedKeyName\n\nThe name of the key you want to remove.\n\nSee Also\nManaging Exported Keys\nvar exportedKeys: [AnyHashable : Any]\nReturns an array of the exported keys.\nfunc exportKey(String, from: Any, withName: String?)\nExports an input or output key of an object in the filter chain.\nfunc setAttributes([AnyHashable : Any], forExportedKey: String)\nSets a dictionary of attributes for an exported key."
  },
  {
    "title": "isCompact",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417codedescriptor/2875194-iscompact",
    "html": "See Also\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload containing the data encoded in the PDF417 code.\nvar rowCount: Int\nThe number of rows in the PDF417 code.\nvar columnCount: Int\nThe number of columns in the PDF417 code."
  },
  {
    "title": "init(payload:isCompact:layerCount:dataCodewordCount:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciazteccodedescriptor/2875188-init",
    "html": "Parameters\nerrorCorrectedPayload\n\nThe data to encode in the Aztec code.\n\nisCompact\n\nA Boolean indicating whether or not the Aztec code is compact.\n\nlayerCount\n\nThe number of layers in the Aztec code.\n\ndataCodewordCount\n\nThe number of codewords in the Aztec code.\n\nReturn Value\n\nAn Aztec code descriptor encoding the specified payload with the specified number of layers and data codewords.\n\nDiscussion\n\nThe CIBarcodeGenerator filter can recreate an Aztec code given the descriptor created using this method."
  },
  {
    "title": "errorCorrectedPayload",
    "url": "https://developer.apple.com/documentation/coreimage/ciazteccodedescriptor/2875187-errorcorrectedpayload",
    "html": "See Also\nExamining a Descriptor\nvar isCompact: Bool\nA Boolean value telling if the Aztec code is compact.\nvar layerCount: Int\nThe number of layers embedded in the Aztec code.\nvar dataCodewordCount: Int\nThe number of data codewords in the Aztec code."
  },
  {
    "title": "isCompact",
    "url": "https://developer.apple.com/documentation/coreimage/ciazteccodedescriptor/2875203-iscompact",
    "html": "See Also\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload containing the data encoded in the Aztec code.\nvar layerCount: Int\nThe number of layers embedded in the Aztec code.\nvar dataCodewordCount: Int\nThe number of data codewords in the Aztec code."
  },
  {
    "title": "load(_:)",
    "url": "https://developer.apple.com/documentation/coreimage/cipluginregistration/1437823-load",
    "html": "Required\n\nParameters\nhost\n\nReserved for future use.\n\nReturn Value\n\nReturns true if the image unit is successfully initialized\n\nDiscussion\n\nThe load method is called once by the host to initialize the image unit when the first filter in the image unit is instantiated. The method provides the image unit with an opportunity to perform custom initialization, such as a registration check.\n\nSee Also\nRelated Documentation\nCore Image Programming Guide\nImage Unit Tutorial"
  },
  {
    "title": "dataCodewordCount",
    "url": "https://developer.apple.com/documentation/coreimage/ciazteccodedescriptor/2875166-datacodewordcount",
    "html": "See Also\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload containing the data encoded in the Aztec code.\nvar isCompact: Bool\nA Boolean value telling if the Aztec code is compact.\nvar layerCount: Int\nThe number of layers embedded in the Aztec code."
  },
  {
    "title": "CIPageCurlTransition",
    "url": "https://developer.apple.com/documentation/coreimage/cipagecurltransition",
    "html": "Topics\nInstance Properties\nangle\nThe angle of the curling page.\n\nRequired\n\nbacksideImage\nThe image that appears on the back of the source image as the page curls to reveal the target image.\n\nRequired\n\nextent\nThe extent of the effect.\n\nRequired\n\nradius\nThe radius of the curl.\n\nRequired\n\nshadingImage\nAn image that looks like a shaded sphere enclosed in a square.\n\nRequired\n\nRelationships\nInherits From\nCITransitionFilter\nSee Also\nRelated Documentation\n+ pageCurlTransitionFilter\nSimulates the curl of a page, revealing the target image."
  },
  {
    "title": "CIModTransition",
    "url": "https://developer.apple.com/documentation/coreimage/cimodtransition",
    "html": "Topics\nInstance Properties\nangle\nThe angle of the mod hole pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the effect.\n\nRequired\n\ncompression\nThe amount of stretching applied to the mod hole pattern.\n\nRequired\n\nradius\nThe radius of the undistorted mod holes in the pattern.\n\nRequired\n\nRelationships\nInherits From\nCITransitionFilter\nSee Also\nRelated Documentation\n+ modTransitionFilter\nTransitions between two images by applying irregularly shaped holes."
  },
  {
    "title": "red",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1437969-red",
    "html": "See Also\nGetting Color Components\nvar colorSpace: CGColorSpace\nThe Quartz 2D color space associated with the color.\nvar components: UnsafePointer<CGFloat>\nThe color components of the color.\nvar numberOfComponents: Int\nReturns the number of color components in the color.\nvar green: CGFloat\nThe unpremultiplied green component of the color.\nvar blue: CGFloat\nThe unpremultiplied blue component of the color.\nvar alpha: CGFloat\nThe alpha value of the color.\nvar stringRepresentation: String\nA formatted string that specifies the components of the color."
  },
  {
    "title": "init(red:green:blue:colorSpace:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1643576-init",
    "html": "Parameters\nr\n\nThe unpremultiplied red component of the color.\n\ng\n\nThe unpremultiplied green component of the color.\n\nb\n\nThe unpremultiplied blue component of the color.\n\ncolorSpace\n\nThe color space in which to create the new color. This color space must conform to the CGColorSpaceModel.rgb color space model.\n\nReturn Value\n\nThe resulting Core Image color.\n\nSee Also\nInitializing Color Objects\ninit(cgColor: CGColor)\nInitializes a Core Image color object with a Core Graphics color.\ninit(color: UIColor)\nInitializes a Core Image color object using a UIKit (or AppKit) color object.\ninit(red: CGFloat, green: CGFloat, blue: CGFloat, alpha: CGFloat)\nInitializes a Core Image color object with the specified red, green, blue, and alpha component values.\ninit?(red: CGFloat, green: CGFloat, blue: CGFloat, alpha: CGFloat, colorSpace: CGColorSpace)\nInitializes a Core Image color object with the specified red, green, blue, and alpha component values as measured in the specified color space."
  },
  {
    "title": "init(red:green:blue:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1437941-init",
    "html": "Parameters\nr\n\nThe value of the red component.\n\ng\n\nThe value of the green component.\n\nb\n\nThe value of the blue component.\n\nReturn Value\n\nA Core Image color object that represents an RGB color in the color space specified by the Quartz 2D constant kCGColorSpaceGenericRGB.\n\nSee Also\nCreating Color Objects\ninit(string: String)\nCreates a color object using the RGBA color component values specified by a string.\nRelated Documentation\n+ colorWithCGColor:\nCreates a color object from a Quartz color.\n+ colorWithRed:green:blue:alpha:\nCreates a color object using the specified RGBA color component values."
  },
  {
    "title": "init(red:green:blue:alpha:colorSpace:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1643572-init",
    "html": "Parameters\nr\n\nThe unpremultiplied red component of the color.\n\ng\n\nThe unpremultiplied green component of the color.\n\nb\n\nThe unpremultiplied blue component of the color.\n\na\n\nThe alpha (opacity) component of the color.\n\ncolorSpace\n\nThe color space in which to create the new color. This color space must conform to the CGColorSpaceModel.rgb color space model.\n\nReturn Value\n\nThe resulting Core Image color.\n\nSee Also\nInitializing Color Objects\ninit(cgColor: CGColor)\nInitializes a Core Image color object with a Core Graphics color.\ninit(color: UIColor)\nInitializes a Core Image color object using a UIKit (or AppKit) color object.\ninit(red: CGFloat, green: CGFloat, blue: CGFloat, alpha: CGFloat)\nInitializes a Core Image color object with the specified red, green, blue, and alpha component values.\ninit?(red: CGFloat, green: CGFloat, blue: CGFloat, colorSpace: CGColorSpace)\nInitializes a Core Image color object with the specified red, green, and blue component values as measured in the specified color space."
  },
  {
    "title": "init(string:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1438059-init",
    "html": "Parameters\nrepresentation\n\nA string that is in one of the formats returned by the stringRepresentation method. For example, the string:\n\n@\"0.5 0.7 0.3 1.0\"\n\nindicates an RGB color whose components are 50% red, 70% green, 30% blue, and 100% opaque (alpha value of 1.0). The string representation always has four components—red, green, blue, and alpha. The default value for the alpha component is 1.0.\n\nReturn Value\n\nA Core Image color object that represents an RGB color in the color space specified by the Quartz 2D constant kCGColorSpaceGenericRGB.\n\nSee Also\nCreating Color Objects\ninit(red: CGFloat, green: CGFloat, blue: CGFloat)\nCreates a color object using the specified RGB color component values\nRelated Documentation\n+ colorWithCGColor:\nCreates a color object from a Quartz color.\n+ colorWithRed:green:blue:alpha:\nCreates a color object using the specified RGBA color component values."
  },
  {
    "title": "init(color:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1528762-init",
    "html": "Parameters\ncolor\n\nThe initial color value, which can belong to any available color space.\n\nReturn Value\n\nThe resulting Core Image color, or (in macOS only) nil if the specified color cannot be converted.\n\nDiscussion\n\nIn iOS and tvOS, this initializer takes a UIColor object and always returns a corresponding Core Image color object.\n\nIn macOS, this initializer takes an NSColor object. some possible NSColor configurations cannot be accurately represented as Core Image (or Core Graphics) colors—in such cases, this initializer returns nil.\n\nSee Also\nInitializing Color Objects\ninit(cgColor: CGColor)\nInitializes a Core Image color object with a Core Graphics color.\ninit(red: CGFloat, green: CGFloat, blue: CGFloat, alpha: CGFloat)\nInitializes a Core Image color object with the specified red, green, blue, and alpha component values.\ninit?(red: CGFloat, green: CGFloat, blue: CGFloat, colorSpace: CGColorSpace)\nInitializes a Core Image color object with the specified red, green, and blue component values as measured in the specified color space.\ninit?(red: CGFloat, green: CGFloat, blue: CGFloat, alpha: CGFloat, colorSpace: CGColorSpace)\nInitializes a Core Image color object with the specified red, green, blue, and alpha component values as measured in the specified color space."
  },
  {
    "title": "init(red:green:blue:alpha:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1438084-init",
    "html": "Parameters\nr\n\nThe unpremultiplied red component of the color.\n\ng\n\nThe unpremultiplied green component of the color.\n\nb\n\nThe unpremultiplied blue component of the color.\n\na\n\nThe alpha (opacity) component of the color.\n\nReturn Value\n\nThe resulting Core Image color.\n\nSee Also\nInitializing Color Objects\ninit(cgColor: CGColor)\nInitializes a Core Image color object with a Core Graphics color.\ninit(color: UIColor)\nInitializes a Core Image color object using a UIKit (or AppKit) color object.\ninit?(red: CGFloat, green: CGFloat, blue: CGFloat, colorSpace: CGColorSpace)\nInitializes a Core Image color object with the specified red, green, and blue component values as measured in the specified color space.\ninit?(red: CGFloat, green: CGFloat, blue: CGFloat, alpha: CGFloat, colorSpace: CGColorSpace)\nInitializes a Core Image color object with the specified red, green, blue, and alpha component values as measured in the specified color space."
  },
  {
    "title": "init(cgColor:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor/1437821-init",
    "html": "Parameters\nc\n\nA Core Graphics color value.\n\nReturn Value\n\nThe resulting Core Image color.\n\nDiscussion\n\nA CGColor object is the fundamental opaque data type used internally by Core Graphics to represent colors. For more information on Core Graphics color and color spaces, see Quartz 2D Programming Guide.\n\nYou can pass a CGColor object that represents any color space, including CMYK, but Core Image converts all color spaces to the Core Image working color space before it passes the color space to the filter kernel. The Core Image working color space uses three color components plus alpha.\n\nSee Also\nInitializing Color Objects\ninit(color: UIColor)\nInitializes a Core Image color object using a UIKit (or AppKit) color object.\ninit(red: CGFloat, green: CGFloat, blue: CGFloat, alpha: CGFloat)\nInitializes a Core Image color object with the specified red, green, blue, and alpha component values.\ninit?(red: CGFloat, green: CGFloat, blue: CGFloat, colorSpace: CGColorSpace)\nInitializes a Core Image color object with the specified red, green, and blue component values as measured in the specified color space.\ninit?(red: CGFloat, green: CGFloat, blue: CGFloat, alpha: CGFloat, colorSpace: CGColorSpace)\nInitializes a Core Image color object with the specified red, green, blue, and alpha component values as measured in the specified color space.\nRelated Documentation\nColor Management Overview\nCore Image Programming Guide"
  },
  {
    "title": "RGBAh",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat/1437998-rgbah",
    "html": "See Also\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance."
  },
  {
    "title": "filter(withName:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilterconstructor/1438018-filter",
    "html": "Required\n\nParameters\nname\n\nThe name of the requested custom filter.\n\nReturn Value\n\nA CIFilter object implementing the custom filter.\n\nDiscussion\n\nCore Image calls this method when a filter is requested by name using the CIFilter class method init(name:) method (or related methods). Your implementation of this method should provide a new instance of the CIFilter subclass for your custom filter.\n\nSee Also\nRelated Documentation\nCore Image Programming Guide"
  },
  {
    "title": "portraitEffectsMatte",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801647-portraiteffectsmatte",
    "html": "See Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "shadowBias",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801656-shadowbias",
    "html": "Discussion\n\nThe default value varies by image.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "neutralTemperature",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801644-neutraltemperature",
    "html": "Discussion\n\nSet this value based on temperature values in the range 2000K...50000K.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "neutralTint",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801645-neutraltint",
    "html": "Discussion\n\nSet this value based on tint values in the range -150...150.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "scaleFactor",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801650-scalefactor",
    "html": "Discussion\n\nThe value should be in the range of 0...1. The default value is 1. A value of 1 results in a full-size output image, and a value less than 1 results in a smaller output image.\n\nTip\n\nSetting this value can greatly improve the drawing performance.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "properties",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801649-properties",
    "html": "Discussion\n\nThis dictionary contains the same contents as the Image Properties accessed using CGImageSourceCopyProperties(_:_:).\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "semanticSegmentationTeethMatte",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801655-semanticsegmentationteethmatte",
    "html": "Discussion\n\nThis matting image segments the teeth from all people in the visible field of view of the image.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "previewImage",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801648-previewimage",
    "html": "See Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "semanticSegmentationHairMatte",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801652-semanticsegmentationhairmatte",
    "html": "Discussion\n\nThis matting image segments hair from all people in the visible field of view of the image.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "semanticSegmentationSkyMatte",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801654-semanticsegmentationskymatte",
    "html": "Discussion\n\nThis matting image segments the sky from the visible field of view of the image.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "makeKernels(source:)",
    "url": "https://developer.apple.com/documentation/coreimage/cikernel/1437876-makekernels",
    "html": "Parameters\ns\n\nA program in the Core Image Kernel Language that contains one or more routines, each of which is marked using the kernel keyword.\n\nReturn Value\n\nAn array of CIKernel objects. The array contains one CIKernel objects for each kernel routine in the supplied string. Each object in the array can be of class CIKernel, CIColorKernel, or CIWarpKernel depending on the corresponding routine specified in the Core Image Kernel Language source code string.\n\nDiscussion\n\nThe Core Image Kernel Language is a dialect of the OpenGL Shading Language. See Core Image Kernel Language Reference and Core Image Programming Guide for more details.\n\nSee Also\nCreating a Kernel Using Core Image Kernel Language\ninit?(source: String)\nCreates a single kernel object.\nDeprecated"
  },
  {
    "title": "name",
    "url": "https://developer.apple.com/documentation/coreimage/cikernel/1438067-name",
    "html": "Discussion\n\nThe name of a kernel routine is the identifier used to declare it in the Core Image Kernel Language source code. For example, if you use the init(source:) method to create a kernel from the source code below, the name of the returned CIKernel object is “moveUpTwoPixels”.\n\nkernel vec4 moveUpTwoPixels (sampler image) {\n    vec2 dc = destCoord();\n    vec2 offset = vec2(0.0, 2.0);\n    return sample (image, samplerTransform (image, dc + offset));\n}\n"
  },
  {
    "title": "apply(extent:roiCallback:arguments:)",
    "url": "https://developer.apple.com/documentation/coreimage/cikernel/1438243-apply",
    "html": "Parameters\nextent\n\nThe extent of the output image.\n\ncallback\n\nA block or closure that computes the region of interest for a given rectangle of destination image pixels. See CIKernelROICallback.\n\nargs\n\nAn array of arguments to pass to the kernel routine. The type of each object in the array must be compatible with the corresponding parameter declared in the kernel routine source code. For details, see Core Image Kernel Language Reference.\n\nReturn Value\n\nA new image object describing the result of applying the kernel.\n\nDiscussion\n\nThis method is analogous to the CIFilter method apply(_:arguments:options:), but it does not require construction of a CIFilter object, and it allows you to specify a callback for determining the kernel’s region of interest as a block or closure. As with the similar CIFilter method, calling this method does not execute the kernel code—filters and their kernel code are evaluated only when rendering a final output image.\n\nWhen applying a filter kernel, the region of interest (ROI) is the area of source image pixels that must be processed to produce a given area of destination image pixels. For a more detailed definition, see The Region of Interest. Core Image calls your callback block or closure to determine the ROI when rendering the filter output. Core Image automatically splits large images into smaller tiles for rendering, so your callback may be called multiple times.\n\nSee Also\nApplying a Kernel to Filter an Image\ntypealias CIKernelROICallback\nThe signature for a block that computes the region of interest (ROI) for a given area of destination image pixels. Core Image calls this block when applying the kernel. You specify this block when using the apply(extent:roiCallback:arguments:) method."
  },
  {
    "title": "CIKernelROICallback",
    "url": "https://developer.apple.com/documentation/coreimage/cikernelroicallback",
    "html": "Discussion\n\nThe block takes the following parameters:\n\nindex\n\nFor a general-purpose kernel or color kernel routine that supports multiple input images, the index of the source image for which Core Image is requesting ROI information. For all other kernel routines, this parameter is always zero.\n\nrect\n\nThe rectangle in destination image pixels for which Core Image is requesting ROI information.\n\nThe block returns a CGRect structure describing the region of interest for the specified rectangle.\n\nWhen applying a filter kernel, the region of interest is the area of source image pixels that must be processed to produce a given area of destination image pixels. (For a more detailed definition, see The Region of Interest.) For example, a kernel that applies a blur effect in a ten-pixel radius must sample source image pixels ten pixels away in each direction from every output pixel. Thus, its region of interest is a rectangle ten pixels larger on each side than the destination rectangle:\n\nCIKernelROICallback callback = ^(int index, CGRect rect) {\n    return CGRectInset(rect, -10, -10);\n};\n\n\nIf your kernel does not need the image at index to produce output in the rectangle rect, your block should return CGRectNull.\n\nSee Also\nApplying a Kernel to Filter an Image\nfunc apply(extent: CGRect, roiCallback: CIKernelROICallback, arguments: [Any]) -> CIImage?\nCreates a new image using the kernel and specified arguments."
  },
  {
    "title": "CISmoothLinearGradient",
    "url": "https://developer.apple.com/documentation/coreimage/cismoothlineargradient",
    "html": "Topics\nInstance Properties\ncolor0\nThe first color to use in the gradient.\n\nRequired\n\ncolor1\nThe second color to use in the gradient.\n\nRequired\n\npoint0\nThe starting position of the gradient.\n\nRequired\n\npoint1\nThe ending position of the gradient.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ smoothLinearGradientFilter\nGenerates a gradient that blends colors along a linear axis between two defined endpoints."
  },
  {
    "title": "CIRadialGradient",
    "url": "https://developer.apple.com/documentation/coreimage/ciradialgradient",
    "html": "Topics\nInstance Properties\ncenter\nThe center of the effect as x and y coordinates.\n\nRequired\n\ncolor0\nThe first color to use in the gradient.\n\nRequired\n\ncolor1\nThe second color to use in the gradient.\n\nRequired\n\nradius0\nThe radius of the starting circle to use in the gradient.\n\nRequired\n\nradius1\nThe radius of the ending circle to use in the gradient.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ radialGradientFilter\nGenerates a gradient that varies radially between two circles having the same center."
  },
  {
    "title": "CIGaussianGradient",
    "url": "https://developer.apple.com/documentation/coreimage/cigaussiangradient",
    "html": "Topics\nInstance Properties\ncenter\nThe center of the effect as x and y coordinates.\n\nRequired\n\ncolor0\nThe first color to use in the gradient.\n\nRequired\n\ncolor1\nThe second color to use in the gradient.\n\nRequired\n\nradius\nThe radius of the Gaussian distribution.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ gaussianGradientFilter\nGenerates a gradient that varies from one color to another using a Gaussian distribution."
  },
  {
    "title": "CILineScreen",
    "url": "https://developer.apple.com/documentation/coreimage/cilinescreen",
    "html": "Topics\nInstance Properties\nangle\nThe angle of the pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the line screen pattern.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nsharpness\nThe sharpness of the pattern.\n\nRequired\n\nwidth\nThe distance between lines in the pattern.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ lineScreenFilter\nCreates a monochrome image with a series of small lines to add detail."
  },
  {
    "title": "paletteCentroidFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228377-palettecentroidfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the palette centroid filter to an image. The filter locates colors in the input image that the palette image defines and outputImage.extent provides the location of the colors of the image. You can combine with other filters to create more sophisticated images.\n\nThe palette centroid filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\npaletteImage\n\nAn image that has the dimensions of N x 1 where N represents the amount of colors in the image, with type CIImage.\n\nperceptual\n\nA Boolean value that specifies if the filter applies the color palette in a perceptual color space.\n\nThe following code creates a filter that calculates the extent of the palette color:\n\nfunc paletteCentroid(inputImage: CIImage, paletteImage: CIImage) -> CIImage {\n    let paletteCentroidFilter = CIFilter.paletteCentroid()\n    paletteCentroidFilter.inputImage = inputImage\n    paletteCentroidFilter.paletteImage = paletteImage\n    paletteCentroidFilter.perceptual = false\n    return paletteCentroidFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "CIHatchedScreen",
    "url": "https://developer.apple.com/documentation/coreimage/cihatchedscreen",
    "html": "Topics\nInstance Properties\nangle\nThe angle of the pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the hatched screen pattern.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nsharpness\nThe amount of sharpening to apply.\n\nRequired\n\nwidth\nThe distance between lines in the pattern.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ hatchedScreenFilter\nCreates a monochrome image with a series of lines to add detail."
  },
  {
    "title": "maskToAlphaFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228354-masktoalphafilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the mask-to-alpha filter to an image. The value of the alpha component is determined by the grayscale value of the input image. Black pixels become completely transparent, white pixels are completely solid.\n\nThe mask-to-alpha filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that makes the input image’s background transparent:\n\nfunc maskToAlpha(inputImage: CIImage) -> CIImage {\n    let maskToAlphaFilter = CIFilter.maskToAlpha()\n    maskToAlphaFilter.inputImage = inputImage\n    return maskToAlphaFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "localToneMapAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801635-localtonemapamount",
    "html": "Discussion\n\nThe value should be in the range of 0...1. The default value varies by image. A value of 0 indicates no local tone curve (linear response), and a value of 1 indicates full global tone curve.\n\nNote\n\nThe isLocalToneMapSupported property is false if the current image doesn’t support this adjustment.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "maximumComponentFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228356-maximumcomponentfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the maximum component filter to an image. The effect applies a preconfigured set of effects that result in the input image becoming grayscale using the maximum RGB color components.\n\nThe maximum component filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds brightness and makes the input image grayscale:\n\nfunc maximumComponent(inputImage: CIImage) -> CIImage {\n    let maximumComponentFilter = CIFilter.maximumComponent()\n    maximumComponentFilter.inputImage = inputImage\n    return maximumComponentFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "colorCrossPolynomialFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228286-colorcrosspolynomialfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the color cross polynomial filter to an image. The effect targets each pixel individually and calculates the coefficients for the red, green, and blue channels according to the polynomial cross product.\n\nThe color cross-polynomial filter uses the following properties:\n\nredCoefficients\n\nA CIVector representing the polynomial coefficients for the red channel.\n\nblueCoefficients\n\nA CIVector representing the polynomial coefficients for the blue channel.\n\ngreenCoefficients\n\nA CIVector representing polynomial coefficients for the green channel.\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds a green hue to the input image:\n\n    func colorCrossPolynomial(inputImage: CIImage) -> CIImage? {\n\n\n        let colorCrossPolynomial = CIFilter.colorCrossPolynomial()\n        let redfloatArr: [CGFloat] = [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n        let greenfloatArr: [CGFloat] = [0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n        let bluefloatArr: [CGFloat] = [0, 0, 1, 0, 0, 0, 0, 1, 1, 0]\n\n\n        colorCrossPolynomial.inputImage = inputImage\n        colorCrossPolynomial.blueCoefficients = CIVector(values: bluefloatArr, count: bluefloatArr.count)\n        colorCrossPolynomial.redCoefficients = CIVector(values: redfloatArr, count: redfloatArr.count)\n        colorCrossPolynomial.greenCoefficients = CIVector(values: greenfloatArr, count: greenfloatArr.count)\n        return colorCrossPolynomial.outputImage\n    }\n\n\nSee Also\nColor Effect Filters\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "ditherFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228315-ditherfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThe effect applies a dithering effect to the input image. The effect applies randomized noise to the input image to produce a processed look.\n\nThe dither filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nintensity\n\nA float representing the desired strength of the effect as an NSNumber.\n\nThe following code creates a filter that adds desaturation to the input image:\n\nfunc dither(inputImage: CIImage) -> CIImage {\n    let ditherFilter = CIFilter.dither()\n    ditherFilter.inputImage = inputImage\n    ditherFilter.intensity = 0.4\n    return ditherFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "documentEnhancerFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228317-documentenhancerfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the document enhancer filter to an image. The effect removes unwanted shadows while whitening the background and enhancing contrast. The filter is commonly used to enhance scanned documents.\n\nThe document enhancer filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\namount\n\nA float representing the desired strength of the effect as an NSNumber.\n\nThe following code creates a filter that adds brightness to the input image:\n\nfunc documentEnhancer(inputImage: CIImage) -> CIImage {\n    let documentEnhancerFilter = CIFilter.documentEnhancer()\n    documentEnhancerFilter.inputImage = inputImage\n    documentEnhancerFilter.amount = 4\n    return documentEnhancerFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "colorCubeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228287-colorcubefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the color cube filter to an image. The effect maps color values in the input image to new color values using a three-dimensional color look-up table, also called a color cube. For each RGBA pixel in the input image, the filter uses the pixel’s red, green, and blue component values to identify a location in the table. The RGBA value at that location becomes the RGBA value of the output pixel.\n\nThe color cube filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncubeData\n\nData containing a 3-dimensional color table of floating-point premultiplied RGBA values. The cells are organized in a standard ordering. The columns and rows of the data are indexed by red and green, respectively. Each data plane is followed by the next higher plane in the data, with planes indexed by blue.\n\nextrapolate\n\nIf true, then the filter extrapolates the color cube for any RGB component values outside the range 0.0 to 1.0.\n\ncubeDimension\n\nThe dimension of the color cube.\n\nThe following code creates a filter that adds a blue hue to the input image:\n\nfunc colorCube(inputImage: CIImage, cubeData: Data) -> CIImage {\n    let colorCubeEffect = CIFilter.colorCube()\n    colorCubeEffect.inputImage = inputImage\n    colorCubeEffect.cubeData = cubeData\n    colorCubeEffect.cubeDimension = 4\n    return colorCubeEffect.outputImage!\n}\n// Create a color cube with size 4.\nvar colorCubeData: [Float32] = []\nlet size = 4\nlet step = 1.0 / Float(size - 1)\nfor b in 0..<size {\n    for g in 0..<size {\n        for r in 0..<size {\n            // Calculate the normalized color component values.\n            let red = Float32(r) * step\n            let green = Float32(g) * step\n            // Shift the blue component to add a blue tint.\n            let blue = min(1.0, Float32(b) * step + 0.5)\n            let alpha: Float = 1.0\n            colorCubeData.append(contentsOf: [red, green, blue, alpha])\n        }\n    }\n}\nlet cubeData = Data(bytes: colorCubeData, count: colorCubeData.count * 4)\nlet result = colorCube(inputImage: ciImage, cubeData: cubeData)\nimageView.image = UIImage(ciImage: result)\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "boostAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801616-boostamount",
    "html": "Discussion\n\nThe value should be in the range of 0...1. A value of 0 indicates no global tone curve (linear response), and a value of 1 indicates full global tone curve. The default value is 1.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "colorNoiseReductionAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801618-colornoisereductionamount",
    "html": "Discussion\n\nThe value should be in the range of 0...1. The default value varies by image. A value of 0 indicates no chroma noise reduction, and a value of 1 indicates maximum chroma noise reduction.\n\nNote\n\nA value of false for isColorNoiseReductionSupported indicates that the current image doesn’t support this adjustment.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "isSharpnessSupported",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801658-issharpnesssupported",
    "html": "Discussion\n\nIf this value is true, you can adjust the amount of sharpness to apply to the image by setting sharpnessAmount.\n\nSee Also\nInspecting supported camera models, decoders, and filters\nclass var supportedCameraModels: [String]\nAn array containing the names of all supported camera models.\nvar supportedDecoderVersions: [CIRAWDecoderVersion]\nAn array of all supported decoder versions for the given image type.\nvar isColorNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports color noise reduction adjustments.\nvar isContrastSupported: Bool\nA Boolean that indicates if the current image supports contrast adjustments.\nvar isDetailSupported: Bool\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nvar isLensCorrectionSupported: Bool\nA Boolean that indicates if you can enable lens correction for the current image.\nvar isLocalToneMapSupported: Bool\nA Boolean that indicates if the current image supports local tone curve adjustments.\nvar isLuminanceNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nvar isMoireReductionSupported: Bool\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nvar nativeSize: CGSize\nThe full native size of the unscaled image."
  },
  {
    "title": "extendedDynamicRangeAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3820998-extendeddynamicrangeamount",
    "html": "Discussion\n\nThe value should be in the range of 0...2. The default value is 0. A value of 0 indicates no EDR. A value of 1 indicates default EDR, and a value of 2 indicates maximum EDR.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "isGamutMappingEnabled",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801631-isgamutmappingenabled",
    "html": "Discussion\n\nThe default value is true.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "isDraftModeEnabled",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801625-isdraftmodeenabled",
    "html": "Discussion\n\nSetting this value to true can improve image decoding speed with minimal loss of quality. The default value is false.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "exposure",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801626-exposure",
    "html": "Discussion\n\nThe default value is 0.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "decoderVersion",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801622-decoderversion",
    "html": "Discussion\n\nThe value should be in the range of 0 to the current decoder version.\n\nA newly initialized object defaults to the newest available decoder version for the given image type. You can request an older version to maintain compatibility with older releases. However, the version you request needs to be a member of supportedDecoderVersions, otherwise the system generates a nil output image.\n\nWhen you request a specific version of the decoder, Core Image produces an image that looks the same across different versions. However, Core Image doesn’t guarantee that the bits are the same across versions, because the rounding behavior of floating-point arithmetic varies due to differences in compilers or hardware.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "isLensCorrectionEnabled",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801632-islenscorrectionenabled",
    "html": "Discussion\n\nThe default value varies by image.\n\nNote\n\nThe isLensCorrectionSupported property is false if the current image doesn’t support this adjustment.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "linearSpaceFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801634-linearspacefilter",
    "html": "See Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "detailAmount",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801623-detailamount",
    "html": "Discussion\n\nThe value should be in the range of 0...3. The default value varies by image. A value of 0 indicates no detail enhancement, and a value of 3 indicates maximum detail enhancement.\n\nNote\n\nA value of false for isDetailSupported indicates that the current image doesn’t support this adjustment.\n\nSee Also\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image."
  },
  {
    "title": "CIDotScreen",
    "url": "https://developer.apple.com/documentation/coreimage/cidotscreen",
    "html": "Topics\nInstance Properties\nangle\nThe angle of the pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the dot screen pattern.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nsharpness\nThe sharpness of the pattern.\n\nRequired\n\nwidth\nThe distance between dots in the pattern.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ dotScreenFilter\nCreates a monochrome image with a series of dots to add detail."
  },
  {
    "title": "CICircularScreen",
    "url": "https://developer.apple.com/documentation/coreimage/cicircularscreen",
    "html": "Topics\nInstance Properties\ncenter\nThe x and y position to use as the center of the circular screen pattern.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nsharpness\nThe sharpness of the circles.\n\nRequired\n\nwidth\nThe distance between each circle in the pattern.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ circularScreenFilter\nAdds a circular overlay to an image."
  },
  {
    "title": "setAttributes(_:forExportedKey:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1438069-setattributes",
    "html": "Parameters\nattributes\n\nA dictionary that describes the attributes associated with the specified key.\n\nkey\n\nThe exported key whose attributes you want to set.\n\nDiscussion\n\nBy default, the exported key inherits the attributes from its original key and target object. You can use this method to change one or more of the existing attributes for the key, such as the default value or maximum value. For more information on attributes, see CIFilter and Core Image Programming Guide.\n\nSee Also\nManaging Exported Keys\nvar exportedKeys: [AnyHashable : Any]\nReturns an array of the exported keys.\nfunc exportKey(String, from: Any, withName: String?)\nExports an input or output key of an object in the filter chain.\nfunc removeExportedKey(String)\nRemoves a key that was previously exported."
  },
  {
    "title": "registerFilterName(_:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/1437891-registerfiltername",
    "html": "Parameters\nname\n\nA unique name for the filter chain you want to register.\n\nDiscussion\n\nThis method allows you to register the filter chain as a named filter in the Core Image filter repository. You can then create a CIFilter object from it using the the init(name:) method of the CIFilter class."
  },
  {
    "title": "Exported Keys",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator/exported_keys",
    "html": "Topics\nConstants\nlet kCIFilterGeneratorExportedKeyName: String\nThe key (CIFilterGeneratorExportedKeyName) for the name used to export the CIFilterGenerator object. The associated value is a string that specifies a unique name for the filter generator object.\nlet kCIFilterGeneratorExportedKey: String\nThe key (CIFilterGeneratorExportedKey) for the exported parameter. The associated value is the key name of the parameter you are exporting, such as inputRadius.\nlet kCIFilterGeneratorExportedKeyTargetObject: String\nThe target object (CIFilterGeneratorExportedKeyTargetObject) for the exported key. The associated value is the name of the object, such as CIMotionBlur."
  },
  {
    "title": "rowCount",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417codedescriptor/2875199-rowcount",
    "html": "See Also\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload containing the data encoded in the PDF417 code.\nvar isCompact: Bool\nA boolean value telling if the PDF417 code is compact.\nvar columnCount: Int\nThe number of columns in the PDF417 code."
  },
  {
    "title": "multiplyBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228370-multiplyblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the multiply-blend mode filter to an image. The effect calculates the colors in the output image by multiplying the color values for the input and background images, resulting in a darker image.\n\nThe multiply-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the image becoming darker with less saturation:\n\nfunc multiplyBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.multiplyBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "sourceAtopCompositingFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228409-sourceatopcompositingfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the source-atop compositing filter to an image. The effect creates the result by overlaying the input image over the background image. The filter then removes the area that doesn’t overlap with the background image.\n\nThe source-atop compositing filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in an output image that shows the background image with the portion of the input image that overlaps it:\n\nfunc sourceAtopCompositing(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.sourceAtopCompositing()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "sourceOutCompositingFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228411-sourceoutcompositingfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the source-out compositing filter to an image. The effect creates the result by overlaying the input image over the background image. The filter then removes the overlapping area of the background image from the result.\n\nThe source-out compositing filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in an output image that shows the portion of the background image that doesn’t overlap with the input image:\n\nfunc sourceOutCompositing(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.sourceOutCompositing()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "subtractBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228418-subtractblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the subtract-blend mode filter to an image. The effect calculates the colors in the output image by subtracting the color values that differ between the background image and the input image.\n\nThe subtract-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the image becoming darker with less detail:\n\nfunc subtractBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.subtractBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image."
  },
  {
    "title": "sourceInCompositingFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228410-sourceincompositingfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the source-in compositing filter to an image. The effect creates the result by overlaying the input image over the background image. The filter then removes the non-overlapping area of both images.\n\nThe source-in compositing filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in an output image that shows the portion of the background image that overlaps with the input:\n\nfunc sourceInCompositing(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.sourceInCompositing()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "localizedReferenceDocumentationForFilterName:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437642-localizedreferencedocumentationf",
    "html": "Parameters\nfilterName\n\nThe filter name.\n\nReturn Value\n\nA URL that specifies the location of the localized documentation, or nil if the filter does not provide localized reference documentation.\n\nDiscussion\n\nThe URL can be a local file or a remote document on a web server. Because filters created prior to OS X v10.5 could return nil, you should be make sure that your code handles this case gracefully.\n\nSee Also\nGetting Localized Information for Registered Filters\n+ localizedNameForFilterName:\nReturns the localized name for the specified filter name.\n+ localizedNameForCategory:\nReturns the localized name for the specified filter category.\n+ localizedDescriptionForFilterName:\nReturns the localized description of a filter for display in the user interface."
  },
  {
    "title": "softLightBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228408-softlightblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the soft-light blend mode filter to an image. The effect calculates the brightness of the colors in the background image. Colors that are lighter than 50 percent gray become lighter, while the filter further darkens colors that are darker than 50 percent. The filter then uses the calculated result to create the output image.\n\nThe soft-light blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the image becoming slightly darker with more saturation and the colors of the gradient image:\n\nfunc softLightBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.softLightBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "perspectiveTransformFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228382-perspectivetransformfilter",
    "html": "Return Value\n\nThe adjusted image.\n\nDiscussion\n\nThis method applies the perspective transform filter to an image. The effect alters the geometry of an image to simulate the observer changing viewing position. You can use the perspective filter to skew an image.\n\nThe perspective transform filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ntopLeft\n\nA CGPoint in the input image mapped to the top-left corner of the output image.\n\ntopRight\n\nA CGPoint in the input image mapped to the top-right corner of the output image.\n\nbottomLeft\n\nA CGPoint in the input image mapped to the bottom-left corner of the output image.\n\nbottomRight\n\nA CGPoint in the input image mapped to the bottom-right corner of the output image.\n\nThe following code creates a filter that changes the perspective of the input image:\n\nfunc perspectiveTransform(inputImage: CIImage) -> CIImage {\n    let perspectiveTransformFilter = CIFilter.perspectiveTransform()\n    perspectiveTransformFilter.inputImage = inputImage\n    perspectiveTransformFilter.topLeft = CGPoint(x: 100, y: 3984)\n    perspectiveTransformFilter.topRight = CGPoint(x: 3732, y: 3025)\n    perspectiveTransformFilter.bottomLeft = CGPoint(x: 0, y: 500)\n    perspectiveTransformFilter.bottomRight = CGPoint(x: 4032, y: 120)\n    return perspectiveTransformFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bicubicScaleTransformFilter\nProduces a high-quality scaled version of an image.\n+ edgePreserveUpsampleFilter\nCreates a high-quality upscaled image.\n+ keystoneCorrectionCombinedFilter\nAdjusts the image vertically and horizontally to remove distortion.\n+ keystoneCorrectionHorizontalFilter\nHorizontally adjusts an image to remove distortion.\n+ keystoneCorrectionVerticalFilter\nVertically adjusts an image to remove distortion.\n+ lanczosScaleTransformFilter\nCreates a high-quality, scaled version of a source image.\n+ perspectiveCorrectionFilter\nTransforms an image’s perspective.\n+ perspectiveRotateFilter\nRotates an image in a 3D space.\n+ perspectiveTransformWithExtentFilter\nAlters an image’s geometry to adjust the perspective while applying constraints.\n+ straightenFilter\nRotates and crops an image."
  },
  {
    "title": "colorCurvesFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228290-colorcurvesfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the color curves filter to an image. The effect uses a three-channel one-dimensional color table to transform the source image pixels. The color table must be comprised of floating-point RGB value.\n\nThe color curves filter uses the following properties:\n\ncolorSpace\n\nA CGColorSpaceRef representing the color space for the color curve.\n\ncurvesData\n\nData containing a color table of floating-point RGB values as NSData.\n\ncurvesDomain\n\nA two-element vector that defines the minimum and maximum values of the curve data as a CIVector.\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds brightness to the input image:\n\nfunc colorCurves(inputImage: CIImage) -> CIImage {\n    let colorCurvesEffect = CIFilter.colorCurves()\n    colorCurvesEffect.inputImage = inputImage\n    colorCurvesEffect.curvesDomain = CIVector(x: 0, y: 1)\n    colorCurvesEffect.curvesData = Data(\n        bytes: [Float32]([\n            0.0,0.0,0.0,\n            0.8,0.8,0.8,\n            1.0,1.0,1.0\n        ]), count: 36)\n    colorCurvesEffect.colorSpace = CGColorSpaceCreateDeviceRGB()\n    return colorCurvesEffect.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "colorCubesMixedWithMaskFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228289-colorcubesmixedwithmaskfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the color cubes mixed with mask filter to an image. The effect uses two color cube tables to modify the input image. The filter uses the mask image to interpolate between the two color cubes.\n\nThe color cubes mixed with mask filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nmaskImage\n\nA mask image with the type CIImage.\n\ncube0Data\n\nData containing a 3-dimensional color table of floating-point premultiplied RGBA values. The cells are organized in a standard ordering. The columns and rows of the data are indexed by red and green, respectively. Each data plane is followed by the next higher plane in the data, with planes indexed by blue.\n\ncube1Data\n\nData containing a 3-dimensional color table of floating-point premultiplied RGBA values. The cells are organized in a standard ordering. The columns and rows of the data are indexed by red and green, respectively. Each data plane is followed by the next higher plane in the data, with planes indexed by blue.\n\ncolorSpace\n\nA CGColorSpaceRef representing the color space for the color cubes.\n\ncubeDimension\n\nThe dimension of the color cubes\n\nThe following code creates a filter that adds colors from the mask image and brightness to the input image:\n\nfunc colorCube(inputImage: CIImage, maskImage: CIImage, cube0Data: Data, cube1Data: Data) -> CIImage {\n    let colorCubeEffect = CIFilter.colorCubesMixedWithMask()\n    colorCubeEffect.inputImage = inputImage\n    colorCubeEffect.colorSpace = CGColorSpaceCreateDeviceRGB()\n    colorCubeEffect.cube0Data = cube1Data\n    colorCubeEffect.cube1Data = cube0Data\n    colorCubeEffect.maskImage = maskImage\n    colorCubeEffect.cubeDimension = 4\n    return colorCubeEffect.outputImage!\n}\nvar colorCube0Data: [Float32] = []\nvar colorCube1Data: [Float32] = []\nlet size = 4\nlet step = 1.0 / Float(size - 1)\nfor b in 0..<size {\n    for g in 0..<size {\n        for r in 0..<size {\n            // Calculate the normalized color component values.\n            let redNormalised = Float32(r) * step\n            let greenNormalised = Float32(g) * step\n            let blueNormalised = Float32(b) * step\n            let alpha: Float = 1.0\n            colorCube0Data.append(contentsOf: [redNormalised*1.2, greenNormalised*0.8, blueNormalised*1.2, alpha])\n            colorCube1Data.append(contentsOf: [redNormalised*1.2, greenNormalised*1.15, blueNormalised*0.9, alpha])\n        }\n    }\n}\nlet cube0Data = Data(bytes:colorCube0Data, count: colorCube0Data.count*4)\nlet cube1Data = Data(bytes:colorCube1Data, count: colorCube1Data.count*4)\nlet maskImage = CIFilter.linearGradient()\nmaskImage.color0 = CIColor(red: 1.0, green: 1.0, blue: 1.0, alpha: 1.0)\nmaskImage.color1 = CIColor(red: 0.0, green: 0.0, blue: 0.0, alpha: 0.0)\nmaskImage.point0 = CGPoint(x:0, y:0)\nmaskImage.point1 = CGPoint(x: ciImage.extent.width, y: 0)\nlet result = colorCube(inputImage: ciImage, maskImage: maskImage.outputImage!, cube0Data: cube0Data, cube1Data: cube1Data)\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "colorCubeWithColorSpaceFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228288-colorcubewithcolorspacefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the color cube with color space filter to an image. The effect applies a mapping from rgb space within the color space defined to color values the cubeData defines. For each pixel, it matches the data and adjusts the color on the output image.\n\nThe color cube with color space filter uses the following properties:\n\ncubeData\n\nData containing a 3-dimensional color table of floating-point premultiplied RGBA values. The cells are organized in a standard ordering. The columns and rows of the data are indexed by red and green, respectively. Each data plane is followed by the next higher plane in the data, with planes indexed by blue.\n\ncolorSpace\n\nA CGColorSpaceRef representing the color space for the color cube.\n\ncubeDimension\n\nThe dimension of the color cube.\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds brightness to the input image:\n\nfunc colorCube(inputImage: CIImage, cubeData: Data) -> CIImage {\n    let colorCubeEffect = CIFilter.colorCubeWithColorSpace()\n    colorCubeEffect.inputImage = inputImage\n    colorCubeEffect.colorSpace = CGColorSpaceCreateDeviceRGB()\n    colorCubeEffect.cubeData = cubeData\n    colorCubeEffect.cubeDimension = 4\n    return colorCubeEffect.outputImage!\n}\n// Create a color cube with size 4.\nvar colorCubeData: [Float32] = []\nlet size = 4\nlet step = 1.0 / Float32(size - 1)\nfor b in 0..<size {\n    for g in 0..<size {\n        for r in 0..<size {\n            // Calculate the normalized color component values.\n            let redNormalised = Float32(r) * step\n            let greenNormalised = Float32(g) * step\n            let blueNormalised = Float32(b) * step\n            let red = pow(redNormalised, 0.5)\n            let green = pow(greenNormalised, 0.5)\n            let blue = pow(blueNormalised, 0.5)\n            let alpha: Float = 1.0\n            colorCubeData.append(contentsOf: [red, green, blue, alpha])\n        }\n    }\n}\nlet cubeData = Data(bytes:colorCubeData, count: colorCubeData.count*4)\nlet result = colorCube(inputImage: ciImage, cubeData: cubeData)\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "filterWithImageData:options:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437879-filterwithimagedata",
    "html": "Deprecated\n\nUse filterWithImageData:identifierHint: instead.\n\nParameters\ndata\n\nThe RAW image data to initialize the object with.\n\noptions\n\nAn options dictionary.\n\nReturn Value\n\nA CIFilter object.\n\nDiscussion\n\nYou can pass any of the keys defined in RAW Image Options along with the appropriate value in options. You should provide a source type identifier hint key (kCGImageSourceTypeIdentifierHint) and the appropriate source type value to help the decoder determine the file type. Otherwise it’s possible to obtain incorrect results.\n\nThe first step when working with RAW images in Core Image is to process the image using either filterWithImageData:options: or filterWithImageURL:options:. These initializers create a CIFilter object with an outputImage which is a CIImage representation of the supplied RAW image. You can process After calling this method, the CIFilter object returns a CIImage object that’s properly processed similar to images retrieved using the outputImage key.\n\nImportant\n\nCore Image doesn’t process the supplied RAW image until the filter’s outputImage is rendered. For this reason, if you supply this initializer with a RAW image of an unsupported format, the filter object will be initialized but its outputImage will be nil.\n\nSee Also\nCreating a Filter from a RAW Image\n+ filterWithCVPixelBuffer:properties:options:\nCreates a filter from a Core Video pixel buffer.\nDeprecated\n+ filterWithImageURL:options:\nCreates a filter that allows the processing of RAW images.\nDeprecated"
  },
  {
    "title": "filterWithImageURL:options:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438096-filterwithimageurl",
    "html": "Deprecated\n\nUse filterWithImageURL: instead.\n\nParameters\nurl\n\nThe location of a RAW image file.\n\noptions\n\nAn options dictionary. You can pass any of the keys defined in RAW Image Options.\n\nReturn Value\n\nA CIFilter object.\n\nDiscussion\n\nThe first step when working with RAW images in Core Image is to process the image using either filterWithImageData:options: or filterWithImageURL:options:. These initializers create a CIFilter object with an outputImage which is a CIImage representation of the supplied RAW image.\n\nThe newly created filter object allows you fine control over the image processing that isn’t available when working with processed images such a JPEG. The following listing shows how to create a Core Image filter based on a URL named imageURL. The image is processed so that its neutral temperature is set to 2,000 Kelvin (giving a blue tint) and its baseline exposure doubled. Finally, a Core Image vignette filter is applied to the processed image in the same way it would be with any other source image:\n\nListing 1 Processing a RAW image.\nlet rawFilter = CIFilter(imageURL: imageURL, options: nil)\nrawFilter?.setValue(2000,    \n                    forKey: kCIInputNeutralTemperatureKey)\nif let baselineExposure = rawFilter?.value(forKey: kCIInputBaselineExposureKey) as? NSNumber {    \n    rawFilter?.setValue(baselineExposure.doubleValue * 2.5,                        forKey: kCIInputBaselineExposureKey)\n}\nlet vignettedImage = rawFilter?.outputImage?.applyingFilter(    \n    \"CIVignette\",    \n    withInputParameters: [kCIInputIntensityKey: 5])\nif let outputImage = vignettedImage {    \n    imageView.image = UIImage(ciImage: outputImage)\n}\n\n\nImportant\n\nCore Image doesn’t process the supplied RAW image until the filter’s outputImage is rendered. For this reason, if you supply this initializer with a RAW image of an unsupported format, the filter object will be initialized but its outputImage will be nil.\n\nSee Also\nCreating a Filter from a RAW Image\n+ filterWithCVPixelBuffer:properties:options:\nCreates a filter from a Core Video pixel buffer.\nDeprecated\n+ filterWithImageData:options:\nCreates a filter that allows the processing of RAW images.\nDeprecated"
  },
  {
    "title": "filterNamesInCategories:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437595-filternamesincategories",
    "html": "Parameters\ncategories\n\nOne or more of the filter category keys defined in Filter Category Keys. Pass nil to get all filters in all categories.\n\nReturn Value\n\nAn array that contains all published filter names that match all the categories specified by the categories argument.\n\nDiscussion\n\nWhen you pass more than one filter category, this method returns the intersection of the filters in the categories. For example, if you pass the categories kCICategoryBuiltIn and kCICategoryColorAdjustment, you obtain all the filters that are members of both the built-in and color adjustment categories. But if you pass in kCICategoryGenerator and kCICategoryStylize, you will not get any filters returned to you because there are no filters that are members of both the generator and stylize categories. If you want to obtain all stylize and generator filters, you must call the filterNamesInCategories: method for each category separately and then merge the results.\n\nSee Also\nAccessing Registered Filters\n+ filterNamesInCategory:\nReturns an array of all published filter names in the specified category."
  },
  {
    "title": "filterNamesInCategory:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438145-filternamesincategory",
    "html": "Parameters\ncategory\n\nA string object that specifies one of the filter categories defined in Filter Category Keys.\n\nReturn Value\n\nAn array that contains all published names of the filter in a category.\n\nSee Also\nAccessing Registered Filters\n+ filterNamesInCategories:\nReturns an array of all published filter names that match all the specified categories."
  },
  {
    "title": "registerFilterName:constructor:classAttributes:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437889-registerfiltername",
    "html": "Parameters\nname\n\nA string object that specifies the name of the filter you want to publish.\n\nanObject\n\nA constructor object that implements the filterWithName method.\n\nattributes\n\nA dictionary that contains the class display name and filter categories attributes along with the appropriate value for each attributes. That is, the kCIAttributeFilterDisplayName attribute and a string that specifies the display name, and the kCIAttributeFilterCategories and an array that specifies the categories to which the filter belongs (such as kCICategoryStillImage and kCICategoryDistortionEffect). All other attributes for the filter should be returned by the custom attributes method implement by the filter.\n\nDiscussion\n\nIn most cases you don’t need to use this method because the preferred way to register a custom filter that you write is to package it as an image unit. You do not need to use this method for a filter packaged as an image unit because you register your filter using the CIPlugInRegistration protocol. (See Core Image Programming Guide for additional details.)"
  },
  {
    "title": "apply:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1562058-apply",
    "html": "Parameters\nk\n\nA CIKernel object that contains a kernel function.\n\n...\n\nA list of arguments to supply to the kernel function. The supplied arguments must be type-compatible with the function signature of the kernel function. The list of arguments must be terminated by the nil object.\n\nDiscussion\n\nIf you are implementing a custom filter, this method needs to be called from within the outputImage method in order to apply your kernel function to the CIImage object. For example, if the kernel function has this signature:\n\nkernel vec4 brightenEffect (sampler src, float k)\n\n\nYou would supply two arguments after the k argument to the apply:k, .. method. In this case, the first argument must be a sampler and the second a floating-point value. For more information on kernels, see Core Image Kernel Language Reference.\n\nSee Also\nApplying a Filter\n- apply:arguments:options:\nProduces a CIImage object by applying arguments to a kernel function and using options to control how the kernel function is evaluated."
  },
  {
    "title": "localizedNameForFilterName:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437697-localizednameforfiltername",
    "html": "Parameters\nfilterName\n\nA filter name.\n\nReturn Value\n\nThe localized name for the filter.\n\nSee Also\nGetting Localized Information for Registered Filters\n+ localizedNameForCategory:\nReturns the localized name for the specified filter category.\n+ localizedDescriptionForFilterName:\nReturns the localized description of a filter for display in the user interface.\n+ localizedReferenceDocumentationForFilterName:\nReturns the location of the localized reference documentation that describes the filter."
  },
  {
    "title": "enabled",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438276-enabled",
    "html": "Discussion\n\nThe filter is applied to its input when this property is set to YES (the default).\n\nUse this property in conjunction with the setName: property when attaching filters to Core Animation layers and accessing or animating filter properties through key-value animations. Core Animation can animate this property on a layer.\n\nSee Also\nGetting Filter Parameters and Attributes\n- setName:\nA name associated with a filter.\nattributes\nA dictionary of key-value pairs that describe the filter.\ninputKeys\nThe names of all input parameters to the filter.\noutputKeys\nThe names of all output parameters from the filter.\noutputImage\nReturns a CIImage object that encapsulates the operations configured in the filter."
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/cirippletransition/3228693-extent",
    "html": "Required"
  },
  {
    "title": "CISwipeTransition",
    "url": "https://developer.apple.com/documentation/coreimage/ciswipetransition",
    "html": "Topics\nInstance Properties\nangle\nThe angle of the swipe.\n\nRequired\n\ncolor\nThe color of the swipe.\n\nRequired\n\nextent\nThe extent of the effect.\n\nRequired\n\nopacity\nThe opacity of the swipe.\n\nRequired\n\nwidth\nThe width of the swipe.\n\nRequired\n\nRelationships\nInherits From\nCITransitionFilter\nSee Also\nRelated Documentation\n+ swipeTransitionFilter\nGradually transitions from one image to another with a swiping motion."
  },
  {
    "title": "localizedNameForCategory:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438057-localizednameforcategory",
    "html": "Parameters\ncategory\n\nA filter category.\n\nReturn Value\n\nThe localized name for the filter category.\n\nSee Also\nGetting Localized Information for Registered Filters\n+ localizedNameForFilterName:\nReturns the localized name for the specified filter name.\n+ localizedDescriptionForFilterName:\nReturns the localized description of a filter for display in the user interface.\n+ localizedReferenceDocumentationForFilterName:\nReturns the location of the localized reference documentation that describes the filter."
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cirippletransition/3228696-width",
    "html": "Required"
  },
  {
    "title": "waitUntilCompleted()",
    "url": "https://developer.apple.com/documentation/coreimage/cirendertask/2881294-waituntilcompleted",
    "html": "Parameters\nerror\n\nnil unless the render task failed\n\nDiscussion\n\nSynchronously blocks execution until the CIRenderTask either completes or fails (with error). Calling this method after startTask(toRender:to:) or startTask(toRender:from:to:at:) makes the render task behave synchronously, as if the CPU and GPU were operating as a single unit."
  },
  {
    "title": "CIBarsSwipeTransition",
    "url": "https://developer.apple.com/documentation/coreimage/cibarsswipetransition",
    "html": "Topics\nInstance Properties\nangle\nThe angle, in radians, of the bars.\n\nRequired\n\nbarOffset\nThe offset of one bar with respect to another.\n\nRequired\n\nwidth\nThe width of each bar.\n\nRequired\n\nRelationships\nInherits From\nCITransitionFilter\nSee Also\nRelated Documentation\n+ barsSwipeTransitionFilter\nTransitions between two images by removing rectangular portions of an image."
  },
  {
    "title": "CITransitionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/citransitionfilter",
    "html": "Topics\nInstance Properties\ninputImage\nThe image to use as an input image.\n\nRequired\n\ntargetImage\nThe target image for a transition.\n\nRequired\n\ntime\nThe parametric time of the transition.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nInherited By\nCIAccordionFoldTransition\nCIBarsSwipeTransition\nCICopyMachineTransition\nCIDisintegrateWithMaskTransition\nCIDissolveTransition\nCIFlashTransition\nCIModTransition\nCIPageCurlTransition\nCIPageCurlWithShadowTransition\nCIRippleTransition\nCISwipeTransition"
  },
  {
    "title": "localizedDescriptionForFilterName:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437591-localizeddescriptionforfilternam",
    "html": "Parameters\nfilterName\n\nThe filter name.\n\nReturn Value\n\nThe localized description of the filter.\n\nSee Also\nGetting Localized Information for Registered Filters\n+ localizedNameForFilterName:\nReturns the localized name for the specified filter name.\n+ localizedNameForCategory:\nReturns the localized name for the specified filter category.\n+ localizedReferenceDocumentationForFilterName:\nReturns the location of the localized reference documentation that describes the filter."
  },
  {
    "title": "init(source:)",
    "url": "https://developer.apple.com/documentation/coreimage/cikernel/1437796-init",
    "html": "Parameters\nstring\n\nA program in the Core Image Kernel Language that contains a single routine marked using the kernel keyword.\n\nReturn Value\n\nA new kernel object. The class of the returned object can be CIKernel, CIColorKernel, or CIWarpKernel depending on the type of routine specified in the Core Image Kernel Language source code string.\n\nDiscussion\n\nThe Core Image Kernel Language is a dialect of the OpenGL Shading Language. See Core Image Kernel Language Reference and Core Image Programming Guide for more details.\n\nSee Also\nCreating a Kernel Using Core Image Kernel Language\nclass func makeKernels(source: String) -> [CIKernel]?\nCreates and returns and array of CIKernel objects.\nDeprecated\nRelated Documentation\nCore Image Programming Guide\nCore Image Kernel Language Reference"
  },
  {
    "title": "straightenFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228416-straightenfilter",
    "html": "Return Value\n\nThe adjusted image.\n\nDiscussion\n\nThis method applies the straighten filter to an image. The effect rotates the image based on the angle property while cropping and scaling the image to remain the same size as the original image.\n\nThe straighten filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nangle\n\nA float representing the angle to rotate the image as an NSNumber.\n\nThe following code creates a filter that rotates the image 135 degrees:\n\nfunc straighten(inputImage: CIImage) -> CIImage {\n    let straightenFilter = CIFilter.straighten()\n    straightenFilter.inputImage = inputImage\n    straightenFilter.angle = 135\n    return straightenFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bicubicScaleTransformFilter\nProduces a high-quality scaled version of an image.\n+ edgePreserveUpsampleFilter\nCreates a high-quality upscaled image.\n+ keystoneCorrectionCombinedFilter\nAdjusts the image vertically and horizontally to remove distortion.\n+ keystoneCorrectionHorizontalFilter\nHorizontally adjusts an image to remove distortion.\n+ keystoneCorrectionVerticalFilter\nVertically adjusts an image to remove distortion.\n+ lanczosScaleTransformFilter\nCreates a high-quality, scaled version of a source image.\n+ perspectiveCorrectionFilter\nTransforms an image’s perspective.\n+ perspectiveRotateFilter\nRotates an image in a 3D space.\n+ perspectiveTransformFilter\nAlters an image’s geometry to adjust the perspective.\n+ perspectiveTransformWithExtentFilter\nAlters an image’s geometry to adjust the perspective while applying constraints."
  },
  {
    "title": "CIImageRepresentationOption",
    "url": "https://developer.apple.com/documentation/coreimage/ciimagerepresentationoption",
    "html": "Topics\nInitializers\ninit(rawValue: String)\nType Properties\nstatic let avDepthData: CIImageRepresentationOption\noptions dictionary key for image export methods to represent data as AVDepthData.\nstatic let avPortraitEffectsMatte: CIImageRepresentationOption\nstatic let avSemanticSegmentationMattes: CIImageRepresentationOption\nstatic let depthImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output depth data.\nstatic let disparityImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output disparity data.\nstatic let portraitEffectsMatteImage: CIImageRepresentationOption\nstatic let semanticSegmentationGlassesMatteImage: CIImageRepresentationOption\nstatic let semanticSegmentationHairMatteImage: CIImageRepresentationOption\nstatic let semanticSegmentationSkinMatteImage: CIImageRepresentationOption\nstatic let semanticSegmentationSkyMatteImage: CIImageRepresentationOption\nstatic let semanticSegmentationTeethMatteImage: CIImageRepresentationOption\nRelationships\nConforms To\nEquatable\nHashable\nRawRepresentable\nSendable"
  },
  {
    "title": "luminosityBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228353-luminosityblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the luminosity-blend mode filter to an image. The effect creates the output image by using the hue and saturation values of the background image while using the luminance values of the input image.\n\nThe luminosity-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the image having accurate gray colors while other colors are added from the background image:\n\nfunc luminosityBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.luminosityBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "screenBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228401-screenblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the screen-blend mode filter to an image. The effect calculates the colors in the output image by multiplying the inverse color values for the input and background images, resulting in a brighter image.\n\nThe screen-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the image becoming lighter with colors from the input and background image:\n\nfunc screenBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let screenBlendMode = CIFilter.screenBlendMode()\n    screenBlendMode.inputImage = inputImage\n    screenBlendMode.backgroundImage = backgroundImage\n    return screenBlendMode.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "saturationBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228400-saturationblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the saturation-blend mode filter to an image. The effect uses the values of the hue and luminance from the background image with the saturation of the input image to produce the output.\n\nThe saturation-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the image taking on the colors of the background image with low saturated values becoming gray.\n\nfunc saturationBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.saturationBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "minimumCompositingFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228361-minimumcompositingfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the minimum compositing filter to an image. The effect calculates the minimum value for each color component in the input and background images. The filter then uses the resulting color to create the output image.\n\nThe minimum compositing filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the output image becoming brighter with both images’ colors:\n\nfunc minimumCompositing(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.minimumCompositing()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "pinLightBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228392-pinlightblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the pin-light-blend mode filter to an image. The effect calculates the brightness of the colors in the background image. Colors that are lighter than 50 percent gray remain unchanged, while the filter replaces colors that are darker than 50 percent with the corresponding color values of the input image. The effect then uses the calculated result to create the output image.\n\nThe pin-light-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the image becoming darker with more saturation and the colors of the gradient image:\n\nfunc pinLightBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.pinLightBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "maximumCompositingFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228357-maximumcompositingfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the maximum compositing filter to an image. The effect calculates the maximum value for each color component in the input and background images. The filter then uses the resulting color to create the output image.\n\nThe maximum compositing filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in an image with a mixture of images’ colors:\n\nfunc maximumCompositing(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.maximumCompositing()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "linearLightBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3801604-linearlightblendmodefilter",
    "html": "Return Value\n\nThe blended image as a CIImage.\n\nDiscussion\n\nThe linear-light blend mode combines the linear-dodge and linear-burn blend modes (rescaled so that neutral colors become middle gray). If the input image’s values are lighter than middle gray, the filter uses dodge; for darker values, the filter uses burn.\n\ninputImage\n\nA CIImage containing the input image\n\nbackgroundImage\n\nA CIImage containing the background image.\n\nThe following code sample applies the linear-light blend mode filter to two images:\n\nfunc linearLightBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let filter = CIFilter.linearLightBlendMode()\n    filter.inputImage = inputImage\n    filter.backgroundImage = backgroundImage\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "linearDodgeBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228350-lineardodgeblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the linear-dodge-blend mode filter to an image. The effect calculates the brightness value for the background image then brightens it to reflect the brightness of the input image. The effect then increases the contrast in the output image.\n\nThe linear-dodge-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the image becoming brighter with both images’ colors:\n\nfunc linearDodgeBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.linearDodgeBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "perspectiveTransformWithExtentFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228383-perspectivetransformwithextentfi",
    "html": "Return Value\n\nThe adjusted image.\n\nDiscussion\n\nThis method applies the perspective transform with extent filter to an image. The effect alters the geometry of an image to simulate the observer changing viewing position. The extent filter crops the image within the bounds specified. You can use the perspective filter to skew an image.\n\nThe perspective transform with extent filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ntopLeft\n\nA CGPoint in the input image mapped to the top-left corner of the output image.\n\ntopRight\n\nA CGPoint in the input image mapped to the top-right corner of the output image.\n\nbottomLeft\n\nA CGPoint in the input image mapped to the bottom-left corner of the output image.\n\nbottomRight\n\nA CGPoint in the input image mapped to the bottom-right corner of the output image.\n\nextent\n\nA CGRect representing the dimensions of the output image.\n\nThe following code creates a filter that changes the perspective of the input image:\n\nfunc perspectiveTransformWithExtent(inputImage: CIImage) -> CIImage {\n    let perspectiveTransformFilter = CIFilter.perspectiveTransformWithExtent()\n    perspectiveTransformFilter.inputImage = inputImage\n    perspectiveTransformFilter.topLeft = CGPoint(x: 100, y: 3984)\n    perspectiveTransformFilter.topRight = CGPoint(x: 3732, y: 3025)\n    perspectiveTransformFilter.bottomLeft = CGPoint(x: 0, y: 500)\n    perspectiveTransformFilter.bottomRight = CGPoint(x: 4032, y: 120)\n    perspectiveTransformFilter.extent = CGRect(x: 0, y: 0, width: 3800, height: 3200)\n    return perspectiveTransformFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bicubicScaleTransformFilter\nProduces a high-quality scaled version of an image.\n+ edgePreserveUpsampleFilter\nCreates a high-quality upscaled image.\n+ keystoneCorrectionCombinedFilter\nAdjusts the image vertically and horizontally to remove distortion.\n+ keystoneCorrectionHorizontalFilter\nHorizontally adjusts an image to remove distortion.\n+ keystoneCorrectionVerticalFilter\nVertically adjusts an image to remove distortion.\n+ lanczosScaleTransformFilter\nCreates a high-quality, scaled version of a source image.\n+ perspectiveCorrectionFilter\nTransforms an image’s perspective.\n+ perspectiveRotateFilter\nRotates an image in a 3D space.\n+ perspectiveTransformFilter\nAlters an image’s geometry to adjust the perspective.\n+ straightenFilter\nRotates and crops an image."
  },
  {
    "title": "colorMonochromeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228295-colormonochromefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the color monochrome filter to an image. The effect remaps the colors of the image to shades of the specified color.\n\nThe color monochrome filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncolor\n\nThe color to map the input image colors to, as a CIColor.\n\nintensity\n\nA float representing the desired strength of the effect as an NSNumber.\n\nThe following code creates a filter that results in the colors of the image becoming shades of red:\n\nfunc colorMonochrome(inputImage: CIImage) -> CIImage {\n    let colorMonochromeFilter = CIFilter.colorMonochrome()\n    colorMonochromeFilter.inputImage = inputImage\n    colorMonochromeFilter.color = CIColor(red: 1, green: 0, blue: 0)\n    colorMonochromeFilter.intensity = 1\n    return colorMonochromeFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "colorInvertFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228292-colorinvertfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a preconfigured set of effects that invert the colors of an image.\n\nThe color-invert filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that inverts the colors of the input image:\n\nfunc colorInvert(inputImage: CIImage) -> CIImage {\n\n\n    let colorInvertFilter = CIFilter.colorInvert()\n    colorInvertFilter.inputImage = inputImage\n    return colorInvertFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "colorPosterizeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228297-colorposterizefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the color posterize filter to an image. The effect remaps red, green, and blue color components to a specified brightness value. The effect mimics the look of a silk-screened poster.\n\nThe color posterize filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nlevels\n\nA float representing the brightness level as an NSNumber.\n\nThe following code creates a filter that flattens the colors in the input image:\n\nfunc colorPosterize(inputImage: CIImage) -> CIImage {\n    let colorPosterizeFilter = CIFilter.colorPosterize()\n    colorPosterizeFilter.inputImage = inputImage\n    colorPosterizeFilter.levels = 6\n    return colorPosterizeFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "colorMapFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228293-colormapfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a color map filter to an image. The effect transforms source color values by converting the unpremultiplied RGB values to luma using the weighting (0.2125, 0.7154, 0.0721). The luma value is then used to look up the new color from the gradient image.\n\nThe color map filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ngradientImage\n\nAn image representing the gradient of colors to be mapped to the input image colors with the type CIImage.\n\nThe following code creates a filter that adds the gradient image colors to the input image:\n\nfunc colorMap(inputImage: CIImage, gradientImage: CIImage) -> CIImage {\n    let colorMap = CIFilter.colorMap()\n    colorMap.inputImage = inputImage\n    colorMap.gradientImage = gradientImage\n    return colorMap.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "convertLabToRGBFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/4018377-convertlabtorgbfilter",
    "html": "Return Value\n\nThe converted CIImage.\n\nDiscussion\n\nThis filter converts an image from CIELAB color space to RGB. The CIELAB color space expresses color as three values: L* for the perceptual lightness, and a*b* for the colors red, green, blue, and yellow. The RGB color space expresses colors using the intensities of the three primary colors: red, green, and blue.\n\ninputImage\n\nA CIImage containing the RGB image.\n\nnormalize\n\nIf true, the three input channels are in the range 0 to 1. If false, the L* channel is in the range 0 to 100, and the a*b* channels are in the range -128 to 128.\n\nThe following code applies the convertLabToRGBFilter to an image with the normalize flag set to the true:\n\nfunc convertLabToRGB(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.convertLabToRGB()\n    filter.inputImage = inputImage\n    filter.normalize = true\n    return filter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "convertRGBtoLabFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/4018378-convertrgbtolabfilter",
    "html": "Return Value\n\nThe converted CIImage.\n\nDiscussion\n\nThis filter converts an image from RGB to CIELAB color space. The CIELAB color space expresses color as three values: L* for the perceptual lightness, and a*b* for the colors red, green, blue, and yellow. The RGB color space expresses colors using the intensities of the three primary colors: red, green, and blue.\n\ninputImage\n\nA CIImage containing the RGB image.\n\nnormalize\n\nIf true, the three output channels are in the range 0 to 1. If false, the L* channel is in the range 0 to 100 and the a*b* channels are in the range -128 to 128.\n\nThe following code applies the convertRGBToLabFilter to an image with the normalize flag set to the true:\n\nfunc convertRGBToLab(inputImage: CIImage) -> CIImage {\n    let convertRGBToLabFilter = CIFilter.convertRGBtoLab()\n    convertRGBToLabFilter.inputImage = inputImage\n    convertRGBToLabFilter.normalize = true\n    return convertRGBToLabFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "LabDeltaE",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228260-labdeltae",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the Lab ΔE filter to an image. The effect creates an image based on the visual color differences between the two input images. The resulting image contains ΔE 1994 values between 0.0 and 100.0.\n\nThe Lab ΔE filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nimage2\n\nAn image with the type CIImage the system uses for comparison.\n\nThe following code creates a filter that removes the background from the input image:\n\nfunc labDeltaE(inputImage: CIImage, inputImage2: CIImage) -> CIImage {\n    let labDeltaEFilter = CIFilter.labDeltaE()\n    labDeltaEFilter.inputImage = inputImage\n    labDeltaEFilter.image2 = inputImage2\n    return labDeltaEFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "linearBurnBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228349-linearburnblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the linear-burn blend mode filter to an image. The effect calculates the brightness value for the background image then darkens it to reflect the brightness of the input image. The effect then decreases the contrast in the output image.\n\nThe linear-burn blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in an output image that’s much darker with very little visible detail:\n\nfunc linearBurnBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.linearBurnBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "filterWithCVPixelBuffer:properties:options:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/2138288-filterwithcvpixelbuffer",
    "html": "Deprecated\n\nUse filterWithCVPixelBuffer:properties: instead.\n\nParameters\npixelBuffer\n\nCVPixelBufferRef with one of the following RAW pixel format types:\n\nkCVPixelFormatType_14Bayer_GRBG\n\nkCVPixelFormatType_14Bayer_RGGB\n\nkCVPixelFormatType_14Bayer_BGGR\n\nkCVPixelFormatType_14Bayer_GBRG\n\nproperties\n\nA properties dictionary. Defines the properties of the pixel buffer.\n\noptions\n\nAn options dictionary. You can pass any of the keys defined in RAW Image Options.\n\nReturn Value\n\nA CIFilter object.\n\nDiscussion\n\nThe first step when working with RAW images in Core Image is to process the image using either filterWithImageData:options: or filterWithImageURL:options:. These initializers create a CIFilter object with an outputImage which is a CIImage representation of the supplied RAW image.\n\nImportant\n\nCore Image doesn't process the supplied RAW image until the filter's outputImage is rendered. For this reason, if you supply this initializer with a RAW image of an unsupported format, the filter object will be initialized but its outputImage will be nil.\n\nSee Also\nCreating a Filter from a RAW Image\n+ filterWithImageData:options:\nCreates a filter that allows the processing of RAW images.\nDeprecated\n+ filterWithImageURL:options:\nCreates a filter that allows the processing of RAW images.\nDeprecated"
  },
  {
    "title": "CICMYKHalftone",
    "url": "https://developer.apple.com/documentation/coreimage/cicmykhalftone",
    "html": "Topics\nInstance Properties\nangle\nThe angle of the pattern.\n\nRequired\n\ncenter\nThe x and y position to use as the center of the halftone pattern.\n\nRequired\n\ngrayComponentReplacement\nThe gray component replacement value.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nsharpness\nThe sharpness of the pattern.\n\nRequired\n\nunderColorRemoval\nThe under color removal value.\n\nRequired\n\nwidth\nThe distance between dots in the pattern.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ CMYKHalftone\nAdds a series of colorful dots to an image."
  },
  {
    "title": "filterWithName:withInputParameters:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437894-filterwithname",
    "html": "Parameters\nname\n\nThe name of the filter. You must make sure the name is spelled correctly, otherwise your app will run but not produce any output images. For that reason, you should check for the existence of the filter after calling this method.\n\nparams\n\nA list of key-value pairs to set as input values to the filter. Each key is a constant that specifies the name of an input parameter for the filter, and the corresponding value is the value for that parameter. See Core Image Filter Reference for built-in filters and their allowed parameters.\n\nReturn Value\n\nA CIFilter object whose input values are initialized.\n\nDiscussion\n\nUse this method to quickly create and configure a CIFilter instance, as in the example below.\n\nCIFilter *f = [CIFilter filterWithName: @\"CIColorControls\"\n                   withInputParameters: @{\n                             @\"inputImage\"      : inImage,\n                             @\"inputSaturation\" : @0.5,\n                             @\"inputBrightness\" : @1.2,\n                             @\"inputContrast\"   : @1.3\n                                         }];\n\nSee Also\nCreating a Filter\n+ filterWithName:\nCreates a CIFilter object for a specific kind of filter.\n+ filterWithName:keysAndValues:\nCreates a CIFilter object for a specific kind of filter and initializes the input values with a nil-terminated list of arguments."
  },
  {
    "title": "filterWithName:keysAndValues:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1562057-filterwithname",
    "html": "Parameters\nname\n\nThe name of the filter. You must make sure the name is spelled correctly, otherwise your app will run but not produce any output images. For that reason, you should check for the existence of the filter after calling this method.\n\nkey0,...\n\nA list of key-value pairs to set as input values to the filter. Each key is a constant that specifies the name of the input value to set, and must be followed by a value. You signal the end of the list by passing a nil value.\n\nReturn Value\n\nA CIFilter object whose input values are initialized.\n\nDiscussion\n\nAs with all Objective-C methods that accept nil-terminated argument lists, to prevent unintended behavior you must take take care not to pass a nil value before the intended end of the argument list. You can avoid such issues by using the filterWithName:withInputParameters: method to create a filter, expressing the parameter list as a dictionary literal.\n\nSee Also\nCreating a Filter\n+ filterWithName:\nCreates a CIFilter object for a specific kind of filter.\n+ filterWithName:withInputParameters:\nCreates a CIFilter object for a specific kind of filter and initializes the input values."
  },
  {
    "title": "filterWithName:",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438255-filterwithname",
    "html": "Parameters\nname\n\nThe name of the filter. You must make sure the name is spelled correctly, otherwise your app will run but not produce any output images. For that reason, you should check for the existence of the filter after calling this method.\n\nReturn Value\n\nA CIFilter object whose input values are undefined.\n\nDiscussion\n\nIn macOS, after creating a filter with this method you must call setDefaults or set parameters individually by calling setValue:forKey:. In iOS, the filter’s parameters are automatically set to default values.\n\nSee Also\nCreating a Filter\n+ filterWithName:withInputParameters:\nCreates a CIFilter object for a specific kind of filter and initializes the input values.\n+ filterWithName:keysAndValues:\nCreates a CIFilter object for a specific kind of filter and initializes the input values with a nil-terminated list of arguments.\nRelated Documentation\nImage Unit Tutorial\nCore Image Filter Reference\nCore Image Programming Guide"
  },
  {
    "title": "lightenBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228346-lightenblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the lighten-blend mode filter to an image. The effect replaces any samples in the background image that are darker than input image.\n\nThe lighten-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the image that blends the input and background image colors:\n\nfunc lightenBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.lightenBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "morphologyRectangleMinimumFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228368-morphologyrectangleminimumfilter",
    "html": "Return Value\n\nThe blurred image.\n\nDiscussion\n\nThis method applies the morphology rectangle minimum filter to an image. The effect targets a rectangular section of the image, calculating the median color values to find colors that make up more than half the working area. Using this calculation, the effect reduces the pixels with contrasting colors to take up more of the less area. The effect is then repeated throughout the image.\n\nThe morphology rectangle minimum filter uses the following properties:\n\nwidth\n\nA float representing the width in pixels of the working area as an NSNumber.\n\nheight\n\nA float representing the height in pixels of the working area as an NSNumber.\n\ninputImage\n\nA CIImage representing the input image to apply the filter to.\n\nThe following code creates a filter that adds an intense blur to the palm trees input image:\n\n    func morphologyRectangleMinimum(inputImage: CIImage) -> CIImage? {\n\n\n        let morphologyRectangleMinimumFilter = CIFilter.morphologyRectangleMinimum()\n        morphologyRectangleMinimumFilter.inputImage = inputImage\n        morphologyRectangleMinimumFilter.width = 5\n        morphologyRectangleMinimumFilter.height = 5\n        return morphologyRectangleMinimumFilter.outputImage\n    }\n\n\nSee Also\nFilters\n+ bokehBlurFilter\nApplies a bokeh effect to an image.\n+ boxBlurFilter\nApplies a square-shaped blur to an area of an image.\n+ discBlurFilter\nApplies a circle-shaped blur to an area of an image.\n+ gaussianBlurFilter\nBlurs an image with a Gaussian distribution pattern.\n+ maskedVariableBlurFilter\nBlurs a specified portion of an image.\n+ medianFilter\nCalculates the median of an image to refine detail.\n+ morphologyGradientFilter\nDetects and highlights edges of objects.\n+ morphologyMaximumFilter\nBlurs a circular area by enlarging contrasting pixels.\n+ morphologyMinimumFilter\nBlurs a circular area by reducing contrasting pixels.\n+ morphologyRectangleMaximumFilter\nBlurs a rectangular area by enlarging contrasting pixels.\n+ motionBlurFilter\nCreates motion blur on an image.\n+ noiseReductionFilter\nReduces noise by sharpening the edges of objects.\n+ zoomBlurFilter\nCreates a zoom blur centered around a single point on the image."
  },
  {
    "title": "hueBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228341-hueblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the hue-blend mode filter to an image. The effect uses the values of the saturation and luminance from the background image with the hue of the input image to produce the output.\n\nThe hue-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that applies the hue-blend mode filter.\n\nfunc hueBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.hueBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "perspectiveRotateFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3325512-perspectiverotatefilter",
    "html": "Return Value\n\nThe adjusted image.\n\nDiscussion\n\nThis method applies the perspective rotate filter to an image. The effect rotates the image in 3D space to simulate the observer changing viewing position.\n\nThe perspective rotate filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\npitch\n\nA float representing the adjustment along the pitch axis in 3D space as an NSNumber.\n\nyaw\n\nA float representing the adjustment along the vertical axis as an NSNumber.\n\nroll\n\nA float representing the amount of horizontal axis in 3D space as an NSNumber.\n\nfocalLength\n\nA float representing the simulated focal length as an NSNumber.\n\nThe following code creates a filter that rotates the image:\n\nfunc perspectiveRotate(inputImage: CIImage) -> CIImage {\n    let perspectiveRotateFilter = CIFilter.perspectiveRotate()\n    perspectiveRotateFilter.inputImage = inputImage\n    perspectiveRotateFilter.pitch = 0\n    perspectiveRotateFilter.yaw = 0.1\n    perspectiveRotateFilter.roll = 0.3\n    perspectiveRotateFilter.focalLength = 18\n    return perspectiveRotateFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bicubicScaleTransformFilter\nProduces a high-quality scaled version of an image.\n+ edgePreserveUpsampleFilter\nCreates a high-quality upscaled image.\n+ keystoneCorrectionCombinedFilter\nAdjusts the image vertically and horizontally to remove distortion.\n+ keystoneCorrectionHorizontalFilter\nHorizontally adjusts an image to remove distortion.\n+ keystoneCorrectionVerticalFilter\nVertically adjusts an image to remove distortion.\n+ lanczosScaleTransformFilter\nCreates a high-quality, scaled version of a source image.\n+ perspectiveCorrectionFilter\nTransforms an image’s perspective.\n+ perspectiveTransformFilter\nAlters an image’s geometry to adjust the perspective.\n+ perspectiveTransformWithExtentFilter\nAlters an image’s geometry to adjust the perspective while applying constraints.\n+ straightenFilter\nRotates and crops an image."
  },
  {
    "title": "hardLightBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228335-hardlightblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the hard-light blend mode filter to an image. The effect calculates the brightness of the colors in the background image. Colors that are lighter than 50 percent gray become lighter. If the brightness of the colors in the input image are darker then 50 percent gray, the effect darkens the colors. The filter then uses the calculated results to create the output image.\n\nThe hard-light blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that mixes the colors and results in an output image that’s darker and more saturated:\n\nfunc hardLightBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.hardLightBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "lanczosScaleTransformFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228344-lanczosscaletransformfilter",
    "html": "Return Value\n\nThe adjusted image.\n\nDiscussion\n\nThis method applies the Lanczos scale transform filter to an image. The effect creates the output image by scaling the input image based on the scale and aspect ratio properties provided.\n\nThe Lanczos scale filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nscale\n\nA float representing the scaling factor used on the image as an NSNumber. Values less than 1.0 scale down the images. Values grater than 1.0 scale up the image.\n\naspectRatio\n\nA float representing the additional horizontal scaling factor used on the image as an NSNumber.\n\nThe following code creates a filter that results in a smaller scaled image with high quality:\n\nfunc lanczosScale(inputImage: CIImage) -> CIImage {    \n    let lanczosScaleFilter = CIFilter.lanczosScaleTransform()\n    lanczosScaleFilter.inputImage = inputImage\n    lanczosScaleFilter.scale =  0.3\n    lanczosScaleFilter.aspectRatio = 1\n    return lanczosScaleFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bicubicScaleTransformFilter\nProduces a high-quality scaled version of an image.\n+ edgePreserveUpsampleFilter\nCreates a high-quality upscaled image.\n+ keystoneCorrectionCombinedFilter\nAdjusts the image vertically and horizontally to remove distortion.\n+ keystoneCorrectionHorizontalFilter\nHorizontally adjusts an image to remove distortion.\n+ keystoneCorrectionVerticalFilter\nVertically adjusts an image to remove distortion.\n+ perspectiveCorrectionFilter\nTransforms an image’s perspective.\n+ perspectiveRotateFilter\nRotates an image in a 3D space.\n+ perspectiveTransformFilter\nAlters an image’s geometry to adjust the perspective.\n+ perspectiveTransformWithExtentFilter\nAlters an image’s geometry to adjust the perspective while applying constraints.\n+ straightenFilter\nRotates and crops an image."
  },
  {
    "title": "divideBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228316-divideblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the divide-blend mode filter to an image. The effect divides the background image color values by the color values of the input image, resulting in the output image.\n\nThe divide-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that brightens and inverts colors in the background image:\n\nfunc divideBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let divideBlendMode = CIFilter.divideBlendMode()\n    divideBlendMode.inputImage = inputImage\n    divideBlendMode.backgroundImage = backgroundImage\n    return divideBlendMode.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "exclusionBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228323-exclusionblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the exclusion-blend mode filter to an image. The effect calculates the brightness value for both images and subtracts the smaller value, resulting in a darker image.\n\nThe exclusion-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that mixes the colors and results in an output image that’s less saturated:\n\nfunc exclusionBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.exclusionBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "perspectiveCorrectionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228380-perspectivecorrectionfilter",
    "html": "Return Value\n\nThe adjusted image.\n\nDiscussion\n\nThis method applies the perspective correction filter to an image. The effect applies a perspective correction transforming nonrectangular area in the source image to a rectangular output image.\n\nThe perspective correction filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ntopLeft\n\nA CGPoint in the input image mapped to the top-left corner of the output image.\n\ntopRight\n\nA CGPoint in the input image mapped to the top-right corner of the output image.\n\nbottomLeft\n\nA CGPoint in the input image mapped to the bottom-left corner of the output image.\n\nbottomRight\n\nA CGPoint in the input image mapped to the bottom-right corner of the output image.\n\nThe following code creates a filter that corrects the perspective to appear straight:\n\nfunc perspectiveCorrection(inputImage: CIImage) -> CIImage {\n    let perspectiveCorrectionFilter = CIFilter.perspectiveCorrection()\n    perspectiveCorrectionFilter.inputImage = inputImage\n    perspectiveCorrectionFilter.topRight = CGPoint(x: 0, y: 3024)\n    perspectiveCorrectionFilter.topLeft = CGPoint(x: 4032, y: 3024)\n    perspectiveCorrectionFilter.bottomRight = CGPoint(x: 200, y: 0)\n    perspectiveCorrectionFilter.bottomLeft = CGPoint(x: 4032, y: 0)\n    return perspectiveCorrectionFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bicubicScaleTransformFilter\nProduces a high-quality scaled version of an image.\n+ edgePreserveUpsampleFilter\nCreates a high-quality upscaled image.\n+ keystoneCorrectionCombinedFilter\nAdjusts the image vertically and horizontally to remove distortion.\n+ keystoneCorrectionHorizontalFilter\nHorizontally adjusts an image to remove distortion.\n+ keystoneCorrectionVerticalFilter\nVertically adjusts an image to remove distortion.\n+ lanczosScaleTransformFilter\nCreates a high-quality, scaled version of a source image.\n+ perspectiveRotateFilter\nRotates an image in a 3D space.\n+ perspectiveTransformFilter\nAlters an image’s geometry to adjust the perspective.\n+ perspectiveTransformWithExtentFilter\nAlters an image’s geometry to adjust the perspective while applying constraints.\n+ straightenFilter\nRotates and crops an image."
  },
  {
    "title": "keystoneCorrectionVerticalFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3325511-keystonecorrectionverticalfilter",
    "html": "Return Value\n\nThe adjusted image.\n\nDiscussion\n\nThis method applies the keystone correction vertical. The effect performs vertical adjustment of the image to shape the image to be rectangular. This effect is commonly used with multimedia projectors to correct the distortion caused by the projector being lower or higher than the projected screen.\n\nThe keystone vertical filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ntopLeft\n\nA CGPoint in the input image mapped to the top-left corner of the output image.\n\ntopRight\n\nA CGPoint in the input image mapped to the top-right corner of the output image.\n\nbottomLeft\n\nA CGPoint in the input image mapped to the bottom-left corner of the output image.\n\nbottomRight\n\nA CGPoint in the input image mapped to the bottom-right corner of the output image.\n\nfocalLength\n\nA float representing the simulated focal length as an NSNumber.\n\nThe following code creates a filter that distorts the image:\n\nfunc keystoneCorrectionVertical(inputImage: CIImage) -> CIImage {\n    let keystoneCorrect = CIFilter.keystoneCorrectionVertical()\n    keystoneCorrect.inputImage = inputImage\n    keystoneCorrect.topLeft = CGPoint(x: 0, y: 3024)\n    keystoneCorrect.topRight = CGPoint(x: 4032, y: 3024)\n    keystoneCorrect.bottomLeft = CGPoint(x: 200, y: 0)\n    keystoneCorrect.bottomRight = CGPoint(x: 4032, y: 0)\n    keystoneCorrect.focalLength = 18\n    return keystoneCorrect.outputImage!\n}\n\n\nSee Also\nFilters\n+ bicubicScaleTransformFilter\nProduces a high-quality scaled version of an image.\n+ edgePreserveUpsampleFilter\nCreates a high-quality upscaled image.\n+ keystoneCorrectionCombinedFilter\nAdjusts the image vertically and horizontally to remove distortion.\n+ keystoneCorrectionHorizontalFilter\nHorizontally adjusts an image to remove distortion.\n+ lanczosScaleTransformFilter\nCreates a high-quality, scaled version of a source image.\n+ perspectiveCorrectionFilter\nTransforms an image’s perspective.\n+ perspectiveRotateFilter\nRotates an image in a 3D space.\n+ perspectiveTransformFilter\nAlters an image’s geometry to adjust the perspective.\n+ perspectiveTransformWithExtentFilter\nAlters an image’s geometry to adjust the perspective while applying constraints.\n+ straightenFilter\nRotates and crops an image."
  },
  {
    "title": "keystoneCorrectionHorizontalFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3325510-keystonecorrectionhorizontalfilt",
    "html": "Return Value\n\nThe adjusted image.\n\nDiscussion\n\nThis method applies the keystone correction horizontal filter to an image. The effect applies a set of horizontal guides and simulated focal length to adjust the shape of the input image. This effect is commonly used when cropping an image to correct distortion. In the figure below, both vertical and horizontal adjustments are made, resulting in a trapezoid-shaped image.\n\nThe keystone correction horizontal filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ntopLeft\n\nA CGPoint in the input image mapped to the top-left corner of the output image.\n\ntopRight\n\nA CGPoint in the input image mapped to the top-right corner of the output image.\n\nbottomLeft\n\nA CGPoint in the input image mapped to the bottom-left corner of the output image.\n\nbottomRight\n\nA CGPoint in the input image mapped to the bottom-right corner of the output image.\n\nfocalLength\n\nA float representing the simulated focal length as an NSNumber.\n\nThe following code creates a filter that distorts the image:\n\nfunc keystoneCorrectionHorizontal(inputImage: CIImage) -> CIImage {    \n    let keystoneCorrect = CIFilter.keystoneCorrectionHorizontal()\n    keystoneCorrect.inputImage = inputImage\n    keystoneCorrect.topLeft = CGPoint(x: 0, y: 2448)\n    keystoneCorrect.topRight = CGPoint(x: 3264, y: 2248)\n    keystoneCorrect.bottomLeft = CGPoint(x: 400, y: 0)\n    keystoneCorrect.bottomRight = CGPoint(x: 3264, y: 150)\n    keystoneCorrect.focalLength = 18\n    return keystoneCorrect.outputImage!\n}\n\n\nSee Also\nFilters\n+ bicubicScaleTransformFilter\nProduces a high-quality scaled version of an image.\n+ edgePreserveUpsampleFilter\nCreates a high-quality upscaled image.\n+ keystoneCorrectionCombinedFilter\nAdjusts the image vertically and horizontally to remove distortion.\n+ keystoneCorrectionVerticalFilter\nVertically adjusts an image to remove distortion.\n+ lanczosScaleTransformFilter\nCreates a high-quality, scaled version of a source image.\n+ perspectiveCorrectionFilter\nTransforms an image’s perspective.\n+ perspectiveRotateFilter\nRotates an image in a 3D space.\n+ perspectiveTransformFilter\nAlters an image’s geometry to adjust the perspective.\n+ perspectiveTransformWithExtentFilter\nAlters an image’s geometry to adjust the perspective while applying constraints.\n+ straightenFilter\nRotates and crops an image."
  },
  {
    "title": "additionCompositingFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228264-additioncompositingfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThe filter calculates the sum of color components in the two input images to produce a brightening effect. People typically use this filter to add highlights and lens flares.\n\nThe addition compositing filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image that the filter applies the effect on with the type CIImage.\n\nThe following code creates a filter that combines the images’ colors to produce one image:\n\nfunc additionCompositing(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let additionCompositeFilter = CIFilter.additionCompositing()\n    additionCompositeFilter.inputImage = inputImage\n    additionCompositeFilter.backgroundImage = backgroundImage\n    return additionCompositeFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "CIContextOption",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption",
    "html": "Topics\nInitializers\ninit(rawValue: String)\nType Properties\nstatic let allowLowPower: CIContextOption\nstatic let cacheIntermediates: CIContextOption\nAn option for whether the context caches the contents of any intermediate pixel buffers it uses during rendering.\nstatic let highQualityDownsample: CIContextOption\nAn option controlling the quality of image downsampling operations performed by the context.\nstatic let memoryTarget: CIContextOption\nstatic let name: CIContextOption\nstatic let outputColorSpace: CIContextOption\nA key for the color space to use for images before they are rendered to the context.\nstatic let outputPremultiplied: CIContextOption\nAn option for whether output rendering by the context produces alpha-premultiplied pixels.\nstatic let priorityRequestLow: CIContextOption\nA key for enabling low-priority GPU use.\nstatic let useSoftwareRenderer: CIContextOption\nA key for enabling software renderer use. If the associated NSNumber object is true, the software renderer is required.\nstatic let workingColorSpace: CIContextOption\nA key for the color space to use for image operations.\nstatic let workingFormat: CIContextOption\nAn option for the color format to use for intermediate results when rendering with the context.\nRelationships\nConforms To\nEquatable\nHashable\nRawRepresentable\nSendable"
  },
  {
    "title": "CIImageOption",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption",
    "html": "Topics\nInitializers\ninit(rawValue: String)\nType Properties\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryHDRGainMap: CIImageOption\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte.\nstatic let auxiliarySemanticSegmentationGlassesMatte: CIImageOption\nstatic let auxiliarySemanticSegmentationHairMatte: CIImageOption\nstatic let auxiliarySemanticSegmentationSkinMatte: CIImageOption\nstatic let auxiliarySemanticSegmentationSkyMatte: CIImageOption\nstatic let auxiliarySemanticSegmentationTeethMatte: CIImageOption\nstatic let cacheImmediately: CIImageOption\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let expandToHDR: CIImageOption\nA Boolean value that indicates whether to read Gain Map HDR images as HDR.\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let toneMapHDRtoSDR: CIImageOption\nRelationships\nConforms To\nEquatable\nHashable\nRawRepresentable\nSendable"
  },
  {
    "title": "CIImageAutoAdjustmentOption",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageautoadjustmentoption",
    "html": "Topics\nInitializers\ninit(rawValue: String)\nType Properties\nstatic let crop: CIImageAutoAdjustmentOption\nA key used to specify whether to return a filter that crops the image to focus on detected features.\nstatic let enhance: CIImageAutoAdjustmentOption\nA key used to specify whether to return enhancement filters.\nstatic let features: CIImageAutoAdjustmentOption\nA key used to specify an array of features that you want to apply enhancement and red eye filters to.\nstatic let level: CIImageAutoAdjustmentOption\nA key used to specify whether to return a filter that rotates the image to keep a level perspective.\nstatic let redEye: CIImageAutoAdjustmentOption\nA key used to specify whether to return a red eye filter.\nRelationships\nConforms To\nEquatable\nHashable\nRawRepresentable\nSendable"
  },
  {
    "title": "CIFormat",
    "url": "https://developer.apple.com/documentation/coreimage/ciformat",
    "html": "Topics\nImage Formats\nstatic var A16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var A8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is alpha.\nstatic var ABGR8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the blue, green, and red color components.\nstatic var ARGB8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the alpha value precedes the red, green, and blue color components.\nstatic var Af: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is alpha.\nstatic var Ah: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is alpha.\nstatic var BGRA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the blue, green, and red color components precede the alpha value.\nstatic var R16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var R8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is a red color value.\nstatic var RG16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RG8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only red and green color components.\nstatic var RGBA16: CIFormat\nA 64-bit-per-pixel, fixed-point pixel format.\nstatic var RGBA8: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format in which the red, green, and blue color components precede the alpha value.\nstatic var RGBAf: CIFormat\nA 128-bit-per-pixel, floating-point pixel format.\nstatic var RGBAh: CIFormat\nA 64-bit-per-pixel, floating-point pixel format.\nstatic var RGf: CIFormat\nA 64-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var RGh: CIFormat\nA 32-bit-per-pixel, floating-point pixel format with only red and green color components.\nstatic var Rf: CIFormat\nA 32-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var Rh: CIFormat\nA 16-bit-per-pixel, floating-point pixel format in which the sole component is a red color value.\nstatic var L16: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var L8: CIFormat\nAn 8-bit-per-pixel, fixed-point pixel format in which the sole component is luminance.\nstatic var LA16: CIFormat\nA 32-bit-per-pixel, fixed-point pixel format with only 16-bit luminance and alpha components.\nstatic var LA8: CIFormat\nA 16-bit-per-pixel, fixed-point pixel format with only 8-bit luminance and alpha components.\nstatic var LAf: CIFormat\nA 64-bit-per-pixel, full-width floating-point pixel format with 32-bit luminance and alpha components.\nstatic var LAh: CIFormat\nA 32-bit-per-pixel, half-width floating-point pixel format with 16-bit luminance and alpha components.\nstatic var Lf: CIFormat\nA 32-bit-per-pixel, full-width floating-point pixel format in which the sole component is luminance.\nstatic var Lh: CIFormat\nA 16-bit-per-pixel, half-width floating-point pixel format in which the sole component is luminance.\nInitializers\ninit(rawValue: Int32)\nType Properties\nstatic var RGB10: CIFormat\nstatic var RGBX16: CIFormat\nstatic var rgbXf: CIFormat\nstatic var rgbXh: CIFormat\nRelationships\nConforms To\nEquatable\nHashable\nRawRepresentable\nSendable\nSee Also\nImage and Kernel Definition Parameters\nclass CIFilterShape\nA description of the bounding shape of a filter and the domain of definition for a filter operation."
  },
  {
    "title": "CIRAWDecoderVersion",
    "url": "https://developer.apple.com/documentation/coreimage/cirawdecoderversion",
    "html": "Topics\nInitializers\ninit(rawValue: String)\nType Properties\nstatic let none: CIRAWDecoderVersion\nstatic let version6: CIRAWDecoderVersion\nstatic let version6DNG: CIRAWDecoderVersion\nstatic let version7: CIRAWDecoderVersion\nstatic let version7DNG: CIRAWDecoderVersion\nstatic let version8: CIRAWDecoderVersion\nstatic let version8DNG: CIRAWDecoderVersion\nRelationships\nConforms To\nEquatable\nHashable\nRawRepresentable\nSendable"
  },
  {
    "title": "bounds",
    "url": "https://developer.apple.com/documentation/coreimage/cifeature/1437782-bounds",
    "html": "Discussion\n\nThe rectangle is in the coordinate system of the image.\n\nSee Also\nFeature Properties\nvar type: String\nThe type of feature that was discovered."
  },
  {
    "title": "medianFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228358-medianfilter",
    "html": "Return Value\n\nThe blurred image.\n\nDiscussion\n\nThis method applies the median filter to an image. The effect computes the median value of colors for a group of neighboring pixels and replaces each pixel with calculated data.\n\nThe median filter uses the following properties:\n\ninputImage\n\nA CIImage representing the input image to apply the filter to.\n\nThe following code creates a filter that refines the detail in the input image:\n\n    func medianBlur(inputImage: CIImage) -> CIImage? {\n\n\n        let medianBlurFilter = CIFilter.median()\n        medianBlurFilter.inputImage = inputImage\n        return medianBlurFilter.outputImage\n    }\n\n\nSee Also\nFilters\n+ bokehBlurFilter\nApplies a bokeh effect to an image.\n+ boxBlurFilter\nApplies a square-shaped blur to an area of an image.\n+ discBlurFilter\nApplies a circle-shaped blur to an area of an image.\n+ gaussianBlurFilter\nBlurs an image with a Gaussian distribution pattern.\n+ maskedVariableBlurFilter\nBlurs a specified portion of an image.\n+ morphologyGradientFilter\nDetects and highlights edges of objects.\n+ morphologyMaximumFilter\nBlurs a circular area by enlarging contrasting pixels.\n+ morphologyMinimumFilter\nBlurs a circular area by reducing contrasting pixels.\n+ morphologyRectangleMaximumFilter\nBlurs a rectangular area by enlarging contrasting pixels.\n+ morphologyRectangleMinimumFilter\nBlurs a rectangular area by reducing contrasting pixels.\n+ motionBlurFilter\nCreates motion blur on an image.\n+ noiseReductionFilter\nReduces noise by sharpening the edges of objects.\n+ zoomBlurFilter\nCreates a zoom blur centered around a single point on the image."
  },
  {
    "title": "overlayBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228374-overlayblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the overlay-blend mode filter to an image. The effect creates the output image by overlapping the input image over the background image while preserving highlights and shadows of the background image.\n\nThe overlay-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the background image becoming darker with the input image overlaid on top:\n\nfunc overlayBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.overlayBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "hasTrackingFrameCount",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1437731-hastrackingframecount",
    "html": "See Also\nTracking Distinct Faces in Video\nvar hasTrackingID: Bool\nA Boolean value that indicates whether the face object has a tracking ID.\nvar trackingID: Int32\nThe tracking identifier of the face object.\nvar trackingFrameCount: Int32\nThe tracking frame count of the face."
  },
  {
    "title": "rightEyeClosed",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1437615-righteyeclosed",
    "html": "Discussion\n\n“Right” is relative to the original (non-mirrored) image orientation, not to the owner of the eye.\n\nFor closed eyes to be detected, the key CIDetectorEyeBlink must be present with a value of true in the dictionary passed to a detector’s features(in:options:) method.\n\nSee Also\nIdentifying Facial Features\nvar hasLeftEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s left eye.\nvar hasRightEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s right eye.\nvar hasMouthPosition: Bool\nA Boolean value that indicates whether the detector found the face’s mouth.\nvar leftEyePosition: CGPoint\nThe coordinates of the left eye, in image coordinates.\nvar rightEyePosition: CGPoint\nThe coordinates of the right eye, in image coordinates\nvar mouthPosition: CGPoint\nThe coordinates of the mouth, in image coordinates\nvar hasSmile: Bool\nA Boolean value that indicates whether a smile is detected in the face.\nvar leftEyeClosed: Bool\nA Boolean value that indicates whether a closed left eye is detected in the face."
  },
  {
    "title": "hasRightEyePosition",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1438076-hasrighteyeposition",
    "html": "See Also\nIdentifying Facial Features\nvar hasLeftEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s left eye.\nvar hasMouthPosition: Bool\nA Boolean value that indicates whether the detector found the face’s mouth.\nvar leftEyePosition: CGPoint\nThe coordinates of the left eye, in image coordinates.\nvar rightEyePosition: CGPoint\nThe coordinates of the right eye, in image coordinates\nvar mouthPosition: CGPoint\nThe coordinates of the mouth, in image coordinates\nvar hasSmile: Bool\nA Boolean value that indicates whether a smile is detected in the face.\nvar leftEyeClosed: Bool\nA Boolean value that indicates whether a closed left eye is detected in the face.\nvar rightEyeClosed: Bool\nA Boolean value that indicates whether a closed right eye is detected in the face."
  },
  {
    "title": "faceAngle",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1437689-faceangle",
    "html": "Discussion\n\nRotation is measured counterclockwise in degrees, with zero indicating that a line drawn between the eyes is horizontal relative to the image orientation.\n\nSee Also\nLocating Faces\nvar bounds: CGRect\nA rectangle indicating the position and dimensions of the face in image coordinates.\nvar hasFaceAngle: Bool\nA Boolean value that indicates whether information about face rotation is available."
  },
  {
    "title": "motionBlurFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228369-motionblurfilter",
    "html": "Return Value\n\nThe blurred image.\n\nDiscussion\n\nThis method applies the motion blur filter to an image. The filter uses the angle of a single row of pixels to determine the direction of the motion effect.\n\nThe motion blur filter uses the following properties:\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\nangle\n\nA float representing the angle of the motion, in radians, that determines which direction the blur smears as an NSNumber.\n\ninputImage\n\nA CIImage representing the input image to apply the filter to.\n\nThe following code creates a filter that adds a motion blur to the input image:\n\n    func motionBlur(inputImage: CIImage) -> CIImage? {\n\n\n        let motionBlurFilter = CIFilter.motionBlur()\n        motionBlurFilter.inputImage = inputImage\n        motionBlurFilter.angle = 0\n        motionBlurFilter.radius = 20\n        return motionBlurFilter.outputImage\n    }\n\n\nSee Also\nFilters\n+ bokehBlurFilter\nApplies a bokeh effect to an image.\n+ boxBlurFilter\nApplies a square-shaped blur to an area of an image.\n+ discBlurFilter\nApplies a circle-shaped blur to an area of an image.\n+ gaussianBlurFilter\nBlurs an image with a Gaussian distribution pattern.\n+ maskedVariableBlurFilter\nBlurs a specified portion of an image.\n+ medianFilter\nCalculates the median of an image to refine detail.\n+ morphologyGradientFilter\nDetects and highlights edges of objects.\n+ morphologyMaximumFilter\nBlurs a circular area by enlarging contrasting pixels.\n+ morphologyMinimumFilter\nBlurs a circular area by reducing contrasting pixels.\n+ morphologyRectangleMaximumFilter\nBlurs a rectangular area by enlarging contrasting pixels.\n+ morphologyRectangleMinimumFilter\nBlurs a rectangular area by reducing contrasting pixels.\n+ noiseReductionFilter\nReduces noise by sharpening the edges of objects.\n+ zoomBlurFilter\nCreates a zoom blur centered around a single point on the image."
  },
  {
    "title": "zoomBlurFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228434-zoomblurfilter",
    "html": "Return Value\n\nThe blurred image.\n\nDiscussion\n\nThis method applies the zoom blur filter to an image. This effect mimics the zoom of a camera when capturing the image.\n\nThe zoom blur filter uses the following properties:\n\namount\n\nA float representing the zoom-in amount as an NSNumber.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\ninputImage\n\nA CIImage representing the input image to apply the filter to.\n\nThe following code creates a filter that adds a zoom blur to the input image:\n\n    func zoomBlur(inputImage: CIImage) -> CIImage? {\n\n\n        let zoomBlurFilter = CIFilter.zoomBlur()\n        zoomBlurFilter.inputImage = inputImage\n        zoomBlurFilter.amount = 5\n        zoomBlurFilter.center = CGPoint(x: 150, y: 150)\n        return zoomBlurFilter.outputImage\n    }\n\n\nSee Also\nFilters\n+ bokehBlurFilter\nApplies a bokeh effect to an image.\n+ boxBlurFilter\nApplies a square-shaped blur to an area of an image.\n+ discBlurFilter\nApplies a circle-shaped blur to an area of an image.\n+ gaussianBlurFilter\nBlurs an image with a Gaussian distribution pattern.\n+ maskedVariableBlurFilter\nBlurs a specified portion of an image.\n+ medianFilter\nCalculates the median of an image to refine detail.\n+ morphologyGradientFilter\nDetects and highlights edges of objects.\n+ morphologyMaximumFilter\nBlurs a circular area by enlarging contrasting pixels.\n+ morphologyMinimumFilter\nBlurs a circular area by reducing contrasting pixels.\n+ morphologyRectangleMaximumFilter\nBlurs a rectangular area by enlarging contrasting pixels.\n+ morphologyRectangleMinimumFilter\nBlurs a rectangular area by reducing contrasting pixels.\n+ motionBlurFilter\nCreates motion blur on an image.\n+ noiseReductionFilter\nReduces noise by sharpening the edges of objects."
  },
  {
    "title": "noiseReductionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228372-noisereductionfilter",
    "html": "Return Value\n\nThe blurred image.\n\nDiscussion\n\nThis method applies the noise reduction filter to an image. The effect calculates changes in luminance below the noise level and locally blurs the area. Values above the threshold are determined to be edges, and become sharpened.\n\nThe morphology noise reduction filter uses the following properties:\n\nnoiseLevel\n\nA float representing the amount of noise reduction as an NSNumber.\n\nsharpness\n\nA float representing the sharpness of the final image as an NSNumber.\n\ninputImage\n\nA CIImage representing the input image to apply the filter to.\n\nThe following code creates a filter that reduces noise in the input image:\n\n    func noiseReduction(inputImage: CIImage) -> CIImage? {\n\n\n        let noiseReductionfilter = CIFilter.noiseReduction()\n        noiseReductionfilter.inputImage = inputImage\n        noiseReductionfilter.noiseLevel = 0.2\n        noiseReductionfilter.sharpness = 0.4\n        return noiseReductionfilter.outputImage\n    }\n\n\nSee Also\nFilters\n+ bokehBlurFilter\nApplies a bokeh effect to an image.\n+ boxBlurFilter\nApplies a square-shaped blur to an area of an image.\n+ discBlurFilter\nApplies a circle-shaped blur to an area of an image.\n+ gaussianBlurFilter\nBlurs an image with a Gaussian distribution pattern.\n+ maskedVariableBlurFilter\nBlurs a specified portion of an image.\n+ medianFilter\nCalculates the median of an image to refine detail.\n+ morphologyGradientFilter\nDetects and highlights edges of objects.\n+ morphologyMaximumFilter\nBlurs a circular area by enlarging contrasting pixels.\n+ morphologyMinimumFilter\nBlurs a circular area by reducing contrasting pixels.\n+ morphologyRectangleMaximumFilter\nBlurs a rectangular area by enlarging contrasting pixels.\n+ morphologyRectangleMinimumFilter\nBlurs a rectangular area by reducing contrasting pixels.\n+ motionBlurFilter\nCreates motion blur on an image.\n+ zoomBlurFilter\nCreates a zoom blur centered around a single point on the image."
  },
  {
    "title": "morphologyMinimumFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228366-morphologyminimumfilter",
    "html": "Return Value\n\nThe blurred image.\n\nDiscussion\n\nThis method applies the morphology minimum filter to an image. The effect targets a circular section of the image, calculating the median color values to find colors that make up more than half the working area. Using this calculation, the effect reduces the pixels with contrasting colors to take up less of the working area. The effect is then repeated throughout the image.\n\nThe morphology minimum filter uses the following properties:\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\ninputImage\n\nA CIImage representing the input image to apply the filter to.\n\nThe following code creates a filter that adds a blur that adds darkness to the input image:\n\n    func morphologyMinimum(inputImage: CIImage) -> CIImage? {\n\n\n        let morphologyMinimumFilter = CIFilter.morphologyMinimum()\n        morphologyMinimumFilter.inputImage = inputImage\n        morphologyMinimumFilter.radius = 5\n        return morphologyMinimumFilter.outputImage\n    }\n\n\nSee Also\nFilters\n+ bokehBlurFilter\nApplies a bokeh effect to an image.\n+ boxBlurFilter\nApplies a square-shaped blur to an area of an image.\n+ discBlurFilter\nApplies a circle-shaped blur to an area of an image.\n+ gaussianBlurFilter\nBlurs an image with a Gaussian distribution pattern.\n+ maskedVariableBlurFilter\nBlurs a specified portion of an image.\n+ medianFilter\nCalculates the median of an image to refine detail.\n+ morphologyGradientFilter\nDetects and highlights edges of objects.\n+ morphologyMaximumFilter\nBlurs a circular area by enlarging contrasting pixels.\n+ morphologyRectangleMaximumFilter\nBlurs a rectangular area by enlarging contrasting pixels.\n+ morphologyRectangleMinimumFilter\nBlurs a rectangular area by reducing contrasting pixels.\n+ motionBlurFilter\nCreates motion blur on an image.\n+ noiseReductionFilter\nReduces noise by sharpening the edges of objects.\n+ zoomBlurFilter\nCreates a zoom blur centered around a single point on the image."
  },
  {
    "title": "morphologyMaximumFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228365-morphologymaximumfilter",
    "html": "Return Value\n\nThe blurred image.\n\nDiscussion\n\nThis method applies the morphology maximum filter to an image. The effect targets a circular section of the image, calculating the median color values to find colors that make up more than half the working area. Using this calculation, the effect enlarges the pixels with contrasting colors to take up more of the working area. The effect is then repeated throughout the image.\n\nThe morphology maximum filter uses the following properties:\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\ninputImage\n\nA CIImage representing the input image to apply the filter to.\n\nThe following code creates a filter that adds an intense blur to the input image:\n\n    func morphologyMaximum(inputImage: CIImage) -> CIImage? {\n\n\n        let morphologyMaximumFilter = CIFilter.morphologyMaximum()\n        morphologyMaximumFilter.inputImage = inputImage\n        morphologyMaximumFilter.radius = 5\n        return morphologyMaximumFilter.outputImage\n    }\n\n\nSee Also\nFilters\n+ bokehBlurFilter\nApplies a bokeh effect to an image.\n+ boxBlurFilter\nApplies a square-shaped blur to an area of an image.\n+ discBlurFilter\nApplies a circle-shaped blur to an area of an image.\n+ gaussianBlurFilter\nBlurs an image with a Gaussian distribution pattern.\n+ maskedVariableBlurFilter\nBlurs a specified portion of an image.\n+ medianFilter\nCalculates the median of an image to refine detail.\n+ morphologyGradientFilter\nDetects and highlights edges of objects.\n+ morphologyMinimumFilter\nBlurs a circular area by reducing contrasting pixels.\n+ morphologyRectangleMaximumFilter\nBlurs a rectangular area by enlarging contrasting pixels.\n+ morphologyRectangleMinimumFilter\nBlurs a rectangular area by reducing contrasting pixels.\n+ motionBlurFilter\nCreates motion blur on an image.\n+ noiseReductionFilter\nReduces noise by sharpening the edges of objects.\n+ zoomBlurFilter\nCreates a zoom blur centered around a single point on the image."
  },
  {
    "title": "morphologyRectangleMaximumFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228367-morphologyrectanglemaximumfilter",
    "html": "Return Value\n\nThe blurred image.\n\nDiscussion\n\nThis method applies the morphology rectangle maximum filter to an image. The effect targets a rectangular section of the image, calculating the median color values to find colors that make up more than half the working area. Using this calculation, the effect enlarges the pixels with contrasting colors to take up more of the working area. The effect is then repeated throughout the image.\n\nThe morphology rectangle maximum filter uses the following properties:\n\nwidth\n\nA float representing the width in pixels of the working area as an NSNumber.\n\nheight\n\nA float representing the height in pixels of the working area as an NSNumber.\n\ninputImage\n\nA CIImage representing the input image to apply the filter to.\n\nThe following code creates a filter that adds a blur to the input image while brighting the palm trees:\n\n    func morphologyRectangleMaximum(inputImage: CIImage) -> CIImage? {\n\n\n        let morphologyRectangleMaximumFilter = CIFilter.morphologyRectangleMaximum()\n        morphologyRectangleMaximumFilter.inputImage = inputImage\n        morphologyRectangleMaximumFilter.width = 5\n        morphologyRectangleMaximumFilter.height = 5\n        return morphologyRectangleMaximumFilter.outputImage\n    }\n\n\nSee Also\nFilters\n+ bokehBlurFilter\nApplies a bokeh effect to an image.\n+ boxBlurFilter\nApplies a square-shaped blur to an area of an image.\n+ discBlurFilter\nApplies a circle-shaped blur to an area of an image.\n+ gaussianBlurFilter\nBlurs an image with a Gaussian distribution pattern.\n+ maskedVariableBlurFilter\nBlurs a specified portion of an image.\n+ medianFilter\nCalculates the median of an image to refine detail.\n+ morphologyGradientFilter\nDetects and highlights edges of objects.\n+ morphologyMaximumFilter\nBlurs a circular area by enlarging contrasting pixels.\n+ morphologyMinimumFilter\nBlurs a circular area by reducing contrasting pixels.\n+ morphologyRectangleMinimumFilter\nBlurs a rectangular area by reducing contrasting pixels.\n+ motionBlurFilter\nCreates motion blur on an image.\n+ noiseReductionFilter\nReduces noise by sharpening the edges of objects.\n+ zoomBlurFilter\nCreates a zoom blur centered around a single point on the image."
  },
  {
    "title": "morphologyGradientFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228364-morphologygradientfilter",
    "html": "Return Value\n\nThe blurred image.\n\nDiscussion\n\nThis method applies the morphology gradient filter to an image. The effect uses the radius to compute and highlight edges of items within the image.\n\nThe morphology gradient filter uses the following properties:\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\ninputImage\n\nA CIImage representing the input image to apply the filter to.\n\nThe following code creates a filter that adds darkness to the overall image while the edges in the input photo become brighter:\n\n    func morphologyGradient(inputImage: CIImage) -> CIImage? {\n\n\n        let morphologyGradientFilter = CIFilter.morphologyGradient()\n        morphologyGradientFilter.inputImage = inputImage\n        morphologyGradientFilter.radius = 1\n        return morphologyGradientFilter.outputImage\n    }\n\n\nSee Also\nFilters\n+ bokehBlurFilter\nApplies a bokeh effect to an image.\n+ boxBlurFilter\nApplies a square-shaped blur to an area of an image.\n+ discBlurFilter\nApplies a circle-shaped blur to an area of an image.\n+ gaussianBlurFilter\nBlurs an image with a Gaussian distribution pattern.\n+ maskedVariableBlurFilter\nBlurs a specified portion of an image.\n+ medianFilter\nCalculates the median of an image to refine detail.\n+ morphologyMaximumFilter\nBlurs a circular area by enlarging contrasting pixels.\n+ morphologyMinimumFilter\nBlurs a circular area by reducing contrasting pixels.\n+ morphologyRectangleMaximumFilter\nBlurs a rectangular area by enlarging contrasting pixels.\n+ morphologyRectangleMinimumFilter\nBlurs a rectangular area by reducing contrasting pixels.\n+ motionBlurFilter\nCreates motion blur on an image.\n+ noiseReductionFilter\nReduces noise by sharpening the edges of objects.\n+ zoomBlurFilter\nCreates a zoom blur centered around a single point on the image."
  },
  {
    "title": "differenceBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228310-differenceblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the difference-blend mode filter to an image. The effect calculates the brightness value for both images and subtracts the smaller value, resulting in a slightly darker image. This effect doesn’t modify images that are black.\n\nThe difference-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that blends colors in the background image and input image:\n\nfunc differenceBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.differenceBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "colorBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228282-colorblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the color-blend mode filter to an image. The effect creates the output image by using the luminance values of the background image, while using the hue and saturation values of the input image. The effect preserves gray levels from the background image.\n\nThe color-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that replaces the colors the image:\n\nfunc colorBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendModeFilter = CIFilter.colorBlendMode()\n    colorBlendModeFilter.inputImage = inputImage\n    colorBlendModeFilter.backgroundImage = backgroundImage\n    return colorBlendModeFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "symbolDescriptor",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodefeature/2875553-symboldescriptor",
    "html": "Discussion\n\nConcrete subclass of CIBarcodeDescriptor. Contains the payload, version of the symbol, mask pattern, and error correction level, so the QR Code can be reproduced.\n\nSee Also\nDecoding a Detected Barcode\nvar messageString: String?\nThe string decoded from the detected barcode."
  },
  {
    "title": "darkenBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228307-darkenblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the darken-blend mode filter to an image. The effect targets lighter colors in the background image and darkens them, while darker colors remain unchanged. The result is the sum of the adjusted color values from both images.\n\nThe darken-blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that darkens the image:\n\nfunc darkenBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.darkenBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "colorDodgeBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228291-colordodgeblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the color-dodge blend mode filter to an image. The effect divides the background image color values by the inverted color values of the input image. The effect then decreases contrast between the input image and the background image.\n\nThe color-dodge mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that darkens the background image:\n\nfunc colorDodgeBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.colorDodgeBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "colorBurnBlendModeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228283-colorburnblendmodefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the color-burn blend mode filter to an image. The effect darkens the background image to reflect the input image’s color.\n\nThe color-burn blend mode filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that darkens the background image:\n\nfunc colorBurnBlendMode(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.colorBurnBlendMode()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "keystoneCorrectionCombinedFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3325509-keystonecorrectioncombinedfilter",
    "html": "Return Value\n\nThe adjusted image.\n\nDiscussion\n\nThis method applies the keystone correction combined filter to an image. The effect applies a combined set of horizontal and vertical guides to adjust the shape of the input image. This effect is commonly used when cropping an image to correct distortion. In the figure below, both vertical and horizontal adjustments are made, resulting in a trapezoid-shaped image.\n\nThe keystone correction filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ntopLeft\n\nA CGPoint in the input image mapped to the top-left corner of the output image.\n\ntopRight\n\nA CGPoint in the input image mapped to the top-right corner of the output image.\n\nbottomLeft\n\nA CGPoint in the input image mapped to the bottom-left corner of the output image.\n\nbottomRight\n\nA CGPoint in the input image mapped to the bottom-right corner of the output image.\n\nfocalLength\n\nA float representing the simulated focal length as an NSNumber.\n\nThe following code creates a filter that distorts the image:\n\nfunc keystoneCorrectionCombined(inputImage: CIImage) -> CIImage {\n    let keystoneCorrect = CIFilter.keystoneCorrectionCombined()\n    keystoneCorrect.inputImage = inputImage\n    keystoneCorrect.topLeft = CGPoint(x: 0, y: 2448)\n    keystoneCorrect.topRight = CGPoint(x: 3164, y: 2248)\n    keystoneCorrect.bottomLeft = CGPoint(x: 0, y: 0)\n    keystoneCorrect.bottomRight = CGPoint(x: 3164, y: 150)\n    keystoneCorrect.focalLength = 35\n    return keystoneCorrect.outputImage!\n}\n\n\nSee Also\nFilters\n+ bicubicScaleTransformFilter\nProduces a high-quality scaled version of an image.\n+ edgePreserveUpsampleFilter\nCreates a high-quality upscaled image.\n+ keystoneCorrectionHorizontalFilter\nHorizontally adjusts an image to remove distortion.\n+ keystoneCorrectionVerticalFilter\nVertically adjusts an image to remove distortion.\n+ lanczosScaleTransformFilter\nCreates a high-quality, scaled version of a source image.\n+ perspectiveCorrectionFilter\nTransforms an image’s perspective.\n+ perspectiveRotateFilter\nRotates an image in a 3D space.\n+ perspectiveTransformFilter\nAlters an image’s geometry to adjust the perspective.\n+ perspectiveTransformWithExtentFilter\nAlters an image’s geometry to adjust the perspective while applying constraints.\n+ straightenFilter\nRotates and crops an image."
  },
  {
    "title": "bicubicScaleTransformFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228271-bicubicscaletransformfilter",
    "html": "Return Value\n\nThe adjusted image.\n\nDiscussion\n\nThis method applies the bicubic scale transform filter to an image. The effect produces a high-quality, scaled version of the input image. The parameters of B and C determine the sharpness and softness of the resampling.\n\nThe bicubic scale transform filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\naspectRatio\n\nA float representing the aspect ratio as an NSNumber.\n\nparameterB\n\nA float representing the value of B used for cubic resampling as an NSNumber.\n\nparameterC\n\nA float representing the value of C used for cubic resampling as an NSNumber.\n\nThe following code creates a filter that results in the image becoming square:\n\nfunc bicubicScale(inputImage: CIImage) -> CIImage {\n    let bicubicScaleFilter = CIFilter.bicubicScaleTransform()\n    bicubicScaleFilter.inputImage = inputImage\n    bicubicScaleFilter.aspectRatio = 0.7\n    bicubicScaleFilter.parameterB = 1\n    bicubicScaleFilter.parameterC = 0.75\n    return bicubicScaleFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ edgePreserveUpsampleFilter\nCreates a high-quality upscaled image.\n+ keystoneCorrectionCombinedFilter\nAdjusts the image vertically and horizontally to remove distortion.\n+ keystoneCorrectionHorizontalFilter\nHorizontally adjusts an image to remove distortion.\n+ keystoneCorrectionVerticalFilter\nVertically adjusts an image to remove distortion.\n+ lanczosScaleTransformFilter\nCreates a high-quality, scaled version of a source image.\n+ perspectiveCorrectionFilter\nTransforms an image’s perspective.\n+ perspectiveRotateFilter\nRotates an image in a 3D space.\n+ perspectiveTransformFilter\nAlters an image’s geometry to adjust the perspective.\n+ perspectiveTransformWithExtentFilter\nAlters an image’s geometry to adjust the perspective while applying constraints.\n+ straightenFilter\nRotates and crops an image."
  },
  {
    "title": "edgePreserveUpsampleFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228319-edgepreserveupsamplefilter",
    "html": "Return Value\n\nThe adjusted image.\n\nDiscussion\n\nThis method applies the edge preserve upsample filter to an image. The effect upsamples a small input image to be the size of the scale image using the luminance of the input image to preserve detail.\n\nThe edge preserve upsample filter uses the following properties:\n\ninputImage\n\nAn image representing the image to upscale with the type CIImage.\n\nscaleImage\n\nAn image representing the reference for scaling the input image with the type CIImage.\n\nspatialSigma\n\nA float representing the influence of the input image’s spatial information on the upsampling operation as an NSNumber.\n\nlumaSimga\n\nA float representing influence of the input image’s luma information on the upsampling operation as an NSNumber.\n\nThe following code creates a filter that upscales the smaller image to the size of the scale image:\n\nfunc edgePerserveUp(inputImage: CIImage, smallImage: CIImage) -> CIImage {\n    let edgePerserveUpFilter = CIFilter.edgePreserveUpsample()\n    edgePerserveUpFilter.inputImage = inputImage\n    edgePerserveUpFilter.smallImage = smallImage\n    edgePerserveUpFilter.spatialSigma = 5\n    edgePerserveUpFilter.lumaSigma = 0.15\n    return edgePerserveUpFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bicubicScaleTransformFilter\nProduces a high-quality scaled version of an image.\n+ keystoneCorrectionCombinedFilter\nAdjusts the image vertically and horizontally to remove distortion.\n+ keystoneCorrectionHorizontalFilter\nHorizontally adjusts an image to remove distortion.\n+ keystoneCorrectionVerticalFilter\nVertically adjusts an image to remove distortion.\n+ lanczosScaleTransformFilter\nCreates a high-quality, scaled version of a source image.\n+ perspectiveCorrectionFilter\nTransforms an image’s perspective.\n+ perspectiveRotateFilter\nRotates an image in a 3D space.\n+ perspectiveTransformFilter\nAlters an image’s geometry to adjust the perspective.\n+ perspectiveTransformWithExtentFilter\nAlters an image’s geometry to adjust the perspective while applying constraints.\n+ straightenFilter\nRotates and crops an image."
  },
  {
    "title": "gaussianBlurFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228331-gaussianblurfilter",
    "html": "Return Value\n\nThe blurred image.\n\nDiscussion\n\nThis method applies a Gaussian blur filter to an image. The effect targets the pixels within a circle defined by a radius and uses Gaussian ditribution to blur the image from the center out.\n\nThe Gaussian blur filter uses the following properties:\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\ninputImage\n\nA CIImage representing the input image to apply the filter to.\n\nThe following code creates a filter that adds a heavy blur to the input image:\n\n    func gaussianBlur(inputImage: CIImage) -> CIImage? {\n\n\n        let gaussianBlurFilter = CIFilter.gaussianBlur()\n        gaussianBlurFilter.inputImage = inputImage\n        gaussianBlurFilter.radius = 10\n        return gaussianBlurFilter.outputImage\n    }\n\n\nSee Also\nFilters\n+ bokehBlurFilter\nApplies a bokeh effect to an image.\n+ boxBlurFilter\nApplies a square-shaped blur to an area of an image.\n+ discBlurFilter\nApplies a circle-shaped blur to an area of an image.\n+ maskedVariableBlurFilter\nBlurs a specified portion of an image.\n+ medianFilter\nCalculates the median of an image to refine detail.\n+ morphologyGradientFilter\nDetects and highlights edges of objects.\n+ morphologyMaximumFilter\nBlurs a circular area by enlarging contrasting pixels.\n+ morphologyMinimumFilter\nBlurs a circular area by reducing contrasting pixels.\n+ morphologyRectangleMaximumFilter\nBlurs a rectangular area by enlarging contrasting pixels.\n+ morphologyRectangleMinimumFilter\nBlurs a rectangular area by reducing contrasting pixels.\n+ motionBlurFilter\nCreates motion blur on an image.\n+ noiseReductionFilter\nReduces noise by sharpening the edges of objects.\n+ zoomBlurFilter\nCreates a zoom blur centered around a single point on the image."
  },
  {
    "title": "boxBlurFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228278-boxblurfilter",
    "html": "Return Value\n\nThe blurred image.\n\nDiscussion\n\nThis method applies the box blur filter to an image. The effect targets a square area and calculates the median color value of the pixels to create the output image. The radius is the width of a square area with a larger area, resulting in a stronger blur effect on the output image.\n\nThe box blur filter uses the following properties:\n\nradius\n\nA float representing the area of effect as a NSNumber.\n\ninputImage\n\nA CIImage representing the input image to apply the filter to.\n\nThe following code creates a filter that results in less detail in the input image:\n\n    func boxBlur(inputImage: CIImage) -> CIImage? {\n\n\n        let boxBlurFilter = CIFilter.boxBlur()\n        boxBlurFilter.inputImage = inputImage\n        boxBlurFilter.radius = 10\n        return boxBlurFilter.outputImage\n    }\n\n\nSee Also\nFilters\n+ bokehBlurFilter\nApplies a bokeh effect to an image.\n+ discBlurFilter\nApplies a circle-shaped blur to an area of an image.\n+ gaussianBlurFilter\nBlurs an image with a Gaussian distribution pattern.\n+ maskedVariableBlurFilter\nBlurs a specified portion of an image.\n+ medianFilter\nCalculates the median of an image to refine detail.\n+ morphologyGradientFilter\nDetects and highlights edges of objects.\n+ morphologyMaximumFilter\nBlurs a circular area by enlarging contrasting pixels.\n+ morphologyMinimumFilter\nBlurs a circular area by reducing contrasting pixels.\n+ morphologyRectangleMaximumFilter\nBlurs a rectangular area by enlarging contrasting pixels.\n+ morphologyRectangleMinimumFilter\nBlurs a rectangular area by reducing contrasting pixels.\n+ motionBlurFilter\nCreates motion blur on an image.\n+ noiseReductionFilter\nReduces noise by sharpening the edges of objects.\n+ zoomBlurFilter\nCreates a zoom blur centered around a single point on the image."
  },
  {
    "title": "maskedVariableBlurFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228355-maskedvariableblurfilter",
    "html": "Return Value\n\nThe blurred image.\n\nDiscussion\n\nThis method applies the masked variable blur to an image. The effect blurs the image in an area defined by the mask image. The mask image contains shades of grey that define the strength of the blur. Black colors in the mask cause no blurring, and white colors cause maximum blur.\n\nThe masked variable blur filter uses the following properties:\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\nmask\n\nAn image that masks an area on the input image with the type CIImage.\n\ninputImage\n\nA CIImage representing the input image to apply the filter to.\n\nThe following code creates a filter that adds a blur to the bottom of the input image:\n\nfunc maskedVariableBlur(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.maskedVariableBlur()\n    filter.inputImage = inputImage\n    // Create a mask that goes from white to black vertially.\n    let mask = CIFilter.smoothLinearGradient()\n    mask.color0 = CIColor.white\n    mask.color1 = CIColor.black\n    mask.point0 = CGPoint(x: 0, y: 0)\n    mask.point1 = CGPoint(x:0, y: inputImage.extent.height)\n    filter.mask = mask.outputImage\n    filter.radius = 25\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bokehBlurFilter\nApplies a bokeh effect to an image.\n+ boxBlurFilter\nApplies a square-shaped blur to an area of an image.\n+ discBlurFilter\nApplies a circle-shaped blur to an area of an image.\n+ gaussianBlurFilter\nBlurs an image with a Gaussian distribution pattern.\n+ medianFilter\nCalculates the median of an image to refine detail.\n+ morphologyGradientFilter\nDetects and highlights edges of objects.\n+ morphologyMaximumFilter\nBlurs a circular area by enlarging contrasting pixels.\n+ morphologyMinimumFilter\nBlurs a circular area by reducing contrasting pixels.\n+ morphologyRectangleMaximumFilter\nBlurs a rectangular area by enlarging contrasting pixels.\n+ morphologyRectangleMinimumFilter\nBlurs a rectangular area by reducing contrasting pixels.\n+ motionBlurFilter\nCreates motion blur on an image.\n+ noiseReductionFilter\nReduces noise by sharpening the edges of objects.\n+ zoomBlurFilter\nCreates a zoom blur centered around a single point on the image."
  },
  {
    "title": "trackingID",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1437709-trackingid",
    "html": "Discussion\n\nCore Image provides a tracking identifier for faces it detects in a video stream, which you can use to identify when a CIFaceFeature objects detected in one video frame is the same face detected in a previous video frame.\n\nThis identifier persists only as long as a face is in the frame and is not associated with a specific face. In other words, if a face moves out of the video frame and comes back into the frame later, another ID is assigned. (Core Image detects faces, but does not recognize specific faces.)\n\nSee Also\nTracking Distinct Faces in Video\nvar hasTrackingID: Bool\nA Boolean value that indicates whether the face object has a tracking ID.\nvar hasTrackingFrameCount: Bool\nA Boolean value that indicates the face object has a tracking frame count.\nvar trackingFrameCount: Int32\nThe tracking frame count of the face."
  },
  {
    "title": "trackingFrameCount",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1437953-trackingframecount",
    "html": "See Also\nTracking Distinct Faces in Video\nvar hasTrackingID: Bool\nA Boolean value that indicates whether the face object has a tracking ID.\nvar trackingID: Int32\nThe tracking identifier of the face object.\nvar hasTrackingFrameCount: Bool\nA Boolean value that indicates the face object has a tracking frame count."
  },
  {
    "title": "leftEyeClosed",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1437630-lefteyeclosed",
    "html": "Discussion\n\n“Left” is relative to the original (non-mirrored) image orientation, not to the owner of the eye.\n\nFor closed eyes to be detected, the key CIDetectorEyeBlink must be present with a value of true in the dictionary passed to a detector’s features(in:options:) method.\n\nSee Also\nIdentifying Facial Features\nvar hasLeftEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s left eye.\nvar hasRightEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s right eye.\nvar hasMouthPosition: Bool\nA Boolean value that indicates whether the detector found the face’s mouth.\nvar leftEyePosition: CGPoint\nThe coordinates of the left eye, in image coordinates.\nvar rightEyePosition: CGPoint\nThe coordinates of the right eye, in image coordinates\nvar mouthPosition: CGPoint\nThe coordinates of the mouth, in image coordinates\nvar hasSmile: Bool\nA Boolean value that indicates whether a smile is detected in the face.\nvar rightEyeClosed: Bool\nA Boolean value that indicates whether a closed right eye is detected in the face."
  },
  {
    "title": "hasTrackingID",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1437683-hastrackingid",
    "html": "See Also\nTracking Distinct Faces in Video\nvar trackingID: Int32\nThe tracking identifier of the face object.\nvar hasTrackingFrameCount: Bool\nA Boolean value that indicates the face object has a tracking frame count.\nvar trackingFrameCount: Int32\nThe tracking frame count of the face."
  },
  {
    "title": "hasSmile",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1437882-hassmile",
    "html": "Discussion\n\nFor smiles to be detected, the key CIDetectorSmile must be present with a value of true in the dictionary passed to a detector’s features(in:options:) method.\n\nSee Also\nIdentifying Facial Features\nvar hasLeftEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s left eye.\nvar hasRightEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s right eye.\nvar hasMouthPosition: Bool\nA Boolean value that indicates whether the detector found the face’s mouth.\nvar leftEyePosition: CGPoint\nThe coordinates of the left eye, in image coordinates.\nvar rightEyePosition: CGPoint\nThe coordinates of the right eye, in image coordinates\nvar mouthPosition: CGPoint\nThe coordinates of the mouth, in image coordinates\nvar leftEyeClosed: Bool\nA Boolean value that indicates whether a closed left eye is detected in the face.\nvar rightEyeClosed: Bool\nA Boolean value that indicates whether a closed right eye is detected in the face."
  },
  {
    "title": "mouthPosition",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1438020-mouthposition",
    "html": "See Also\nIdentifying Facial Features\nvar hasLeftEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s left eye.\nvar hasRightEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s right eye.\nvar hasMouthPosition: Bool\nA Boolean value that indicates whether the detector found the face’s mouth.\nvar leftEyePosition: CGPoint\nThe coordinates of the left eye, in image coordinates.\nvar rightEyePosition: CGPoint\nThe coordinates of the right eye, in image coordinates\nvar hasSmile: Bool\nA Boolean value that indicates whether a smile is detected in the face.\nvar leftEyeClosed: Bool\nA Boolean value that indicates whether a closed left eye is detected in the face.\nvar rightEyeClosed: Bool\nA Boolean value that indicates whether a closed right eye is detected in the face."
  },
  {
    "title": "rightEyePosition",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1438213-righteyeposition",
    "html": "See Also\nIdentifying Facial Features\nvar hasLeftEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s left eye.\nvar hasRightEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s right eye.\nvar hasMouthPosition: Bool\nA Boolean value that indicates whether the detector found the face’s mouth.\nvar leftEyePosition: CGPoint\nThe coordinates of the left eye, in image coordinates.\nvar mouthPosition: CGPoint\nThe coordinates of the mouth, in image coordinates\nvar hasSmile: Bool\nA Boolean value that indicates whether a smile is detected in the face.\nvar leftEyeClosed: Bool\nA Boolean value that indicates whether a closed left eye is detected in the face.\nvar rightEyeClosed: Bool\nA Boolean value that indicates whether a closed right eye is detected in the face."
  },
  {
    "title": "hasMouthPosition",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1437976-hasmouthposition",
    "html": "See Also\nIdentifying Facial Features\nvar hasLeftEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s left eye.\nvar hasRightEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s right eye.\nvar leftEyePosition: CGPoint\nThe coordinates of the left eye, in image coordinates.\nvar rightEyePosition: CGPoint\nThe coordinates of the right eye, in image coordinates\nvar mouthPosition: CGPoint\nThe coordinates of the mouth, in image coordinates\nvar hasSmile: Bool\nA Boolean value that indicates whether a smile is detected in the face.\nvar leftEyeClosed: Bool\nA Boolean value that indicates whether a closed left eye is detected in the face.\nvar rightEyeClosed: Bool\nA Boolean value that indicates whether a closed right eye is detected in the face."
  },
  {
    "title": "leftEyePosition",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1437923-lefteyeposition",
    "html": "See Also\nIdentifying Facial Features\nvar hasLeftEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s left eye.\nvar hasRightEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s right eye.\nvar hasMouthPosition: Bool\nA Boolean value that indicates whether the detector found the face’s mouth.\nvar rightEyePosition: CGPoint\nThe coordinates of the right eye, in image coordinates\nvar mouthPosition: CGPoint\nThe coordinates of the mouth, in image coordinates\nvar hasSmile: Bool\nA Boolean value that indicates whether a smile is detected in the face.\nvar leftEyeClosed: Bool\nA Boolean value that indicates whether a closed left eye is detected in the face.\nvar rightEyeClosed: Bool\nA Boolean value that indicates whether a closed right eye is detected in the face."
  },
  {
    "title": "hasLeftEyePosition",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1437900-haslefteyeposition",
    "html": "See Also\nIdentifying Facial Features\nvar hasRightEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s right eye.\nvar hasMouthPosition: Bool\nA Boolean value that indicates whether the detector found the face’s mouth.\nvar leftEyePosition: CGPoint\nThe coordinates of the left eye, in image coordinates.\nvar rightEyePosition: CGPoint\nThe coordinates of the right eye, in image coordinates\nvar mouthPosition: CGPoint\nThe coordinates of the mouth, in image coordinates\nvar hasSmile: Bool\nA Boolean value that indicates whether a smile is detected in the face.\nvar leftEyeClosed: Bool\nA Boolean value that indicates whether a closed left eye is detected in the face.\nvar rightEyeClosed: Bool\nA Boolean value that indicates whether a closed right eye is detected in the face."
  },
  {
    "title": "CIDataMatrixCodeDescriptor.ECCVersion",
    "url": "https://developer.apple.com/documentation/coreimage/cidatamatrixcodedescriptor/eccversion",
    "html": "Overview\n\nECC 000 - 140 symbols offer five levels of error correction using convolutional code error correction. Each successive level or error correction offers more protection for the message data but increases the size of the symbol required to carry a given message. See the ISO/IEC 16022:2006 spec for other modes.\n\nECC 200 symbols utilize Reed-Solomon error correction, with error correction capacity determined by symbol size (in rows and columns).\n\nTopics\nEnumeration Cases\ncase v000\nIndicates error correction using convolutional code error correction with zero data protection.\ncase v050\nIndicates 1/4 (25%) of the symbol dedicated convolutional code error correction.\ncase v080\nIndicates 1/3 (33%) of the symbol dedicated convolutional code error correction.\ncase v100\nIndicates 1/2 (50%) of the symbol dedicated convolutional code error correction.\ncase v140\nIndicates 3/4 (75%) of the symbol dedicated convolutional code error correction.\ncase v200\nIndicates Reed-Solomon error correction. Data protection overhead varies based on symbol size.\nRelationships\nConforms To\nSendable"
  },
  {
    "title": "eccVersion",
    "url": "https://developer.apple.com/documentation/coreimage/cidatamatrixcodedescriptor/2875170-eccversion",
    "html": "Discussion\n\nValid values are 000, 050, 080, 100, 140, and 200. Any symbol with an even number of rows and columns will be ECC 200.\n\nSee Also\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload that comprises the Data Matrix code symbol.\nvar rowCount: Int\nThe number of module rows.\nvar columnCount: Int\nThe number of module columns."
  },
  {
    "title": "rowCount",
    "url": "https://developer.apple.com/documentation/coreimage/cidatamatrixcodedescriptor/2875200-rowcount",
    "html": "Discussion\n\nRefer to ISO/IEC 16022:2006(E) for valid module row and column count combinations.\n\nSee Also\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload that comprises the Data Matrix code symbol.\nvar columnCount: Int\nThe number of module columns.\nvar eccVersion: CIDataMatrixCodeDescriptor.ECCVersion\nThe Data Matrix code ECC version."
  },
  {
    "title": "CIDetectorImageOrientation",
    "url": "https://developer.apple.com/documentation/coreimage/cidetectorimageorientation",
    "html": "Discussion\n\nThe value of this key is an NSNumber object whose value is an integer between 1 and 8. The TIFF and EXIF specifications define these values to indicate where the pixel coordinate origin (0,0) of the image should appear when it is displayed. The default value is 1, indicating that the origin is in the top left corner of the image. For further details, see kCGImagePropertyOrientation.\n\nCore Image detects only faces whose orientation matches that of the image. You should provide a value for this key if you want to detect faces in a different orientation."
  },
  {
    "title": "bounds",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1438068-bounds",
    "html": "See Also\nLocating Faces\nvar hasFaceAngle: Bool\nA Boolean value that indicates whether information about face rotation is available.\nvar faceAngle: Float\nThe rotation of the face."
  },
  {
    "title": "topRight",
    "url": "https://developer.apple.com/documentation/coreimage/citextfeature/1438282-topright",
    "html": "See Also\nIdentifying the Corners of a Detected Text Region\nvar bottomLeft: CGPoint\nThe lower-left corner of the detected text region, in image coordinates.\nvar bottomRight: CGPoint\nThe lower-right corner of the detected text region, in image coordinates.\nvar topLeft: CGPoint\nThe upper-left corner of the detected text region, in image coordinates."
  },
  {
    "title": "hasFaceAngle",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature/1438165-hasfaceangle",
    "html": "See Also\nLocating Faces\nvar bounds: CGRect\nA rectangle indicating the position and dimensions of the face in image coordinates.\nvar faceAngle: Float\nThe rotation of the face."
  },
  {
    "title": "errorCorrectedPayload",
    "url": "https://developer.apple.com/documentation/coreimage/cidatamatrixcodedescriptor/2875173-errorcorrectedpayload",
    "html": "Discussion\n\nThe error-corrected payload contains the de-interleaved bits of the message.\n\nData Matrix symbols are specified by ISO/IEC 16022:2006(E). ECC 200-type symbols will always have an even number of rows and columns.\n\nSee Also\nExamining a Descriptor\nvar rowCount: Int\nThe number of module rows.\nvar columnCount: Int\nThe number of module columns.\nvar eccVersion: CIDataMatrixCodeDescriptor.ECCVersion\nThe Data Matrix code ECC version."
  },
  {
    "title": "CIDetectorTypeText",
    "url": "https://developer.apple.com/documentation/coreimage/cidetectortypetext",
    "html": "Discussion\n\nThe text detector finds areas that are likely to contain upright text, but does not perform optical character recognition."
  },
  {
    "title": "init(payload:rowCount:columnCount:eccVersion:)",
    "url": "https://developer.apple.com/documentation/coreimage/cidatamatrixcodedescriptor/2875201-init",
    "html": "Parameters\nerrorCorrectedPayload\n\nThe data to encode in the Data Matrix code.\n\nrowCount\n\nThe number of rows in the matrix.\n\ncolumnCount\n\nThe number of columns in the matrix.\n\neccVersion\n\nThe code's ECC version. Valid values are 000, 050, 080, 100, 140, and 200.\n\nAny symbol with an even number of rows and columns will be ECC 200.\n\nReturn Value\n\nA Data Matrix code descriptor encoding the specified payload in the specified row and column count under the specified ECC version.\n\nDiscussion\n\nThe CIBarcodeGenerator filter can recreate a Data Matrix code given the descriptor created using this method."
  },
  {
    "title": "topLeft",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodefeature/1437780-topleft",
    "html": "See Also\nIdentifying the Corners of a Detected Barcode\nvar bottomLeft: CGPoint\nThe lower-left corner of the detected barcode, in image coordinates.\nvar bottomRight: CGPoint\nThe lower-right corner of the detected barcode, in image coordinates.\nvar topRight: CGPoint\nThe upper-right corner of the detected barcode, in image coordinates."
  },
  {
    "title": "Detector Configuration Keys",
    "url": "https://developer.apple.com/documentation/coreimage/cidetector/detector_configuration_keys",
    "html": "Topics\nConstants\nlet CIDetectorAccuracy: String\nA key used to specify the desired accuracy for the detector.\nlet CIDetectorTracking: String\nA key used to enable or disable face tracking for the detector. Use this option when you want to track faces across frames in a video.\nlet CIDetectorMinFeatureSize: String\nA key used to specify the minimum size that the detector will recognize as a feature.\nlet CIDetectorNumberOfAngles: String\nThe number of perspectives to use for detecting a face in video input.\nlet CIDetectorMaxFeatureCount: String\nThe key to the configuration dictionary whose value represents the maximum number of features the detector should return."
  },
  {
    "title": "messageString",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodefeature/1438035-messagestring",
    "html": "See Also\nDecoding a Detected Barcode\nvar symbolDescriptor: CIQRCodeDescriptor?\nAn abstract representation of a QR Code symbol."
  },
  {
    "title": "load(_:allowExecutableCode:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciplugin/1438187-load",
    "html": "Parameters\nurl\n\nThe location of the image unit to load.\n\nallowExecutableCode\n\ntrue to load all filters from the image unit, or false to load only those filters without CPU executable code.\n\nDiscussion\n\nYou need to call this method only once to load a specific image unit. The behavior of this method is not defined for multiple calls for the same image unit. If you pass false for the allowExecutableCode parameter, Core Image will load only pure kernel filters that run entirely on the GPU, ignoring filters implemented using compiled Objective-C code.\n\nSee Also\nLoading Plug-ins\nclass func loadAllPlugIns()\nScans directories for files that have the .plugin extension and then loads the image units.\nDeprecated\nclass func loadNonExecutablePlugIns()\nScans directories for files that have the .plugin extension and then loads only those filters that are marked by the image unit as non-executable filters."
  },
  {
    "title": "loadNonExecutablePlugIns()",
    "url": "https://developer.apple.com/documentation/coreimage/ciplugin/1437599-loadnonexecutableplugins",
    "html": "Discussion\n\nThis call does not execute any of the code in the image unit; it simply loads the code. You need to call this method only once to load a specific image unit. The behavior of this method is not defined for multiple calls for the same image unit.\n\nSee Also\nLoading Plug-ins\nclass func loadAllPlugIns()\nScans directories for files that have the .plugin extension and then loads the image units.\nDeprecated\nclass func load(URL!, allowExecutableCode: Bool)\nLoads filters from an image unit that have the appropriate executable status.\nDeprecated"
  },
  {
    "title": "bounds",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodefeature/1438153-bounds",
    "html": "Discussion\n\nThis property identifies the rectangular region of the image containing the detected barcode, not necessarily the shape of the barcode. A detected barcode is rectangular in space, but may appear in perspective in the image. Use the properties listed in Identifying the Corners of a Detected Barcode to find the corners of the barcode as it appears in perspective."
  },
  {
    "title": "Feature Detection Keys",
    "url": "https://developer.apple.com/documentation/coreimage/cidetector/feature_detection_keys",
    "html": "Topics\nConstants\nlet CIDetectorImageOrientation: String\nAn option for the display orientation of the image whose features you want to detect.\nlet CIDetectorEyeBlink: String\nAn option for whether Core Image will perform additional processing to recognize closed eyes in detected faces.\nlet CIDetectorSmile: String\nAn option for whether Core Image will perform additional processing to recognize smiles in detected faces.\nlet CIDetectorFocalLength: String\nAn option identifying the focal length in pixels used in capturing images to be processed by the detector.\nlet CIDetectorAspectRatio: String\nAn option specifying the aspect ratio (width divided by height) of rectangles to search for.\nlet CIDetectorReturnSubFeatures: String\nAn option specifying whether to return feature information for components of detected features."
  },
  {
    "title": "Detector Accuracy Options",
    "url": "https://developer.apple.com/documentation/coreimage/cidetector/detector_accuracy_options",
    "html": "Topics\nConstants\nlet CIDetectorAccuracyLow: String\nIndicates that the detector should choose techniques that are lower in accuracy, but can be processed more quickly.\nlet CIDetectorAccuracyHigh: String\nIndicates that the detector should choose techniques that are higher in accuracy, even if it requires more processing time."
  },
  {
    "title": "loadAllPlugIns()",
    "url": "https://developer.apple.com/documentation/coreimage/ciplugin/1437653-loadallplugins",
    "html": "Discussion\n\nThis method scans the following directories:\n\n/Library/Graphics/Image Units\n\n~/Library/Graphics/Image Units\n\nCall this method once. If you call this method more than once, Core Image loads newly added image units, but image units (and the filters they contain) that are already loaded are not removed.\n\nSee Also\nLoading Plug-ins\nclass func loadNonExecutablePlugIns()\nScans directories for files that have the .plugin extension and then loads only those filters that are marked by the image unit as non-executable filters.\nclass func load(URL!, allowExecutableCode: Bool)\nLoads filters from an image unit that have the appropriate executable status.\nDeprecated\nRelated Documentation\nImage Unit Tutorial\nCore Image Programming Guide"
  },
  {
    "title": "Detector Types",
    "url": "https://developer.apple.com/documentation/coreimage/cidetector/detector_types",
    "html": "Topics\nConstants\nlet CIDetectorTypeFace: String\nA detector that searches for faces in a still image or video, returning CIFaceFeature objects that provide information about detected faces.\nlet CIDetectorTypeRectangle: String\nA detector that searches for rectangular areas in a still image or video, returning CIRectangleFeature objects that provide information about detected regions.\nlet CIDetectorTypeQRCode: String\nA detector that searches for Quick Response codes (a type of 2D barcode) in a still image or video, returning CIQRCodeFeature objects that provide information about detected barcodes.\nlet CIDetectorTypeText: String\nA detector that searches for text in a still image or video, returning CITextFeature objects that provide information about detected regions."
  },
  {
    "title": "CIDetectorFocalLength",
    "url": "https://developer.apple.com/documentation/coreimage/cidetectorfocallength",
    "html": "Discussion\n\nThe value of this key is an NSNumber object whose value is a floating-point number. Use this option with the CIDetectorTypeRectangle detector type to control the effect of the CIDetectorAspectRatio option on feature detection.\n\nThis option’s value can be 0.0, -1.0, or any positive value:\n\nThe special value of -1.0 (the default) disables the aspect ratio test for the returned rectangle.\n\nThe special value of 0.0 enables a less precise test of aspect ratio that approximates an orthographic (non-perspective) projection. Use this value if you want to specify the aspect ratio of the rectangle via the CIDetectorAspectRatio option, but have no means of determining the value for the focal length in pixels. See below for a method to compute an approximate value for the focal length in pixels.\n\nAny other value specifies the camera focal length, in pixels, allowing the aspect ratio specification to account for perspective distortion of rectangles in the input image.\n\nIf you know the diagonal field of view of the camera (the scene angle subtended by the diagonal corners of an image), you can use the following formula to compute an approximate focal length in pixels:\n\nfocal_length_pixels = (image_diagonal_pixels/2)/tan(FOV/2)\n\nIn this formula, image_diagonal_pixels is the length (in pixels) of the image diagonal of the maximum resolution of the camera sensor. (For example, this value is 4080 pixels for a 3264 x 2448 (8 megapixel) sensor, and 5000 pixels for a 4096x3024 (12 megapixel) sensor.)\n\nTo measure diagonal field of view, put the camera on a tripod so that it is perpendicular to a surface and the center of the image is oriented on a mark on the surface. Measure the distance from the mark to one of the corner points of the image (Y). Measure the distance from the camera to the surface (Z). The field of view is then 2*arctan(Y/Z).\n\nYou must specify this value in terms of the maximum sensor resolution. If the supplied CIImage has been scaled relative relative to the maximum sensor resolution, the supplied focal length must also be similarly scaled."
  },
  {
    "title": "features(in:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cidetector/1438189-features",
    "html": "Parameters\nimage\n\nThe image you want to examine.\n\noptions\n\nA dictionary that specifies feature detection options. See Feature Detection Keys for allowed keys and their possible values.\n\nReturn Value\n\nAn array of CIFeature objects. Each object represents a feature detected in the image.\n\nSee Also\nUsing a Detector Object to Find Features\nfunc features(in: CIImage) -> [CIFeature]\nSearches for features in an image."
  },
  {
    "title": "CIDetectorAspectRatio",
    "url": "https://developer.apple.com/documentation/coreimage/cidetectoraspectratio",
    "html": "Discussion\n\nThe value of this key is an NSNumber object whose value is a positive floating-point number. Use this option with the CIDetectorTypeRectangle detector type to fine-tune the accuracy of the detector. For example, to more accurately find a business card (3.5 x 2 inches) in an image, specify an aspect ratio of 1.75 (3.5 / 2)."
  },
  {
    "title": "CIDetectorTypeRectangle",
    "url": "https://developer.apple.com/documentation/coreimage/cidetectortyperectangle",
    "html": "Discussion\n\nThe rectangle detector finds areas that are likely to represent rectangular objects that appear in perspective in the image, such as papers or books seen on a desktop."
  },
  {
    "title": "topRight",
    "url": "https://developer.apple.com/documentation/coreimage/cirectanglefeature/1438071-topright",
    "html": "See Also\nIdentifying the Corners of a Detected Rectangle\nvar bottomLeft: CGPoint\nThe lower-left corner of the detected rectangle, in image coordinates.\nvar bottomRight: CGPoint\nThe lower-right corner of the detected rectangle, in image coordinates.\nvar topLeft: CGPoint\nThe upper-left corner of the detected rectangle, in image coordinates."
  },
  {
    "title": "features(in:)",
    "url": "https://developer.apple.com/documentation/coreimage/cidetector/1438049-features",
    "html": "Parameters\nimage\n\nThe image you want to examine.\n\nReturn Value\n\nAn array of CIFeature objects. Each object represents a feature detected in the image.\n\nSee Also\nUsing a Detector Object to Find Features\nfunc features(in: CIImage, options: [String : Any]?) -> [CIFeature]\nSearches for features in an image based on the specified image orientation."
  },
  {
    "title": "init(ofType:context:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cidetector/1437884-init",
    "html": "Parameters\ntype\n\nA string indicating the kind of detector you are interested in. See Detector Types.\n\ncontext\n\nA Core Image context that the detector can use when analyzing an image.\n\noptions\n\nA dictionary containing details on how you want the detector to be configured. See Detector Configuration Keys.\n\nReturn Value\n\nA configured detector.\n\nDiscussion\n\nA CIDetector object can potentially create and hold a significant amount of resources. Where possible, reuse the same CIDetector instance. Also, when processing images with a detector object, your application performs better if the CIContext used to initialize the detector is the same context used to process the ciImage objects."
  },
  {
    "title": "topLeft",
    "url": "https://developer.apple.com/documentation/coreimage/cirectanglefeature/1437951-topleft",
    "html": "See Also\nIdentifying the Corners of a Detected Rectangle\nvar bottomLeft: CGPoint\nThe lower-left corner of the detected rectangle, in image coordinates.\nvar bottomRight: CGPoint\nThe lower-right corner of the detected rectangle, in image coordinates.\nvar topRight: CGPoint\nThe upper-right corner of the detected rectangle, in image coordinates."
  },
  {
    "title": "bottomLeft",
    "url": "https://developer.apple.com/documentation/coreimage/cirectanglefeature/1437878-bottomleft",
    "html": "See Also\nIdentifying the Corners of a Detected Rectangle\nvar bottomRight: CGPoint\nThe lower-right corner of the detected rectangle, in image coordinates.\nvar topLeft: CGPoint\nThe upper-left corner of the detected rectangle, in image coordinates.\nvar topRight: CGPoint\nThe upper-right corner of the detected rectangle, in image coordinates."
  },
  {
    "title": "bottomRight",
    "url": "https://developer.apple.com/documentation/coreimage/cirectanglefeature/1437888-bottomright",
    "html": "See Also\nIdentifying the Corners of a Detected Rectangle\nvar bottomLeft: CGPoint\nThe lower-left corner of the detected rectangle, in image coordinates.\nvar topLeft: CGPoint\nThe upper-left corner of the detected rectangle, in image coordinates.\nvar topRight: CGPoint\nThe upper-right corner of the detected rectangle, in image coordinates."
  },
  {
    "title": "type",
    "url": "https://developer.apple.com/documentation/coreimage/cifeature/1438092-type",
    "html": "See Also\nFeature Properties\nvar bounds: CGRect\nThe rectangle that holds discovered feature.\nRelated Documentation"
  },
  {
    "title": "CIFeatureTypeFace",
    "url": "https://developer.apple.com/documentation/coreimage/cifeaturetypeface",
    "html": "Discussion\n\nUse the CIFaceFeature class to find more information about the detected feature.\n\nSee Also\nFeature Types\nlet CIFeatureTypeRectangle: String\nThe discovered feature is a rectangular object, though it might appear in perspective in the image.\nlet CIFeatureTypeQRCode: String\nThe discovered feature is a Quick Response code (2D barcode).\nlet CIFeatureTypeText: String\nThe discovered feature is a region likely to contain upright text."
  },
  {
    "title": "init(width:height:pixelFormat:commandBuffer:mtlTextureProvider:)",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination/2880274-init",
    "html": "Parameters\nwidth\n\nWidth of the MTLTexture that will be returned by block.\n\nheight\n\nHeight of the MTLTexture that will be returned by block.\n\npixelFormat\n\nPixel format of the MTLTexture that will be returned by block.\n\ncommandBuffer\n\nAn optional MTLCommandBuffer used for rendering to the MTLTexture.\n\nblock\n\nMTLTexture-rendering provider block to be called lazily when the destination is rendered to. The block must return a texture of MTLTextureType of MTLTextureType.type2D.\n\nReturn Value\n\nA CIRenderDestination object for rendering to a Metal texture.\n\nDiscussion\n\nThe destination's colorSpace property will default to a CGColorSpace created with sRGB, extendedSRGB, or genericGrayGamma2_2.\n\nSee Also\nCreating a Render Destination\ninit(pixelBuffer: CVPixelBuffer)\nCreates a render destination based on a Core Video pixel buffer.\ninit(ioSurface: IOSurface)\nCreates a render destination based on an IOSurface object.\ninit(mtlTexture: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?)\nCreates a render destination based on a Metal texture.\ninit(glTexture: UInt32, target: UInt32, width: Int, height: Int)\nCreates a render destination based on an OpenGL texture.\ninit(bitmapData: UnsafeMutableRawPointer, width: Int, height: Int, bytesPerRow: Int, format: CIFormat)\nCreates a render destination based on a client-managed buffer."
  },
  {
    "title": "CIFeatureTypeQRCode",
    "url": "https://developer.apple.com/documentation/coreimage/cifeaturetypeqrcode",
    "html": "Discussion\n\nUse the CIQRCodeFeature class to find more information about the detected feature.\n\nSee Also\nFeature Types\nlet CIFeatureTypeFace: String\nThe discovered feature is a person’s face.\nlet CIFeatureTypeRectangle: String\nThe discovered feature is a rectangular object, though it might appear in perspective in the image.\nlet CIFeatureTypeText: String\nThe discovered feature is a region likely to contain upright text."
  },
  {
    "title": "CIFeatureTypeRectangle",
    "url": "https://developer.apple.com/documentation/coreimage/cifeaturetyperectangle",
    "html": "Discussion\n\nUse the CIRectangleFeature class to find more information about the detected feature.\n\nSee Also\nFeature Types\nlet CIFeatureTypeFace: String\nThe discovered feature is a person’s face.\nlet CIFeatureTypeQRCode: String\nThe discovered feature is a Quick Response code (2D barcode).\nlet CIFeatureTypeText: String\nThe discovered feature is a region likely to contain upright text."
  },
  {
    "title": "init(glTexture:target:width:height:)",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination/2875438-init",
    "html": "Parameters\ntexture\n\nGLTexture-backed texture data.\n\ntarget\n\nA value denoting the type of destination. Use GL_TEXTURE_2D if your texture dimensions are a power of two, or GL_TEXTURE_RECTANGLE_EXT otherwise.\n\nwidth\n\nWidth of the texture in texels.\n\nheight\n\nHeight of the texture in texels.\n\nReturn Value\n\nA CIRenderDestination object for rendering to a GLTexture supported by GLContext-backed CIContext.\n\nDiscussion\n\nRendering to a GLTexture-backed CIRenderDestination is supported by only GLContext-backed CIContext.\n\nThe destination's colorSpace property will default to a CGColorSpace created with sRGB, extendedSRGB, or genericGrayGamma2_2.\n\nSee Also\nCreating a Render Destination\ninit(pixelBuffer: CVPixelBuffer)\nCreates a render destination based on a Core Video pixel buffer.\ninit(ioSurface: IOSurface)\nCreates a render destination based on an IOSurface object.\ninit(mtlTexture: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?)\nCreates a render destination based on a Metal texture.\ninit(width: Int, height: Int, pixelFormat: MTLPixelFormat, commandBuffer: (any MTLCommandBuffer)?, mtlTextureProvider: (() -> any MTLTexture)?)\nCreates a render destination based on a Metal texture with specified pixel format.\ninit(bitmapData: UnsafeMutableRawPointer, width: Int, height: Int, bytesPerRow: Int, format: CIFormat)\nCreates a render destination based on a client-managed buffer."
  },
  {
    "title": "pixellateFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228393-pixellatefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the pixelate filter to an image. The effect produces a result by mapping the image to colored squares with colors defined by the pixels of the input image.\n\nThe pixelate filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nscale\n\nA float representing the size of the squares as an NSNumber.\n\nThe following code creates a filter that results in a distorted image made of squares:\n\nfunc pixelate(inputImage: CIImage) -> CIImage {\n    let pixelate = CIFilter.pixellate()\n    pixelate.inputImage = inputImage\n    pixelate.center = CGPoint(x: 150, y: 150)\n    pixelate.scale = 30\n    return pixelate.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "init(bitmapData:width:height:bytesPerRow:format:)",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination/2875427-init",
    "html": "Parameters\ndata\n\nPointer to raw bits of a client-managed buffer that is at least (bytesPerRow * height) bytes in size.\n\nwidth\n\nWidth of the bitmap image in pixels.\n\nheight\n\nHeight of the bitmap image in pixels.\n\nbytesPerRow\n\nNumber of bytes per row of data.\n\nformat\n\nColor format specifying how the colors are laid out in memory (for example, RGBA8).\n\nReturn Value\n\nA CIRenderDestination object for rendering to a client-managed buffer.\n\nDiscussion\n\nThe destination's colorSpace property will default to a CGColorSpace created with sRGB, extendedSRGB, or genericGrayGamma2_2.\n\nSee Also\nCreating a Render Destination\ninit(pixelBuffer: CVPixelBuffer)\nCreates a render destination based on a Core Video pixel buffer.\ninit(ioSurface: IOSurface)\nCreates a render destination based on an IOSurface object.\ninit(mtlTexture: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?)\nCreates a render destination based on a Metal texture.\ninit(width: Int, height: Int, pixelFormat: MTLPixelFormat, commandBuffer: (any MTLCommandBuffer)?, mtlTextureProvider: (() -> any MTLTexture)?)\nCreates a render destination based on a Metal texture with specified pixel format.\ninit(glTexture: UInt32, target: UInt32, width: Int, height: Int)\nCreates a render destination based on an OpenGL texture."
  },
  {
    "title": "CIRenderDestinationAlphaMode",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestinationalphamode",
    "html": "Topics\nEnumeration Cases\ncase none\nDesignates a destination with no alpha compositing.\ncase premultiplied\nDesignates a destination that expects premultiplied alpha values.\ncase unpremultiplied\nDesignates a destination that expects non-premultiplied alpha values.\nRelationships\nConforms To\nSendable\nSee Also\nCustomizing Rendering\nvar alphaMode: CIRenderDestinationAlphaMode\nThe render destination's representation of alpha (transparency) values.\nvar blendKernel: CIBlendKernel?\nThe destination's blend kernel.\nvar blendsInDestinationColorSpace: Bool\nIndicator of whether to blend in the destination's color space.\nvar colorSpace: CGColorSpace?\nThe destination's color space.\nvar width: Int\nThe render destination's row width.\nvar height: Int\nThe render destination's buffer height.\nvar isClamped: Bool\nIndicator of whether or not the destination clamps.\nvar isDithered: Bool\nIndicator of whether or not the destination dithers.\nvar isFlipped: Bool\nIndicator of whether the destination is flipped."
  },
  {
    "title": "blendKernel",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination/2875452-blendkernel",
    "html": "See Also\nCustomizing Rendering\nvar alphaMode: CIRenderDestinationAlphaMode\nThe render destination's representation of alpha (transparency) values.\nenum CIRenderDestinationAlphaMode\nDifferent ways of representing alpha.\nvar blendsInDestinationColorSpace: Bool\nIndicator of whether to blend in the destination's color space.\nvar colorSpace: CGColorSpace?\nThe destination's color space.\nvar width: Int\nThe render destination's row width.\nvar height: Int\nThe render destination's buffer height.\nvar isClamped: Bool\nIndicator of whether or not the destination clamps.\nvar isDithered: Bool\nIndicator of whether or not the destination dithers.\nvar isFlipped: Bool\nIndicator of whether the destination is flipped."
  },
  {
    "title": "blendsInDestinationColorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination/2875437-blendsindestinationcolorspace",
    "html": "See Also\nCustomizing Rendering\nvar alphaMode: CIRenderDestinationAlphaMode\nThe render destination's representation of alpha (transparency) values.\nenum CIRenderDestinationAlphaMode\nDifferent ways of representing alpha.\nvar blendKernel: CIBlendKernel?\nThe destination's blend kernel.\nvar colorSpace: CGColorSpace?\nThe destination's color space.\nvar width: Int\nThe render destination's row width.\nvar height: Int\nThe render destination's buffer height.\nvar isClamped: Bool\nIndicator of whether or not the destination clamps.\nvar isDithered: Bool\nIndicator of whether or not the destination dithers.\nvar isFlipped: Bool\nIndicator of whether the destination is flipped."
  },
  {
    "title": "colorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination/2875439-colorspace",
    "html": "See Also\nCustomizing Rendering\nvar alphaMode: CIRenderDestinationAlphaMode\nThe render destination's representation of alpha (transparency) values.\nenum CIRenderDestinationAlphaMode\nDifferent ways of representing alpha.\nvar blendKernel: CIBlendKernel?\nThe destination's blend kernel.\nvar blendsInDestinationColorSpace: Bool\nIndicator of whether to blend in the destination's color space.\nvar width: Int\nThe render destination's row width.\nvar height: Int\nThe render destination's buffer height.\nvar isClamped: Bool\nIndicator of whether or not the destination clamps.\nvar isDithered: Bool\nIndicator of whether or not the destination dithers.\nvar isFlipped: Bool\nIndicator of whether the destination is flipped."
  },
  {
    "title": "CIRAWFilterOption",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilteroption",
    "html": "Topics\nInitializers\ninit(rawValue: String)\nType Properties\nstatic let activeKeys: CIRAWFilterOption\nA key for the set of input keys available for use. The associated value is an NSSet object containing the set of input keys which may be used to affect the output image. (Depending on the input image type and the decoder version, some input keys may be unavailable.) This key is read-only.\nstatic let allowDraftMode: CIRAWFilterOption\nA key for allowing draft mode. The associated value is a Boolean value packaged as an NSNumber object. It’s best not to use draft mode if the image needs to be drawn without draft mode at a later time, because changing the value from true to false is an expensive operation. If the optional scale factor is smaller than a certain value, additionally setting draft mode can improve image decoding speed without any perceivable loss of quality. However, turning on draft mode does not have any effect if the scale factor is not below this threshold.\nstatic let baselineExposure: CIRAWFilterOption\nA key for an NSNumber object containing a float that expresses the amount of baseline exposure applied to an image.\nstatic let boostAmount: CIRAWFilterOption\nA key for the amount of boost to apply to an image. The associated value is a floating-point value packaged as an NSNumber object. The value must be in the range of 0...1. A value of 0 indicates no boost, that is, a linear response. The default value is 1, which indicates full boost.\nstatic let boostShadowAmount: CIRAWFilterOption\nA key for the amount to boost the shadow areas of the image. The associated value must be an NSNumber object that specifies floating-point value. The value has no effect if the image used for initialization is not RAW.\nstatic let ciInputEnableEDRModeKey: CIRAWFilterOption\nstatic let ciInputLocalToneMapAmountKey: CIRAWFilterOption\nstatic let colorNoiseReductionAmount: CIRAWFilterOption\nA key for the amount of noise reduction to apply to color data in the image.\nstatic let decoderVersion: CIRAWFilterOption\nA key for the version number of the method to be used for decoding. A newly initialized object defaults to the newest available decoder version for the given image type. You can request an alternative, older version to maintain compatibility with older releases. Must be one of the values listed for the supportedDecoderVersions key, otherwise a nil output image is generated. The associated value must be an NSNumber object that specifies an integer value in range of 0 to the current decoder version. When you request a specific version of the decoder, Core Image produces an image that is visually the same across different versions of the operating system. Core Image, however, does not guarantee that the same bits are produced across different versions of the operating system. That’s because the rounding behavior of floating-point arithmetic can vary due to differences in compilers or hardware. Note that this option has no effect if the image used for initialization is not RAW.\nstatic let disableGamutMap: CIRAWFilterOption\nA key for an NSNumber object containing a Boolean value that determines whether or not to disable gamut mapping.\nstatic let enableChromaticNoiseTracking: CIRAWFilterOption\nA key for progressive chromatic noise tracking (based on ISO and exposure time). The associated value must be an NSNumber object that specifies a BOOL value (true or false). The default is true. This option has no effect if the image used for initialization is not RAW.\nstatic let enableSharpening: CIRAWFilterOption\nA key for the sharpening state. The associated value must be an NSNumber object that specifies a BOOL value (true or false). The default is true. This option has no effect if the image used for initialization is not RAW.\nstatic let enableVendorLensCorrection: CIRAWFilterOption\nA key for whether to automatically correct for image distortion from known lenses.\nstatic let ignoreImageOrientation: CIRAWFilterOption\nA key for specifying whether to ignore the image orientation. The associated value is a Boolean value packaged as an NSNumber object. The default value is false. An image is usually loaded in its proper orientation, as long as the associated metadata records its orientation. For special purposes you might want to load the image in its physical orientation. The exact meaning of \"physical orientation” is dependent on the specific image.\nstatic let imageOrientation: CIRAWFilterOption\nA key for the image orientation. The associated value is an integer value packaged as an NSNumber object. Valid values are in range 1...8 and follow the EXIF specification. The value is disregarded when the kCIIgnoreImageOrientationKey flag is set. You can change the orientation of the image by overriding this value. By changing this value you can rotate an image in 90-degree increments.\nstatic let linearSpaceFilter: CIRAWFilterOption\nA key for the filter to apply to the image while it is temporarily in a linear color space as part of RAW image processing. The associated value must be a CIFilter object.\nstatic let luminanceNoiseReductionAmount: CIRAWFilterOption\nA key for the amount of noise reduction to apply to luminance data in the image.\nstatic let moireAmount: CIRAWFilterOption\nA key for an NSNumber object containing a double that expresses the amount of moiré reduction applied to an image.\nstatic let neutralChromaticityX: CIRAWFilterOption\nThe x value of the chromaticity. The associated value is a floating-point value packaged as an NSNumber object. You can query this value to get the current x value for neutral x, y.\nstatic let neutralChromaticityY: CIRAWFilterOption\nThe y value of the chromaticity. The associated value is a floating-point value packaged as an NSNumber object. You can query this value to get the current y value for neutral x, y.\nstatic let neutralLocation: CIRAWFilterOption\nA key for the neutral position. Use this key to set the location in geometric coordinates of the unrotated output image that should be used as neutral. You cannot query this value; it is undefined for reading. The associated value is a two-element CIVector object that specifies the location (x, y).\nstatic let neutralTemperature: CIRAWFilterOption\nA key for neutral temperature. The associated value is a floating-point value packaged as an NSNumber object. You can query this value to get the current temperature value.\nstatic let neutralTint: CIRAWFilterOption\nA key for the neutral tint. The associated value is a floating-point value packaged as an NSNumber object. Use this key to set or fetch the temperature and tint values. You can query this value to get the current tint value.\nstatic let noiseReductionAmount: CIRAWFilterOption\nA key for the amount to reduce noise in the image. The associated value must be an NSNumber object that specifies a floating-point value between 0.0 and 1.0. The value has no effect if the image used for initialization is not RAW.\nstatic let noiseReductionContrastAmount: CIRAWFilterOption\nA key for the amount of contrast enhancement to apply during noise reduction.\nstatic let noiseReductionDetailAmount: CIRAWFilterOption\nA key for the amount of detail enhancement to apply during noise reduction.\nstatic let noiseReductionSharpnessAmount: CIRAWFilterOption\nA key for the amount of sharpness enhancement to apply during noise reduction.\nstatic let outputNativeSize: CIRAWFilterOption\nA key for the full native size of the original, non-transformed RAW image. The associated value is a CIVector object whose X and Y values are the image’s width and height. This key is read-only.\nstatic let propertiesKey: CIRAWFilterOption\nstatic let scaleFactor: CIRAWFilterOption\nA key for the scale factor. The associated value is a floating-point value packaged as an NSNumber object that specifies the desired scale factor at which the image will be drawn. Setting this value can greatly improve the drawing performance. A value of 1 is the identity. In some cases, if you change the scale factor and enable draft mode, performance can decrease. See allowDraftMode.\nstatic let supportedDecoderVersions: CIRAWFilterOption\nA key for the supported decoder versions. The associated value is an NSArray object that contains all supported decoder versions for the given image type, sorted in increasingly newer order. Each entry is an NSDictionary object that contains key-value pairs. All entries represent a valid version identifier that can be passed as the kCIDecoderVersion value for the key kCIDecoderMethodKey. Version values are read-only; attempting to set this value raises an exception. Currently, the only defined key is @\"version\" which has as its value an NSString that uniquely describing a given decoder version. This string might not be suitable for user interface display..\nRelationships\nConforms To\nEquatable\nHashable\nRawRepresentable\nSendable"
  },
  {
    "title": "bokehBlurFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228277-bokehblurfilter",
    "html": "Return Value\n\nThe blurred image.\n\nDiscussion\n\nThis method applies the bokeh blur filter to an image. The effect targets a circular area of pixels defined by the radius and blurs the area. The filter adds smaller intense blur rings.\n\nThe bokeh blur filter uses the following properties:\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\nringSize\n\nA float representing the ring size of the bokeh as an NSNumber.\n\nringAmount\n\nA float representing the emphasis at the ring of the bokeh as an NSNumber.\n\nsoftness\n\nA float representing the softness of the bokeh effect as an NSNumber.\n\ninputImage\n\nA CIImage representing the input image to apply the filter to.\n\nThe following code creates a filter that adds a softer blur to the input image:\n\n    func bokehBlur(inputImage: CIImage) -> CIImage? {\n\n\n        let bokehBlurFilter = CIFilter.bokehBlur()\n        bokehBlurFilter.inputImage = inputImage\n        bokehBlurFilter.ringSize = 0.1\n        bokehBlurFilter.ringAmount = 0\n        bokehBlurFilter.softness = 1\n        bokehBlurFilter.radius = 20\n        return bokehBlurFilter.outputImage\n    }\n\n\nSee Also\nFilters\n+ boxBlurFilter\nApplies a square-shaped blur to an area of an image.\n+ discBlurFilter\nApplies a circle-shaped blur to an area of an image.\n+ gaussianBlurFilter\nBlurs an image with a Gaussian distribution pattern.\n+ maskedVariableBlurFilter\nBlurs a specified portion of an image.\n+ medianFilter\nCalculates the median of an image to refine detail.\n+ morphologyGradientFilter\nDetects and highlights edges of objects.\n+ morphologyMaximumFilter\nBlurs a circular area by enlarging contrasting pixels.\n+ morphologyMinimumFilter\nBlurs a circular area by reducing contrasting pixels.\n+ morphologyRectangleMaximumFilter\nBlurs a rectangular area by enlarging contrasting pixels.\n+ morphologyRectangleMinimumFilter\nBlurs a rectangular area by reducing contrasting pixels.\n+ motionBlurFilter\nCreates motion blur on an image.\n+ noiseReductionFilter\nReduces noise by sharpening the edges of objects.\n+ zoomBlurFilter\nCreates a zoom blur centered around a single point on the image."
  },
  {
    "title": "discBlurFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228311-discblurfilter",
    "html": "Return Value\n\nThe blurred image.\n\nDiscussion\n\nThis method applies the disc blur filter to an image. The effect targets the pixels within a circle defined by a radius and calculates the median color value to create to the output image.\n\nThe disc blur filter uses the following properties:\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\ninputImage\n\nA CIImage representing the input image to apply the filter to.\n\nThe following code creates a filter that adds a strong blur to the input image:\n\n    func discBlur(inputImage: CIImage) -> CIImage? {\n\n\n        let discBlurFilter = CIFilter.discBlur()\n        discBlurFilter.inputImage = inputImage\n        discBlurFilter.radius = 8\n        return discBlurFilter.outputImage\n    }\n\n\nSee Also\nFilters\n+ bokehBlurFilter\nApplies a bokeh effect to an image.\n+ boxBlurFilter\nApplies a square-shaped blur to an area of an image.\n+ gaussianBlurFilter\nBlurs an image with a Gaussian distribution pattern.\n+ maskedVariableBlurFilter\nBlurs a specified portion of an image.\n+ medianFilter\nCalculates the median of an image to refine detail.\n+ morphologyGradientFilter\nDetects and highlights edges of objects.\n+ morphologyMaximumFilter\nBlurs a circular area by enlarging contrasting pixels.\n+ morphologyMinimumFilter\nBlurs a circular area by reducing contrasting pixels.\n+ morphologyRectangleMaximumFilter\nBlurs a rectangular area by enlarging contrasting pixels.\n+ morphologyRectangleMinimumFilter\nBlurs a rectangular area by reducing contrasting pixels.\n+ motionBlurFilter\nCreates motion blur on an image.\n+ noiseReductionFilter\nReduces noise by sharpening the edges of objects.\n+ zoomBlurFilter\nCreates a zoom blur centered around a single point on the image."
  },
  {
    "title": "init(imageURL:)",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter/3801630-init",
    "html": "Parameters\nurl\n\nThe URL location of the image.\n\nSee Also\nCreating a filter\ninit?(cvPixelBuffer: CVPixelBuffer, properties: [AnyHashable : Any])\nCreates a RAW filter from the pixel buffer and its properties that you specify.\ninit?(imageData: Data, identifierHint: String?)\nCreates a RAW filter from the image data and type hint that you specify."
  },
  {
    "title": "outputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cifilterprotocol/3228048-outputimage",
    "html": "Required"
  },
  {
    "title": "columnCount",
    "url": "https://developer.apple.com/documentation/coreimage/cidatamatrixcodedescriptor/2875202-columncount",
    "html": "Discussion\n\nRefer to ISO/IEC 16022:2006(E) for valid module row and column count combinations.\n\nSee Also\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload that comprises the Data Matrix code symbol.\nvar rowCount: Int\nThe number of module rows.\nvar eccVersion: CIDataMatrixCodeDescriptor.ECCVersion\nThe Data Matrix code ECC version."
  },
  {
    "title": "subFeatures",
    "url": "https://developer.apple.com/documentation/coreimage/citextfeature/1437810-subfeatures",
    "html": "Discussion\n\nA text detector can identify both a major region that is likely to contain text as well as the areas within that region that likely to contain individual text features. Such features might be single characters, groups of closely-packed characters, or entire words.\n\nCore Image populates this array only if you enable the CIDetectorReturnSubFeatures option when retrieving features."
  },
  {
    "title": "destination",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867422-destination",
    "html": "Discussion\n\nFigure 1 The result of using the destination blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "hue",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867408-hue",
    "html": "Discussion\n\nFigure 1 The result of using the hue blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "luminosity",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867423-luminosity",
    "html": "Discussion\n\nFigure 1 The result of using the luminosity blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "topRight",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodefeature/1437896-topright",
    "html": "See Also\nIdentifying the Corners of a Detected Barcode\nvar bottomLeft: CGPoint\nThe lower-left corner of the detected barcode, in image coordinates.\nvar bottomRight: CGPoint\nThe lower-right corner of the detected barcode, in image coordinates.\nvar topLeft: CGPoint\nThe upper-left corner of the detected barcode, in image coordinates."
  },
  {
    "title": "topLeft",
    "url": "https://developer.apple.com/documentation/coreimage/citextfeature/1438221-topleft",
    "html": "See Also\nIdentifying the Corners of a Detected Text Region\nvar bottomLeft: CGPoint\nThe lower-left corner of the detected text region, in image coordinates.\nvar bottomRight: CGPoint\nThe lower-right corner of the detected text region, in image coordinates.\nvar topRight: CGPoint\nThe upper-right corner of the detected text region, in image coordinates."
  },
  {
    "title": "bottomRight",
    "url": "https://developer.apple.com/documentation/coreimage/citextfeature/1437659-bottomright",
    "html": "See Also\nIdentifying the Corners of a Detected Text Region\nvar bottomLeft: CGPoint\nThe lower-left corner of the detected text region, in image coordinates.\nvar topLeft: CGPoint\nThe upper-left corner of the detected text region, in image coordinates.\nvar topRight: CGPoint\nThe upper-right corner of the detected text region, in image coordinates."
  },
  {
    "title": "multiply",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867419-multiply",
    "html": "Discussion\n\nFigure 1 The result of using the multiply blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "bottomLeft",
    "url": "https://developer.apple.com/documentation/coreimage/citextfeature/1438004-bottomleft",
    "html": "See Also\nIdentifying the Corners of a Detected Text Region\nvar bottomRight: CGPoint\nThe lower-right corner of the detected text region, in image coordinates.\nvar topLeft: CGPoint\nThe upper-left corner of the detected text region, in image coordinates.\nvar topRight: CGPoint\nThe upper-right corner of the detected text region, in image coordinates."
  },
  {
    "title": "pixelBuffer",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorinput/1639649-pixelbuffer",
    "html": "Required\n\nDiscussion\n\nUse this property if you plan to process the image using routines that can make use of memory shared with Core Video. For example, if your image processing technology uses OpenGL or OpenGL ES, you can create a GL texture from a Core Video pixel buffer with the CVOpenGLBufferPool or CVOpenGLESTextureCache API.\n\nDo not modify the contents of this buffer.\n\nSee Also\nAccessing Input Image Data\nvar baseAddress: UnsafeRawPointer\nA pointer to the image data in CPU memory to be processed.\n\nRequired\n\nvar metalTexture: (any MTLTexture)?\nA Metal texture containing the image data to be processed.\n\nRequired\n\nvar surface: IOSurfaceRef\nAn IOSurface object containing the image data to be processed.\n\nRequired"
  },
  {
    "title": "region",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorinput/1639633-region",
    "html": "Required\n\nDiscussion\n\nThis region will always contain, but may be larger than, the region produced by the roiCallback block parameter of the withExtent(_:processorDescription:argumentDigest:inputFormat:outputFormat:options:roiCallback:processor:) method.\n\nSee Also\nGetting Supplemental Information for Image Processing\nvar bytesPerRow: Int\nThe number of bytes per row of pixels in the input image data.\n\nRequired\n\nvar format: CIFormat\nThe per-pixel data format of the image to be processed.\n\nRequired"
  },
  {
    "title": "format",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorinput/1639639-format",
    "html": "Required\n\nSee Also\nGetting Supplemental Information for Image Processing\nvar region: CGRect\nThe area within the input image to be processed.\n\nRequired\n\nvar bytesPerRow: Int\nThe number of bytes per row of pixels in the input image data.\n\nRequired"
  },
  {
    "title": "bytesPerRow",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorinput/1639655-bytesperrow",
    "html": "Required\n\nSee Also\nGetting Supplemental Information for Image Processing\nvar region: CGRect\nThe area within the input image to be processed.\n\nRequired\n\nvar format: CIFormat\nThe per-pixel data format of the image to be processed.\n\nRequired"
  },
  {
    "title": "metalTexture",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorinput/1639651-metaltexture",
    "html": "Required\n\nDiscussion\n\nUse this property if you plan to process the image using a Metal shader.\n\nDo not modify the contents of this texture.\n\nSee Also\nAccessing Input Image Data\nvar baseAddress: UnsafeRawPointer\nA pointer to the image data in CPU memory to be processed.\n\nRequired\n\nvar pixelBuffer: CVPixelBuffer?\nA CoreVideo pixel buffer containing the image data to be processed.\n\nRequired\n\nvar surface: IOSurfaceRef\nAn IOSurface object containing the image data to be processed.\n\nRequired"
  },
  {
    "title": "vividLight",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867358-vividlight",
    "html": "Discussion\n\nFigure 1 The result of using the vivid light blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color."
  },
  {
    "title": "sourceOut",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867415-sourceout",
    "html": "Discussion\n\nFigure 1 The result of using the source out blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "height",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination/2875433-height",
    "html": "See Also\nCustomizing Rendering\nvar alphaMode: CIRenderDestinationAlphaMode\nThe render destination's representation of alpha (transparency) values.\nenum CIRenderDestinationAlphaMode\nDifferent ways of representing alpha.\nvar blendKernel: CIBlendKernel?\nThe destination's blend kernel.\nvar blendsInDestinationColorSpace: Bool\nIndicator of whether to blend in the destination's color space.\nvar colorSpace: CGColorSpace?\nThe destination's color space.\nvar width: Int\nThe render destination's row width.\nvar isClamped: Bool\nIndicator of whether or not the destination clamps.\nvar isDithered: Bool\nIndicator of whether or not the destination dithers.\nvar isFlipped: Bool\nIndicator of whether the destination is flipped."
  },
  {
    "title": "sourceIn",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867428-sourcein",
    "html": "Discussion\n\nFigure 1 The result of using the source in blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "softLight",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867434-softlight",
    "html": "Discussion\n\nFigure 1 The result of using the soft light blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "subtract",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867370-subtract",
    "html": "Discussion\n\nFigure 1 The result of using the subtract blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "digest",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessoroutput/4048311-digest",
    "html": "Required"
  },
  {
    "title": "surface",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessoroutput/1639627-surface",
    "html": "Required\n\nDiscussion\n\nUse this property if you plan to process the image using routines that can make use of memory shared with other processes or technologies.\n\nSee Also\nProviding Output Image Data\nvar baseAddress: UnsafeMutableRawPointer\nA pointer to CPU memory at which to write output pixel data.\n\nRequired\n\nvar metalTexture: (any MTLTexture)?\nA Metal texture to which you can write output pixel data.\n\nRequired\n\nvar pixelBuffer: CVPixelBuffer?\nA CoreVideo pixel buffer to which you can write output pixel data.\n\nRequired"
  },
  {
    "title": "region",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessoroutput/1639629-region",
    "html": "Required\n\nDiscussion\n\nYour image processor block may be invoked multiple times to provide output for multiple regions, and the region for which output is needed may not match the bounds of the output buffer, texture, or surface.\n\nSee Also\nGetting Supplemental Information for Image Processing\nvar metalCommandBuffer: (any MTLCommandBuffer)?\nA command buffer to use for image processing using Metal.\n\nRequired\n\nvar bytesPerRow: Int\nThe number of bytes per row of pixels for the output image.\n\nRequired\n\nvar format: CIFormat\nThe per-pixel data format expected of the output image.\n\nRequired"
  },
  {
    "title": "pixelBuffer",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessoroutput/1639647-pixelbuffer",
    "html": "Required\n\nDiscussion\n\nUse this property to process the image using routines that can make use of memory shared with Core Video. For example, if your image processing technology uses OpenGL or OpenGL ES, you can create a GL texture from a Core Video pixel buffer with the CVOpenGLBufferPool or CVOpenGLESTextureCache API.\n\nSee Also\nProviding Output Image Data\nvar baseAddress: UnsafeMutableRawPointer\nA pointer to CPU memory at which to write output pixel data.\n\nRequired\n\nvar metalTexture: (any MTLTexture)?\nA Metal texture to which you can write output pixel data.\n\nRequired\n\nvar surface: IOSurfaceRef\nAn IOSurface object to which you can write output pixel data.\n\nRequired"
  },
  {
    "title": "screen",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867356-screen",
    "html": "Discussion\n\nFigure 1 The result of using the screen blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "metalCommandBuffer",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessoroutput/1639641-metalcommandbuffer",
    "html": "Required\n\nDiscussion\n\nIf you perform image processing with a Metal shader (and write output to the metalTexture property), encode your render or compute commands to this buffer. Core Image uses the same command buffer to render other effects that precede or follow your processor in a filter chain.\n\nSee Also\nGetting Supplemental Information for Image Processing\nvar region: CGRect\nThe rectangular region of the output image that your processor must provide.\n\nRequired\n\nvar bytesPerRow: Int\nThe number of bytes per row of pixels for the output image.\n\nRequired\n\nvar format: CIFormat\nThe per-pixel data format expected of the output image.\n\nRequired"
  },
  {
    "title": "metalTexture",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessoroutput/1639631-metaltexture",
    "html": "Required\n\nDiscussion\n\nFor image processing using a Metal shader, bind this texture as an attachment in a render pass or as an output texture in a compute pass.\n\nSee Also\nProviding Output Image Data\nvar baseAddress: UnsafeMutableRawPointer\nA pointer to CPU memory at which to write output pixel data.\n\nRequired\n\nvar pixelBuffer: CVPixelBuffer?\nA CoreVideo pixel buffer to which you can write output pixel data.\n\nRequired\n\nvar surface: IOSurfaceRef\nAn IOSurface object to which you can write output pixel data.\n\nRequired"
  },
  {
    "title": "baseAddress",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessoroutput/1639626-baseaddress",
    "html": "Required\n\nDiscussion\n\nUse this property if you process the image using a CPU-based routine that cannot make use of higher-level constructs for sharing memory.\n\nNote\n\nIf your image processing routine is GPU-based, use the the pixelBuffer, surface, or metalTexture property instead.\n\nSee Also\nProviding Output Image Data\nvar metalTexture: (any MTLTexture)?\nA Metal texture to which you can write output pixel data.\n\nRequired\n\nvar pixelBuffer: CVPixelBuffer?\nA CoreVideo pixel buffer to which you can write output pixel data.\n\nRequired\n\nvar surface: IOSurfaceRef\nAn IOSurface object to which you can write output pixel data.\n\nRequired"
  },
  {
    "title": "apply(foreground:background:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2919728-apply",
    "html": "Parameters\nforeground\n\nThe first input image to be blended\n\nbackground\n\nThe second input image to be blended\n\nReturn Value\n\nA CIImage blending the foreground and background images. Its extent will be the union of the foreground and background image extents.\n\nDiscussion\n\nThe foreground and background images are not treated differently in the blending. You can think of them as equivalents A and B; the foreground is not given any precedence over the background."
  },
  {
    "title": "swipeTransitionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228420-swipetransitionfilter",
    "html": "Return Value\n\nThe transition image.\n\nDiscussion\n\nThis method applies the swipe transition filter to an image. The effect transitions from the input image to the target image by simulating a swiping motion.\n\nThe swipe transition filter uses the following properties:\n\ninputImage\n\nThe starting image with the type CIImage.\n\ntargetImage\n\nThe ending image with the type CIImage.\n\nextent\n\nA CGRect representing the size of the rounded rectangle.\n\ntime\n\nA float representing the parametric time of the transition from start (at time 0) to end (at time 1) as an NSNumber.\n\nangle\n\nA float representing the angle of the motion of the swipe as an NSNumber.\n\nwidth\n\nA float representing the width of the swipe effect as an NSNumber.\n\nopacity\n\nA float representing the transparency of the swipe as an NSNumber.\n\nThe following code creates a filter that transitions from the input image to the target image with a gradual fade from left to right.\n\nfunc swipe(inputImage: CIImage, targetImage: CIImage) -> CIImage {\n    let swipeTransiton = CIFilter.swipeTransition()\n    swipeTransiton.inputImage = inputImage\n    swipeTransiton.targetImage = targetImage\n    swipeTransiton.extent = CGRect(x: 0, y: 0, width: 300, height: 300)\n    swipeTransiton.time = 0.5\n    swipeTransiton.angle = -0.7\n    swipeTransiton.width = 203\n    swipeTransiton.opacity = 0\n    return swipeTransiton.outputImage!\n}\n\n\nSee Also\nFilters\n+ accordionFoldTransitionFilter\nTransitions by folding and crossfading an image to reveal the target image.\n+ barsSwipeTransitionFilter\nTransitions between two images by removing rectangular portions of an image.\n+ copyMachineTransitionFilter\nSimulates the effect of a copy machine scanner light to transiton between two images.\n+ disintegrateWithMaskTransitionFilter\nTransitions between two images using a mask image.\n+ dissolveTransitionFilter\nTransitions between two images with a fade effect.\n+ flashTransitionFilter\nCreates a flash of light to transition between two images.\n+ modTransitionFilter\nTransitions between two images by applying irregularly shaped holes.\n+ pageCurlTransitionFilter\nSimulates the curl of a page, revealing the target image.\n+ pageCurlWithShadowTransitionFilter\nSimulates the curl of a page, revealing the target image with added shadow.\n+ rippleTransitionFilter\nSimulates a ripple in a pond to transiton from one image to another."
  },
  {
    "title": "disintegrateWithMaskTransitionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228312-disintegratewithmasktransitionfi",
    "html": "Return Value\n\nThe transition image.\n\nDiscussion\n\nThis method applies the disintegrate with mask transition filter to an image. The effect transitions from one image to another using the shapes defined by the mask image.\n\nThe disintegrate with mask transition filter uses the following properties:\n\ninputImage\n\nThe starting image with the type CIImage.\n\ntargetImage\n\nThe ending image with the type CIImage.\n\nmaskImage\n\nAn image with the type CIImage.\n\ntime\n\nA float representing the parametric time of the transition from start (at time 0) to end (at time 1) as an NSNumber.\n\nshadowRadius\n\nA float representing the size of the shadow as a NSNumber.\n\nshadowDensity\n\nA float representing the strength of the shadow as a NSNumber.\n\nshadowOffset\n\nA CGPoint representing the size of the shadow from the mask image.\n\nThe following code creates a filter that produces a transition between the input and target images starting in the area’s outline in the mask image:\n\nfunc disintergrate(inputImage: CIImage, targetImage: CIImage, maskImage: CIImage) -> CIImage {\n    let disintergrateTransition  = CIFilter.disintegrateWithMaskTransition()\n    disintergrateTransition.inputImage = inputImage\n    disintergrateTransition.targetImage = targetImage\n    disintergrateTransition.maskImage = maskImage\n    disintergrateTransition.time = 0.5\n    disintergrateTransition.shadowRadius = 8\n    disintergrateTransition.shadowDensity = 0.65\n    disintergrateTransition.shadowOffset = CGPoint(x: 0, y: -1)\n    return disintergrateTransition.outputImage!\n}\n\n\nSee Also\nFilters\n+ accordionFoldTransitionFilter\nTransitions by folding and crossfading an image to reveal the target image.\n+ barsSwipeTransitionFilter\nTransitions between two images by removing rectangular portions of an image.\n+ copyMachineTransitionFilter\nSimulates the effect of a copy machine scanner light to transiton between two images.\n+ dissolveTransitionFilter\nTransitions between two images with a fade effect.\n+ flashTransitionFilter\nCreates a flash of light to transition between two images.\n+ modTransitionFilter\nTransitions between two images by applying irregularly shaped holes.\n+ pageCurlTransitionFilter\nSimulates the curl of a page, revealing the target image.\n+ pageCurlWithShadowTransitionFilter\nSimulates the curl of a page, revealing the target image with added shadow.\n+ rippleTransitionFilter\nSimulates a ripple in a pond to transiton from one image to another.\n+ swipeTransitionFilter\nGradually transitions from one image to another with a swiping motion."
  },
  {
    "title": "copyMachineTransitionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228304-copymachinetransitionfilter",
    "html": "Return Value\n\nThe transition image.\n\nDiscussion\n\nThis method applies the copy machine transition filter to an image. The effect transitions from one image to another by simulating the scanning light effect of a copy machine.\n\nThe copy machine transition filter uses the following properties:\n\ninputImage\n\nThe starting image with the type CIImage.\n\ntargetImage\n\nThe ending image with the type CIImage.\n\ntime\n\nA float representing the parametric time of the transition from start (at time 0) to end (at time 1) as an NSNumber.\n\nangle\n\nA float representing the angle of the copier light, in radians as an NSNumber.\n\nwidth\n\nA float representing the width of the effect as a NSNumber.\n\nextent\n\nA CGRect representing the area of the copy machine effect.\n\ncolor\n\nA CIColor representing the color of the light.\n\nopacity\n\nA float representing the transparency of the copier light as an NSNumber.\n\nThe following code creates a filter that produces a light bar that glides across the input image revealing the target image:\n\nfunc copyMachine(inputImage: CIImage, targetImage: CIImage) -> CIImage {\n    let copyMachineTransition = CIFilter.copyMachineTransition()\n    copyMachineTransition.inputImage = inputImage\n    copyMachineTransition.targetImage = targetImage\n    copyMachineTransition.time = 0.5\n    copyMachineTransition.angle = 0.9\n    copyMachineTransition.extent = CGRect(x: 54.1, y: 90.2, width: 300, height: 300)\n    copyMachineTransition.color = .white\n    copyMachineTransition.width = 200\n    copyMachineTransition.opacity = 1.30   \n    return copyMachineTransition.outputImage!\n}\n\n\nSee Also\nFilters\n+ accordionFoldTransitionFilter\nTransitions by folding and crossfading an image to reveal the target image.\n+ barsSwipeTransitionFilter\nTransitions between two images by removing rectangular portions of an image.\n+ disintegrateWithMaskTransitionFilter\nTransitions between two images using a mask image.\n+ dissolveTransitionFilter\nTransitions between two images with a fade effect.\n+ flashTransitionFilter\nCreates a flash of light to transition between two images.\n+ modTransitionFilter\nTransitions between two images by applying irregularly shaped holes.\n+ pageCurlTransitionFilter\nSimulates the curl of a page, revealing the target image.\n+ pageCurlWithShadowTransitionFilter\nSimulates the curl of a page, revealing the target image with added shadow.\n+ rippleTransitionFilter\nSimulates a ripple in a pond to transiton from one image to another.\n+ swipeTransitionFilter\nGradually transitions from one image to another with a swiping motion."
  },
  {
    "title": "clear",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867388-clear",
    "html": "See Also\nBuiltin Blend Kernels\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "componentMultiply",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867406-componentmultiply",
    "html": "Discussion\n\nFigure 1 The result of using the component multiply blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "componentMin",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867425-componentmin",
    "html": "Discussion\n\nFigure 1 The result of using the component min blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "colorDodge",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867417-colordodge",
    "html": "Discussion\n\nFigure 1 The result of using the color dodge blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "darken",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867348-darken",
    "html": "Discussion\n\nFigure 1 The result of using the darken blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "darkerColor",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867351-darkercolor",
    "html": "Discussion\n\nFigure 1 The result of using the darker color blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "bounds",
    "url": "https://developer.apple.com/documentation/coreimage/citextfeature/1437885-bounds",
    "html": "Discussion\n\nThis property identifies the rectangular region of the image containing the detected text region, not necessarily the shape of the region. A detected feature is rectangular in space, but may appear in perspective in the image. Use the properties listed in Identifying the Corners of a Detected Text Region to find the corners of the rectangle as it appears in perspective.\n\nSee Also\nRelated Documentation\nCore Image Programming Guide"
  },
  {
    "title": "init(source:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867353-init",
    "html": "Parameters\nstring\n\nA program in the Core Image Kernel Language that contains a single routine marked using the kernel keyword.\n\nReturn Value\n\nA new blend kernel object, or nil if the specified source code does not contain a valid blend kernel routine.\n\nDiscussion\n\nThis method is similar to the init(source:) method of the superclass CIKernel, but creates only blend kernels. Use this method when you want to ensure that the type of kernel object returned (if any) is always CIBlendKernel."
  },
  {
    "title": "bottomRight",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodefeature/1438245-bottomright",
    "html": "See Also\nIdentifying the Corners of a Detected Barcode\nvar bottomLeft: CGPoint\nThe lower-left corner of the detected barcode, in image coordinates.\nvar topLeft: CGPoint\nThe upper-left corner of the detected barcode, in image coordinates.\nvar topRight: CGPoint\nThe upper-right corner of the detected barcode, in image coordinates."
  },
  {
    "title": "bottomLeft",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodefeature/1437985-bottomleft",
    "html": "See Also\nIdentifying the Corners of a Detected Barcode\nvar bottomRight: CGPoint\nThe lower-right corner of the detected barcode, in image coordinates.\nvar topLeft: CGPoint\nThe upper-left corner of the detected barcode, in image coordinates.\nvar topRight: CGPoint\nThe upper-right corner of the detected barcode, in image coordinates."
  },
  {
    "title": "bounds",
    "url": "https://developer.apple.com/documentation/coreimage/cirectanglefeature/1438024-bounds",
    "html": "Discussion\n\nThis property identifies the rectangular region of the image containing the detected rectangle, not necessarily the shape of the rectangle. A detected feature is rectangular in space, but may appear in perspective in the image. Use the properties listed in CIRectangleFeature to find the corners of the rectangle as it appears in perspective."
  },
  {
    "title": "init(pixelBuffer:)",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination/2875436-init",
    "html": "Parameters\npixelBuffer\n\nThe CVPixelBuffer render target.\n\nReturn Value\n\nA CIRenderDestination object for rendering to a CVPixelBuffer.\n\nDiscussion\n\nThe destination's colorSpace property will default to a CGColorSpace created by querying the CVPixelBuffer object's attributes.\n\nSee Also\nCreating a Render Destination\ninit(ioSurface: IOSurface)\nCreates a render destination based on an IOSurface object.\ninit(mtlTexture: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?)\nCreates a render destination based on a Metal texture.\ninit(width: Int, height: Int, pixelFormat: MTLPixelFormat, commandBuffer: (any MTLCommandBuffer)?, mtlTextureProvider: (() -> any MTLTexture)?)\nCreates a render destination based on a Metal texture with specified pixel format.\ninit(glTexture: UInt32, target: UInt32, width: Int, height: Int)\nCreates a render destination based on an OpenGL texture.\ninit(bitmapData: UnsafeMutableRawPointer, width: Int, height: Int, bytesPerRow: Int, format: CIFormat)\nCreates a render destination based on a client-managed buffer."
  },
  {
    "title": "init(ioSurface:)",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination/2876044-init",
    "html": "Parameters\nsurface\n\nThe IOSurface render target.\n\nReturn Value\n\nA CIRenderDestination object for rendering to an IOSurface object.\n\nDiscussion\n\nThe destination's colorSpace property will default to a CGColorSpace created by querying the IOSurface object's attributes.\n\nSee Also\nCreating a Render Destination\ninit(pixelBuffer: CVPixelBuffer)\nCreates a render destination based on a Core Video pixel buffer.\ninit(mtlTexture: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?)\nCreates a render destination based on a Metal texture.\ninit(width: Int, height: Int, pixelFormat: MTLPixelFormat, commandBuffer: (any MTLCommandBuffer)?, mtlTextureProvider: (() -> any MTLTexture)?)\nCreates a render destination based on a Metal texture with specified pixel format.\ninit(glTexture: UInt32, target: UInt32, width: Int, height: Int)\nCreates a render destination based on an OpenGL texture.\ninit(bitmapData: UnsafeMutableRawPointer, width: Int, height: Int, bytesPerRow: Int, format: CIFormat)\nCreates a render destination based on a client-managed buffer."
  },
  {
    "title": "init(mtlTexture:commandBuffer:)",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination/2880273-init",
    "html": "Parameters\ntexture\n\nThe MTLTexture object for rendering with MTLTextureType of MTLTextureType.type2D.\n\ncommandBuffer\n\nAn optional MTLCommandBuffer to use for rendering to the MTLTexture destination.\n\nReturn Value\n\nA CIRenderDestination object for rendering to a Metal buffer.\n\nDiscussion\n\nRendering to a MTLTexture-backed CIRenderDestination is supported by only MTLTexture-backed CIContext objects. The texture must have MTLTextureType of MTLTextureType.type2D.\n\nThe destination's colorSpace property will default to a CGColorSpace created with sRGB, extendedSRGB, or genericGrayGamma2_2.\n\nSee Also\nCreating a Render Destination\ninit(pixelBuffer: CVPixelBuffer)\nCreates a render destination based on a Core Video pixel buffer.\ninit(ioSurface: IOSurface)\nCreates a render destination based on an IOSurface object.\ninit(width: Int, height: Int, pixelFormat: MTLPixelFormat, commandBuffer: (any MTLCommandBuffer)?, mtlTextureProvider: (() -> any MTLTexture)?)\nCreates a render destination based on a Metal texture with specified pixel format.\ninit(glTexture: UInt32, target: UInt32, width: Int, height: Int)\nCreates a render destination based on an OpenGL texture.\ninit(bitmapData: UnsafeMutableRawPointer, width: Int, height: Int, bytesPerRow: Int, format: CIFormat)\nCreates a render destination based on a client-managed buffer."
  },
  {
    "title": "CIFeatureTypeText",
    "url": "https://developer.apple.com/documentation/coreimage/cifeaturetypetext",
    "html": "Discussion\n\nUse the CITextFeature class to find more information about the detected feature.\n\nSee Also\nFeature Types\nlet CIFeatureTypeFace: String\nThe discovered feature is a person’s face.\nlet CIFeatureTypeRectangle: String\nThe discovered feature is a rectangular object, though it might appear in perspective in the image.\nlet CIFeatureTypeQRCode: String\nThe discovered feature is a Quick Response code (2D barcode)."
  },
  {
    "title": "pointillizeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228394-pointillizefilter",
    "html": "Return Value\n\nA CIImage containing the pointillized image.\n\nDiscussion\n\nThis filter applies a pointillize effect to an image. The effect generates an output image made of small, single-color, circular points distributed on a randomly perturbed grid.\n\nThe pointillize filter uses the following properties:\n\ninputImage\n\nA CIImage containing the input image.\n\nradius\n\nThe radius in pixels of the circular points.\n\ncenter\n\nDetermines the origin of the grid.\n\nThe following code applies the pointillize filter with a radius of 40 pixels.\n\nfunc pointillize(inputImage: CIImage) -> CIImage {\n    let pointillizeFilter = CIFilter.pointillize()\n    pointillizeFilter.inputImage = inputImage\n    pointillizeFilter.radius = 40\n    pointillizeFilter.center = CGPoint(x: 0,y: 0)\n    return pointillizeFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "exclusion",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867343-exclusion",
    "html": "Discussion\n\nFigure 1 The result of using the exclusion blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "destinationIn",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867352-destinationin",
    "html": "Discussion\n\nFigure 1 The result of using the destination in blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "hardMix",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867347-hardmix",
    "html": "Discussion\n\nFigure 1 The result of using the hard mix blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "destinationOver",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867432-destinationover",
    "html": "Discussion\n\nFigure 1 The result of using the destination over blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "destinationOut",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867368-destinationout",
    "html": "Discussion\n\nFigure 1 The result of using the destination out blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "difference",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867416-difference",
    "html": "Discussion\n\nFigure 1 The result of using the difference blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "exclusiveOr",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867421-exclusiveor",
    "html": "Discussion\n\nFigure 1 The result of using the exclusive or blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "divide",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867410-divide",
    "html": "Discussion\n\nFigure 1 The result of using the divide blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "hardLight",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867418-hardlight",
    "html": "Discussion\n\nFigure 1 The result of using the hard light blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "destinationAtop",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867385-destinationatop",
    "html": "Discussion\n\nFigure 1 The result of using the destination atop blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "lighten",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867424-lighten",
    "html": "Discussion\n\nFigure 1 The result of using the lighten blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "lighterColor",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867427-lightercolor",
    "html": "Discussion\n\nFigure 1 The result of using the lighter color blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "linearBurn",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867409-linearburn",
    "html": "Discussion\n\nFigure 1 The result of using the linear burn blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "linearLight",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867435-linearlight",
    "html": "Discussion\n\nFigure 1 The result of using the linear light blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "linearDodge",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867354-lineardodge",
    "html": "Discussion\n\nFigure 1 The result of using the linear dodge blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "overlay",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867411-overlay",
    "html": "Discussion\n\nFigure 1 The result of using the overlay blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "pinLight",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867420-pinlight",
    "html": "Discussion\n\nFigure 1 The result of using the pin light blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "saturation",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867431-saturation",
    "html": "Discussion\n\nFigure 1 The result of using the saturation blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "apply(withExtent:inputs:arguments:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorkernel/2138284-apply",
    "html": "Parameters\nextent\n\nThe extent of the image to which to apply the kernel.\n\ninput\n\nThe input source image to process.\n\nargs\n\nDictionary of arguments mapping keys such as \"thresholdValue\" to their values.\n\nerror\n\nPointer to the NSError object into which processing errors will be written.\n\nReturn Value\n\nThe output image resulting from applying the custom image processor kernel.\n\nDiscussion\n\nApplies a custom image processor kernel to an image.\n\nImportant\n\nCore Image will concatenate filters in a network into as fewer kernels as possible, avoiding the creation of intermediate buffers. However, it is unable to do this with image processor kernels."
  },
  {
    "title": "formatForInput(at:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorkernel/2138289-formatforinput",
    "html": "Parameters\ninput\n\nIndex of this image processor in a multi-step workflow.\n\nReturn Value\n\nCore Image pixel format CIFormat constant revealing the processor's input pixel format.\n\nDiscussion\n\nOverride this method and the outputFormat property getter to customize your processor's input and output pixel format.\n\nThe format must be one of CIFormat: BGRA8, RGBAh, RGBAf, or R8. If the outputFormat is 0, then the output will be a supported format that best matches the rendering context's workingFormat."
  },
  {
    "title": "roi(forInput:arguments:outputRect:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorkernel/2138287-roi",
    "html": "Parameters\ninput\n\nIndex of the input image.\n\narguments\n\nDictionary of arguments mapping keys such as \"thresholdValue\" to their values.\n\noutputRect\n\nRectangle defining the area of output that must be rendered.\n\nReturn Value\n\nRectangle defining the region of the input image over which the kernel should execute.\n\nDiscussion\n\nOverride this method if your image processor needs to work with a larger or smaller region of interest in the input image than each corresponding region of the output image (for example, a blur filter, which samples several input pixels for each output pixel).\n\nThis will be called one or more times per render to determine the portion of the input images needed to render a given outputRect of the output. This will not be called if there are 0 input images.\n\nThe default implementation simply returns outputRect."
  },
  {
    "title": "process(with:arguments:output:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorkernel/2138290-process",
    "html": "Parameters\ninputs\n\nInputs to this processor stage.\n\narguments\n\nDictionary of arguments mapping keys such as \"thresholdValue\" to their values.\n\noutput\n\nThe output image following processing.\n\nerror\n\nPointer to the NSError object into which processing errors will be written.\n\nReturn Value\n\nReturns true if processing succeeded, and false if processing failed.\n\nDiscussion\n\nOverride this method to perform custom image processing."
  },
  {
    "title": "outputIsOpaque",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorkernel/2867389-outputisopaque",
    "html": "Discussion\n\nOverride this property if your processor's output stores 1.0 into the alpha channel of all pixels within the output extent. If not overridden, false is returned."
  },
  {
    "title": "outputFormat",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorkernel/2143065-outputformat",
    "html": "Discussion\n\nOverride this class property if you want your processor's output to be in a specific CIFormat: BGRA8, RGBAh, RGBAf, or R8. If the outputFormat is 0, then the output will be a supported format that best matches the rendering context's workingFormat.\n\nIf a processor returns data in a colorspace other than the context working space, then call matchedToWorkingSpace(from:) on the processor output.\n\nIf a processor returns data as alpha-unpremultiplied RGBA data, then call premultiplyingAlpha() on the processor output."
  },
  {
    "title": "synchronizeInputs",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorkernel/2143066-synchronizeinputs",
    "html": "Discussion\n\nSet to return false if you want your processor to be given CIImageProcessorInput objects not synchronized for CPU access.Set to return false if your subclass uses the GPU.\n\nDefaults to true if not overridden."
  },
  {
    "title": "surface",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorinput/1639657-surface",
    "html": "Required\n\nDiscussion\n\nUse this property if you plan to process the image using routines that can make use of memory shared with other processes or technologies.\n\nDo not modify the contents of this surface.\n\nSee Also\nAccessing Input Image Data\nvar baseAddress: UnsafeRawPointer\nA pointer to the image data in CPU memory to be processed.\n\nRequired\n\nvar metalTexture: (any MTLTexture)?\nA Metal texture containing the image data to be processed.\n\nRequired\n\nvar pixelBuffer: CVPixelBuffer?\nA CoreVideo pixel buffer containing the image data to be processed.\n\nRequired"
  },
  {
    "title": "baseAddress",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorinput/1639645-baseaddress",
    "html": "Required\n\nDiscussion\n\nUse this property if you plan to process the image using a CPU-based routine that cannot make use of higher-level constructs for sharing memory.\n\nNote\n\nIf your image processing routine is GPU-based, use the the pixelBuffer, surface, or metalTexture property instead.\n\nDo not modify the memory addressed by this pointer.\n\nSee Also\nAccessing Input Image Data\nvar metalTexture: (any MTLTexture)?\nA Metal texture containing the image data to be processed.\n\nRequired\n\nvar pixelBuffer: CVPixelBuffer?\nA CoreVideo pixel buffer containing the image data to be processed.\n\nRequired\n\nvar surface: IOSurfaceRef\nAn IOSurface object containing the image data to be processed.\n\nRequired"
  },
  {
    "title": "roiTileCount",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorinput/4172814-roitilecount",
    "html": "Required"
  },
  {
    "title": "CIFilterShape",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltershape",
    "html": "Overview\n\nYou use CIFilterShape objects in conjunction with Core Image classes, such as CIFilter, CIKernel, and CISampler, to create custom filters.\n\nTopics\nInitializing a Filter Shape\ninit(rect: CGRect)\nInitializes a filter shape object with a rectangle.\nInspecting a Filter Shape\nvar extent: CGRect\nThe extent of the filter shape.\nModifying a Filter Shape\nfunc insetBy(x: Int32, y: Int32) -> CIFilterShape\nModifies a filter shape object so that it is inset by the specified x and y values.\nfunc intersect(with: CIFilterShape) -> CIFilterShape\nCreates a filter shape object that represents the intersection of the current filter shape and the specified filter shape object.\nfunc intersect(with: CGRect) -> CIFilterShape\nCreates a filter shape that represents the intersection of the current filter shape and a rectangle.\nfunc transform(by: CGAffineTransform, interior: Bool) -> CIFilterShape\nCreates a filter shape that results from applying a transform to the current filter shape.\nfunc union(with: CIFilterShape) -> CIFilterShape\nCreates a filter shape that results from the union of the current filter shape and another filter shape object.\nfunc union(with: CGRect) -> CIFilterShape\nCreates a filter shape that results from the union of the current filter shape and a rectangle.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nSee Also\nImage and Kernel Definition Parameters\nstruct CIFormat\nPixel data formats for image input, output, and processing."
  },
  {
    "title": "digest",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorinput/4048310-digest",
    "html": "Required"
  },
  {
    "title": "roiTileIndex",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorinput/4172815-roitileindex",
    "html": "Required"
  },
  {
    "title": "init(image:)",
    "url": "https://developer.apple.com/documentation/coreimage/cisampler/1438117-init",
    "html": "Parameters\nim\n\nThe image object to initialize the sampler with.\n\nSee Also\nInitializing a Sampler\ninit(image: CIImage, options: [AnyHashable : Any]?)\nInitializes the sampler with an image object using options specified in a dictionary.\nRelated Documentation\n- initWithImage:keysAndValues:\nInitializes the sampler with an image object using options specified as key-value pairs."
  },
  {
    "title": "init(image:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cisampler/1437963-init",
    "html": "Parameters\nim\n\nThe image to initialize the sampler with.\n\ndict\n\nA dictionary that contains options specified as key-value pairs. See Sampler Option Keys.\n\nSee Also\nInitializing a Sampler\ninit(image: CIImage)\nInitializes a sampler with an image object.\nRelated Documentation\n- initWithImage:keysAndValues:\nInitializes the sampler with an image object using options specified as key-value pairs."
  },
  {
    "title": "minimumComponentFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228360-minimumcomponentfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the minimum component filter to an image. The effect applies a preconfigured set of effects that result in the input image becoming grayscale using the minimum RGB color components.\n\nThe minimum component filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds darkness and makes the input image grayscale:\n\nfunc minimumComponent(inputImage: CIImage) -> CIImage {\n    let minimumComponentFilter = CIFilter.minimumComponent()\n    minimumComponentFilter.inputImage = inputImage\n    return minimumComponentFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "multiplyCompositingFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228371-multiplycompositingfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the multiply compositing filter to an image. The effect calculates the color value of the output image by multiplying the color values from the input image and the background image.\n\nThe multiply compositing filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the image becoming darker with more saturation:\n\nfunc multiplyCompositing(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.multiplyCompositing()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ sourceOverCompositingFilter\nPlaces one image over a second image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "definition",
    "url": "https://developer.apple.com/documentation/coreimage/cisampler/1437877-definition",
    "html": "Discussion\n\nThe DOD contains all nontransparent pixels produced by referencing the sampler.\n\nSee Also\nGetting Information About the Sampler Object\nvar extent: CGRect\nThe rectangle that specifies the extent of the sampler"
  },
  {
    "title": "isDithered",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination/2875441-isdithered",
    "html": "See Also\nCustomizing Rendering\nvar alphaMode: CIRenderDestinationAlphaMode\nThe render destination's representation of alpha (transparency) values.\nenum CIRenderDestinationAlphaMode\nDifferent ways of representing alpha.\nvar blendKernel: CIBlendKernel?\nThe destination's blend kernel.\nvar blendsInDestinationColorSpace: Bool\nIndicator of whether to blend in the destination's color space.\nvar colorSpace: CGColorSpace?\nThe destination's color space.\nvar width: Int\nThe render destination's row width.\nvar height: Int\nThe render destination's buffer height.\nvar isClamped: Bool\nIndicator of whether or not the destination clamps.\nvar isFlipped: Bool\nIndicator of whether the destination is flipped."
  },
  {
    "title": "width",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination/2875434-width",
    "html": "See Also\nCustomizing Rendering\nvar alphaMode: CIRenderDestinationAlphaMode\nThe render destination's representation of alpha (transparency) values.\nenum CIRenderDestinationAlphaMode\nDifferent ways of representing alpha.\nvar blendKernel: CIBlendKernel?\nThe destination's blend kernel.\nvar blendsInDestinationColorSpace: Bool\nIndicator of whether to blend in the destination's color space.\nvar colorSpace: CGColorSpace?\nThe destination's color space.\nvar height: Int\nThe render destination's buffer height.\nvar isClamped: Bool\nIndicator of whether or not the destination clamps.\nvar isDithered: Bool\nIndicator of whether or not the destination dithers.\nvar isFlipped: Bool\nIndicator of whether the destination is flipped."
  },
  {
    "title": "isClamped",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination/2875451-isclamped",
    "html": "See Also\nCustomizing Rendering\nvar alphaMode: CIRenderDestinationAlphaMode\nThe render destination's representation of alpha (transparency) values.\nenum CIRenderDestinationAlphaMode\nDifferent ways of representing alpha.\nvar blendKernel: CIBlendKernel?\nThe destination's blend kernel.\nvar blendsInDestinationColorSpace: Bool\nIndicator of whether to blend in the destination's color space.\nvar colorSpace: CGColorSpace?\nThe destination's color space.\nvar width: Int\nThe render destination's row width.\nvar height: Int\nThe render destination's buffer height.\nvar isDithered: Bool\nIndicator of whether or not the destination dithers.\nvar isFlipped: Bool\nIndicator of whether the destination is flipped."
  },
  {
    "title": "Sampler Option Keys",
    "url": "https://developer.apple.com/documentation/coreimage/cisampler/sampler_option_keys",
    "html": "Topics\nConstants\nlet kCISamplerAffineMatrix: String\nThe key for an affine matrix. The associated value is an NSArray object ([a b c d tx ty]) that defines the transformation to apply to the sampler.\nlet kCISamplerWrapMode: String\nThe key for the sampler wrap mode. The wrap mode specifies how Core Image produces pixels that are outside the extent of the sample. Possible values are kCISamplerWrapBlack and kCISamplerWrapClamp.\nlet kCISamplerFilterMode: String\nThe key for the filtering to use when sampling the image. Possible values are kCISamplerFilterNearest and kCISamplerFilterLinear.\nlet kCISamplerColorSpace: String\nThe key for the color space to use when sampling the image. The associated value must be an RGB CGColorSpace object. Using this option specifies that samples should be converted to this color space before being passed to a kernel. If not specified, samples will be passed to the kernel in the working color space of the Core Image context used to render the image."
  },
  {
    "title": "format",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessoroutput/1639628-format",
    "html": "Required\n\nDiscussion\n\nYour image processing routine must provide data in this pixel format.\n\nSee Also\nGetting Supplemental Information for Image Processing\nvar region: CGRect\nThe rectangular region of the output image that your processor must provide.\n\nRequired\n\nvar metalCommandBuffer: (any MTLCommandBuffer)?\nA command buffer to use for image processing using Metal.\n\nRequired\n\nvar bytesPerRow: Int\nThe number of bytes per row of pixels for the output image.\n\nRequired"
  },
  {
    "title": "bytesPerRow",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessoroutput/1639635-bytesperrow",
    "html": "Required\n\nSee Also\nGetting Supplemental Information for Image Processing\nvar region: CGRect\nThe rectangular region of the output image that your processor must provide.\n\nRequired\n\nvar metalCommandBuffer: (any MTLCommandBuffer)?\nA command buffer to use for image processing using Metal.\n\nRequired\n\nvar format: CIFormat\nThe per-pixel data format expected of the output image.\n\nRequired"
  },
  {
    "title": "Sampler Option Values",
    "url": "https://developer.apple.com/documentation/coreimage/cisampler/sampler_option_values",
    "html": "Topics\nConstants\nlet kCISamplerWrapBlack: String\nPixels are transparent black.\nlet kCISamplerWrapClamp: String\nCoordinates are clamped to the extent.\nlet kCISamplerFilterNearest: String\nNearest neighbor sampling.\nlet kCISamplerFilterLinear: String\nBilinear interpolation."
  },
  {
    "title": "isFlipped",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination/2875442-isflipped",
    "html": "See Also\nCustomizing Rendering\nvar alphaMode: CIRenderDestinationAlphaMode\nThe render destination's representation of alpha (transparency) values.\nenum CIRenderDestinationAlphaMode\nDifferent ways of representing alpha.\nvar blendKernel: CIBlendKernel?\nThe destination's blend kernel.\nvar blendsInDestinationColorSpace: Bool\nIndicator of whether to blend in the destination's color space.\nvar colorSpace: CGColorSpace?\nThe destination's color space.\nvar width: Int\nThe render destination's row width.\nvar height: Int\nThe render destination's buffer height.\nvar isClamped: Bool\nIndicator of whether or not the destination clamps.\nvar isDithered: Bool\nIndicator of whether or not the destination dithers."
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/cisampler/1437872-extent",
    "html": "Discussion\n\nExtent is the rectangle that specifies the area outside which the wrap mode set for the sampler is invoked.\n\nSee Also\nGetting Information About the Sampler Object\nvar definition: CIFilterShape\nThe domain of definition (DOD) of the sampler"
  },
  {
    "title": "source",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867407-source",
    "html": "Discussion\n\nFigure 1 The result of using the source blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "sourceAtop",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867357-sourceatop",
    "html": "Discussion\n\nFigure 1 The result of using the source atop blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "sourceOver",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867413-sourceover",
    "html": "Discussion\n\nFigure 1 The result of using the source over blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "componentAdd",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867384-componentadd",
    "html": "Discussion\n\nFigure 1 The result of using the component add blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "componentMax",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867433-componentmax",
    "html": "Discussion\n\nFigure 1 The result of using the component max blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "colorBurn",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867391-colorburn",
    "html": "Discussion\n\nFigure 1 The result of using the color burn blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "color",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel/2867350-color",
    "html": "Discussion\n\nFigure 1 The result of using the color blend kernel (background image is top left, foreground image is bottom left)\n\nSee Also\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color."
  },
  {
    "title": "fourfoldReflectedTileFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228327-fourfoldreflectedtilefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis method applies the four-fold reflected tile filter to an image. The effect produces a four-way reflected tile image.\n\nThe four-fold reflected tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nangle\n\nA float representing the direction of distortion, in radians as an NSNumber.\n\nwidth\n\nA float representing the set width of each tile as an NSNumber.\n\nacute angle\n\nA float representing the primary angle for the repeating parallelogram tile as an NSNumber.\n\nThe following code creates a filter that results in a four-fold pattern:\n\nfunc fourFoldReflected(inputImage: CIImage) -> CIImage {\n    let fourFoldReflectedTile = CIFilter.fourfoldReflectedTile()\n    fourFoldReflectedTile.inputImage = inputImage\n    fourFoldReflectedTile.center = CGPoint(x: 150, y: 150)\n    fourFoldReflectedTile.width = 10\n    fourFoldReflectedTile.angle = 7\n    fourFoldReflectedTile.acuteAngle = 1\n    return fourFoldReflectedTile.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ triangleTileFilter\nTiles a triangular area of an image.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "glideReflectedTileFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228333-glidereflectedtilefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis method applies the glide reflected tile filter to an image. The effect produces a tiled image by rotating and reflecting a tile from the input image.\n\nThe glide reflected tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nangle\n\nA float representing the direction of distortion, in radians as an NSNumber.\n\nwidth\n\nA float representing the set width of each tile as an NSNumber.\n\nThe following code creates a filter that results in flipping the image and then tiling the result:\n\nfunc glideReflected(inputImage: CIImage) -> CIImage {\n    let glideReflectedTile = CIFilter.glideReflectedTile()\n    glideReflectedTile.inputImage = inputImage\n    glideReflectedTile.center = CGPoint(x: 150, y: 150)\n    glideReflectedTile.angle = 10\n    glideReflectedTile.width = 10\n    return glideReflectedTile.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ triangleTileFilter\nTiles a triangular area of an image.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "sepiaToneFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228402-sepiatonefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the sepia tone filter to an image. The effect maps the colors of the inputImage to various shades of brown.\n\nThe sepia tone filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the input image transforming to a brown hue:\n\nfunc sepiaTone(inputImage: CIImage ) -> CIImage {\n    let sepiaToneFilter = CIFilter.sepiaTone()\n    sepiaToneFilter.inputImage = inputImage\n    sepiaToneFilter.intensity = 1\n    return sepiaToneFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ falseColorFilter\nReplaces an image’s colors with specified colors.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "flashTransitionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228326-flashtransitionfilter",
    "html": "Return Value\n\nThe transition image.\n\nDiscussion\n\nThis method applies the flash transition filter to an image. The effect transitions from the input image to the target image by creating a flash that fills the image and fades to the target image.\n\nThe flash transition filter uses the following properties:\n\ninputImage\n\nThe starting image with the type CIImage.\n\ntargetImage\n\nThe ending image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nextent\n\nA CGRect representing the size of the rounded rectangle.\n\ncolor\n\nA CIColor representing the color of the flash effect.\n\ntime\n\nA float representing the parametric time of the transition from start (at time 0) to end (at time 1) as an NSNumber.\n\nmaxStiriationRadius\n\nA float representing the radius of the light rays emanating from the flash as a NSNumber.\n\nstriationStrength\n\nA float representing the strength of the light rays emanating from the flash as a NSNumber.\n\nstriationContrast\n\nA float representing the contrast that’s added to each output pixel as a NSNumber.\n\nfadeThreshold\n\nA float representing the amount of fade between the flash and the target image as a NSNumber.\n\nThe following code creates a filter that transitions from the input image with a large flash of light and fades to the target image.\n\nfunc flash (inputImage: CIImage, targetImage: CIImage) -> CIImage {\n    let flashTransition = CIFilter.flashTransition()\n    flashTransition.inputImage = inputImage\n    flashTransition.targetImage = targetImage\n    flashTransition.center = CGPoint(x: 253, y: 372)\n    flashTransition.extent = CGRect(x: 0, y: 0, width: 300, height: 300)\n    flashTransition.color = .white\n    flashTransition.time = 0.5\n    flashTransition.maxStriationRadius = 2.58\n    flashTransition.striationStrength = 0.5\n    flashTransition.striationContrast = 1.375\n    flashTransition.fadeThreshold = 0.06\n    return flashTransition.outputImage!\n}\n\n\nSee Also\nFilters\n+ accordionFoldTransitionFilter\nTransitions by folding and crossfading an image to reveal the target image.\n+ barsSwipeTransitionFilter\nTransitions between two images by removing rectangular portions of an image.\n+ copyMachineTransitionFilter\nSimulates the effect of a copy machine scanner light to transiton between two images.\n+ disintegrateWithMaskTransitionFilter\nTransitions between two images using a mask image.\n+ dissolveTransitionFilter\nTransitions between two images with a fade effect.\n+ modTransitionFilter\nTransitions between two images by applying irregularly shaped holes.\n+ pageCurlTransitionFilter\nSimulates the curl of a page, revealing the target image.\n+ pageCurlWithShadowTransitionFilter\nSimulates the curl of a page, revealing the target image with added shadow.\n+ rippleTransitionFilter\nSimulates a ripple in a pond to transiton from one image to another.\n+ swipeTransitionFilter\nGradually transitions from one image to another with a swiping motion."
  },
  {
    "title": "dissolveTransitionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228314-dissolvetransitionfilter",
    "html": "Return Value\n\nThe transition image.\n\nDiscussion\n\nThis method applies the disintegrate transition filter to an image. The effect transitions from one image to another by using a fade effect.\n\nThe dissolve transition filter uses the following properties:\n\ninputImage\n\nThe starting image with the type CIImage.\n\ntargetImage\n\nThe ending image with the type CIImage.\n\ntime\n\nA float representing the parametric time of the transition from start (at time 0) to end (at time 1) as an NSNumber.\n\nThe following code creates a filter that produces a fade transition from the input image and the target image:\n\nfunc dissolve(inputImage: CIImage, targetImage: CIImage) -> CIImage {\n    let dissolveTransition = CIFilter.dissolveTransition()\n    dissolveTransition.inputImage = inputImage\n    dissolveTransition.targetImage = targetImage\n    dissolveTransition.time = 0.5\n    return dissolveTransition.outputImage!\n}\n\n\nSee Also\nFilters\n+ accordionFoldTransitionFilter\nTransitions by folding and crossfading an image to reveal the target image.\n+ barsSwipeTransitionFilter\nTransitions between two images by removing rectangular portions of an image.\n+ copyMachineTransitionFilter\nSimulates the effect of a copy machine scanner light to transiton between two images.\n+ disintegrateWithMaskTransitionFilter\nTransitions between two images using a mask image.\n+ flashTransitionFilter\nCreates a flash of light to transition between two images.\n+ modTransitionFilter\nTransitions between two images by applying irregularly shaped holes.\n+ pageCurlTransitionFilter\nSimulates the curl of a page, revealing the target image.\n+ pageCurlWithShadowTransitionFilter\nSimulates the curl of a page, revealing the target image with added shadow.\n+ rippleTransitionFilter\nSimulates a ripple in a pond to transiton from one image to another.\n+ swipeTransitionFilter\nGradually transitions from one image to another with a swiping motion."
  },
  {
    "title": "unsharpMaskFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228428-unsharpmaskfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the unsharp mask filter to an image. The effect increases the contrast of the edge between pixels of different colors within the defined radius property.\n\nThe unsharp mask filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\nintensity\n\nA float representing the desired strength of the effect as an NSNumber.\n\nThe following code creates a filter that results in the objects within the image becoming darker:\n\nfunc unsharp (inputImage: CIImage) -> CIImage? {    \n    let unsharpMask = CIFilter.unsharpMask()\n    unsharpMask.inputImage = inputImage\n    unsharpMask.radius = 5\n    unsharpMask.intensity = 2.5\n    return unsharpMask.outputImage!\n}\n\n\nSee Also\nFilters\n+ sharpenLuminanceFilter\nApplies a sharpening effect to an image."
  },
  {
    "title": "shadedMaterialFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228403-shadedmaterialfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the shaded material filter to an image. The effect produces a shaded image from a height-field image. Areas of the height field image that have a darker shaded area produce a stronger effect. You can combine the filter with CIHeightFieldFromMask to produce quick shadings of masks, such as text.\n\nThe shaded material filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nshadingImage\n\nAn image representing the color shading effect with type CIImage.\n\nscale\n\nA float representing the strength of effect as an NSNumber.\n\nThe following code creates a filter that results in an image containing glossy text by applying the shading image.\n\nfunc shadowMaterial(inputImage: CIImage, shadeImage: CIImage) -> CIImage {\n    let shadowMaterialFilter = CIFilter.shadedMaterial()\n    shadowMaterialFilter.inputImage = inputImage\n    shadowMaterialFilter.shadingImage = shadeImage\n    shadowMaterialFilter.scale = 10\n    return shadowMaterialFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "sobelGradientsFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/4190864-sobelgradientsfilter",
    "html": "Return Value\n\nA CIImage containing the Sobel gradients.\n\nDiscussion\n\nThis filter applies the Sobel operator to the color components of the input image. You would typically use the Sobel filter as part of an edge-detection algorithm for performing.\n\ninputImage\n\nA CIImage containing the image to process.\n\nThe following code applies the sobelGradientsFilter filter to an image.\n\nfunc sobelGradients(inputImage: CIImage) -> CIImage {\n    let sobel = CIFilter.sobelGradients()\n    sobel.inputImage = inputImage\n    return sobel.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "personSegmentationFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3750390-personsegmentationfilter",
    "html": "Return Value\n\nA CIImage containing the mask.\n\nDiscussion\n\nThe person-segmentation filter creates a mask that contains red pixels in the areas of the input image that are likely to contain people.\n\nThe person-segmentation filter takes the following properties:\n\ninputIImage\n\nA CIImage containing the image to segment.\n\nqualityLevel\n\nThe size and quality of the resulting segmentation mask. 0 is accurate, 1 is balanced, and 2 is fast.\n\nThe following code applies the person-segmentation filter to an image:\n\nfunc personSegmentation(inputImage: CIImage) -> CIImage {\n    let personSegmentationFilter = CIFilter.personSegmentation()\n    personSegmentationFilter.inputImage = inputImage\n    personSegmentationFilter.qualityLevel = 0\n    return personSegmentationFilter.outputImage!\n}\n\n\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "cannyEdgeDetectorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/4190863-cannyedgedetectorfilter",
    "html": "Return Value\n\nA CIImage with the detected edges.\n\nDiscussion\n\nThis filter performs a Canny edge-detection on the input image, producing a black-and-white image with the detected edges. White pixels indicate an edge, and black pixels indicate no edge.\n\nThe Canny edge-detection filter uses the following properties:\n\ninputImage\n\nThe CIImage to use as an input for the effect.\n\ngaussianSigma\n\nA float specifying the sigma of the Gaussian blur to apply, reducing high-frequency noise. Defaults to 1.6.\n\nperceptual\n\nA Boolean specifying whether to use a perceptual color space to compute the edge thresholds. Defaults to false.\n\nthresholdLow\n\nA float specifying the threshold for weak edges. Defaults to 0.02.\n\nthresholdHigh\n\nA float specifying the threshold for strong edges. Defaults to 0.05.\n\nhysteresisPasses\n\nThe number of hysteresis passes to apply to promote weak edge pixels. Minimum value is 0, maximum value is 20, and defaults to 1.\n\nThe following code applies Canny edge-detection to an image:\n\nfunc cannyEdgeDetector(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.cannyEdgeDetector()\n    filter.inputImage = inputImage\n    filter.gaussianSigma = 5\n    filter.perceptual = false\n    filter.thresholdLow = 0.02\n    filter.thresholdHigh = 0.05\n    filter.hysteresisPasses = 1\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "convolution7X7Filter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228301-convolution7x7filter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the convolution 7 x 7 filter to an image. The effect uses a 7 x 7 area surrounding an input pixel, the pixel itself, and those within a distance of 3 pixels horizontally and vertically. The effect repeats this for every pixel within the image. The work area is then combined with the weight property vector to produce the processed image. This filter differs from the convolutionRGB7X7Filter filter, which only processes the RGB components.\n\nThe convolution 7 x 7 filter uses the following properties:\n\nbias\n\nA float representing the value that’s added to each output pixel as a NSNumber.\n\nweights\n\nA CIVector representing the convolution kernel.\n\ninputImage\n\nAn image with the type CIImage.\n\nNote\n\nWhen using a nonzero bias value, the output image has an infinite extent. You should crop the output image before attempting to render it.\n\nThe following code creates a filter that highlights edges in the input image:\n\nfunc convolution7X7(inputImage: CIImage) -> CIImage? {\n    let convolutionFilter = CIFilter.convolution7X7()\n    convolutionFilter.inputImage = inputImage\n    let weights: [CGFloat] = [\n        0, 0, -1, -1, -1, 0, 0,\n        0, -1, -3, -3, -3, -1, 0,\n        -1, -3, 0, 7, 0, -3, -1,\n        -1, -3, 7, 25, 7, -3, -1,\n        -1, -3, 0, 7, 0, -3, -1,\n        0, -1, -3, -3, -3, -1, 0,\n        0, 0, -1, -1, -1, 0, 0\n    ]\n    let kernel = CIVector(values: weights, count: 49)\n    convolutionFilter.weights = kernel\n    convolutionFilter.bias = 0.0\n    return convolutionFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ convolution3X3Filter\nApplies a convolution 3 x 3 filter to the RGBA components of an image.\n+ convolution5X5Filter\nApplies a convolution 5 x 5 filter to the RGBA components image.\n+ convolution9HorizontalFilter\nApplies a convolution-9 horizontal filter to the RGBA components of an image.\n+ convolution9VerticalFilter\nApplies a convolution-9 vertical filter to the RGBA components of an image.\n+ convolutionRGB3X3Filter\nApplies a convolution 3 x 3 filter to the RGB components of an image.\n+ convolutionRGB5X5Filter\nApplies a convolution 5 x 5 filter to the RGB components of an image.\n+ convolutionRGB7X7Filter\nApplies a convolution 7 x 7 filter to the RGB components of an image.\n+ convolutionRGB9HorizontalFilter\nApplies a convolution 9 x 1 filter to the RGB components of an image.\n+ convolutionRGB9VerticalFilter\nApplies a convolution 1 x 9 filter to the RGB components of an image."
  },
  {
    "title": "bloomFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228276-bloomfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the bloom filter to an image. The effect softens edges and adds a slight blur effect.\n\nThe bloom filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\nintensity\n\nA float representing the desired strength of the effect as an NSNumber.\n\nThe following code creates a filter that results in a hazy effect on the image:\n\nfunc bloom(inputImage: CIImage) -> CIImage {\n    let bloomFilter = CIFilter.bloom()\n    bloomFilter.inputImage = inputImage\n    bloomFilter.radius = 10\n    bloomFilter.intensity = 1\n    return bloomFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "comicEffectFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228298-comiceffectfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the comic effect filter to an image. The effect simulates a comic book drawing by outlining edges and applying a color halftone effect.\n\nThe comic effect filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the image appearing image in comic book style.\n\nfunc comicEffect(inputImage: CIImage) -> CIImage {\n    let comicEffectFilter = CIFilter.comicEffect()\n    comicEffectFilter.inputImage = inputImage\n    return comicEffectFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "coreMLModelFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228305-coremlmodelfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the Core ML model filter to an image. The effect filters the image using a trained Core ML model to produce the result. Specifying the head index allows you to produce a result from various components of a multiheaded coreML model.\n\nThe Core ML model filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nheadIndex\n\nA float representing which output of a multihead Core ML model should be used for applying the effect to an image.\n\nsoftmaxNormalization\n\nA Boolean value representing the softmax normalization to be applied to the output image created by the model.\n\ninputModel\n\nThe Core ML model to be used for applying effect on the image.\n\nThe following code creates a filter that results in the flowers appearing to be glass panes:\n\nfunc coreML(inputImage: CIImage) -> CIImage {\n    let coreMLFilter = CIFilter.coreMLModel()\n    let model = GlassModel().model\n    coreMLFilter.inputImage = inputImage\n    coreMLFilter.headIndex = 0\n    coreMLFilter.softmaxNormalization = false\n    return coreMLFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "blendWithMaskFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228274-blendwithmaskfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the blend with mask filter to an image. The effect uses values from the green mask image to interpolate between the input and background images. The mask image consists of shades of green that define the strength of the interpolation from zero (where the mask image is black) to the specified radius (where the mask image is green).\n\nThe blend with mask filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nmaskImage\n\nAn image that masks an area on the background image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the replacement of green in the mask image with the detail of the input image:\n\nfunc blendWithMask(inputImage: CIImage, backgroundImage: CIImage, maskImage: CIImage) -> CIImage {\n    let blendWithMaskFilter = CIFilter.blendWithMask()\n    blendWithMaskFilter.backgroundImage = backgroundImage\n    blendWithMaskFilter.inputImage = inputImage\n    blendWithMaskFilter.maskImage = maskImage\n    return blendWithMaskFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "blendWithRedMaskFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228275-blendwithredmaskfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the blend with red mask filter to an image. The effect uses values from the red mask image to interpolate between the input and background images. The mask image consists of shades of red that define the strength of the interpolation from zero (where the mask image is black) to the specified radius (where the mask image is red).\n\nThe blend with red mask filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nmaskImage\n\nAn image that masks an area on the background image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the replacement of green in the mask image with the detail of the input image:\n\nfunc blendWithRedMask(inputimage: CIImage, backgroundimage: CIImage, maskimage: CIImage) -> CIImage {\n    let  blendWithRedMaskFilter = CIFilter.blendWithRedMask()\n    blendWithRedMaskFilter.inputImage = inputimage\n    blendWithRedMaskFilter.maskImage = maskimage\n    blendWithRedMaskFilter.backgroundImage = backgroundimage\n    return blendWithRedMaskFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "blendWithAlphaMaskFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228272-blendwithalphamaskfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the blend with alpha mask filter to an image. The effect uses values from the grayscale mask image to interpolate between the input and background images. The mask image consists of shades of gray that define the strength of the interpolation from zero (where the mask image is black) to the specified radius (where the mask image is white).\n\nThe blend with alpha mask filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nmaskImage\n\nAn image that masks an area on the background image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the replacement of white in the mask image with the detail of the input image:\n\nfunc blendWithAlphaMask(inputimage: CIImage, backgroundimage: CIImage, maskimage: CIImage) -> CIImage {\n    let blendWithAlphaMaskFilter = CIFilter.blendWithAlphaMask()\n    blendWithAlphaMaskFilter.inputImage = inputimage\n    blendWithAlphaMaskFilter.maskImage = maskimage\n    blendWithAlphaMaskFilter.backgroundImage = backgroundimage\n    return blendWithAlphaMaskFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "blendWithBlueMaskFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228273-blendwithbluemaskfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the blend with blue mask filter to an image. The effect uses values from the blue mask image to interpolate between the input and background images. The mask image is made of shades of blue that define the strength of the interpolation from zero (where the mask image is black) to the specified radius (where the mask image is blue).\n\nThe blend with blue mask filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nmaskImage\n\nAn image that masks an area on the background image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in the replacement of blue in the mask image with the detail of the input image:\n\nfunc blendWithBlueMask(inputimage: CIImage, backgroundimage: CIImage, maskimage: CIImage) -> CIImage {\n    let  blendWithBlueMaskFilter = CIFilter.blendWithBlueMask()\n    blendWithBlueMaskFilter.inputImage = inputimage\n    blendWithBlueMaskFilter.maskImage = maskimage\n    blendWithBlueMaskFilter.backgroundImage = backgroundimage\n    return blendWithBlueMaskFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "torusLensDistortionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600152-toruslensdistortionfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the torus lens distortion filter to an image. This effect distorts an image by creating a torus-shaped object, placing it over the input image, and applying the refraction.\n\nThe torus lens distortion filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nradius\n\nA float representing the amount of pixels the filter uses in the tours as an NSNumber.\n\nrefraction\n\nA float representing the refraction of the glass as an NSNumber.\n\nwidth\n\nA float representing the width of the torus ring as an NSNumber.\n\nThe following code creates a filter that results in a torus-shaped object placed over the image:\n\nfunc torusLens(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.torusLensDistortion()\n    filter.inputImage = inputImage\n    filter.radius = 620\n    filter.refraction = 1.7\n    filter.center = CGPoint(x: 1791, y: 1344)\n    filter.width = 360\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "stripesGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228417-stripesgeneratorfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a vertical stripped line pattern as an image.\n\nThe stripes generator filter uses the following properties:\n\ncenter\n\nA CIVector representing the center of the image.\n\ncolor0\n\nA CIColor representing the stripes color.\n\ncolor1\n\nA CIColor representing the background color.\n\nwidth\n\nA float representing the width of the lines as an NSNumber.\n\nsharpness\n\nA float representing the sharpness of the lines as an NSNumber.\n\nThe following code creates a filter that generates a black and white vertical striped image:\n\nfunc stripes() -> CIImage {\n    let stripesGenerator = CIFilter.stripesGenerator()\n    stripesGenerator.center = CGPoint(x: 150, y: 150)\n    stripesGenerator.color0 = .white\n    stripesGenerator.color1 = .black\n    stripesGenerator.width = 80\n    stripesGenerator.sharpness = 1\n    return stripesGenerator.outputImage!\n}\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "sunbeamsGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228419-sunbeamsgeneratorfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a sunbeam as an image. The effect generates a center-textured sun with striations. You can combine with other filters to create more sophisticated images.\n\nThe sunbeams generator filter uses the following properties:\n\ncenter\n\nA vector representing the center of the image as a CIVector.\n\ncolor\n\nA CIColor representing the color of the sun.\n\nsunRadius\n\nA float representing the radius of the center sun as an NSNumber.\n\nmaxStriationRadius\n\nA float representing the striation radius as an NSNumber.\n\nstriationStrength\n\nA float representing the striation strength as an NSNumber.\n\nstriationContrast\n\nA float representing the striation contrast as an NSNumber.\n\ntime\n\nA float representing the time as an NSNumber.\n\nThe following code creates a filter that generates an image that resembles a yellow sun with sunbeams:\n\n    func sunBeam () -> CIImage {\n        let sunBeamGenerator = CIFilter.sunbeamsGenerator()\n        sunBeamGenerator.center = CGPoint(x: 150, y: 150)\n        sunBeamGenerator.color = CIColor(red: 0.96, green: 1, blue: 1, alpha: 1)\n        sunBeamGenerator.sunRadius = 40\n        sunBeamGenerator.maxStriationRadius = 2.58\n        sunBeamGenerator.striationStrength = 0.50\n        sunBeamGenerator.striationContrast = 1.38\n        sunBeamGenerator.time = 0\n        return sunBeamGenerator.outputImage!\n    }\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "starShineGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228415-starshinegeneratorfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a star-shine image. The effect is similar to a supernova effect. You can use this filter to simulate a lens flare.\n\nThe star-shine generator filter uses the following properties:\n\ncenter\n\nA vector representing the center of the flare as a CGPoint.\n\ncolor\n\nA color representing the color of the flare as a CGColor.\n\nradius\n\nA float representing the radius of the flare as an NSNumber.\n\ncrossScale\n\nA float representing the cross flare size relative to the round central flare as an NSNumber.\n\ncrossAngle\n\nA float representing the angle of the flare as an NSNumber.\n\ncrossOpacity\n\nA float representing the thickness of the cross opacity as an NSNumber.\n\ncrossWidth\n\nA float representing the cross width as an NSNumber.\n\nepsilon\n\nA float representing the epsilon as an NSNumber.\n\nThe following code generates a star-shaped silhouette with a black background.\n\nfunc starShine() -> CIImage {\n    let starShineGenerator = CIFilter.starShineGenerator()\n    starShineGenerator.center = CGPoint(x: 150, y: 150)\n    starShineGenerator.color = .green\n    starShineGenerator.radius = 50\n    starShineGenerator.crossScale = 15\n    starShineGenerator.crossAngle = 0.60\n    starShineGenerator.crossOpacity = -2\n    starShineGenerator.crossWidth = 2.5\n    starShineGenerator.epsilon = -2.0\n    return starShineGenerator.outputImage!\n}\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "code128BarcodeGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228281-code128barcodegeneratorfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a Code 128 barcode as an image. Code 128 is a high-density linear barcode defined in the ISO/IEC 15417:2007 standard. Use this filter to generate alphanumeric or numeric-only barcodes. The barcode can contain any of the 128 ASCII characters.\n\nThe Code 128 barcode filter uses the following properties:\n\nmessage\n\nNSData containing the message to encode in the Code 128 barcode.\n\nquietSpace\n\nNSNumber containing the number of empty white pixels that should surround the barcode.\n\nbarcodeHeight\n\nNSNumber containing the height of the generated barcode in pixels.\n\nThe following code creates a filter that generates a Code 128 barcode:\n\nfunc code128Barcode(barcode: String) -> CIImage {\n    let code128Barcode = CIFilter.code128BarcodeGenerator()\n    code128Barcode.message = barcode.data(using: .ascii)!\n    code128Barcode.quietSpace = 5\n    code128Barcode.barcodeHeight = 20\n    return code128Barcode.outputImage!\n}\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "roundedRectangleStrokeGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/4126861-roundedrectanglestrokegeneratorf",
    "html": "Return Value\n\nA CIImage containing the stroked rectangle.\n\nDiscussion\n\nThis filter creates an outline of a rounded rectangle.\n\nThe filter takes the following properties:\n\nextent\n\nA CGRect containing the position and size of the rectangle.\n\nwidth\n\nThe width of the stroke to draw.\n\nradius\n\nThe corner radius.\n\nThe following code generates an image containing a stroked rounded rectangle:\n\nfunc roundedRectangleStroke() -> CIImage {\n    let filter = CIFilter.roundedRectangleStrokeGenerator()\n    filter.extent = CGRect(x: 0, y: 0, width: 200, height: 100)\n    filter.color = CIColor.red\n    filter.width = 5\n    filter.radius = 20\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "blurredRectangleGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/4273903-blurredrectanglegeneratorfilter",
    "html": "Return Value\n\nA CIImage containing a blurred rectangle.\n\nDiscussion\n\nCreates a CIImage containing a blurred rectangle. The resulting image size is the extent of the rectangle plus any additional space required for the blur effect.\n\nThe blurred rectangle filter uses the following properties:\n\nextent\n\nA CGRect that defines the extent of the effect.\n\ncolor\n\nA CIColor specifying the color of the rectangle.\n\nsigma\n\nA float specifying the sigma for the Gaussian blur.\n\nThe following code creates a filter that generates a blurred red rectangle with a width of 200 x 100 pixels.\n\nfunc blurredRectangle() -> CIImage {\n    let filter = CIFilter.blurredRectangleGenerator()\n    filter.extent = CGRect(x: 0, y: 0, width: 200, height: 100)\n    filter.color = CIColor.red\n    filter.sigma = 10.0\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "checkerboardGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228279-checkerboardgeneratorfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a checkerboard pattern as an image. The effect requires the size, sharpness, and color properties to create the pattern.\n\nThe checkerboard generator filter uses the following properties:\n\ncenter\n\nA vector representing the center of the image as a CIVector.\n\ncolor0\n\nA CIColor representing the first color of the pattern.\n\ncolor1\n\nA CIColor representing the second color of the pattern.\n\nsharpness\n\nA float representing the sharpness of the pattern as an NSNumber.\n\nwidth\n\nA float representing the width of the checkerboard squares as an NSNumber.\n\nThe following code creates a filter that generates a black-and-white checkered pattern:\n\nfunc checkerBoard() -> CIImage {\n    let checkerBoardGenerator = CIFilter.checkerboardGenerator()\n    checkerBoardGenerator.setDefaults()\n    checkerBoardGenerator.center = CGPoint(x: 0, y: 0)\n    checkerBoardGenerator.color0 = .white\n    checkerBoardGenerator.color1 = .black\n    checkerBoardGenerator.width = 40\n    checkerBoardGenerator.sharpness = 1\n    return checkerBoardGenerator.outputImage!\n}\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "stretchCropFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600151-stretchcropfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the stretch crop filter to an image. This effect distorts an image by stretching an image and then applies the crop extent. If the crop value is 0, the filter only uses stretching. If the value is 1, then the filter only uses cropping.\n\nThe stretch crop filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenterStretchAmount\n\nA float representing the amount of stretching of the center of the image as an NSNumber.\n\nsize\n\nA CGPoint representing the desired size of the output image.\n\ncropAmount\n\nA float representing the amount of cropping you apply to achieve the target size as an NSNumber.\n\nThe following code creates a filter that results in a smaller image that’s distorted and cropped to be the defined size:\n\nfunc stretchCrop(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.stretchCrop()\n    filter.inputImage = inputImage\n    filter.cropAmount = 0.25\n    filter.centerStretchAmount = 0.25\n    filter.size = CGPoint(\n        x: inputImage.extent.width * 2,\n        y: inputImage.extent.size.height * 0.8\n    )\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "circleSplashDistortionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600140-circlesplashdistortionfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the circle splash distortion filter to an image. This effect distorts the pixels starting at the circumference of a circle and emanating outward.\n\nThe circle splash distortion filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nradius\n\nA float representing the amount in pixels the filter uses to create the distortion as an NSNumber.\n\ncenter\n\nA CGPoint representing the center of the image.\n\nThe following code creates a filter that results in a ripple effect applied to the image:\n\nfunc circularSplash(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.circleSplashDistortion()\n    filter.inputImage = inputImage\n    filter.center = CGPoint(x: 50.0, y: 50.0)\n    filter.radius = 2.0\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "circularWrapFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600141-circularwrapfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the circular wrap filter to an image. This effect wraps an image around a transparent circle. The distortion of the image increases with the distance from the center of the circle.\n\nThe circular wrap filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nangle\n\nA float representing the angle of the wrap, in radians, as an NSNumber.\n\nradius\n\nA float representing the amount of pixels the filter uses to create the distortion as an NSNumber.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nThe following code creates a filter that results in a circular image generated from the input image:\n\nfunc circularWrap(inputImage: CIImage) -> CIImage {    let filter = CIFilter.circularWrap()\n    filter.inputImage = inputImage\n    filter.center = CGPoint(\n        x: inputImage.extent.size.width/2,\n        y: inputImage.extent.size.height/2\n    )\n    filter.angle = .pi\n    filter.radius = 90\n    return filter.outputImage!\n}\n\n\nlet text = CIFilter.textImageGenerator()\ntext.text = \"Core Image\"\ntext.fontSize = 100\ntext.fontName = \"Chalkboard\"\ntext.outputImage!\ncircularWrap(inputImage: text.outputImage!)\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "convolutionRGB9VerticalFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3750389-convolutionrgb9verticalfilter",
    "html": "Return Value\n\nThis method applies a 1 x 9 convolution to the RGB components of an image. The effect uses a 1 x 9 area surrounding an input pixel, the pixel itself, and those within a distance of 4 pixels vertically. The effect repeats this for every pixel within the image. Unlike the convolution filters, which use square matrices, this filter can only produce effects along a vertical axis. You can combine this filter with the convolutionRGB9HorizontalFilter to apply separable 9 x 9 convolutions. This filter differs from the convolution9VerticalFilter filter, which processes all of the color components including the alpha component.\n\nThe convolution-RGB-9-vertical filter uses the following properties:\n\ninputImage\n\nA CIImage containing the image to process.\n\nweights\n\nA CIVector representing the convolution kernel.\n\nbias\n\nA float representing the value that’s added to each output pixel.\n\nNote\n\nWhen using a nonzero bias value, the output image has an infinite extent. You should crop the image before attempting to render it.\n\nThe following code creates a filter that blurs the image in the vertical direction:\n\nfunc convolutionRGB9Vertical(inputImage: CIImage) -> CIImage {\n    let convolutionFilter = CIFilter.convolutionRGB9Vertical()\n    convolutionFilter.inputImage = inputImage\n    let weights: [CGFloat] = [1, 1, 1, 1, 1, 1, 1, 1, 1].map { $0/9.0 }\n    let kernel = CIVector(values: weights, count: 9)\n    convolutionFilter.weights = kernel\n    convolutionFilter.bias = 0.0\n    return convolutionFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ convolution3X3Filter\nApplies a convolution 3 x 3 filter to the RGBA components of an image.\n+ convolution5X5Filter\nApplies a convolution 5 x 5 filter to the RGBA components image.\n+ convolution7X7Filter\nApplies a convolution 7 x 7 filter to the RGBA color components of an image.\n+ convolution9HorizontalFilter\nApplies a convolution-9 horizontal filter to the RGBA components of an image.\n+ convolution9VerticalFilter\nApplies a convolution-9 vertical filter to the RGBA components of an image.\n+ convolutionRGB3X3Filter\nApplies a convolution 3 x 3 filter to the RGB components of an image.\n+ convolutionRGB5X5Filter\nApplies a convolution 5 x 5 filter to the RGB components of an image.\n+ convolutionRGB7X7Filter\nApplies a convolution 7 x 7 filter to the RGB components of an image.\n+ convolutionRGB9HorizontalFilter\nApplies a convolution 9 x 1 filter to the RGB components of an image."
  },
  {
    "title": "perspectiveTileFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228381-perspectivetilefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis method applies the perspective tile filter to an image. The effect adjusts the perspective of the image and then tiles the result.\n\nThe perspective tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ntopLeft\n\nA CGPoint of the input image mapped to the top-left corner of the tile.\n\ntopRight\n\nA CGPoint of the input image mapped to the top-right corner of the tile.\n\nbottomLeft\n\nA CGPoint of the input image mapped to the bottom-left corner of the tile.\n\nbottomRight\n\nA CGPoint of the input image mapped to the bottom-right corner of the tile.\n\nThe following code creates a filter that tiles the image and adjusts the perspective to add depth:\n\nfunc perspective(inputImage: CIImage) -> CIImage {\n    let perspectiveTile = CIFilter.perspectiveTile()\n    perspectiveTile.inputImage = inputImage\n    perspectiveTile.topLeft = CGPoint(x: 118, y: 484)\n    perspectiveTile.topRight = CGPoint(x: 646, y: 507)\n    perspectiveTile.bottomLeft = CGPoint(x: 548, y: 140)\n    perspectiveTile.bottomRight = CGPoint(x: 155, y: 153)\n    return perspectiveTile.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ triangleTileFilter\nTiles a triangular area of an image.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "twelvefoldReflectedTileFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228427-twelvefoldreflectedtilefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis method applies the 12-fold reflected tile filter to an image. The effect produces a 12-way reflected tile image.\n\nThe 12-fold reflected tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nangle\n\nA float representing the direction of distortion, in radians as an NSNumber.\n\nwidth\n\nA float representing the set width of each tile as an NSNumber.\n\nThe following code creates a filter that results in a 12-fold pattern angled at 30 degrees and then repeated:\n\nfunc twelveFoldReflected(inputImage: CIImage) -> CIImage {\n    let twelveFoldReflectedTile = CIFilter.twelvefoldReflectedTile()\n    twelveFoldReflectedTile.inputImage = inputImage\n    twelveFoldReflectedTile.center = CGPoint(x: 150, y: 150)\n    twelveFoldReflectedTile.angle = 0\n    twelveFoldReflectedTile.width = 100\n    return twelveFoldReflectedTile.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ triangleTileFilter\nTiles a triangular area of an image."
  },
  {
    "title": "sixfoldReflectedTileFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228405-sixfoldreflectedtilefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis filter produces a tiled image from a source image by applying a six-way reflected symmetry.\n\nThe six-fold reflected tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nangle\n\nA float representing the direction of distortion , in radians as an NSNumber.\n\nwidth\n\nA float representing the set width of each tile as an NSNumber.\n\nThe following code creates a filter that results in a six-fold pattern repeated and angled for distortion:\n\nfunc sixFoldReflected(inputImage: CIImage) -> CIImage {\n    let sixFoldReflectedTile = CIFilter.sixfoldReflectedTile()\n    sixFoldReflectedTile.inputImage = inputImage\n    sixFoldReflectedTile.center = CGPoint(x: 150, y: 150)\n    sixFoldReflectedTile.angle = 0\n    sixFoldReflectedTile.width = 100\n    return sixFoldReflectedTile.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ triangleTileFilter\nTiles a triangular area of an image.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "kaleidoscopeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228343-kaleidoscopefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis method applies the kaleidoscope tile filter to an image. The effect produces a complex 12-way symmetrical reflected pattern from the input image.\n\nThe kaleidoscope tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nangle\n\nA float representing the direction of distortion, in radians as an NSNumber.\n\ncount\n\nA float representing the number of reflections in the pattern as an NSNumber.\n\nThe following code creates a filter that results in the creation of a kaleidoscope effect from the input image:\n\nfunc kaleidoscope(inputImage: CIImage) -> CIImage {\n    let kaleidoscopeEffect = CIFilter.kaleidoscope()\n    kaleidoscopeEffect.inputImage = inputImage\n    kaleidoscopeEffect.count = 6\n    kaleidoscopeEffect.center = CGPoint(x: 150, y: 150)\n    kaleidoscopeEffect.angle = 0\n    return kaleidoscopeEffect.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ triangleTileFilter\nTiles a triangular area of an image.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "opTileFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228373-optilefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis filter extracts a tile from the image, applies any specified scaling and rotation, and then assembles the image again to give an optical illusion effect.\n\nThe optical illusion tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nangle\n\nA float representing the direction of distortion, in radians as an NSNumber.\n\nwidth\n\nA float representing the set width of each tile as an NSNumber.\n\nscale\n\nA float representing the scale of numbers of tiles in the output image as an NSNumber.\n\nThe following code creates a filter that results in a distorted image with less detail:\n\nfunc op(inputImage: CIImage) -> CIImage {\n    let opTile = CIFilter.opTile()\n    opTile.inputImage = inputImage\n    opTile.center = CGPoint(x: 150, y: 150)\n    opTile.scale = 2.80\n    opTile.angle = 0\n    opTile.width = 65\n    return opTile.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ triangleTileFilter\nTiles a triangular area of an image.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "parallelogramTileFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228379-parallelogramtilefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis method applies the parallelogram tile filter to an image. The effect warps the input image to create a parallelogram and then tiles the result.\n\nThe parallelogram tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nangle\n\nA float representing the direction of distortion, in radians as an NSNumber.\n\nwidth\n\nA float representing the set width of each tile as an NSNumber.\n\nacuteAngle\n\nA float representing the primary angle for the repeating parallelogram tile.\n\nThe following code creates a filter that results in the image being cropped to a parallelogram and then tiled:\n\nfunc parallelogram(inputImage: CIImage) -> CIImage {\n    let parallelogramTile = CIFilter.parallelogramTile()\n    parallelogramTile.inputImage = inputImage\n    parallelogramTile.center = CGPoint(x: 150, y: 150)\n    parallelogramTile.angle = 0\n    parallelogramTile.acuteAngle = 1.57\n    parallelogramTile.width = 100\n    return parallelogramTile.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ triangleTileFilter\nTiles a triangular area of an image.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "linearGradientFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228351-lineargradientfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a linear-gradient image. The effect creates a gradient that varies linearly between the two input properties of point0 and point1.\n\nThe linear-gradient filter uses the following properties:\n\npoint0\n\nA CGPoint representing the starting position of the gradient.\n\npoint1\n\nA CGPoint representing the ending position of the gradient.\n\ncolor0\n\nA CIColor representing the first color to use in the gradient.\n\ncolor1\n\nA CIColor representing the second color to use the gradient.\n\nThe following code creates a filter that generates a gradient image:\n\nfunc linear() -> CIImage {\n    let linearGradient = CIFilter.linearGradient()\n    linearGradient.point0 = CGPoint(x: 0, y: 0)\n    linearGradient.point1 = CGPoint(x: 200, y: 200)\n    linearGradient.color0 = CIColor(red: 216/255, green: 232/255, blue: 146/255)\n    linearGradient.color1 = CIColor(red: 0/255, green: 112/255, blue: 201/255)\n    return linearGradient.outputImage!\n}\n\n\nSee Also\nFilters\n+ gaussianGradientFilter\nGenerates a gradient that varies from one color to another using a Gaussian distribution.\n+ hueSaturationValueGradientFilter\nGenerates a gradient representing a specified color space.\n+ radialGradientFilter\nGenerates a gradient that varies radially between two circles having the same center.\n+ smoothLinearGradientFilter\nGenerates a gradient that blends colors along a linear axis between two defined endpoints."
  },
  {
    "title": "radialGradientFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228395-radialgradientfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a radial-gradient image. The effect generates a color shift between the radius0 and radius1 properties.\n\nThe radial-gradient filter uses the following properties:\n\ncenter\n\nA CGPoint representing the center of the effect as x and y coordinates.\n\ncolor0\n\nA CIColor representing the first color to use in the gradient.\n\ncolor1\n\nA CIColor representing the second color to use in the gradient.\n\nradius0\n\nA float representing the radius of the starting circle to use in the gradient as a NSNumber.\n\nradius1\n\nA float representing the radius of the ending circle to use in the gradient as a NSNumber.\n\nThe following code creates a filter that generates a gradient image:\n\nfunc radial() -> CIImage {\n    let radialGradient = CIFilter.radialGradient()\n    radialGradient.center = CGPoint(x: 150, y: 150)\n    radialGradient.radius0 = 5\n    radialGradient.radius1 = 100\n    radialGradient.color0 = CIColor(red: 246/255, green: 145/255, blue: 181/255)\n    radialGradient.color1 = CIColor(red: 110/255, green: 81/255, blue: 161/255)\n    return radialGradient.outputImage!\n}\n\n\nSee Also\nFilters\n+ gaussianGradientFilter\nGenerates a gradient that varies from one color to another using a Gaussian distribution.\n+ hueSaturationValueGradientFilter\nGenerates a gradient representing a specified color space.\n+ linearGradientFilter\nGenerates a color gradient that varies along a linear axis between two defined endpoints.\n+ smoothLinearGradientFilter\nGenerates a gradient that blends colors along a linear axis between two defined endpoints."
  },
  {
    "title": "hueSaturationValueGradientFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228342-huesaturationvaluegradientfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a hue-saturation-value gradient image. The filter creates a color wheel that shows the hues and saturations for a specified CGColorSpaceRef.\n\nThe hue-saturation-value gradient uses the following properties:\n\ncolorSpace\n\nA CGColorSpaceRef representing the color space for the generated color wheel.\n\ndither\n\nA boolean value specifying whether the distort the generated output.\n\nradius\n\nA float representing the distance from the center of the effect as an NSNumber.\n\nsoftness\n\nA float representing the softness of the generated color wheel as an NSNumber.\n\nvalue\n\nA float representing the lightness of the hue-saturation gradient as an NSNumber.\n\nThe following code creates a filter that generates a color-space image:\n\nfunc hueSaturationValue() -> CIImage {\n    let hueSaturationValueGradient = CIFilter.hueSaturationValueGradient()\n    hueSaturationValueGradient.colorSpace = CGColorSpaceCreateDeviceRGB()\n    hueSaturationValueGradient.dither = 1\n    hueSaturationValueGradient.radius = 100\n    hueSaturationValueGradient.softness = 2\n    hueSaturationValueGradient.value = 1\n    return hueSaturationValueGradient.outputImage!\n}\n\n\nSee Also\nFilters\n+ gaussianGradientFilter\nGenerates a gradient that varies from one color to another using a Gaussian distribution.\n+ linearGradientFilter\nGenerates a color gradient that varies along a linear axis between two defined endpoints.\n+ radialGradientFilter\nGenerates a gradient that varies radially between two circles having the same center.\n+ smoothLinearGradientFilter\nGenerates a gradient that blends colors along a linear axis between two defined endpoints."
  },
  {
    "title": "gaussianGradientFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228332-gaussiangradientfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a Gaussian gradient image. The effect uses the Gaussian kernel to calculate the even dispersal of the first color in the center to the second color in the image’s periphery.\n\nThe Gaussian gradient filter uses the following properties:\n\ncenter\n\nA CGPoint representing the center of the effect as x and y coordinates.\n\ncolor0\n\nA CIColor representing the first color to use in the gradient.\n\ncolor1\n\nA CIColor representing the second color to use in the gradient.\n\nradius\n\nA float representing the radius of the Gaussian distribution as an NSNumber.\n\nThe following code creates a filter that generates a gradient image:\n\nfunc gaussian() -> CIImage {\n    let gaussianGradient = CIFilter.gaussianGradient()\n    gaussianGradient.center = CGPoint (x: 150, y: 150)\n    gaussianGradient.color0 = CIColor(red: 88/255, green: 201\n/255, blue: 175/255)\n    gaussianGradient.color1 = CIColor(red: 153/255, green: 153/255, blue: 204/255)\n    gaussianGradient.radius = 10\n    return gaussianGradient.outputImage!\n}\n\n\nSee Also\nFilters\n+ hueSaturationValueGradientFilter\nGenerates a gradient representing a specified color space.\n+ linearGradientFilter\nGenerates a color gradient that varies along a linear axis between two defined endpoints.\n+ radialGradientFilter\nGenerates a gradient that varies radially between two circles having the same center.\n+ smoothLinearGradientFilter\nGenerates a gradient that blends colors along a linear axis between two defined endpoints."
  },
  {
    "title": "modTransitionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228363-modtransitionfilter",
    "html": "Return Value\n\nThe transition image.\n\nDiscussion\n\nThis method applies the mod transition filter to an image. The effect transitions from the input image to the output image by revealing the target image through irregularly shaped holes.\n\nThe mod transition filter uses the following properties:\n\ninputImage\n\nThe starting image with the type CIImage.\n\ntargetImage\n\nThe ending image with the type CIImage.\n\ncenter\n\nA CGPoint representing the center of the image.\n\nangle\n\nA float representing the angle of the effect as an NSNumber.\n\nradius\n\nA float representing the size of the area of effect as an NSNumber.\n\ncompression\n\nA float representing the amount of stretching applied to the mod hole pattern as an NSNumber.\n\ntime\n\nA float representing the parametric time of the transition from start (at time 0) to end (at time 1) as an NSNumber.\n\nThe following code creates a filter that transitions from the input image to the target image by creating a series of irregular shaped holes.\n\nfunc mod(inputImage: CIImage, targetImage: CIImage) -> CIImage {\n    let modTransition = CIFilter.modTransition()\n    modTransition.inputImage = inputImage\n    modTransition.targetImage = targetImage\n    modTransition.center = CGPoint(x: 390, y: 392)\n    modTransition.time = 0.5\n    modTransition.angle = 0.09\n    modTransition.radius = 150\n    modTransition.compression = 523   \n    return modTransition.outputImage!\n}\n\n\nSee Also\nFilters\n+ accordionFoldTransitionFilter\nTransitions by folding and crossfading an image to reveal the target image.\n+ barsSwipeTransitionFilter\nTransitions between two images by removing rectangular portions of an image.\n+ copyMachineTransitionFilter\nSimulates the effect of a copy machine scanner light to transiton between two images.\n+ disintegrateWithMaskTransitionFilter\nTransitions between two images using a mask image.\n+ dissolveTransitionFilter\nTransitions between two images with a fade effect.\n+ flashTransitionFilter\nCreates a flash of light to transition between two images.\n+ pageCurlTransitionFilter\nSimulates the curl of a page, revealing the target image.\n+ pageCurlWithShadowTransitionFilter\nSimulates the curl of a page, revealing the target image with added shadow.\n+ rippleTransitionFilter\nSimulates a ripple in a pond to transiton from one image to another.\n+ swipeTransitionFilter\nGradually transitions from one image to another with a swiping motion."
  },
  {
    "title": "histogramDisplayFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3547122-histogramdisplayfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method applies the histogram display filter to the result of the output from the areaHistogramFilter filter. This effect shows a graphical representation of the tonal distribution of colors in the image.\n\nThe histogram display filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage. Typically this is the output from the area histogram filter.\n\nheight\n\nA float representing the height of the generated histogram image as an NSNumber.\n\nlowLimit\n\nA float representing the fraction of the left portion of the histogram image to make darker as an NSNumber.\n\nhightLimit\n\nA float representing the fraction of the right portion of the histogram to make lighter as an NSNumber.\n\nThe following code creates a filter that results in a histogram diagram generated from the input image:\n\nfunc areaHistogram(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.areaHistogram()\n    filter.inputImage = inputImage\n    filter.count = 256\n    filter.scale = 50\n    filter.extent = CGRect(\n        x: inputImage.extent.width/2-250,\n        y: inputImage.extent.height/2-250,\n        width: 500,\n        height: 500)\n    return filter.outputImage!\n}\n\n\nfunc histogramDisplay(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.histogramDisplay()\n    filter.inputImage = areaHistogram(inputImage: inputImage)\n    filter.highLimit = 1\n    filter.height = 100\n    filter.lowLimit = 0\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ areaAverageFilter\nReturns a 1 x 1 pixel image that contains the average color for the region of interest.\n+ areaHistogramFilter\nReturns a histogram of a specified area of the image.\n+ areaLogarithmicHistogramFilter\nReturns a logarithmic histogram of a specified area of the image.\n+ areaMaximumFilter\nCalculates the maximum color components of a specified area of the image.\n+ areaMaximumAlphaFilter\nFinds the pixel with the highest alpha value.\n+ areaMinimumFilter\nCalculates the minimum color component values for a specified area of the image.\n+ areaMinimumAlphaFilter\nCalculates the pixel within a specified area that has the smallest alpha value.\n+ areaMinMaxFilter\nCalculates minimum and maximum color components for a specified area of the image.\n+ areaMinMaxRedFilter\nCalculates the minimum and maximum red component value.\n+ columnAverageFilter\nCalculates the average color for a specified column of an image.\n+ KMeansFilter\nApplies the k-means algorithm to find the most common colors in an image.\n+ rowAverageFilter\nCalculates the average color for the specified row of pixels in an image."
  },
  {
    "title": "rowAverageFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3547123-rowaveragefilter",
    "html": "Return Value\n\nDiscussion\n\nThis method applies the row average filter to an image. This effect calculates the average color for a horizontal row over a region defined by extent. The height of the extent determines the width of the resulting image. The height is always 1 pixel.\n\nThe row average filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nextent\n\nA CGRect that specifies the subregion of the image that you want to process.\n\nThe following code creates a filter that calculates the row average for the middle section of an image:\n\nfunc rowAverage(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.rowAverage()\n    filter.inputImage = inputImage\n    filter.extent = CGRect(x: inputImage.extent.width/3, y: 0, width: inputImage.extent.width/3, height: inputImage.extent.height)\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ areaAverageFilter\nReturns a 1 x 1 pixel image that contains the average color for the region of interest.\n+ areaHistogramFilter\nReturns a histogram of a specified area of the image.\n+ areaLogarithmicHistogramFilter\nReturns a logarithmic histogram of a specified area of the image.\n+ areaMaximumFilter\nCalculates the maximum color components of a specified area of the image.\n+ areaMaximumAlphaFilter\nFinds the pixel with the highest alpha value.\n+ areaMinimumFilter\nCalculates the minimum color component values for a specified area of the image.\n+ areaMinimumAlphaFilter\nCalculates the pixel within a specified area that has the smallest alpha value.\n+ areaMinMaxFilter\nCalculates minimum and maximum color components for a specified area of the image.\n+ areaMinMaxRedFilter\nCalculates the minimum and maximum red component value.\n+ columnAverageFilter\nCalculates the average color for a specified column of an image.\n+ histogramDisplayFilter\nGenerates a histogram map from the image.\n+ KMeansFilter\nApplies the k-means algorithm to find the most common colors in an image."
  },
  {
    "title": "sharpenLuminanceFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228404-sharpenluminancefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the sharpen luminance filter to an image. The effect increases image detail by adjusting the luminance of each pixel within the radius property. Sharpening the luminance doesn’t effect the chroma data of each pixel.\n\nThe bicubic sharpen luminance filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\nsharpness\n\nA float representing the desired strength of the effect as an NSNumber.\n\nThe following code creates a filter that results in detail from the sign in the image to be more visible:\n\nfunc sharpen (inputImage: CIImage) -> CIImage? {\n    let sharpenLuminance = CIFilter.sharpenLuminance()\n    sharpenLuminance.inputImage = inputImage\n    sharpenLuminance.radius = 10\n    sharpenLuminance.sharpness = 1\n    return sharpenLuminance.outputImage!\n}\n\n\nSee Also\nFilters\n+ unsharpMaskFilter\nIncreases an image’s contrast between two colors."
  },
  {
    "title": "columnAverageFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3547121-columnaveragefilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method applies the column average filter to an image. This effect calculates the average color for a vertical column over a region defined by extent. The width of the resulting image is set by the width of the extent. The height of the resulting image is always 1 pixel.\n\nThe column average filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nextent\n\nA CGRect that specifies the subregion of the image that you want to process.\n\nThe following code creates an image containing the average values in the columns from the middle of the image:\n\nfunc columnAverage(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.columnAverage()\n    filter.inputImage = inputImage\n    filter.extent = CGRect(x: 0, y: inputImage.extent.height/3, width: inputImage.extent.width, height: inputImage.extent.height/3)\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ areaAverageFilter\nReturns a 1 x 1 pixel image that contains the average color for the region of interest.\n+ areaHistogramFilter\nReturns a histogram of a specified area of the image.\n+ areaLogarithmicHistogramFilter\nReturns a logarithmic histogram of a specified area of the image.\n+ areaMaximumFilter\nCalculates the maximum color components of a specified area of the image.\n+ areaMaximumAlphaFilter\nFinds the pixel with the highest alpha value.\n+ areaMinimumFilter\nCalculates the minimum color component values for a specified area of the image.\n+ areaMinimumAlphaFilter\nCalculates the pixel within a specified area that has the smallest alpha value.\n+ areaMinMaxFilter\nCalculates minimum and maximum color components for a specified area of the image.\n+ areaMinMaxRedFilter\nCalculates the minimum and maximum red component value.\n+ histogramDisplayFilter\nGenerates a histogram map from the image.\n+ KMeansFilter\nApplies the k-means algorithm to find the most common colors in an image.\n+ rowAverageFilter\nCalculates the average color for the specified row of pixels in an image."
  },
  {
    "title": "areaMinMaxFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3547115-areaminmaxfilter",
    "html": "Return Value\n\nA 2 x 1 pixel image containing the minimum and maximum color components.\n\nDiscussion\n\nThis filter returns the maximum and minimum color components in the region defined by extent. The result is a 2 x 1 pixel image with the left pixel containing the minimum components and the right pixel containing the maximum components.\n\nThe area min max filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nextent\n\nA CGRect that specifies the subregion of the image that you want to process.\n\nThe following code creates a filter that results in a 2 x 1 image where the minimum components are in the left pixel and the maximum components are in the right pixel.\n\nfunc areaMinMax(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.areaMinMax()\n    filter.inputImage = inputImage\n    filter.extent = CGRect(        x: inputImage.extent.width/2-250,\n        y: inputImage.extent.height/2-250,\n        width: 500,\n        height: 500)\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ areaAverageFilter\nReturns a 1 x 1 pixel image that contains the average color for the region of interest.\n+ areaHistogramFilter\nReturns a histogram of a specified area of the image.\n+ areaLogarithmicHistogramFilter\nReturns a logarithmic histogram of a specified area of the image.\n+ areaMaximumFilter\nCalculates the maximum color components of a specified area of the image.\n+ areaMaximumAlphaFilter\nFinds the pixel with the highest alpha value.\n+ areaMinimumFilter\nCalculates the minimum color component values for a specified area of the image.\n+ areaMinimumAlphaFilter\nCalculates the pixel within a specified area that has the smallest alpha value.\n+ areaMinMaxRedFilter\nCalculates the minimum and maximum red component value.\n+ columnAverageFilter\nCalculates the average color for a specified column of an image.\n+ histogramDisplayFilter\nGenerates a histogram map from the image.\n+ KMeansFilter\nApplies the k-means algorithm to find the most common colors in an image.\n+ rowAverageFilter\nCalculates the average color for the specified row of pixels in an image."
  },
  {
    "title": "areaMinMaxRedFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3547116-areaminmaxredfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method applies the area-minimum-maximum-red filter to an image. This effect calculates the darkest and lightest red color value in the region defined by extent. The red and green components of the 1 x 1 pixel output image contain the result.\n\nThe area-minimum-maximum-red filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nextent\n\nA CGRect that specifies the subregion of the image that you want to process.\n\nThe following code creates a filter that results in a 1 x 1 pixel image with the red and green color components populated:\n\nfunc areaMinMaxRed(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.areaMinMaxRed()\n    filter.inputImage = inputImage\n    filter.extent = CGRect(\n        x: inputImage.extent.width/2-250,\n        y: inputImage.extent.height/2-250,\n        width: 500,\n        height: 500)\n     return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ areaAverageFilter\nReturns a 1 x 1 pixel image that contains the average color for the region of interest.\n+ areaHistogramFilter\nReturns a histogram of a specified area of the image.\n+ areaLogarithmicHistogramFilter\nReturns a logarithmic histogram of a specified area of the image.\n+ areaMaximumFilter\nCalculates the maximum color components of a specified area of the image.\n+ areaMaximumAlphaFilter\nFinds the pixel with the highest alpha value.\n+ areaMinimumFilter\nCalculates the minimum color component values for a specified area of the image.\n+ areaMinimumAlphaFilter\nCalculates the pixel within a specified area that has the smallest alpha value.\n+ areaMinMaxFilter\nCalculates minimum and maximum color components for a specified area of the image.\n+ columnAverageFilter\nCalculates the average color for a specified column of an image.\n+ histogramDisplayFilter\nGenerates a histogram map from the image.\n+ KMeansFilter\nApplies the k-means algorithm to find the most common colors in an image.\n+ rowAverageFilter\nCalculates the average color for the specified row of pixels in an image."
  },
  {
    "title": "areaMinimumAlphaFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3547117-areaminimumalphafilter",
    "html": "Return Value\n\nA 1 x 1 pixel image containing the color with the smallest alpha value.\n\nDiscussion\n\nThis method applies the area minimum alpha filter to an image. This effect finds and returns the pixel with the lowest alpha value in the region defined by extent.\n\nThe area minimum alpha filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nextent\n\nA CGRect that specifies the subregion of the image that you want to process.\n\nThe following code creates a filter that results in a 1 x 1 pixel image containing the color with the lowest alpha value:\n\nfunc areaMinimumAlpha(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.areaMinimumAlpha()\n    filter.inputImage = inputImage\n    filter.extent = CGRect(\n        x: inputImage.extent.width/2-250,\n        y: inputImage.extent.height/2-250,\n        width: 500,\n        height: 500)\n     return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ areaAverageFilter\nReturns a 1 x 1 pixel image that contains the average color for the region of interest.\n+ areaHistogramFilter\nReturns a histogram of a specified area of the image.\n+ areaLogarithmicHistogramFilter\nReturns a logarithmic histogram of a specified area of the image.\n+ areaMaximumFilter\nCalculates the maximum color components of a specified area of the image.\n+ areaMaximumAlphaFilter\nFinds the pixel with the highest alpha value.\n+ areaMinimumFilter\nCalculates the minimum color component values for a specified area of the image.\n+ areaMinMaxFilter\nCalculates minimum and maximum color components for a specified area of the image.\n+ areaMinMaxRedFilter\nCalculates the minimum and maximum red component value.\n+ columnAverageFilter\nCalculates the average color for a specified column of an image.\n+ histogramDisplayFilter\nGenerates a histogram map from the image.\n+ KMeansFilter\nApplies the k-means algorithm to find the most common colors in an image.\n+ rowAverageFilter\nCalculates the average color for the specified row of pixels in an image."
  },
  {
    "title": "areaMinimumFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3547118-areaminimumfilter",
    "html": "Return Value\n\nA 1 x 1 size image containing the minimum color component values.\n\nDiscussion\n\nThis filter returns the minimum color components in the region defined by extent.\n\nThe area minimum filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nextent\n\nA CGRect that specifies the subregion of the image that you want to process.\n\nThe following code creates a filter that calculates the minimum color components of a 500 x 500 set of pixels from the center of the image:\n\nfunc areaMinimum(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.areaMinimum()\n    filter.inputImage = inputImage\n    filter.extent = CGRect(\n        x: inputImage.extent.width/2-250,\n        y: inputImage.extent.height/2-250,\n        width: 500,\n        height: 500)\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ areaAverageFilter\nReturns a 1 x 1 pixel image that contains the average color for the region of interest.\n+ areaHistogramFilter\nReturns a histogram of a specified area of the image.\n+ areaLogarithmicHistogramFilter\nReturns a logarithmic histogram of a specified area of the image.\n+ areaMaximumFilter\nCalculates the maximum color components of a specified area of the image.\n+ areaMaximumAlphaFilter\nFinds the pixel with the highest alpha value.\n+ areaMinimumAlphaFilter\nCalculates the pixel within a specified area that has the smallest alpha value.\n+ areaMinMaxFilter\nCalculates minimum and maximum color components for a specified area of the image.\n+ areaMinMaxRedFilter\nCalculates the minimum and maximum red component value.\n+ columnAverageFilter\nCalculates the average color for a specified column of an image.\n+ histogramDisplayFilter\nGenerates a histogram map from the image.\n+ KMeansFilter\nApplies the k-means algorithm to find the most common colors in an image.\n+ rowAverageFilter\nCalculates the average color for the specified row of pixels in an image."
  },
  {
    "title": "sourceOverCompositingFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228412-sourceovercompositingfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the source-over compositing filter to an image. The effect creates the result by overlaying the input image over the background image. Unlike the other source compositing filters, source-over doesn’t subtract parts of the image.\n\nThe source-over compositing filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in both of the input images becoming visible with no subtraction:\n\nfunc sourceOverCompositing(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let colorBlendFilter = CIFilter.sourceOverCompositing()\n    colorBlendFilter.inputImage = inputImage\n    colorBlendFilter.backgroundImage = backgroundImage\n    return colorBlendFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ additionCompositingFilter\nBlends colors from two images by addition.\n+ colorBlendModeFilter\nBlends color from two images using the luminance values from the background image and the hue and saturation values from the input image.\n+ colorBurnBlendModeFilter\nBlends color from two images while darkening the image.\n+ colorDodgeBlendModeFilter\nBlends color from two images using dodging.\n+ darkenBlendModeFilter\nBlends colors from two images while darkening lighter pixels.\n+ differenceBlendModeFilter\nSubtracts color values to blend colors.\n+ divideBlendModeFilter\nDivides color values to blend colors.\n+ exclusionBlendModeFilter\nSubtracts color values to blend colors with less contrast.\n+ hardLightBlendModeFilter\nBlends colors of two images by screening and multiplying.\n+ hueBlendModeFilter\nBlends colors of two images by computing the sum of image color values.\n+ lightenBlendModeFilter\nBlends colors from two images by brightening colors.\n+ linearBurnBlendModeFilter\nBlends color from two images while increasing contrast.\n+ linearDodgeBlendModeFilter\nBlends colors of two images with dodging.\n+ linearLightBlendModeFilter\nA combination of linear burn and linear dodge blend modes.\n+ luminosityBlendModeFilter\nBlends color from two images by calculating the color, hue, and saturation.\n+ minimumCompositingFilter\nBlends colors from two images by computing minimum values.\n+ maximumCompositingFilter\nApplies a maximum compositing filter to an image.\n+ multiplyBlendModeFilter\nBlends colors from two images by multiplying color components.\n+ multiplyCompositingFilter\nBlurs the colors of two images by multiplying color components.\n+ overlayBlendModeFilter\nBlends colors by overlaying images.\n+ pinLightBlendModeFilter\nBlends colors of two images by replacing brighter colors.\n+ saturationBlendModeFilter\nBlends the colors and saturation values of two images.\n+ screenBlendModeFilter\nBlends colors of two images by multiplying colors.\n+ softLightBlendModeFilter\nBlurs the colors of two images by calculating luminance.\n+ sourceAtopCompositingFilter\nOverlaps two images to create one cropped image.\n+ sourceInCompositingFilter\nSubtracts non-overlapping areas of two images, resulting in one image.\n+ sourceOutCompositingFilter\nSubtracts overlapping area of two images to create the output image.\n+ subtractBlendModeFilter\nBlends colors by subtracting color values from two images."
  },
  {
    "title": "pageCurlWithShadowTransitionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228376-pagecurlwithshadowtransitionfilt",
    "html": "Return Value\n\nThe transition image.\n\nDiscussion\n\nThis method applies the page curl with shadow transition filter to an image. The effect transitions from one image to another by simulating a curling page, revealing the target image as the page curls with a shadow effect from the backside image.\n\nThe page curl with shadow transition filter uses the following properties:\n\ninputImage\n\nThe starting image with the type CIImage.\n\ntargetImage\n\nThe ending image with the type CIImage.\n\nbacksideImage\n\nAn image used as the backside of the curl with the type CIImage.\n\nextent\n\nA CIVector representing the extent of the effect.\n\nangle\n\nA float representing the angle of the motion, in radians as an NSNumber.\n\nshadowAmount\n\nA float representing the strength of the shadow as an NSNumber.\n\nshadowExtent\n\nA CIVector representing the rectangular portion of the input image that is used to create the shadow.\n\nshadowSize\n\nA float representing the maximum amount of pixels to make up the shadow as an NSNumber.\n\ntime\n\nA float representing the parametric time of the transition from start (at time 0) to end (at time 1) as an NSNumber.\n\nThe following code creates a page curling back to reveal the target image with an added shadow.\n\nfunc pageCurl(inputImage: CIImage, targetImage: CIImage, backsideImage: CIImage) -> CIImage {\n    let pageCurlTransition = CIFilter.pageCurlWithShadowTransition()\n    pageCurlTransition.inputImage = inputImage\n    pageCurlTransition.targetImage = targetImage\n    pageCurlTransition.backsideImage = backsideImage\n    pageCurlTransition.extent = CGRect(x: 54, y: 90, width: 300, height: 300)\n    pageCurlTransition.time = 0.5\n    pageCurlTransition.angle = 4\n    pageCurlTransition.radius = 100\n    pageCurlTransition.shadowAmount = 10\n    pageCurlTransition.shadowSize = 6\n    pageCurlTransition.shadowExtent = CGRect(x: 32, y: 56, width: 400, height: 400)\n    return pageCurlTransition.outputImage!\n}\n\n\nSee Also\nFilters\n+ accordionFoldTransitionFilter\nTransitions by folding and crossfading an image to reveal the target image.\n+ barsSwipeTransitionFilter\nTransitions between two images by removing rectangular portions of an image.\n+ copyMachineTransitionFilter\nSimulates the effect of a copy machine scanner light to transiton between two images.\n+ disintegrateWithMaskTransitionFilter\nTransitions between two images using a mask image.\n+ dissolveTransitionFilter\nTransitions between two images with a fade effect.\n+ flashTransitionFilter\nCreates a flash of light to transition between two images.\n+ modTransitionFilter\nTransitions between two images by applying irregularly shaped holes.\n+ pageCurlTransitionFilter\nSimulates the curl of a page, revealing the target image.\n+ rippleTransitionFilter\nSimulates a ripple in a pond to transiton from one image to another.\n+ swipeTransitionFilter\nGradually transitions from one image to another with a swiping motion."
  },
  {
    "title": "pageCurlTransitionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228375-pagecurltransitionfilter",
    "html": "Return Value\n\nThe transition image.\n\nDiscussion\n\nThis method applies the page curl transition filter to an image. The effect transitions from one image to another by simulating a curling page, revealing the target image as the page curls.\n\nThe page curl transition filter uses the following properties:\n\ninputImage\n\nThe starting image with the type CIImage.\n\ntargetImage\n\nThe ending image with the type CIImage.\n\nbacksideImage\n\nAn image used as the backside of the curl with the type CIImage.\n\nextent\n\nA CGRect representing the size of the effect.\n\ntime\n\nA float representing the parametric time of the transition from start (at time 0) to end (at time 1) as an NSNumber.\n\nangle\n\nA float representing the angle of the motion of the curl as an NSNumber.\n\nradius\n\nA float representing the radius of the curl as an NSNumber.\n\nThe following code creates a filter that produces a page curling back to reveal the target image.\n\nfunc pageCurl(inputImage: CIImage, targetImage: CIImage, backsideImage: CIImage) -> CIImage {\n    let pageCurlTransition = CIFilter.pageCurlTransition()\n    pageCurlTransition.inputImage = inputImage\n    pageCurlTransition.targetImage = targetImage\n    pageCurlTransition.backsideImage = backsideImage\n    pageCurlTransition.extent = CGRect(x: 54, y: 90, width: 300, height: 300)\n    pageCurlTransition.time = 5.6\n    pageCurlTransition.angle = 0.9\n    pageCurlTransition.radius = 150\n    return pageCurlTransition.outputImage!\n}\n\n\nSee Also\nFilters\n+ accordionFoldTransitionFilter\nTransitions by folding and crossfading an image to reveal the target image.\n+ barsSwipeTransitionFilter\nTransitions between two images by removing rectangular portions of an image.\n+ copyMachineTransitionFilter\nSimulates the effect of a copy machine scanner light to transiton between two images.\n+ disintegrateWithMaskTransitionFilter\nTransitions between two images using a mask image.\n+ dissolveTransitionFilter\nTransitions between two images with a fade effect.\n+ flashTransitionFilter\nCreates a flash of light to transition between two images.\n+ modTransitionFilter\nTransitions between two images by applying irregularly shaped holes.\n+ pageCurlWithShadowTransitionFilter\nSimulates the curl of a page, revealing the target image with added shadow.\n+ rippleTransitionFilter\nSimulates a ripple in a pond to transiton from one image to another.\n+ swipeTransitionFilter\nGradually transitions from one image to another with a swiping motion."
  },
  {
    "title": "rippleTransitionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228397-rippletransitionfilter",
    "html": "Return Value\n\nThe transition image.\n\nDiscussion\n\nThis method applies the ripple transition filter to an image. The effect transitions from one image to another by creating a circular wave that expands from the center point, revealing the target image through the wave effect.\n\nThe ripple transition filter uses the following properties:\n\ninputImage\n\nThe starting image with the type CIImage.\n\ntargetImage\n\nThe ending image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nwidth\n\nA float representing the width of the ripple effect as an NSNumber.\n\nextent\n\nA CGRect representing the size of the ripple effect.\n\nscale\n\nA float representing the scale of the effect as an NSNumber.\n\ntime\n\nA float representing the parametric time of the transition from start (at time 0) to end (at time 1) as an NSNumber.\n\nThe following code creates a filter that transitions from the input image to the target image with a water-like ripple effect.\n\nfunc ripple (inputImage: CIImage, targetImage: CIImage) -> CIImage {\n    let rippleTransition = CIFilter.rippleTransition()\n    rippleTransition.inputImage = inputImage\n    rippleTransition.targetImage = targetImage\n    rippleTransition.center = CGPoint(x: 250, y: 150)\n    rippleTransition.width = 100\n    rippleTransition.extent = CGRect(x: 54, y: 80, width: 300, height: 300)\n    rippleTransition.scale = 22\n    rippleTransition.time = 0.3\n    return rippleTransition.outputImage!\n}\n\n\nSee Also\nFilters\n+ accordionFoldTransitionFilter\nTransitions by folding and crossfading an image to reveal the target image.\n+ barsSwipeTransitionFilter\nTransitions between two images by removing rectangular portions of an image.\n+ copyMachineTransitionFilter\nSimulates the effect of a copy machine scanner light to transiton between two images.\n+ disintegrateWithMaskTransitionFilter\nTransitions between two images using a mask image.\n+ dissolveTransitionFilter\nTransitions between two images with a fade effect.\n+ flashTransitionFilter\nCreates a flash of light to transition between two images.\n+ modTransitionFilter\nTransitions between two images by applying irregularly shaped holes.\n+ pageCurlTransitionFilter\nSimulates the curl of a page, revealing the target image.\n+ pageCurlWithShadowTransitionFilter\nSimulates the curl of a page, revealing the target image with added shadow.\n+ swipeTransitionFilter\nGradually transitions from one image to another with a swiping motion."
  },
  {
    "title": "auxiliaryDepth",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/2890795-auxiliarydepth",
    "html": "Discussion\n\nThe value of this key is an NSNumber containing a Boolean true or false. If the value is true, then calls to imageWithContentsOfURL:options: and imageWithData:options: will return the auxiliary image as a half-float monochrome image instead of the primary image, or nil if no auxiliary image exists.\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "spotColorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228413-spotcolorfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the spot color filter to an image. The effect replaces one or more of the color ranges of the input image with properties.\n\nThe spot color filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenterColor1\n\nA CIColor representing the median value of the first color to be replaced.\n\ncenterColor2\n\nA CIColor representing the median value of the second color to be replaced.\n\ncenterColor3\n\nA CIColor representing the median value of the third color to be replaced.\n\nreplacementColor1\n\nA CIColor to replace the first color.\n\nreplacementColor2\n\nA CIColor to replace the second color.\n\nreplacementColor3\n\nA CIColor to replace the third color.\n\ncloseness1\n\nA float representing how closely the first center color must match before it’s replaced.\n\ncloseness2\n\nA float representing how closely the second center color must match before it’s replaced.\n\ncloseness3\n\nA float representing how closely the third center color must match before it’s replaced.\n\ncontrast1\n\nA float representing the contrast of the first replacement color as an NSNumber.\n\ncontrast2\n\nA float representing the contrast of the second replacement color as an NSNumber.\n\ncontrast3\n\nA float representing the contrast of the third replacement color as an NSNumber.\n\nThe following code creates a filter that replaces the colors of the input image with the specified colors:\n\nfunc spotColor(inputImage: CIImage) -> CIImage {\n    let spotColorFilter = CIFilter.spotColor()\n    spotColorFilter.inputImage = inputImage\n    spotColorFilter.centerColor1 = .red\n    spotColorFilter.replacementColor1 = .green\n    spotColorFilter.closeness1 = 5\n    spotColorFilter.contrast1 = 1\n    return spotColorFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "accordionFoldTransitionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228263-accordionfoldtransitionfilter",
    "html": "Return Value\n\nThe transition image.\n\nDiscussion\n\nThis method applies the accordion fold transition filter to an image. The effect transitions from one image to another by unfolding and crossfading.\n\nThe accordion fold transition filter uses the following properties:\n\ninputImage\n\nThe starting image with the type CIImage.\n\ntargetImage\n\nThe ending image with the type CIImage.\n\ntime\n\nA float representing the parametric time of the transition from start (at time 0) to end (at time 1) as an NSNumber.\n\nnumberOfFolds\n\nA float representing the number of accordion folds as a NSNumber.\n\nfoldShadowAmount\n\nA float representing the strength of the shadow as a NSNumber.\n\nThe following code creates a filter that produces folds in the input image and fades to the target image:\n\nfunc accordionFold(inputImage: CIImage, targetImage: CIImage) -> CIImage {\n    let accordionFoldTransiton = CIFilter.accordionFoldTransition()\n    accordionFoldTransiton.inputImage = inputImage\n    accordionFoldTransiton.targetImage = targetImage\n    accordionFoldTransiton.time = 0.5\n    accordionFoldTransiton.numberOfFolds = 6\n    accordionFoldTransiton.foldShadowAmount = 2\n    return accordionFoldTransiton.outputImage!\n}\n\n\nSee Also\nFilters\n+ barsSwipeTransitionFilter\nTransitions between two images by removing rectangular portions of an image.\n+ copyMachineTransitionFilter\nSimulates the effect of a copy machine scanner light to transiton between two images.\n+ disintegrateWithMaskTransitionFilter\nTransitions between two images using a mask image.\n+ dissolveTransitionFilter\nTransitions between two images with a fade effect.\n+ flashTransitionFilter\nCreates a flash of light to transition between two images.\n+ modTransitionFilter\nTransitions between two images by applying irregularly shaped holes.\n+ pageCurlTransitionFilter\nSimulates the curl of a page, revealing the target image.\n+ pageCurlWithShadowTransitionFilter\nSimulates the curl of a page, revealing the target image with added shadow.\n+ rippleTransitionFilter\nSimulates a ripple in a pond to transiton from one image to another.\n+ swipeTransitionFilter\nGradually transitions from one image to another with a swiping motion."
  },
  {
    "title": "barsSwipeTransitionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228270-barsswipetransitionfilter",
    "html": "Return Value\n\nThe transition image.\n\nDiscussion\n\nThis method applies the bar swipe transition filter to an image. The effect transitions from one image to another by a series of moving bars passing over the target image.\n\nThe bar swipe transition filter uses the following properties:\n\ninputImage\n\nThe starting image with the type CIImage.\n\ntargetImage\n\nThe ending image with the type CIImage.\n\ntime\n\nA float representing the parametric time of the transition from start (at time 0) to end (at time 1) as an NSNumber.\n\nangle\n\nA float representing the angle of the motion as an NSNumber.\n\nwidth\n\nA float representing the width of the bars in pixels as an NSNumber.\n\nbarOffset\n\nA float representing the offset of one bar in relation to others as a NSNumber.\n\nThe following code creates a filter that produces falling bars from the input image to transition to the target image:\n\nfunc barSwipe(inputImage: CIImage, targetImage: CIImage) -> CIImage {\n    let barSwipeTranstion = CIFilter.barsSwipeTransition()\n    barSwipeTranstion.inputImage = inputImage\n    barSwipeTranstion.targetImage = targetImage\n    barSwipeTranstion.time = 0.5\n    barSwipeTranstion.angle = 0.09\n    barSwipeTranstion.width = 30\n    barSwipeTranstion.barOffset = 10   \n    return barSwipeTranstion.outputImage!\n}\n\n\nSee Also\nFilters\n+ accordionFoldTransitionFilter\nTransitions by folding and crossfading an image to reveal the target image.\n+ copyMachineTransitionFilter\nSimulates the effect of a copy machine scanner light to transiton between two images.\n+ disintegrateWithMaskTransitionFilter\nTransitions between two images using a mask image.\n+ dissolveTransitionFilter\nTransitions between two images with a fade effect.\n+ flashTransitionFilter\nCreates a flash of light to transition between two images.\n+ modTransitionFilter\nTransitions between two images by applying irregularly shaped holes.\n+ pageCurlTransitionFilter\nSimulates the curl of a page, revealing the target image.\n+ pageCurlWithShadowTransitionFilter\nSimulates the curl of a page, revealing the target image with added shadow.\n+ rippleTransitionFilter\nSimulates a ripple in a pond to transiton from one image to another.\n+ swipeTransitionFilter\nGradually transitions from one image to another with a swiping motion."
  },
  {
    "title": "mixFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228362-mixfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the mix filter to an image. The effect uses the amount property to interpolate between the input image and the background image, resulting in both images visible in the output image.\n\nThe mix filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nbackgroundImage\n\nAn image representing the background image with the type CIImage.\n\namount\n\nA float representing the strength of of the effect as an NSNumber.\n\nThe following code creates a filter that combines the input and background images to create one image with both images visible:\n\nfunc mix(inputImage: CIImage, backgroundImage: CIImage) -> CIImage {\n    let mixFilter = CIFilter.mix()\n    mixFilter.inputImage = inputImage\n    mixFilter.backgroundImage = backgroundImage\n    mixFilter.amount = 0.25\n    return mixFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "highlightShadowAdjustFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228339-highlightshadowadjustfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the highlight-shadow adjust filter to an image. The effect adjusts shadows, while preserving spatial detail in the image.\n\nThe highlight-shadow adjust filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nshadowAmount\n\nA float representing the amount of generated shadow as an NSNumber.\n\nradius\n\nA float representing the radius of the shadow as an NSNumber.\n\nhighlightAmount\n\nA float representing the strength of the shadow as an NSNumber.\n\nThe following code creates a filter that results in a brighter image with reduced shadows:\n\nfunc highlightShadowAdjust(inputImage: CIImage) -> CIImage {\n    let highlightShadowAdjustFilter = CIFilter.highlightShadowAdjust()\n    highlightShadowAdjustFilter.inputImage = inputImage\n    highlightShadowAdjustFilter.shadowAmount = 1\n    return highlightShadowAdjustFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "lineOverlayFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228347-lineoverlayfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the line overlay filter to an image. The effect creats a sketch that outlines the edges of the image in black, leaving the non-outlined portion of the image transparent.\n\nThe line overlay filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nnrNoiseLevel\n\nA float representing the desired level of noise as an NSNumber.\n\nnrSharpness\n\nA float representing the desired level of sharpness as an NSNumber.\n\nedgeIntensity\n\nA float representing the Sobel gradient information for edge tracing as an NSNumber.\n\nthreshold\n\nA float representing the threshold of edge visibilty as an NSNumber.\n\ncontrast\n\nA float representing the desired contrast as an NSNumber.\n\nThe following code creates a filter that results in a monochrome image with lines outlining the edges of objects:\n\nfunc lineOverlay(inputImage: CIImage) -> CIImage {\n    let lineOverlay = CIFilter.lineOverlay()\n    lineOverlay.inputImage = inputImage\n    lineOverlay.nrNoiseLevel = 0.07\n    lineOverlay.nrSharpness = 0.71\n    lineOverlay.edgeIntensity = 1\n    lineOverlay.threshold = 0.1\n    lineOverlay.contrast = 50.00\n    return lineOverlay.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "hexagonalPixellateFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228338-hexagonalpixellatefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the hexagonal pixelate filter to an image. The effect creates an image containing colored hexagons.\n\nThe hexagonal pixelate filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nscale\n\nA float representing the scale of the hexagons as an NSNumber.\n\nThe following code creates a filter that results in an image made up of hexagons:\n\nfunc hexagonalPixelate (inputImage: CIImage) -> CIImage {\n    let hexagonalPixelateFilter = CIFilter.hexagonalPixellate()\n    hexagonalPixelateFilter.inputImage = inputImage\n    hexagonalPixelateFilter.center = CGPoint(x: 2016, y: 1512)\n    hexagonalPixelateFilter.scale = 50\n    return hexagonalPixelateFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "saliencyMapFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228399-saliencymapfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the saliency map filter to an image. The effect generates a saliency map representation of the input image.\n\nThe saliency map filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that produces an image that’s easier for computers to analyze:\n\nfunc saliencyMap(inputImage: CIImage) -> CIImage {\n    let saliencyMapFilter = CIFilter.saliencyMap()\n    saliencyMapFilter.inputImage = inputImage\n    return saliencyMapFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "gloomFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228334-gloomfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the gloom filter to an image. The effect reduces the highlights of the image resulting in the image looking dull.\n\nThe gloom filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\nintensity\n\nA float representing the desired strength of the effect as an NSNumber.\n\nThe following code creates a filter that results in a darker image with a slight blur:\n\nfunc gloom(inputImage: CIImage) -> CIImage {\n    let gloomFilter = CIFilter.gloom()\n    gloomFilter.inputImage = inputImage\n    gloomFilter.radius = 3\n    gloomFilter.intensity = 10\n    return gloomFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "heightFieldFromMaskFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228337-heightfieldfrommaskfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the height-field from the mask filter to an image. The effect targets the white in the input image and creates realistic shading.\n\nThe height field from mask filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\nThe following code creates a filter that results in the text having a shading effect:\n\nfunc heightFieldFromMask(inputImage: CIImage) -> CIImage {\n    let heightFieldFromMaskFilter = CIFilter.heightFieldFromMask()\n    heightFieldFromMaskFilter.inputImage = inputImage\n    heightFieldFromMaskFilter.radius = 3\n    return heightFieldFromMaskFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "edgeWorkFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228320-edgeworkfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the edge work filter to an image. The effect creates a stylized black-and-white rendition of the image that looks similar to a woodblock print.\n\nThe edge work filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\nThe following code creates a filter that results in a monochrome image with the edges of objects highlighted:\n\nfunc edgeWork(inputImage: CIImage) -> CIImage {\n    let edgeWorkFilter = CIFilter.edgeWork()\n    edgeWorkFilter.inputImage = inputImage\n    edgeWorkFilter.radius = 4\n    return edgeWorkFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "gaborGradientsFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3325508-gaborgradientsfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the Gabor gradients filter to an image. The effect targets the texture of objects within the frame, and is frequently used to find detail in photographs of fingerprints.\n\nThe gabor gradients filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in a darker image with shades of green and red outlining the texture of objects:\n\nfunc garborGradients(inputImage: CIImage) -> CIImage {\n    let garborFilter = CIFilter.gaborGradients()\n    garborFilter.inputImage = inputImage\n    return garborFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "edgesFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228321-edgesfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the edges filter to an image. The effect uses the intensity to compute and highlight edges of items within the image.\n\nThe edges filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nintensity\n\nA float representing the desired strength of the effect as an NSNumber.\n\nThe following code creates a filter that results in a darker image with the edges of objects highlighted with the colors of the input image:\n\nfunc edges(inputImage: CIImage) -> CIImage {\n    let edgesFilter = CIFilter.edges()\n    edgesFilter.inputImage = inputImage\n    edgesFilter.intensity = 15\n    return edgesFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "depthOfFieldFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228308-depthoffieldfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the depth of field filter to an image. The effect simulates changing the focus of the camera before taking a photograph.\n\nThe depth of field filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\npoint0\n\nA set of coordinates marking the first point to be focused on as a CGPoint.\n\npoint1\n\nA set of coordinates marking the second point to be focused on as a CGPoint.\n\nunsharpMaskRadius\n\nA float representing the radius of the unsharpened mask effect applied to the in-focus area of effect as an NSNumber.\n\nunsharpMaskIntensity\n\nA float representing the intensity of the unsharp mask effect as an NSNumber.\n\nThe following code creates a filter that results in the center cilantro being in focus while gradually blurring to the top and bottom of the image:\n\nfunc depthOfField(inputImage: CIImage) -> CIImage {\n    let depthOfFieldFilter = CIFilter.depthOfField()\n    depthOfFieldFilter.inputImage = inputImage\n    depthOfFieldFilter.radius = 5\n    depthOfFieldFilter.point0 = CGPoint(x: 2349, y: 846)\n    depthOfFieldFilter.point1 = CGPoint(x: 571, y: 3121)\n    depthOfFieldFilter.unsharpMaskRadius = 7\n    depthOfFieldFilter.unsharpMaskIntensity = 10\n    return depthOfFieldFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ crystallizeFilter\nCreates an image made with a series of colorful polygons.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "crystallizeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228306-crystallizefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the crystallize filter to an image. The effect creates polygon-shaped color blocks by aggregating pixel-color values.\n\nThe crystallize filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nradius\n\nA float representing the area of effect as an NSNumber.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nThe following code creates a filter that results in an image made of small polygons:\n\nfunc crystalize(inputImage: CIImage) -> CIImage {\n    let crystalizefilter = CIFilter.crystallize()\n    crystalizefilter.inputImage = inputImage\n    crystalizefilter.radius = 50\n    crystalizefilter.center = CGPoint(x: 2016, y: 1512)\n    return crystalizefilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ blendWithAlphaMaskFilter\nBlends two images by using an alpha mask image.\n+ blendWithBlueMaskFilter\nBlends two images by using a blue mask image.\n+ blendWithMaskFilter\nBlends two images by using a mask image.\n+ blendWithRedMaskFilter\nBlends two images by using a red mask image.\n+ bloomFilter\nAdjusts an image’s colors by applying a blur effect.\n+ cannyEdgeDetectorFilter\nApplies the Canny edge-detection algorithm to an image.\n+ comicEffectFilter\nCreates an image with a comic book effect.\n+ coreMLModelFilter\nFilters an image with a Core ML model.\n+ depthOfFieldFilter\nSimulates a depth of field effect.\n+ edgesFilter\nHilghlights edges of objects found within an image.\n+ edgeWorkFilter\nProduces a black-and-white image that looks similar to a woodblock print.\n+ gaborGradientsFilter\nHighlights textures in an image.\n+ gloomFilter\nAdjusts an image’s color by applying a gloom filter.\n+ heightFieldFromMaskFilter\nCreates a realistic shaded height-field image.\n+ hexagonalPixellateFilter\nCreates an image made of a series of colorful hexagons.\n+ highlightShadowAdjustFilter\nAdjusts the highlights of colors to reduce shadows.\n+ lineOverlayFilter\nCreates an image that resembles a sketch of the outlines of objects.\n+ mixFilter\nBlends two images together.\n+ personSegmentationFilter\nCreates a mask where red pixels indicate areas of the image that are likely to contain a person.\n+ pixellateFilter\nEnlarges the colors of the pixels to create a blurred effect.\n+ pointillizeFilter\nApplies a pointillize effect to an image.\n+ saliencyMapFilter\nCreates a saliency map from an image.\n+ shadedMaterialFilter\nCreates a shaded image from a height-field image.\n+ sobelGradientsFilter\nCalculates the Sobel gradients for an image.\n+ spotColorFilter\nReplaces colors of an image with specifed colors.\n+ spotLightFilter\nHighlights a definined area of the image."
  },
  {
    "title": "convolution9VerticalFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228303-convolution9verticalfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a 1 x 9 convolution filter to the RGBA components of an image. The effect uses a 1 x 9 area surrounding an input pixel, the pixel itself, and those within a distance of 4 pixels vertically. The effect repeats this for every pixel within the image. Unlike the convolution filters, which use square matrices, this filter can only produce effects along a vertical axis. You can combine this filter with the convolution9HorizontalFilter to apply separable 9 x 9 convolutions.\n\nThe convolution-9-vertical filter uses the following properties:\n\nbias\n\nA float representing the value that’s added to each output pixel as a NSNumber.\n\nweights\n\nA CIVector representing the convolution kernel.\n\ninputImage\n\nAn image with the type CIImage.\n\nNote\n\nWhen using a nonzero bias value, the output image has an infinite extent. You should crop the output image before attempting to render it.\n\nThe following code creates a filter that detects edges in the input image:\n\nfunc convolution9Vertical(inputImage: CIImage) -> CIImage? {\n    let convolutionFilter = CIFilter.convolution9Vertical()\n    convolutionFilter.inputImage = inputImage\n    convolutionFilter.inputImage = inputImage\n    let weights: [CGFloat] = [1, 1, 1, 1, 1, 1, 1, 1, 1].map { $0/9.0 }\n    let kernel = CIVector(values: weights, count: 9)\n    convolutionFilter.weights = kernel\n    convolutionFilter.bias = 0.0\n    return convolutionFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ convolution3X3Filter\nApplies a convolution 3 x 3 filter to the RGBA components of an image.\n+ convolution5X5Filter\nApplies a convolution 5 x 5 filter to the RGBA components image.\n+ convolution7X7Filter\nApplies a convolution 7 x 7 filter to the RGBA color components of an image.\n+ convolution9HorizontalFilter\nApplies a convolution-9 horizontal filter to the RGBA components of an image.\n+ convolutionRGB3X3Filter\nApplies a convolution 3 x 3 filter to the RGB components of an image.\n+ convolutionRGB5X5Filter\nApplies a convolution 5 x 5 filter to the RGB components of an image.\n+ convolutionRGB7X7Filter\nApplies a convolution 7 x 7 filter to the RGB components of an image.\n+ convolutionRGB9HorizontalFilter\nApplies a convolution 9 x 1 filter to the RGB components of an image.\n+ convolutionRGB9VerticalFilter\nApplies a convolution 1 x 9 filter to the RGB components of an image."
  },
  {
    "title": "convolutionRGB5X5Filter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3750386-convolutionrgb5x5filter",
    "html": "Return Value\n\nThis method applies a 5 x 5 convolution to the RGB components image. The effect uses a 5 x 5 area surrounding an input pixel, the pixel itself, and those within a distance of two pixels horizontally and vertically. The effect repeats this for every pixel within the image. The work area is then combined with the weight property vector to produce the processed image. This filter differs from the convolution5X5Filter filter, which processes all of the color components including the alpha component.\n\nThe convolution-RGB 5 x 5 filter uses the following properties:\n\ninputImage\n\nA CIImage containing the image to process.\n\nweights\n\nA CIVector representing the convolution kernel.\n\nbias\n\nA float representing the value that’s added to each output pixel.\n\nNote\n\nWhen using a nonzero bias value, the output image has an infinite extent. You should crop the output image before attempting to render it.\n\nThe following code creates a filter that applies an unsharp kernel to the input image:\n\nfunc convolutionRGB5X5(inputImage: CIImage) -> CIImage {\n    let convolutionFilter = CIFilter.convolutionRGB5X5()\n    convolutionFilter.inputImage = inputImage\n    let blur: [CGFloat] = [\n         1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1,\n    ].map { $0/25.0 }\n    let kernel = CIVector(values: blur, count: 25)\n    convolutionFilter.weights = kernel\n    convolutionFilter.bias = 0\n    return convolutionFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ convolution3X3Filter\nApplies a convolution 3 x 3 filter to the RGBA components of an image.\n+ convolution5X5Filter\nApplies a convolution 5 x 5 filter to the RGBA components image.\n+ convolution7X7Filter\nApplies a convolution 7 x 7 filter to the RGBA color components of an image.\n+ convolution9HorizontalFilter\nApplies a convolution-9 horizontal filter to the RGBA components of an image.\n+ convolution9VerticalFilter\nApplies a convolution-9 vertical filter to the RGBA components of an image.\n+ convolutionRGB3X3Filter\nApplies a convolution 3 x 3 filter to the RGB components of an image.\n+ convolutionRGB7X7Filter\nApplies a convolution 7 x 7 filter to the RGB components of an image.\n+ convolutionRGB9HorizontalFilter\nApplies a convolution 9 x 1 filter to the RGB components of an image.\n+ convolutionRGB9VerticalFilter\nApplies a convolution 1 x 9 filter to the RGB components of an image."
  },
  {
    "title": "exposureAdjustFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228324-exposureadjustfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the exposure-adjust filter to an image. The effect uses multiplication of color values to simulate the change of exposure within the photo.\n\nThe exposure-adjust filter uses the following properties:\n\nev\n\nA float representing the amount to adjust the exposure as an NSNumber.\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds brightness to the input image:\n\nfunc exposureAdjust(inputImage: CIImage) -> CIImage {\n    let exposureAdjustFilter = CIFilter.exposureAdjust()\n    exposureAdjustFilter.inputImage = inputImage\n    exposureAdjustFilter.ev = 2\n    return exposureAdjustFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "lightTunnelFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600147-lighttunnelfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the light tunnel filter to an image. This effect distorts the input image by warping the image to cylinder shape.\n\nThe light tunnel filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the light tunnel as a CGPoint.\n\nradius\n\nA float representing the amount of pixels the filter uses to create the light tunnel as an NSNumber.\n\nrotation\n\nA float representing the rotation angle of the light tunnel as an NSNumber.\n\nThe following code creates a filter that generates a swirling pattern from the input image:\n\nfunc lightTunnel(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.lightTunnel()\n    filter.inputImage = inputImage\n    filter.radius = 100\n    filter.rotation = .pi\n    filter.center = CGPoint(\n        x: inputImage.extent.width / 2,\n        y: inputImage.extent.size.height / 2\n    )\n    return filter.outputImage!.cropped(to: inputImage.extent)\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "disparityToDepthFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228313-disparitytodepthfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThe method generates the disparity-to-depth filter. The filter converts a depth data image to disparity data. You can combine with other filters to create more sophisticated images.\n\nThe disparity-to-depth filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that generates a disparity depth map image:\n\nfunc disparityToDepth(inputImage: CIImage) -> CIImage {\n    let disparityToDepthFilter = CIFilter.disparityToDepth()\n    disparityToDepthFilter.inputImage = inputImage\n    return disparityToDepthFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "depthToDisparityFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228309-depthtodisparityfilter",
    "html": "Return Value\n\nAn image containing the disparity data.\n\nDiscussion\n\nThis method applies the depth-to-disparity filter. The filter takes depth data as an input and produces disparity data in the output image. You can use the output of this filter to create a stereo image.\n\nThe depth-to-disparity filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that generates a depth map image:\n\nfunc depthToDisparity(inputImage: CIImage) -> CIImage {\n    let depthToDisparityFilter = CIFilter.depthToDisparity()\n    depthToDisparityFilter.inputImage = inputImage\n    return depthToDisparityFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "roundedRectangleGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3335007-roundedrectanglegeneratorfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a rounded rectangle image with the specified size, corner radius, and color properties.\n\nThe rounded rectangle generator filter uses the following properties:\n\ncolor\n\nA CIColor representing the color of the rounded rectangle.\n\nextent\n\nA CGRect representing the size of the rounded rectangle.\n\nradius\n\nA float representing the curve of the rectangle’s corners.\n\nThe following code creates a filter that generates a light blue square with rounded corners:\n\nfunc roundedRectangle () -> CIImage {\n    let roundedRectangleGenerator = CIFilter.roundedRectangleGenerator()\n    roundedRectangleGenerator.color = CIColor(red: 96/255, green: 173/255, blue: 193/255)\n    roundedRectangleGenerator.extent = CGRect(x: 0, y: 1, width: 700, height: 700)\n    roundedRectangleGenerator.radius = 100\n    return roundedRectangleGenerator.outputImage!\n}\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "randomGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228396-randomgeneratorfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates an image with infinite extent. The image pixels values are from one of four independent, uniformly distributed random colors.\n\nThe following code creates a filter that generates a random color image:\n\nfunc random() -> CIImage {\n   let randomGenerator = CIFilter.randomGenerator()\n   return randomGenerator.outputImage!\n}\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "QRCodeGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228262-qrcodegenerator",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a QR code as an image. QR codes are a high-density matrix barcode format defined in the ISO/IEC 18004:2006 standard.\n\nThe QR code generator filter uses the following properties:\n\nmessage\n\nA string representing the data to be encoded as a QR Code as NSData.\n\ncorrectionLevel\n\nA single letter string representing the error-correction format as an NSString. L is 7 precent correction, M is 15 precent correction, Q is 25 precent correction, and H is 30 precent correction.\n\nThe following code creates a filter that generates a QR code:\n\nfunc qrCode(inputMessage: String) -> CIImage {\n    let qrCodeGenerator = CIFilter.qrCodeGenerator()\n    qrCodeGenerator.message = inputMessage.data(using: .ascii)!\n    qrCodeGenerator.correctionLevel = \"H\"\n    return qrCodeGenerator.outputImage!\n}\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "PDF417BarcodeGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228261-pdf417barcodegenerator",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a PDF417 barcode as an image. PDF417 is a high-density stacked linear barcode format defined in the ISO 15438 standard. Use this filter to generate alphanumeric or numeric-only barcodes. Commonly used on identification cards or inventory management because of the large amount of data the barcode can hold.\n\nThe PDF417 barcode generator filter uses the following properties:\n\nmessage\n\nAn NSData object representing the data to be encoded as a barcode.\n\nminWidth\n\nA float representing the minimum width of the barcode’s data area, in pixels as an NSNumber.\n\nmaxWidth\n\nA float representing the maximum width of the barcode’s data area, in pixels, as an NSNumber.\n\nmaxHeight\n\nA float representing the maximum height of the barcode’s data area, in pixels, as an NSNumber.\n\nminHeight\n\nA float representing the minimum height of the barcode’s data area, in pixels, as an NSNumber.\n\ndataColums\n\nA float representing the number of columns in the data area as an NSNumber.\n\nrows\n\nA float representing the number of rows in the data area as an NSNumber.\n\npreferredAspectRatio\n\nA float representing the desired aspect ratio as an NSNumber.\n\ncompactionMode\n\nAn option that determines which method the generator uses to compress data as an NSNumber. See the note below for the possible values.\n\ncompactStyle\n\nA Boolean value of 0 or 1 that determines the omission of redundant elements to make the generated barcode more compact as an NSNumber.\n\ncorrectionLevel\n\nA float between 0 and 8 that determines the amount of redundancy to include in the barcode’s data to prevent errors when the barcode is read. If left unspecified, the generator chooses a correction level based on the size of the message data.\n\nalwaysSpecifyCompaction\n\nA Boolean value of 0 or 1 that determines the inclusion of information about the compaction mode in the barcode as an NSNumber. If a PDF417 barcode doesn’t contain compaction mode information, the reader assumes text-based compaction.\n\nThe compactionMode property takes one of the following numeric values:\n\nValue\n\n\t\n\nName\n\n\t\n\nDescription\n\n\n\n\n1\n\n\t\n\nAutomatic\n\n\t\n\nThe generator automatically chooses a compression method. This option is the default.\n\n\n\n\n2\n\n\t\n\nNumeric\n\n\t\n\nValid only when the message is an ASCII-encoded string of digits, achieving optimal compression for that type of data.\n\n\n\n\n3\n\n\t\n\nText\n\n\t\n\nValid only when the message is all ASCII-encoded alphanumeric and punctuation characters, achieving optimal compression for that type of data.\n\n\n\n\n4\n\n\t\n\nByte\n\n\t\n\nValid for any data, but least compact.\n\nSelect either 1 or the appropriate valid value for your data that gives the most compact output.\n\nThe following code creates a filter that generates a PDF417 barcode:\n\nfunc pdf417Barcode(inputMessage: String) -> CIImage {\n    let pdf417BarcodeGenerator = CIFilter.pdf417BarcodeGenerator()\n    pdf417BarcodeGenerator.message = inputMessage.data(using: .ascii)!\n    pdf417BarcodeGenerator.minWidth = 56\n    pdf417BarcodeGenerator.maxWidth = 58\n    pdf417BarcodeGenerator.maxHeight = 283\n    pdf417BarcodeGenerator.minHeight = 13\n    pdf417BarcodeGenerator.dataColumns = 9\n    pdf417BarcodeGenerator.rows = 6\n    pdf417BarcodeGenerator.preferredAspectRatio = 0.0\n    pdf417BarcodeGenerator.compactionMode = 1\n    pdf417BarcodeGenerator.compactStyle = 1\n    pdf417BarcodeGenerator.correctionLevel = 0.01\n    pdf417BarcodeGenerator.alwaysSpecifyCompaction = 0\n    return pdf417BarcodeGenerator.outputImage!\n}\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "meshGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228359-meshgeneratorfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a mesh generator image. The effect uses an array of line segments to create the resulting image.\n\nThe mesh generator filter uses the following properties:\n\ninputMesh\n\nAn array of line segments stored as an array of CIVector, each containing a start point and end point.\n\ncolor\n\nA CIColor representing the color used to make the mesh.\n\nwidth\n\nA float representing the width of the line segments as an NSNumber\n\nThe following code creates a filter that generates a green star made from mesh segments:\n\nfunc mesh(mesh: NSdata) -> CIImage {\n    let meshGenerator = CIFilter.meshGenerator()\n    meshGenerator.color = CIColor.green\n    meshGenerator.width = 3\n    meshGenerator.inputmesh = mesh\n    return meshGenerator.outputImage!\n}\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "lenticularHaloGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228345-lenticularhalogeneratorfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a lenticular halo image. You commonly combine this effect with an image to simulate a halo generated by the spread of light on a lens.\n\nThe lenticular halo generator filter uses the following properties:\n\ncenter\n\nA vector representing the center of the lens flare as a CIVector.\n\ncolor\n\nA CIColor controlling the proportion of red, green, and blue halos.\n\nhaloWidth\n\nA float representing the halo width as an NSNumber.\n\nhaloRadius\n\nA float representing the halo radius as an NSNumber.\n\nhaloOverlap\n\nA float representing the overlap of red, green, and blue halos as an NSNumber. A value of 1 results in a full overlap.\n\nstriationStrength\n\nA float representing the brightness of the rainbow-colored halo area as an NSNumber.\n\nstriationContrast\n\nA float representing the contrast of the rainbow-colored halo area as an NSNumber.\n\ntime\n\nA float representing the addition of brightness to the halo as an NSNumber.\n\nThe following code creates a filter that generates a lenticular halo image:\n\nfunc lenticularHalo() -> CIImage {\n    let lenticularHaloGenerator = CIFilter.lenticularHaloGenerator()\n    lenticularHaloGenerator.center = CGPoint(x: 200, y: 200)\n    lenticularHaloGenerator.color = CIColor(red: 1.2, green: 2.7, blue: 2.5)\n    lenticularHaloGenerator.haloWidth = 87\n    lenticularHaloGenerator.haloRadius = 70\n    lenticularHaloGenerator.haloOverlap = 0.77\n    lenticularHaloGenerator.striationStrength = 0.50\n    lenticularHaloGenerator.striationContrast = 1.00\n    lenticularHaloGenerator.time = 0.00\n    return lenticularHaloGenerator.outputImage!\n}\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "barcodeGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228269-barcodegeneratorfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a custom barcode as an image. The effect uses barcode descriptors to specify properties of the generated barcode.\n\nThe barcode generator uses the following property:\n\nbarcodeDescriptor\n\nAn instance of CIBarcodeDescriptor with the input parameters supplied.\n\nThe following code creates a filter that generates a QR code containing the text Johnny Appleseed.\n\nfunc barcode(inputMessage: Data) -> CIImage {\n   let barcodeGenerator = CIFilter.barcodeGenerator()\n    barcodeGenerator.barcodeDescriptor = CIQRCodeDescriptor(payload: inputMessage, symbolVersion: 1, maskPattern: 4, errorCorrectionLevel: .levelL)!\n    return barcodeGenerator.outputImage!\n}\n\n\nlet johnnyAppleseed: [UInt8] = [0x41, 0x04, 0xA6, 0xF6, 0x86, 0xE6, 0xE7, 0x92, 0x04, 0x17, 0x07, 0x06, 0xC6, 0x57, 0x36, 0x56, 0x56, 0x40, 0xEC]\nlet data = Data(johnnyAppleseed)\n\n\nlet bImage = barcode(inputMessage: data)\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image.\nRelated Documentation\nCIAztecCodeDescriptor\nA concrete subclass of that represents an Aztec code symbol.\nCIDataMatrixCodeDescriptor\nA concrete subclass of that represents a Data Matrix code symbol.\nCIPDF417CodeDescriptor\nA concrete subclass of that represents a PDF 417 symbol.\nCIQRCodeDescriptor\nA concrete subclass of that represents a square QR code symbol."
  },
  {
    "title": "aztecCodeGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228268-azteccodegeneratorfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates an Aztec code as an image. The Aztec barcode code is a low-density barcode defined in the ISO/IEC 24778:2008 standard. This code is commonly used for transport ticketing or boarding passes.\n\nThe Aztec code generator filter uses the following properties:\n\nmessage\n\nThe data to be encoded as an Aztec code. An NSData object whose display name is Message.\n\ncorrectionLevel\n\nA float representing the percentage of redundancy to add to the message data. A higher correction level allows the barcode to be correctly read even when partially damaged.\n\nlayers\n\nA float representing the number of concentric squares encoding the barcode data. When the value is zero, Core Image automatically determines the appropriate number of layers to encode the message data at the specified correction level.\n\ncompactStyle\n\nA float determining the use of compact or full-size Aztec barcode format. The compact format can store up to 44 bytes of message data in up to 4 layers. The full-size format can store up to 1914 bytes of message data in up to 32 layers. Leave unset, or set to 0 for automatic.\n\nThe following code creates a filter that generates an Aztec barcode:\n\nfunc aztecCode(inputMessage: String) -> CIImage {\n    let aztecCodeGenerator = CIFilter.aztecCodeGenerator()\n    aztecCodeGenerator.correctionLevel = 15\n    aztecCodeGenerator.compactStyle = 0\n    aztecCodeGenerator.message = inputMessage.data(using: .ascii)!\n    aztecCodeGenerator.layers = 10\n    return aztecCodeGenerator.outputImage!\n}\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "displacementDistortionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600142-displacementdistortionfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the displacement distortion filter to an image. This effect distorts an image by applying the grayscale color values of the texture image.\n\nThe displacement distortion filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ndisplacementImage\n\nAn image with the type CIImage.\n\nscale\n\nA float representing the scaling the filter uses to apply the texture to the input image as an NSNumber.\n\nThe following code creates a filter that applies the grayscale values of the displacement image to the input image:\n\nfunc displacementDistortion(inputImage: CIImage) -> CIImage {\n    // Create an interesting grayscale pattern.\n    let displacementImage = CIFilter.checkerboardGenerator()\n    displacementImage.color0 = CIColor.white\n    displacementImage.color1 = CIColor.black\n    displacementImage.width = 200\n    let gaussianBlur = CIFilter.gaussianBlur()\n    gaussianBlur.radius = 40\n    gaussianBlur.inputImage = displacementImage.outputImage\n    // Use it in the displacement filter.\n    let filter = CIFilter.displacementDistortion()\n    filter.displacementImage = gaussianBlur.outputImage\n    filter.inputImage = inputImage\n    filter.scale = 1000\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "bumpDistortionLinearFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600139-bumpdistortionlinearfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the bump distortion linear filter to an image. This effect creates a concave or convex bump to a linear portion of the image. The curvature of the bump is defined by the scale property. A value of 0.0 has no effect, while a positive value creates an outward curvature and a negative value creates an inward curvature.\n\nThe bump distortion linear filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nradius\n\nA float representing the amount of pixels the filter uses to create the distortion as an NSNumber.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nscale\n\nA float representing the curvature of the bump effect as an NSNumber.\n\nangle\n\nA float representing the angle of the distortion, in radians, as an NSNumber.\n\nThe following code creates a filter that results in a vertical bump distorting the image:\n\nfunc bumpDistortionLinear(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.bumpDistortionLinear()\n    filter.inputImage = inputImage\n    filter.center = CGPoint(x: inputImage.extent.midX, y: inputImage.extent.midY)\n    filter.radius = 500\n    filter.scale = 0.2\n    filter.angle = .pi/2\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "bumpDistortionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600138-bumpdistortionfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the bump distortion filter to an image. This effect creates a concave or convex bump defined by the scale. A value of 0.0 has no effect, while a positive value creates an outward curvature and a negative value creates an inward curvature.\n\nThe bump distortion filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nradius\n\nA float representing the amount of pixels the filter uses to create the distortion as an NSNumber.\n\ncenter\n\nA CGPoint representing the center of the effect.\n\nscale\n\nA float representing the curvature of the bump effect as an NSNumber.\n\nThe following code creates a filter that results in a concave bump distorting the image:\n\nfunc bump(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.bumpDistortion()\n    filter.inputImage = inputImage\n    filter.center = CGPoint(x: 500, y: 500)\n    filter.radius = 1200\n    filter.scale = 2\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "convolutionRGB9HorizontalFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3750388-convolutionrgb9horizontalfilter",
    "html": "Return Value\n\nThe convolved image.\n\nDiscussion\n\nThis method applies a 9 x 1 convolution to the RGB components of an image. The effect uses a 9 x 1 area surrounding an input pixel, the pixel itself, and those within a distance of 4 pixels horizontally. The effect repeats this for every pixel within the image. Unlike the convolution filters, which use square matrices, this filter can only produce effects along a vertical axis. You can combine this filter with the convolutionRGB9VerticalFilter to apply separable 9 x 9 convolutions. This filter differs from the convolution9HorizontalFilter filter, which processes all of the color components including the alpha component.\n\nThe convolution-RGB-9-vertical filter uses the following properties:\n\ninputImage\n\nA CIImage containing the image to process.\n\nweights\n\nA CIVector representing the convolution kernel.\n\nbias\n\nA float representing the value that’s added to each output pixel.\n\nNote\n\nWhen using a nonzero bias value, the output image has an infinite extent. You should crop the image before attempting to render it.\n\nThe following code creates a filter that blurs the image in the horizontal direction:\n\nfunc convolutionRGB9Horizontal(inputImage: CIImage) -> CIImage {\n    let convolutionFilter = CIFilter.convolutionRGB9Horizontal()\n    convolutionFilter.inputImage = inputImage\n    let weights: [CGFloat] = [1, 1, 1, 1, 1, 1, 1, 1, 1].map { $0/9.0 }\n    let kernel = CIVector(values: weights, count: 9)\n    convolutionFilter.weights = kernel\n    convolutionFilter.bias = 0.0\n    return convolutionFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ convolution3X3Filter\nApplies a convolution 3 x 3 filter to the RGBA components of an image.\n+ convolution5X5Filter\nApplies a convolution 5 x 5 filter to the RGBA components image.\n+ convolution7X7Filter\nApplies a convolution 7 x 7 filter to the RGBA color components of an image.\n+ convolution9HorizontalFilter\nApplies a convolution-9 horizontal filter to the RGBA components of an image.\n+ convolution9VerticalFilter\nApplies a convolution-9 vertical filter to the RGBA components of an image.\n+ convolutionRGB3X3Filter\nApplies a convolution 3 x 3 filter to the RGB components of an image.\n+ convolutionRGB5X5Filter\nApplies a convolution 5 x 5 filter to the RGB components of an image.\n+ convolutionRGB7X7Filter\nApplies a convolution 7 x 7 filter to the RGB components of an image.\n+ convolutionRGB9VerticalFilter\nApplies a convolution 1 x 9 filter to the RGB components of an image."
  },
  {
    "title": "sixfoldRotatedTileFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228406-sixfoldrotatedtilefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis method applies the six-fold reflected tile filter to an image. The effect produces a tiled image by rotating the image in increments 60 degrees.\n\nThe six-fold rotated tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nangle\n\nA float representing the direction of distortion , in radians as an NSNumber.\n\nwidth\n\nA float representing the set width of each tile as an NSNumber.\n\nThe following code creates a filter that results in flowers in the input image becoming rotated by 60 degrees and tiled to create the output:\n\nfunc sixFoldRotated(inputImage: CIImage) -> CIImage {\n    let sixFoldRotatedTile = CIFilter.sixfoldRotatedTile()\n    sixFoldRotatedTile.inputImage = inputImage\n    sixFoldRotatedTile.center = CGPoint(x: 150, y: 150)\n    sixFoldRotatedTile.angle = 0\n    sixFoldRotatedTile.width = 100\n    return sixFoldRotatedTile.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ triangleTileFilter\nTiles a triangular area of an image.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "triangleKaleidoscopeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228425-trianglekaleidoscopefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis method applies the triangle kaleidoscope filter to an image. The effect produces a complex tiled pattern from a triangular area input image.\n\nThe triangle kaleidoscope tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ndecay\n\nA float representing the intensity of the color fade from the the center of the triangle as an NSNumber.\n\npoint\n\nA set of coordinates marking the center of the triangular area of the input image as a CIVector.\n\nrotation\n\nA float representing the angle of rotation of the triangle as an NSNumber.\n\nsize\n\nA float representing the size in pixels of the triangle as an NSNumber.\n\nThe following code creates a filter that produces a triangle tile of the input image, creating an optical illusion:\n\nfunc triangleKaleidoscope(inputImage: CIImage) -> CIImage {\n    let triangleKaleidoscopeTile = CIFilter.triangleKaleidoscope()\n    triangleKaleidoscopeTile.inputImage = inputImage\n    triangleKaleidoscopeTile.point = CGPoint(x: 150, y: 150)\n    triangleKaleidoscopeTile.size = 700\n    triangleKaleidoscopeTile.rotation = -0.36\n    triangleKaleidoscopeTile.decay = 0.85\n    return triangleKaleidoscopeTile.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleTileFilter\nTiles a triangular area of an image.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "triangleTileFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228426-triangletilefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis method applies the triangle tile filter to an image. The effect creates a tiled pattern from a triangular area from the input image.\n\nThe triangle tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nangle\n\nA float representing the direction of distortion, in radians as an NSNumber.\n\nwidth\n\nA float representing the set width of each tile as an NSNumber.\n\nThe following code creates a filter that produces a triangle of the input image and tiles the result:\n\nfunc triangle(inputImage: CIImage) -> CIImage {\n    let triangleTile = CIFilter.triangleTile()\n    triangleTile.setValue(inputImage, forKey: kCIInputImageKey)\n    triangleTile.center = CGPoint(x: 2016, y: 1512)\n    triangleTile.angle = 1\n    triangleTile.width = 250\n    return triangleTile.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "affineTileFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228266-affinetilefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis method applies the affine tile filter to an image. This effect performs an CGAffineTransform and then tiles the transformed image.\n\nThe affine tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ntransform\n\nA CGAffineTransform to apply to the image.\n\nThe following code creates a filter that results in the image becoming tiled:\n\nfunc affineTile(inputImage: CIImage) -> CIImage {\n    let affineTileEffect = CIFilter.affineTile()\n    affineTileEffect.inputImage = inputImage\n    affineTileEffect.transform = CGAffineTransform(a: 1, b: 2, c: 2, d: 3, tx: 4, ty: 4)\n    return affineTileEffect.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ triangleTileFilter\nTiles a triangular area of an image.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "fourfoldTranslatedTileFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228329-fourfoldtranslatedtilefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis method applies the four-fold translated tile filter to an image. The effect produces a four-way tiled image by applying four translation operations. Translation operations map the position of each element in the photo to a new position in the output image.\n\nThe four-fold translated tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint. This controls the source of the tile contents.\n\nangle\n\nA float representing the direction of the tiled patten, in radians as an NSNumber.\n\nwidth\n\nA float representing the set width of each tile as an NSNumber.\n\nacuteAngle\n\nA float representing the primary angle for the repeating translated tile as an NSNumber.\n\nThe following code creates a filter that performs a four-fold translated tile operation on the image:\n\nfunc fourFoldTranslated(inputImage: CIImage) -> CIImage {\n    let fourFoldTranslatedTile = CIFilter.fourfoldTranslatedTile()\n    fourFoldTranslatedTile.inputImage = inputImage\n    fourFoldTranslatedTile.center = CGPoint(x: inputImage.extent.midX, y: inputImage.extent.midY)\n    fourFoldTranslatedTile.angle = 1\n    fourFoldTranslatedTile.width = 400\n    fourFoldTranslatedTile.acuteAngle = 1\n    return fourFoldTranslatedTile.outputImage!.cropped(to: inputImage.extent)\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ triangleTileFilter\nTiles a triangular area of an image.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "eightfoldReflectedTileFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228322-eightfoldreflectedtilefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis method applies the eight-fold reflected tile filter to an image. The effect creates an eight-way symmetry from the input image and tiles it to create the output image.\n\nThe eight-fold reflected tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nangle\n\nA float representing the direction of distortion, in radians as an NSNumber.\n\nwidth\n\nA float representing the set width of each tile as an NSNumber.\n\nThe following code creates a filter that results in small symmetrical repeated tiles:\n\nfunc eightFoldReflected(inputImage: CIImage) -> CIImage {\n    let eightFoldReflectedTile = CIFilter.eightfoldReflectedTile()\n    eightFoldReflectedTile.inputImage = inputImage\n    eightFoldReflectedTile.center = CGPoint(x: inputImage.extent.midX, y: inputImage.extent.midY)\n    eightFoldReflectedTile.width = 400\n    eightFoldReflectedTile.angle = 1\n    return eightFoldReflectedTile.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ triangleTileFilter\nTiles a triangular area of an image.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "fourfoldRotatedTileFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228328-fourfoldrotatedtilefilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis method applies the four-fold rotated tile filter to an image. The effect produces a tiled image by rotating a tile from the iput image in increments of 90 degrees.\n\nThe four-fold rotated tile filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nangle\n\nA float representing the direction of distortion, in radians as an NSNumber.\n\nwidth\n\nA float representing the set width of each tile as an NSNumber.\n\nThe following code creates a filter that results in the flowers in the input image becoming rotated by 90 degrees and tiled:\n\nfunc fourFoldRotated(inputImage: CIImage) -> CIImage {\n    let fourFoldRotatedTile = CIFilter.fourfoldRotatedTile()\n    fourFoldRotatedTile.inputImage = inputImage\n    fourFoldRotatedTile.center = CGPoint(x: 150, y: 150)\n    fourFoldRotatedTile.angle = 10\n    fourFoldRotatedTile.width = 10\n    return fourFoldRotatedTile.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineClampFilter\nPerforms a transform on the image and extends the image edges to infinity.\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ triangleTileFilter\nTiles a triangular area of an image.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "smoothLinearGradientFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228407-smoothlineargradientfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a smooth linear-gradient image. The effect creates a gradient by gradually blending colors between point0 and point1 using the sigmoid curve function.\n\nThe smooth linear-gradient filter uses the following properties:\n\npoint0\n\nA CGPoint representing the starting position of the gradient.\n\npoint1\n\nA CGPoint representing the ending position of the gradient.\n\ncolor0\n\nA CIColor representing the first color used in the gradient.\n\ncolor1\n\nA CIColor representing the second color used in the gradient.\n\nThe following code creates a filter that generates a gradient image:\n\nfunc smoothLinear() -> CIImage {\n    let smoothLinearGradient = CIFilter.smoothLinearGradient()\n    smoothLinearGradient.point0 = CGPoint(x: 0, y: 0)\n    smoothLinearGradient.point1 = CGPoint(x: 200, y: 200)\n    smoothLinearGradient.color0 = CIColor(red: 0/255, green: 112/255, blue: 201/255)\n    smoothLinearGradient.color1 = CIColor(red: 216/255, green: 232/255, blue: 146/255)\n    return smoothLinearGradient.outputImage!\n}\n\n\nSee Also\nFilters\n+ gaussianGradientFilter\nGenerates a gradient that varies from one color to another using a Gaussian distribution.\n+ hueSaturationValueGradientFilter\nGenerates a gradient representing a specified color space.\n+ linearGradientFilter\nGenerates a color gradient that varies along a linear axis between two defined endpoints.\n+ radialGradientFilter\nGenerates a gradient that varies radially between two circles having the same center."
  },
  {
    "title": "affineClampFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228265-affineclampfilter",
    "html": "Return Value\n\nThe tiled image.\n\nDiscussion\n\nThis method applies the affine clamp filter to an image. This effect performs similarly to the affine transform filter except that it produces an infinite image. You can use this filter when you need to blur an image but you want to avoid a soft, black fringe along the edges.\n\nThe affine clamp filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ntransform\n\nA CGAffineTransform to constrain to the image.\n\nThe following code creates a filter that produces a cropped image with colored edges to fill the rest of the image:\n\nfunc affineClamp(inputImage: CIImage) -> CIImage {\n    let affineClamp = CIFilter.affineClamp()\n    affineClamp.inputImage = inputImage\n    affineClamp.transform = CGAffineTransform(a: 1.0, b: 0.0, c: 0.0, d: 1.0, tx: 0.0, ty: 0.0)\n    return affineClamp.outputImage!\n}\n\n\nSee Also\nFilters\n+ affineTileFilter\nPerforms a transform on the image and tiles the result.\n+ eightfoldReflectedTileFilter\nCreates an eight-way reflected pattern.\n+ fourfoldReflectedTileFilter\nCreates a four-way reflected pattern.\n+ fourfoldRotatedTileFilter\nCreates a tiled image by rotating a tile in increments of 90 degrees.\n+ fourfoldTranslatedTileFilter\nCreates a tiled image by applying four translation operations.\n+ glideReflectedTileFilter\nTiles an image by rotating and reflecting a tile from the image.\n+ kaleidoscopeFilter\nCreates a 12-way kaleidoscopic image from an image.\n+ opTileFilter\nProduces an effect that mimics a style of visual art that uses optical illusions.\n+ parallelogramTileFilter\nWarps the image to create a parallelogram and tiles the result.\n+ perspectiveTileFilter\nTiles an image by adjusting the perspective of the image.\n+ sixfoldReflectedTileFilter\nProduces a tiled image from a source image by applying a six-way reflected symmetry.\n+ sixfoldRotatedTileFilter\nCreates a tiled image by rotating in increments of 60 degrees.\n+ triangleKaleidoscopeFilter\nCreate a triangular kaleidoscope effect and then tiles the result.\n+ triangleTileFilter\nTiles a triangular area of an image.\n+ twelvefoldReflectedTileFilter\nCreates a tiled image by rotating in increments of 30 degrees."
  },
  {
    "title": "KMeansFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3547110-kmeansfilter",
    "html": "Return Value\n\nA one-dimensional CIImage containing the colors.\n\nDiscussion\n\nThis filter uses the k-means clustering algorithm to find the most common colors in an input image. The result is a CIImage with count x 1 dimensions. Each RGBA pixel in the result image represents the center of a k-means cluster. The RGB components contain the color and the alpha component represents the weight of the color. You typically use the KMeansFilter filter in conjunction with the palettizeFilter filter to produce an image with a reduced number of colors.\n\ninputImage\n\nA CIImage to process.\n\nextent\n\nA CGRect specifying the area of the image to analyze.\n\nmeans\n\nAn optional CIImage containing a set of colors to use as seeds for the k-means clustering.\n\ncount\n\nThe number of k-means color clusters that should be created. Maximum is 128, and default is 8.\n\npasses\n\nThe number of k-means passes that should run. Maximum is 20, and default is 5.\n\nperceptual\n\nWhether the k-means color palette should use a perceptual color space.\n\nTip\n\nThe colors in the result of the KMeansFilter filter have an alpha component that indicates the weight of the color. You should set this value one using imageBySettingAlphaOneInExtent: before using the palette.\n\nThe following code example uses the KMeansFilter filter followed by the palettizeFilter filter to reduce the colors in the image to four:\n\nfunc kMeans(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.kMeans()\n    filter.inputImage = inputImage\n    filter.extent = inputImage.extent\n    filter.count = 4\n    filter.passes = 5\n    return filter.outputImage!\n}\n\n\nfunc palettize(inputImage: CIImage, paletteImage: CIImage) -> CIImage {\n    let palettize = CIFilter.palettize()\n    palettize.inputImage = inputImage\n    palettize.paletteImage = paletteImage\n    return palettize.outputImage!\n}\n\n\nlet palette = kMeans(inputImage: image)\nlet palettized = palettize(inputImage: image, palette.settingAlphaOne(in: palette.extent))\n\n\nSee Also\nFilters\n+ areaAverageFilter\nReturns a 1 x 1 pixel image that contains the average color for the region of interest.\n+ areaHistogramFilter\nReturns a histogram of a specified area of the image.\n+ areaLogarithmicHistogramFilter\nReturns a logarithmic histogram of a specified area of the image.\n+ areaMaximumFilter\nCalculates the maximum color components of a specified area of the image.\n+ areaMaximumAlphaFilter\nFinds the pixel with the highest alpha value.\n+ areaMinimumFilter\nCalculates the minimum color component values for a specified area of the image.\n+ areaMinimumAlphaFilter\nCalculates the pixel within a specified area that has the smallest alpha value.\n+ areaMinMaxFilter\nCalculates minimum and maximum color components for a specified area of the image.\n+ areaMinMaxRedFilter\nCalculates the minimum and maximum red component value.\n+ columnAverageFilter\nCalculates the average color for a specified column of an image.\n+ histogramDisplayFilter\nGenerates a histogram map from the image.\n+ rowAverageFilter\nCalculates the average color for the specified row of pixels in an image."
  },
  {
    "title": "areaHistogramFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3547112-areahistogramfilter",
    "html": "Return Value\n\nA 1 pixel high image containing the calculated histogram.\n\nDiscussion\n\nThis filter calculates histograms of the red, green, blue, and alpha colors in the region defined by extent. The count property controls the number of bins (or width) of the histogram. The filter scales the histogram so that the total of all the counts in the bins equals scale.\n\nThe area histogram filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nextent\n\nA CGRect that specifies the subregion of the image that you want to process.\n\nscale\n\nThe scale value to use for the histogram values. If the scale is 1, then the total of all the counts in the histogram equals 1.\n\ncount\n\nThe number of bins for the histogram. This value determines the width of the output image. Minimum value 1, maximum value 2048.\n\nThe following code creates a filter that results in an image that has a height of 1 pixel and a width of 256 pixels. The pixel color components contain the histogram values.\n\nfunc areaHistogram(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.areaHistogram()\n    filter.inputImage = inputImage\n    filter.count = 256\n    filter.scale = 50\n    filter.extent = CGRect(\n        x: inputImage.extent.width/2-250,\n        y: inputImage.extent.height/2-250,\n        width: 500,\n        height: 500)\n    return filter.outputImage!\n}\n\n\nTo display the histogram, you can use the histogramDisplayFilter filter:\n\nfunc histogramDisplay(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.histogramDisplay()\n    filter.inputImage = areaHistogram(inputImage: inputImage)\n    filter.highLimit = 1\n    filter.height = 100\n    filter.lowLimit = 0\n    return filter.outputImage!\n}\n\n\n\n\nSee Also\nFilters\n+ areaAverageFilter\nReturns a 1 x 1 pixel image that contains the average color for the region of interest.\n+ areaLogarithmicHistogramFilter\nReturns a logarithmic histogram of a specified area of the image.\n+ areaMaximumFilter\nCalculates the maximum color components of a specified area of the image.\n+ areaMaximumAlphaFilter\nFinds the pixel with the highest alpha value.\n+ areaMinimumFilter\nCalculates the minimum color component values for a specified area of the image.\n+ areaMinimumAlphaFilter\nCalculates the pixel within a specified area that has the smallest alpha value.\n+ areaMinMaxFilter\nCalculates minimum and maximum color components for a specified area of the image.\n+ areaMinMaxRedFilter\nCalculates the minimum and maximum red component value.\n+ columnAverageFilter\nCalculates the average color for a specified column of an image.\n+ histogramDisplayFilter\nGenerates a histogram map from the image.\n+ KMeansFilter\nApplies the k-means algorithm to find the most common colors in an image.\n+ rowAverageFilter\nCalculates the average color for the specified row of pixels in an image."
  },
  {
    "title": "areaAverageFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3547111-areaaveragefilter",
    "html": "Return Value\n\nA 1 x 1 pixel image containing the average color for the region of interest.\n\nDiscussion\n\nThis filter calculates the average color of the area defined by extent and creates a 1 x 1 pixel image with the result. The filter processes each color component (red, green, blue, alpha) of the input image independently.\n\nThe area average filter uses the following properties:\n\ninputImage\n\nThe CIImage containing the image you want to process.\n\nextent\n\nA CGRect that specifies the region of the image that you want to process.\n\nThe following code creates a filter that calculates the average color of a 500 x 500 set of pixels from the center of the image:\n\nfunc averageArea(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.areaAverage()\n    filter.inputImage = inputImage\n    filter.extent = CGRect(\n        x: inputImage.extent.width/2-250,\n        y: inputImage.extent.height/2-250,\n        width: 500,\n        height: 500)\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ areaHistogramFilter\nReturns a histogram of a specified area of the image.\n+ areaLogarithmicHistogramFilter\nReturns a logarithmic histogram of a specified area of the image.\n+ areaMaximumFilter\nCalculates the maximum color components of a specified area of the image.\n+ areaMaximumAlphaFilter\nFinds the pixel with the highest alpha value.\n+ areaMinimumFilter\nCalculates the minimum color component values for a specified area of the image.\n+ areaMinimumAlphaFilter\nCalculates the pixel within a specified area that has the smallest alpha value.\n+ areaMinMaxFilter\nCalculates minimum and maximum color components for a specified area of the image.\n+ areaMinMaxRedFilter\nCalculates the minimum and maximum red component value.\n+ columnAverageFilter\nCalculates the average color for a specified column of an image.\n+ histogramDisplayFilter\nGenerates a histogram map from the image.\n+ KMeansFilter\nApplies the k-means algorithm to find the most common colors in an image.\n+ rowAverageFilter\nCalculates the average color for the specified row of pixels in an image."
  },
  {
    "title": "areaLogarithmicHistogramFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3987921-arealogarithmichistogramfilter",
    "html": "Return Value\n\nA 1-pixel-high image containing the calculated histogram.\n\nDiscussion\n\nThis filter calculates histograms of the red,green,blue, and alpha colors for the specified area of an image. A base two-logarithm function is applied to the values before binning. The count property controls the number of bins (or width) of the histogram. The histogram is scaled so that all the values sum to scale.\n\ninputImage\n\nAn image with the type CIImage.\n\nextent\n\nA CGRect that specifies the subregion of the image you want to process.\n\nscale\n\nThe scale value for the histogram values. If the scale is 1, then the bins in the resulting image sum to 1.\n\ncount\n\nThe number of bins for the histogram. This value determines the width of the output image. Minimum value 1, and maximum value 2048.\n\nminimumStop\n\nThe minimum of the range of color channel values in the logarithmic histogram image. Defaults to -10.\n\nmaximumStop\n\nThe maximum of the range of color channel values in the logarithmic histogram image. Defaults to 4.\n\nThe following code creates a filter that results in a 1-pixel-tall image with a width of 256. The pixel color components contain the logarithmic histogram values:\n\nfunc areaLogarithmicHistogram(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.areaLogarithmicHistogram()\n    filter.inputImage = inputImage\n    filter.count = 256\n    filter.scale = 15\n    filter.extent = CGRect(\n        x: inputImage.extent.width/2-250,\n        y: inputImage.extent.height/2-250,\n        width: 500,\n        height: 500)\n    return filter.outputImage!\n}\n\n\nUse the histogramDisplayFilter filter to display the histogram:\n\nfunc logarithmicHistogramDisplay(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.histogramDisplay()\n    filter.inputImage = areaLogarithmicHistogram(inputImage: inputImage)\n    filter.highLimit = 1\n    filter.height = 100\n    filter.lowLimit = 0\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ areaAverageFilter\nReturns a 1 x 1 pixel image that contains the average color for the region of interest.\n+ areaHistogramFilter\nReturns a histogram of a specified area of the image.\n+ areaMaximumFilter\nCalculates the maximum color components of a specified area of the image.\n+ areaMaximumAlphaFilter\nFinds the pixel with the highest alpha value.\n+ areaMinimumFilter\nCalculates the minimum color component values for a specified area of the image.\n+ areaMinimumAlphaFilter\nCalculates the pixel within a specified area that has the smallest alpha value.\n+ areaMinMaxFilter\nCalculates minimum and maximum color components for a specified area of the image.\n+ areaMinMaxRedFilter\nCalculates the minimum and maximum red component value.\n+ columnAverageFilter\nCalculates the average color for a specified column of an image.\n+ histogramDisplayFilter\nGenerates a histogram map from the image.\n+ KMeansFilter\nApplies the k-means algorithm to find the most common colors in an image.\n+ rowAverageFilter\nCalculates the average color for the specified row of pixels in an image."
  },
  {
    "title": "textImageGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228422-textimagegeneratorfilter",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates a text image. The effect takes the input string property and the scale factor to scale up the text. You commonly combine this filter with other filters to create a watermark on images.\n\nThe text image generator filter uses the following properties:\n\ntext\n\nThe string to render. The string can contain non-ASCII characters.\n\nfontName\n\nA string representing the name of the font to be used to generate the image.\n\nfontSize\n\nA float representing the size of the font as an NSNumber.\n\nscaleFactor\n\nA float representing the scale of the font for the generated text as an NSNumber.\n\nThe following code creates a filter that generates a string of text as a grayscale image:\n\nfunc textImage(inputText: String) -> CIImage {\n    let textImageGenerator = CIFilter.textImageGenerator()\n    textImageGenerator.text = inputText\n    textImageGenerator.fontName = \"Helvetica\"\n    textImageGenerator.fontSize = 25\n    textImageGenerator.scaleFactor = 4\n    return textImageGenerator.outputImage!\n}\n\n\nSee Also\nFilters\n+ attributedTextImageGeneratorFilter\nGenerates an attributed-text image.\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun."
  },
  {
    "title": "areaMaximumFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3547114-areamaximumfilter",
    "html": "Return Value\n\nA 1 x 1 size image containing the maximum color components.\n\nDiscussion\n\nThis filter returns the maximum color components in the region defined by extent.\n\nThe area maximum filter uses the following properties:.\n\ninputImage\n\nAn image with the type CIImage.\n\nextent\n\nA CGRect that specifies the subregion of the image that you want to process.\n\nThe following code creates a filter that calculates the maximum color components of a 500 x 500 set of pixels from the center of the image:\n\nfunc areaMaximum(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.areaMaximum()\n    filter.inputImage = inputImage\n    filter.extent = CGRect(\n        x: inputImage.extent.width/2,\n        y: inputImage.extent.height/2,\n        width: 100,\n        height: 100)\n     return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ areaAverageFilter\nReturns a 1 x 1 pixel image that contains the average color for the region of interest.\n+ areaHistogramFilter\nReturns a histogram of a specified area of the image.\n+ areaLogarithmicHistogramFilter\nReturns a logarithmic histogram of a specified area of the image.\n+ areaMaximumAlphaFilter\nFinds the pixel with the highest alpha value.\n+ areaMinimumFilter\nCalculates the minimum color component values for a specified area of the image.\n+ areaMinimumAlphaFilter\nCalculates the pixel within a specified area that has the smallest alpha value.\n+ areaMinMaxFilter\nCalculates minimum and maximum color components for a specified area of the image.\n+ areaMinMaxRedFilter\nCalculates the minimum and maximum red component value.\n+ columnAverageFilter\nCalculates the average color for a specified column of an image.\n+ histogramDisplayFilter\nGenerates a histogram map from the image.\n+ KMeansFilter\nApplies the k-means algorithm to find the most common colors in an image.\n+ rowAverageFilter\nCalculates the average color for the specified row of pixels in an image."
  },
  {
    "title": "areaMaximumAlphaFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3547113-areamaximumalphafilter",
    "html": "Return Value\n\nA 1 x 1 size image containing the pixel with the maximum alpha value.\n\nDiscussion\n\nThis filter returns the pixel with highest alpha value in the region defined by extent.\n\nThe area maximum alpha filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nextent\n\nA CGRect that specifies the subregion of the image that you want to process.\n\nThe following code creates a filter that results in a single pixel image containing the pixel with the highest alpha value:\n\nfunc areaMaximumAlpha(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.areaMaximumAlpha()\n    filter.inputImage = inputImage\n    filter.extent = CGRect(\n        x: inputImage.extent.width/2-250,\n        y: inputImage.extent.height/2-250,\n        width: 500,\n        height: 500)\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ areaAverageFilter\nReturns a 1 x 1 pixel image that contains the average color for the region of interest.\n+ areaHistogramFilter\nReturns a histogram of a specified area of the image.\n+ areaLogarithmicHistogramFilter\nReturns a logarithmic histogram of a specified area of the image.\n+ areaMaximumFilter\nCalculates the maximum color components of a specified area of the image.\n+ areaMinimumFilter\nCalculates the minimum color component values for a specified area of the image.\n+ areaMinimumAlphaFilter\nCalculates the pixel within a specified area that has the smallest alpha value.\n+ areaMinMaxFilter\nCalculates minimum and maximum color components for a specified area of the image.\n+ areaMinMaxRedFilter\nCalculates the minimum and maximum red component value.\n+ columnAverageFilter\nCalculates the average color for a specified column of an image.\n+ histogramDisplayFilter\nGenerates a histogram map from the image.\n+ KMeansFilter\nApplies the k-means algorithm to find the most common colors in an image.\n+ rowAverageFilter\nCalculates the average color for the specified row of pixels in an image."
  },
  {
    "title": "textureFormat",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/1437934-textureformat",
    "html": "Discussion\n\nThe value for this key must be an NSNumber object containing a Core Image pixel format constant. (See Pixel Formats.) You may only use this key when initializing an image using the init(texture:size:flipped:options:) method.\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "nearestSampling",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/2867426-nearestsampling",
    "html": "Discussion\n\nThe value for this key is an NSNumber containing a Boolean value specifying whether the image should be sampled using nearest neighbor behavior. An unspecified value defaults to linear sampling.\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "portraitEffectsMatte",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2976440-portraiteffectsmatte",
    "html": "See Also\nAccessing Original Image Content\nvar cgImage: CGImage?\nThe CoreGraphics image object this image was created from, if applicable.\nvar pixelBuffer: CVPixelBuffer?\nThe CoreVideo pixel buffer this image was created from, if applicable.\nvar depthData: AVDepthData?\nAVDepthData representation of the depth image."
  },
  {
    "title": "colorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/1438131-colorspace",
    "html": "Discussion\n\nFor more information on this data type see CGColorSpace. Typically you use this option when you need to load an elevation, mask, normal vector, or RAW sensor data directly from a file without color correcting it. This constant specifies to override Core Image, which, by default, assumes that data is in GenericRGB.\n\nThe value you supply for this dictionary key must be a CGColorSpace data type. If a value for this key isn’t supplied, the image’s colorSpace dictionary are populated automatically by calling CGImageSourceCopyPropertiesAtIndex(_:_:_:). To request that Core Image perform no color management, specify the NSNull object as the value for this key. Use this option for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables).\n\nSee Also\nImage Dictionary Keys\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "linearToSRGBToneCurveFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228352-lineartosrgbtonecurvefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the linear-to-sRGB tone curve filter to an image. The effect converts an image in linear color space to sRGB.\n\nThe linear-to-sRGB tone curve filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds brightness to the input image:\n\nfunc linearTosRGB(inputImage: CIImage) -> CIImage {\n    let linearTosRGB = CIFilter.linearToSRGBToneCurve()\n    linearTosRGB.inputImage = inputImage\n    return linearTosRGB.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "properties",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437733-properties",
    "html": "Discussion\n\nIf the CIImage object is the output of a filter (or filter chain), this property’s value is the metadata from the filter’s original input image.\n\nSee Also\nGetting Image Information\nvar definition: CIFilterShape\nReturns a filter shape object that represents the domain of definition of the image.\nvar extent: CGRect\nA rectangle that specifies the extent of the image.\nvar url: URL?\nThe URL from which the image was loaded.\nvar colorSpace: CGColorSpace?\nThe color space of the image.\nfunc orientationTransform(forExifOrientation: Int32) -> CGAffineTransform\nReturns the transformation needed to reorient the image to the specified orientation."
  },
  {
    "title": "insertingIntermediate()",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2966521-insertingintermediate",
    "html": "Return Value\n\nThe image obtained from inserting the intermediate.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "convolutionRGB7X7Filter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3750387-convolutionrgb7x7filter",
    "html": "Return Value\n\nThe convolved image.\n\nDiscussion\n\nThis method applies a 7 x 7 convolution to the RGB components image. The effect uses a 7 x 7 area surrounding an input pixel, the pixel itself, and those within a distance of 3 pixels horizontally and vertically. The effect repeats this for every pixel within the image. The work area is then combined with the weight property vector to produce the processed image. This filter differs from the convolution7X7Filter filter, which processes all of the color components including the alpha component.\n\nThe convolution-RGB 7 x 7 filter uses the following properties:\n\ninputImage\n\nA CIImage containing the image to process.\n\nweights\n\nA CIVector representing the convolution kernel.\n\nbias\n\nA float representing the value that’s added to each output pixel.\n\nNote\n\nWhen using a nonzero bias value, the output image has an infinite extent. You should crop the output image before attempting to render it.\n\nThe following code creates a filter that highlights edges in the input image:\n\nfunc convolutionRGB7X7(inputImage: CIImage) -> CIImage {\n    let convolutionFilter = CIFilter.convolutionRGB7X7()\n    convolutionFilter.inputImage = inputImage\n    let weights: [CGFloat] = [\n        0, 0, -1, -1, -1, 0, 0,\n        0, -1, -3, -3, -3, -1, 0,\n        -1, -3, 0, 7, 0, -3, -1,\n        -1, -3, 7, 25, 7, -3, -1,\n        -1, -3, 0, 7, 0, -3, -1,\n        0, -1, -3, -3, -3, -1, 0,\n        0, 0, -1, -1, -1, 0, 0\n    ]\n    let kernel = CIVector(values: weights, count: 49)\n    convolutionFilter.weights = kernel\n    convolutionFilter.bias = 0.0\n    return convolutionFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ convolution3X3Filter\nApplies a convolution 3 x 3 filter to the RGBA components of an image.\n+ convolution5X5Filter\nApplies a convolution 5 x 5 filter to the RGBA components image.\n+ convolution7X7Filter\nApplies a convolution 7 x 7 filter to the RGBA color components of an image.\n+ convolution9HorizontalFilter\nApplies a convolution-9 horizontal filter to the RGBA components of an image.\n+ convolution9VerticalFilter\nApplies a convolution-9 vertical filter to the RGBA components of an image.\n+ convolutionRGB3X3Filter\nApplies a convolution 3 x 3 filter to the RGB components of an image.\n+ convolutionRGB5X5Filter\nApplies a convolution 5 x 5 filter to the RGB components of an image.\n+ convolutionRGB9HorizontalFilter\nApplies a convolution 9 x 1 filter to the RGB components of an image.\n+ convolutionRGB9VerticalFilter\nApplies a convolution 1 x 9 filter to the RGB components of an image."
  },
  {
    "title": "convolutionRGB3X3Filter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3750385-convolutionrgb3x3filter",
    "html": "Return Value\n\nThe convolved image.\n\nDiscussion\n\nThis method applies a 3 x 3 convolution to the RGB components of an image. The effect uses a 3 x 3 area surrounding an input pixel, the pixel itself, and those within a distance of 1 pixel horizontally and vertically. This filter differs from the convolution3X3Filter filter, which processes all of the color components including the alpha component.\n\nThe convolution-RGB 3 x 3 filter uses the following properties:\n\nbias\n\nA float representing the value that’s added to each output pixel.\n\nweights\n\nA CIVector representing the convolution kernel.\n\ninputImage\n\nA CIImage containing the image to process.\n\nNote\n\nWhen using a nonzero bias value, the output image has an infinite extent. You should crop the output image before attempting to render it.\n\nThe following code creates a filter that sharpens the input image:\n\nfunc convolutionRGB3X3(inputImage: CIImage) -> CIImage {\n    let convolutionFilter = CIFilter.convolutionRGB3X3()\n    convolutionFilter.inputImage = inputImage\n    let kernel = CIVector(values: [\n        0, -2, 0,\n        -2, 9, -2,\n        0, -2, 0\n    ], count: 9)\n    convolutionFilter.weights = kernel\n    convolutionFilter.bias = 0.0\n    return convolutionFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ convolution3X3Filter\nApplies a convolution 3 x 3 filter to the RGBA components of an image.\n+ convolution5X5Filter\nApplies a convolution 5 x 5 filter to the RGBA components image.\n+ convolution7X7Filter\nApplies a convolution 7 x 7 filter to the RGBA color components of an image.\n+ convolution9HorizontalFilter\nApplies a convolution-9 horizontal filter to the RGBA components of an image.\n+ convolution9VerticalFilter\nApplies a convolution-9 vertical filter to the RGBA components of an image.\n+ convolutionRGB5X5Filter\nApplies a convolution 5 x 5 filter to the RGB components of an image.\n+ convolutionRGB7X7Filter\nApplies a convolution 7 x 7 filter to the RGB components of an image.\n+ convolutionRGB9HorizontalFilter\nApplies a convolution 9 x 1 filter to the RGB components of an image.\n+ convolutionRGB9VerticalFilter\nApplies a convolution 1 x 9 filter to the RGB components of an image."
  },
  {
    "title": "hueAdjustFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228340-hueadjustfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the hue-adjust filter to an image. The effect changes the hue or color of the pixels by using the angle to modify the image’s color data.\n\nThe hue-adjust filter uses the following properties:\n\nangle\n\nA float representing the angle in radians to adjust the current hue of the image as an NSNumber.\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that shifts the hue of the image by 5 radians.\n\nfunc hueAdjust(inputImage: CIImage) -> CIImage {\n    let hueAdjustFilter = CIFilter.hueAdjust()\n    hueAdjustFilter.inputImage = inputImage\n    hueAdjustFilter.angle = 5\n    return hueAdjustFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "gammaAdjustFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228330-gammaadjustfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the gamma-adjust filter to an image. The effect adjusts the image’s mid-tone brightness. This filter is typically used to compensate for distortion caused by displays.\n\nThe gamma-adjust filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds darker colors to the input image:\n\nfunc gammaAdjust(inputImage: CIImage) -> CIImage {\n    let gammaAdjustFilter = CIFilter.gammaAdjust()\n    gammaAdjustFilter.inputImage = inputImage\n    gammaAdjustFilter.power = 4\n    return gammaAdjustFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "colorThresholdOtsuFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3584770-colorthresholdotsufilter",
    "html": "Return Value\n\nAn image containing pixels with color components that are either 1 or 0.\n\nDiscussion\n\nThe filter applies Otsu’s algorithm to the reg, green, and blue color components. The filter uses these thresholds to set the color to components to 1 or 0. The alpha component remains unchanged.\n\nThe color threshold Otsu filter uses the following property:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in an image where each color component is either 1 or 0:\n\nfunc colorThresholdOTSU(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.colorThresholdOtsu()\n    filter.inputImage = inputImage\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "colorControlsFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228285-colorcontrolsfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the color controls filter to an image. The effect calculates saturation by linearly interpolating between a grayscale image with a saturation of 0.0 and the original image saturation of 1.0.\n\nThe color controls filter uses the following properties:\n\nbrightness\n\nA float representing the amount of brightness applied as a NSNumber.\n\ncontrast\n\nA float representing the amount of contrast applied as a NSNumber.\n\nsaturation\n\nA float representing the amount of saturation applied as a NSNumber.\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that results in a darker image:\n\nfunc colorControls(inputImage: CIImage) -> CIImage {\n    let colorControlsFilter = CIFilter.colorControls()\n    colorControlsFilter.inputImage = inputImage\n    colorControlsFilter.brightness = -0.4\n    colorControlsFilter.contrast = 1\n    colorControlsFilter.saturation = 1\n    return colorControlsFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "colorPolynomialFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228296-colorpolynomialfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the color polynomial filter to an image. The effect calculates the sum of each pixel’s color component value and the coefficient properties together to produce the output image.\n\nThe color polynomial filter uses the following properties:\n\nredCoefficients\n\nA vector representing the polynomial coefficients for the red channel as a CIVector.\n\ngreenCoefficients\n\nA vector representing the polynomial coefficients for the green channel as a CIVector.\n\nblueCoefficients\n\nA vector representing the polynomial coefficients for the blue channel as a CIVector.\n\nalphaCoefficients\n\nA vector representing the polynomial coefficients for the alpha channel as a CIVector.\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds a lighter contrast to the input image:\n\nfunc colorPolynomial(inputImage: CIImage) -> CIImage {\n    let colorPolynomialFilter = CIFilter.colorPolynomial()\n    colorPolynomialFilter.alphaCoefficients = CIVector (x: 0, y: 0.6, z: 0, w: 0)\n    colorPolynomialFilter.redCoefficients = CIVector (x: 0, y: 1, z: 0.1, w: 0)\n    colorPolynomialFilter.greenCoefficients = CIVector(x: 0, y: 1, z: 0, w: 0)\n    colorPolynomialFilter.blueCoefficients = CIVector(x: 0, y: 1, z: 0, w: 0)\n    colorPolynomialFilter.inputImage = inputImage\n    return colorPolynomialFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "colorMatrixFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228294-colormatrixfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the color matrix filter to an image. The effect calculates the color matrix by multiplying the vector properties with the color values from the input image.\n\nThe color matrix filter uses the following properties:\n\nrVector\n\nA CIVector representing the amount of red to multiply the source color values by.\n\ngVector\n\nA CIVector representing the amount of green to multiply the source color values by.\n\nbVector\n\nA CIVector representing the amount of blue to multiply the source color values by.\n\naVector\n\nA CIVector representing the amount of alpha to multiply the source color values by.\n\nbiasVector\n\nA CIVector representing the amount of each vector that’s added to each color component.\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds a green hue to the input image:\n\nfunc colorMatrix(inputImage: CIImage) -> CIImage {\n    let colorMatrixFilter = CIFilter.colorMatrix()\n    colorMatrixFilter.inputImage = inputImage\n    colorMatrixFilter.rVector = CIVector (x: 1, y: 0, z: 0.2, w: 0)\n    colorMatrixFilter.gVector = CIVector (x: 0, y: 1, z: 0, w: 0.9)\n    colorMatrixFilter.bVector = CIVector (x: 0, y: 0, z: 1, w: 0)\n    colorMatrixFilter.aVector = CIVector (x: 0, y: 0, z: 0, w: 1)\n    colorMatrixFilter.biasVector = CIVector (x: 0, y: 0, z: 0, w: 0)\n    return colorMatrixFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "colorClampFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228284-colorclampfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the color clamp filter to an image. The effect calculates each pixel’s color component value. Using this calculation, the effect adjusts the values that are outside the range of the minComponents or maxComponents properties and clamps them within the range.\n\nThe color clamp filter uses the following properties:\n\nminComponents\n\nRGBA values for the lower end of the range as a CIVector.\n\nmaxComponents\n\nRGBA values for the upper end of the range as a CIVector.\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds a red hue to the input image:\n\nfunc colorClamp(inputImage: CIImage) -> CIImage {\n    let colorClampFilter = CIFilter.colorClamp()\n    colorClampFilter.inputImage = inputImage\n    colorClampFilter.minComponents = CIVector(x: 1, y: 0, z: 0, w: 0)\n    colorClampFilter.maxComponents = CIVector (x: 1, y: 0.9, z: 1, w: 1)\n    return colorClampFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "colorThresholdFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3547120-colorthresholdfilter",
    "html": "Return Value\n\nAn image containing pixels with color components that are either 1 or 0.\n\nDiscussion\n\nThis method applies the color threshold filter to an image. The filter compares the value of each color component (red, green, and blue) in the image against the threshold value. Any component higher than the threshold becomes 1 and any component lower becomes 0. The alpha component remains unchanged.\n\nThe color threshold filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nthreshold\n\nA float representing the threshold of color values as an NSNumber.\n\nThe following code creates a filter that results in an image where each color component is either 1 or 0.\n\nfunc colorThreshold(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.colorThreshold()\n    filter.inputImage = inputImage\n    filter.threshold = 0.5\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "colorAbsoluteDifferenceFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3547119-colorabsolutedifferencefilter",
    "html": "Return Value\n\nAn image containing the absolute color difference between the two input images.\n\nDiscussion\n\nThis method applies the color absolute difference filter to an image. This filter calculates the absolute color difference of the red, green, and blue values between the two input images. The alpha channel is the product of the alpha channels from the two input images.\n\nThe absolute difference filter uses the following properties:\n\ninputImage\n\nThe first CIImage for differencing.\n\ninputImage2\n\nThe second CIImage for differencing.\n\nThe following code creates a filter that results in the color difference between two images:\n\nfunc colorAbsolute(inputImage: CIImage, inputImage2: CIImage) -> CIImage {\n    let filter = CIFilter.colorAbsoluteDifference()\n    filter.inputImage = inputImage\n    filter.inputImage2 = inputImage2\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "twirlDistortionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600153-twirldistortionfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the twirl distortion filter to an image. This effect distorts an image by rotating pixels around the defined center to create a twirling effect. You can specify the number of rotations to control the strength of the effect.\n\nThe twirl distortion filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nradius\n\nA float representing the amount of pixels the filter uses to create the distortion as an NSNumber.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nangle\n\nA float representing the angle of the twirl, in radians, as an NSNumber.\n\nThe following code creates a filter that results in the center of the image becoming twirled:\n\nfunc twirlDistort(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.twirlDistortion()\n    filter.inputImage = inputImage\n    filter.radius = 600\n    filter.angle = 3.141592653589793\n    filter.center = CGPoint(x: 1791, y: 1344)\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "pinchDistortionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600150-pinchdistortionfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the pinch distortion filter to an image. This effect creates a rectangular area that pinches source pixels inward, distorting those pixels closest to the rectangle the most.\n\nThe pinch distortion filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nscale\n\nA float representing the amount of pinching effect as an NSNumber.\n\nradius\n\nA float representing the amount of pixels used to create the distortion as an NSNumber.\n\nThe following code creates a filter that results in a distorted image from the center of the photo:\n\nfunc pinch(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.pinchDistortion()\n    filter.inputImage = inputImage\n    filter.radius = 400\n    filter.scale = 0.5\n    filter.center = CGPoint(x: 1791, y: 1344)\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "attributedTextImageGeneratorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228267-attributedtextimagegeneratorfilt",
    "html": "Return Value\n\nThe generated image.\n\nDiscussion\n\nThis method generates an attributed-text image. The effect takes the input string property and the scale factor to scale up the text. You commonly combine this filter with other filters to create a watermark on images.\n\nThe attributed-text image generator filter uses the following properties:\n\ntext\n\nAn NSAttributedString.\n\nscaleFactor\n\nA float representing the scale of the font to use for the generated text.\n\npadding\n\nA float representing the value for an additional number of pixels to pad around the text’s bounding box.\n\nThe following code creates a filter that generates an attributed-text image:\n\nfunc attributedTextImage() -> CIImage {\n    let attributedTextImageFilter = CIFilter.attributedTextImageGenerator()\n    attributedTextImageFilter.text = NSAttributedString(string: \"Hello world! 👋\")\n    attributedTextImageFilter.scaleFactor = 10\n    attributedTextImageFilter.padding = 5\n    return attributedTextImageFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ aztecCodeGeneratorFilter\nGenerates a low-density barcode.\n+ barcodeGeneratorFilter\nGenerates a barcode as an image from the descriptor.\n+ blurredRectangleGeneratorFilter\nGenerates a blurred rectangle.\n+ checkerboardGeneratorFilter\nGenerates a checkerboard image.\n+ code128BarcodeGeneratorFilter\nGenerates a high-density, linear barcode.\n+ lenticularHaloGeneratorFilter\nGenerates a lenticular halo image.\n+ meshGeneratorFilter\nGenerates a pattern made from an array of line segments.\n+ PDF417BarcodeGenerator\nGenerates a high-density linear barcode.\n+ QRCodeGenerator\nGenerates a quick response (QR) code image.\n+ randomGeneratorFilter\nGenerates a random filter image.\n+ roundedRectangleGeneratorFilter\nGenerates a rounded rectangle image.\n+ roundedRectangleStrokeGeneratorFilter\nCreates an image containing the outline of a rounded rectangle.\n+ starShineGeneratorFilter\nGenerates a star-shine image.\n+ stripesGeneratorFilter\nGenerates a line of stripes as an image\n+ sunbeamsGeneratorFilter\nGenerates an image resembling the sun.\n+ textImageGeneratorFilter\nGenerates a text image."
  },
  {
    "title": "vortexDistortionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600154-vortexdistortionfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the vortex distortion filter to an image. This effect distorts an image by rotating pixels around the defined center to simulate a vortex. You can specify the number of rotations to control the strength of the effect.\n\nThe vortex distortion filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nangle\n\nA float representing the angle of the vortex, in radians, as an NSNumber.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nradius\n\nA float representing the amount of pixels the filter uses to create the distortion as an NSNumber.\n\nThe following code creates a filter that results in a small vortex effect:\n\nfunc vortexDistort(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.vortexDistortion()\n    filter.inputImage = inputImage\n    filter.radius = 700\n    filter.angle = 56.54866776461628\n    filter.center = CGPoint(x: 1124, y: 778)\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point."
  },
  {
    "title": "glassLozengeFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600145-glasslozengefilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the glass lozenge filter to an image. This effect distorts an image by creating a lozenge shape placed over the input image.\n\nThe absolute threshold filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nradius\n\nA float representing the radius of the lozenge distortion as an NSNumber.\n\nrefraction\n\nA float representing the refraction of the glass as an NSNumber.\n\ninputPoint1\n\nA CGPoint representing the x and y positions that define the center of the circle at the first end of the lozenge.\n\ninputPoint2\n\nA CGPoint representing the x and y positions that define the center of the circle at the second end of the lozenge.\n\nThe following code creates a filter that results in a large glass lozenge distorting the image:\n\nfunc glassLozenge(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.glassLozenge()\n    filter.inputImage = inputImage\n    filter.refraction = 1.7\n    filter.point0 = CGPoint(x: 150, y: 1050)\n    filter.point1 = CGPoint(x: 3050, y: 150)\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "holeDistortionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600146-holedistortionfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the hole distortion filter to an image. This effect distorts the image by generating a circular area that displaces pixels in the image by pushing them outward from the hole defined by the radius.\n\nThe absolute threshold filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nradius\n\nA float representing the amount of pixels the filter uses to create the distortion as an NSNumber.\n\nThe following code creates a filter that results in an image becoming distorted from the center outward:\n\nfunc holeDistortion(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.holeDistortion()\n    filter.inputImage = inputImage\n    filter.radius = 300\n    filter.center = CGPoint(x: 1791, y: 1344)\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "ninePartTiledFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600149-nineparttiledfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the nine-part tiled filter to an image. This effect distorts an image by tiling an image based on the breakpoints properties.\n\nThe nine-part tiled filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nflipYTiles\n\nA Boolean value representing if the y-axis should be flipped.\n\ngrowAmount\n\nA CGPoint representing the amount of stretching applied.\n\nbreakpoint1\n\nA CGPoint representing the upper-right corner of the image to retain after tiling ends.\n\nbreakpoint0\n\nA CGPoint representing the lower-left corner of image to retain before stretching begins.\n\nThe following code creates a filter that results in distorted tiles of the image becoming flipped:\n\nfunc ninePartTiled(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.ninePartTiled()\n    filter.inputImage = inputImage\n    filter.setDefaults()\n    filter.breakpoint0 = CGPoint(x: 200, y: 200)\n    filter.breakpoint1 = CGPoint(x: inputImage.extent.size.width-200, y: inputImage.extent.size.height - 200)\n    filter.growAmount = CGPoint(x: 500, y: 500)\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "glassDistortionFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600144-glassdistortionfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the glass distortion filter to an image. This effect distorts an image by applying a glass texture from the raised portions of the texture map image.\n\nThe glass distortion filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ntexture\n\nAn image with the type CIImage.\n\nscale\n\nThe amount of texturing to apply. Larger values increase the effect. Defaults to 200.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nThe following code creates a filter that results in a glass-like distortion applied to the image:\n\nfunc glassDistortion(inputImage: CIImage, textureImage: CIImage) -> CIImage {\n    let filter = CIFilter.glassDistortion()\n    filter.inputImage = inputImage\n    filter.textureImage = textureImage\n    filter.center = CGPoint(x: 1791, y: 1344)\n    filter.scale = 500\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "ninePartStretchedFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600148-ninepartstretchedfilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the nine-part stretched filter to an image. This effect distorts an image by stretching an image to the breakpoint properties while distorting the image based on the grow amount.\n\nThe nine-part stretched filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ngrowAmount\n\nA CGPoint representing the amount of stretching applied.\n\nbreakpoint0\n\nA CGPoint representing the lower-left corner of the image to retain before stretching begins.\n\nbreakpoint1\n\nA CGPoint representing the upper-right corner of the image to retain after stretching ends.\n\nThe following code creates a filter that results in a significantly warped image:\n\nfunc ninePartStretch(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.ninePartStretched()\n    filter.inputImage = inputImage\n    filter.setDefaults()\n    filter.breakpoint0 = CGPoint(x: 200, y: 200)\n    filter.breakpoint1 = CGPoint(x: inputImage.extent.size.width-200, y: inputImage.extent.size.height - 200)\n    filter.growAmount = CGPoint(x: 500, y: 500)\n    return filter.outputImage!\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ drosteFilter\nStylizes an image with the Droste effect.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "drosteFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3600143-drostefilter",
    "html": "Return Value\n\nThe distorted image.\n\nDiscussion\n\nThis method applies the Droste filter to an image. This effect creates a Droste effect that distorts the image by repeating smaller versions of the same image within itself.\n\nThe Droste filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nrotation\n\nA float representing the angle of the rotation, in radians, as an NSNumber.\n\nzoom\n\nA float representing the zoom of the effect as an NSNumber.\n\nperiodicity\n\nA float representing the amount of intervals as an NSNumber.\n\ninputInsetPoint1\n\nA CGPoint representing the x and y position that defines the first inset point.\n\ninputInsetPoint0\n\nA CGPoint representing the x and y position that defines the second inset point.\n\ninputStrands\n\nA float representing the amount of strands as an NSNumber.\n\nThe following code creates a filter that results in the image becoming a repeated, scaled pattern:\n\nfunc drosteFilter(inputImage: CIImage) -> CIImage {\n    let filter = CIFilter.droste()\n    filter.inputImage = inputImage\n    filter.insetPoint1 = CGPoint(\n        x: inputImage.extent.size.width * 0.2,\n        y: inputImage.extent.size.height * 0.2\n    )\n    filter.insetPoint0 = CGPoint(\n        x: inputImage.extent.size.width * 0.8,\n        y: inputImage.extent.size.height * 0.8\n    )\n    filter.periodicity = 1\n    filter.rotation = 0\n    filter.strands = 1\n    filter.zoom = 1\n    return filter.outputImage!.cropped(to: inputImage.extent)\n}\n\n\nSee Also\nFilters\n+ bumpDistortionFilter\nDistorts an image with a concave or convex bump.\n+ bumpDistortionLinearFilter\nLinearly distorts an image with a concave or convex bump.\n+ circleSplashDistortionFilter\nDistorts an image with radiating circles to the periphery of the image.\n+ circularWrapFilter\nDistorts an image by increasing the distance of the center of the image.\n+ displacementDistortionFilter\nApplies the grayscale values of the second image to the first image.\n+ glassDistortionFilter\nDistorts an image by applying a glass-like texture.\n+ glassLozengeFilter\nCreates a lozenge-shaped lens and distorts the image.\n+ holeDistortionFilter\nDistorts an image with a circular area that pushes the image outward.\n+ lightTunnelFilter\nDistorts an image by generating a light tunnel.\n+ ninePartStretchedFilter\nDistorts an image by stretching it between two breakpoints.\n+ ninePartTiledFilter\nDistorts an image by tiling portions of it.\n+ pinchDistortionFilter\nDistorts an image by creating a pinch effect with stronger distortion in the center.\n+ stretchCropFilter\nDistorts an image by stretching or cropping to fit a specified size.\n+ torusLensDistortionFilter\nCreates a torus-shaped lens to distort the image.\n+ twirlDistortionFilter\nDistorts an image by rotating pixels around a center point.\n+ vortexDistortionFilter\nDistorts an image by using a vortex effect created by rotating pixels around a point."
  },
  {
    "title": "startTask(toRender:from:to:at:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2875448-starttask",
    "html": "Parameters\nimage\n\nA CIImage to render.\n\nfromRect\n\nThe part of the image to render, as if cropped.\n\ndestination\n\nA CIRenderDestination into which to render the image.\n\natPoint\n\nAn origin point in the destination at which to place the image.\n\nReturn Value\n\nAn asynchronous CIRenderTask to render the image to the specified destination.\n\nDiscussion\n\nThis method crops the image to the specified rectangle and renders the result at the indicated origin point. If the image’s extent property and fromRect argument values are infinite, this call renders the image’s (0, 0) point starting from the origin atPoint.\n\nYou must use an MTLTexture-backed CIContext to support an MTLTexture-backed CIRenderDestination. Similarly, you must use GLContext-backed CIContext to support a GLTexture-backed CIRenderDestination.\n\nThis call returns as soon as it enqueues all work required to render the image on the context’s device. In many situations, after issuing a render, you may need to wait for it to complete. In these cases, use the returned CIRenderTask as follows:\n\nlet renderTask = try context.startTask(toRender: image, from: fromRect, to: destination, at: point)\n\n\nlet renderInfo = try renderTask.waitUntilCompleted()\n\n\nSee Also\nCustomizing Render Destination\nfunc prepareRender(CIImage, from: CGRect, to: CIRenderDestination, at: CGPoint)\nAn optional call to warm up a CIContext so that subsequent calls to render with the same arguments run more efficiently.\nfunc startTask(toClear: CIRenderDestination) -> CIRenderTask\nFills the entire destination with black or clear depending on its alphaMode.\nfunc startTask(toRender: CIImage, to: CIRenderDestination) -> CIRenderTask\nRenders an image to a destination so that point (0, 0) of the image maps to point (0, 0) of the destination."
  },
  {
    "title": "auxiliaryPortraitEffectsMatte",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/2976441-auxiliaryportraiteffectsmatte",
    "html": "Discussion\n\nThe value of this key is an NSNumber containing a Boolean true or false.\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image."
  },
  {
    "title": "auxiliaryDisparity",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/2890794-auxiliarydisparity",
    "html": "Discussion\n\nThe value of this key is an NSNumber containing a Boolean true or false. If the value is true, then calls to imageWithContentsOfURL:options: and imageWithData:options: will return the auxiliary image as a half-float monochrome image instead of the primary image, or nil if no auxiliary image exists.\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "textureTarget",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/1437613-texturetarget",
    "html": "Discussion\n\nThe value for this key must be an NSNumber object containing a supported OpenGL texture target constant, either GL_TEXTURE_2D or GL_TEXTURE_RECTANGLE_ARB. You may only use this key when initializing an image using the init(texture:size:flipped:options:) method.\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "properties",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/1437679-properties",
    "html": "Discussion\n\nTo ensure that an image has no metadata properties, set the value of this key to [NSNull null].\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "applyOrientationProperty",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/2915369-applyorientationproperty",
    "html": "Discussion\n\nImages can contain metadata that reveals the orientation at capture time. You can load this metadata into CIImage with imageWithContentsOfURL: or init(data:) when the captured image contains orientation metadata. Use any of the initWith:options: methods if the properties (NSDictionary of metadata properties) option is also provided.\n\nIf the value of this key is true, then calls to imageWithContentsOfURL:options: and imageWithData:options: will return the image transformed according to its orientation metadata.\n\nSee Also\nImage Dictionary Keys\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte."
  },
  {
    "title": "depthData",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2902251-depthdata",
    "html": "Discussion\n\nReturns an AVDepthData if the CIImage was created with imageWithData: or imageWithContentsOfURL: and one of the options auxiliaryDepth or auxiliaryDisparity, otherwise nil.\n\nSee Also\nAccessing Original Image Content\nvar cgImage: CGImage?\nThe CoreGraphics image object this image was created from, if applicable.\nvar pixelBuffer: CVPixelBuffer?\nThe CoreVideo pixel buffer this image was created from, if applicable.\nvar portraitEffectsMatte: AVPortraitEffectsMatte?\nAVPortraitEffectsMatte representation of portrait effects."
  },
  {
    "title": "pixelBuffer",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1687604-pixelbuffer",
    "html": "Discussion\n\nIf this image was create using the init(cvPixelBuffer:) initializer, this property’s value is the CVPixelBuffer object that provides the image’s underlying image data. Do not modify the contents of this pixel buffer; doing so will cause undefined rendering results.\n\nOtherwise, this property’s value is nil—in this case you can obtain a pixel buffer by rendering the image with the CIContext render(_:to:) method.\n\nSee Also\nAccessing Original Image Content\nvar cgImage: CGImage?\nThe CoreGraphics image object this image was created from, if applicable.\nvar depthData: AVDepthData?\nAVDepthData representation of the depth image.\nvar portraitEffectsMatte: AVPortraitEffectsMatte?\nAVPortraitEffectsMatte representation of portrait effects."
  },
  {
    "title": "cgImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1687603-cgimage",
    "html": "Discussion\n\nIf this image was created using the init(cgImage:) or init(contentsOf:) initializer, this property’s value is the CGImage object that provides the image’s underlying image data. Otherwise, this property’s value is nil—in this case you can obtain a CoreGraphics image by rendering the image with the CIContext createCGImage(_:from:) method.\n\nSee Also\nAccessing Original Image Content\nvar pixelBuffer: CVPixelBuffer?\nThe CoreVideo pixel buffer this image was created from, if applicable.\nvar depthData: AVDepthData?\nAVDepthData representation of the depth image.\nvar portraitEffectsMatte: AVPortraitEffectsMatte?\nAVPortraitEffectsMatte representation of portrait effects."
  },
  {
    "title": "orientationTransform(forExifOrientation:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437930-orientationtransform",
    "html": "Parameters\norientation\n\nAn integer specifying an image orientation according to the EXIF specification. For details, see kCGImagePropertyOrientation.\n\nReturn Value\n\nAn affine transform that will rotate or mirror the image to match the specified orientation when applied.\n\nDiscussion\n\nThis method determines the transformation needed to match the specified orientation, but does not apply that transformation to the image. To apply the transformation (possibly after concatenating it with other transformations), use the transformed(by:) method or the CIAffineTransform filter. To determine and apply the transformation in a single step, use the oriented(forExifOrientation:) method.\n\nSee Also\nGetting Image Information\nvar definition: CIFilterShape\nReturns a filter shape object that represents the domain of definition of the image.\nvar extent: CGRect\nA rectangle that specifies the extent of the image.\nvar properties: [String : Any]\nA dictionary containing metadata about the image.\nvar url: URL?\nThe URL from which the image was loaded.\nvar colorSpace: CGColorSpace?\nThe color space of the image."
  },
  {
    "title": "colorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437750-colorspace",
    "html": "Discussion\n\nThis property’s value is nil if the image’s color space cannot be determined.\n\nSee Also\nGetting Image Information\nvar definition: CIFilterShape\nReturns a filter shape object that represents the domain of definition of the image.\nvar extent: CGRect\nA rectangle that specifies the extent of the image.\nvar properties: [String : Any]\nA dictionary containing metadata about the image.\nvar url: URL?\nThe URL from which the image was loaded.\nfunc orientationTransform(forExifOrientation: Int32) -> CGAffineTransform\nReturns the transformation needed to reorient the image to the specified orientation."
  },
  {
    "title": "extent",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437996-extent",
    "html": "Discussion\n\nThis rectangle specifies the extent of the image in working space coordinates.\n\nSee Also\nGetting Image Information\nvar definition: CIFilterShape\nReturns a filter shape object that represents the domain of definition of the image.\nvar properties: [String : Any]\nA dictionary containing metadata about the image.\nvar url: URL?\nThe URL from which the image was loaded.\nvar colorSpace: CGColorSpace?\nThe color space of the image.\nfunc orientationTransform(forExifOrientation: Int32) -> CGAffineTransform\nReturns the transformation needed to reorient the image to the specified orientation."
  },
  {
    "title": "url",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438195-url",
    "html": "Discussion\n\nA URL is available only if the image object was created with a URL (such as with the init(contentsOf:) method or related methods). Otherwise, this property’s value is nil.\n\nSee Also\nGetting Image Information\nvar definition: CIFilterShape\nReturns a filter shape object that represents the domain of definition of the image.\nvar extent: CGRect\nA rectangle that specifies the extent of the image.\nvar properties: [String : Any]\nA dictionary containing metadata about the image.\nvar colorSpace: CGColorSpace?\nThe color space of the image.\nfunc orientationTransform(forExifOrientation: Int32) -> CGAffineTransform\nReturns the transformation needed to reorient the image to the specified orientation."
  },
  {
    "title": "definition",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437804-definition",
    "html": "Return Value\n\nA filter shape object.\n\nSee Also\nGetting Image Information\nvar extent: CGRect\nA rectangle that specifies the extent of the image.\nvar properties: [String : Any]\nA dictionary containing metadata about the image.\nvar url: URL?\nThe URL from which the image was loaded.\nvar colorSpace: CGColorSpace?\nThe color space of the image.\nfunc orientationTransform(forExifOrientation: Int32) -> CGAffineTransform\nReturns the transformation needed to reorient the image to the specified orientation."
  },
  {
    "title": "insertingIntermediate(cache:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2966522-insertingintermediate",
    "html": "Parameters\ncache\n\nA Boolean value indicating whether to cache the intermediate.\n\nReturn Value\n\nThe image obtained from inserting the intermediate.\n\nDiscussion\n\nIntermediate buffers created through setting cache to true have a higher priority than others.\n\nThis setting is independent of of CIContext's cacheIntermediates option.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate."
  },
  {
    "title": "settingProperties(_:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645895-settingproperties",
    "html": "Parameters\nproperties\n\nA dictionary of metadata properties to associate with the image.\n\nReturn Value\n\nAn image object with the specified properties.\n\nDiscussion\n\nWhen you create an image, Core Image sets an image’s properties dictionary to the metadata you specify (using the properties key in an options dictionary), or to the underlying image’s metadata (by calling the CGImageSourceCopyPropertiesAtIndex(_:_:_:) function). Use this method to override an image’s metadata properties with new values.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "createCGLayer(with:info:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1438267-createcglayer",
    "html": "Parameters\nsize\n\nThe size, in default user space units, of the layer relative to the graphics context.\n\nd\n\nA dictionary, which is passed to CGLayerCreateWithContext as the auxiliaryInfo parameter. Pass NULL because this parameter is reserved for future use.\n\nReturn Value\n\nA CGLayer object.\n\nDiscussion\n\nAfter calling this method, Core Image draws content into the CGLayer object. Core Image creates a CGLayer object by calling the Quartz 2D function init(_:size:auxiliaryInfo:), whose prototype is:\n\nCGLayerRef CGLayerCreateWithContext (\n   CGContextRef context,\n   CGSize size,\n   CFDictionaryRef auxiliaryInfo\n);\n\n\nCore Image passes the CIContext object as the context parameter, the size as the size parameter, and the dictionary as the auxiliaryInfo parameter. For more information on CGLayer objects, see Quartz 2D Programming Guide and CGLayer.\n\nSee Also\nRendering Images\nfunc createCGImage(CIImage, from: CGRect) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?, deferred: Bool) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object with deferred rendering.\nfunc render(CIImage, toBitmap: UnsafeMutableRawPointer, rowBytes: Int, bounds: CGRect, format: CIFormat, colorSpace: CGColorSpace?)\nRenders to the given bitmap.\nfunc render(CIImage, to: CVPixelBuffer)\nRenders an image into a pixel buffer.\nfunc render(CIImage, to: CVPixelBuffer, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into a pixel buffer.\nfunc render(CIImage, to: IOSurfaceRef, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into an IOSurface object.\nfunc render(CIImage, to: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?, bounds: CGRect, colorSpace: CGColorSpace)\nRenders a region of an image to a Metal texture.\nRelated Documentation\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object.\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options."
  },
  {
    "title": "createCGImage(_:from:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437784-createcgimage",
    "html": "Parameters\nim\n\nA Core Image image object.\n\nr\n\nThe region of the image to render.\n\nReturn Value\n\nA Quartz 2D image. You are responsible for releasing the returned image when you no longer need it.\n\nDiscussion\n\nRenders a region of an image into a temporary buffer using the context, then creates and returns a Quartz 2D image with the results.\n\nSee Also\nRendering Images\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?, deferred: Bool) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object with deferred rendering.\nfunc createCGLayer(with: CGSize, info: CFDictionary?) -> CGLayer?\nCreates a CGLayer object from the provided parameters.\nDeprecated\nfunc render(CIImage, toBitmap: UnsafeMutableRawPointer, rowBytes: Int, bounds: CGRect, format: CIFormat, colorSpace: CGColorSpace?)\nRenders to the given bitmap.\nfunc render(CIImage, to: CVPixelBuffer)\nRenders an image into a pixel buffer.\nfunc render(CIImage, to: CVPixelBuffer, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into a pixel buffer.\nfunc render(CIImage, to: IOSurfaceRef, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into an IOSurface object.\nfunc render(CIImage, to: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?, bounds: CGRect, colorSpace: CGColorSpace)\nRenders a region of an image to a Metal texture."
  },
  {
    "title": "temperatureAndTintFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228421-temperatureandtintfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the image temperature and tint filter to an image. The effect adjusts the white balance of the input image to match the targetNeutral property, resulting in a cooler or warmer tone image.\n\nThe temperature and tint filter uses the following properties:\n\nneutral\n\nA vector containing the source white point as a CIVector.\n\ntargetNeutral\n\nA vector containing the desired white point as a CIVector.\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds an orange hue to the input image:\n\nfunc tempatureAndTint(inputImage: CIImage) -> CIImage {\n    let tempatureAndTintFilter = CIFilter.temperatureAndTint()\n    tempatureAndTintFilter.inputImage = inputImage\n    tempatureAndTintFilter.neutral = CIVector(x: 11500, y: 10)\n    tempatureAndTintFilter.targetNeutral = CIVector(x: 4000, y: 0)\n    return tempatureAndTintFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "convolution9HorizontalFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228302-convolution9horizontalfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a 9 x 1 convolution to the RGBA components of an image. The effect uses a 9 x 1 area surrounding an input pixel, the pixel itself, and those within a distance of 4 pixels horizontally. The effect repeats this for every pixel within the image. Unlike the convolution filters, which use square matrices, this filter can only produce effects along a horizontal axis. You can combine this filter with the convolution9VerticalFilter to apply separable 9 x 9 convolutions.\n\nThe convolution 9-horizontal filter uses the following properties:\n\nbias\n\nA float representing the value that’s added to each output pixel as a NSNumber.\n\nweights\n\nA CIVector representing the convolution kernel.\n\ninputImage\n\nAn image with the type CIImage.\n\nNote\n\nWhen using a nonzero bias value, the output image has an infinite extent. You should crop the output image before attempting to render it.\n\nThe following code creates a filter that detects edges in the input image:\n\nfunc convolution9Horizontal(inputImage: CIImage) -> CIImage? {\n    let convolutionFilter = CIFilter.convolution9Horizontal()\n    convolutionFilter.inputImage = inputImage\n    let weights: [CGFloat] = [1, 1, 1, 1, 1, 1, 1, 1, 1].map { $0/9.0 }\n    let kernel = CIVector(values: weights, count: 9)\n    convolutionFilter.weights = kernel\n    convolutionFilter.bias = 0.0\n    return convolutionFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ convolution3X3Filter\nApplies a convolution 3 x 3 filter to the RGBA components of an image.\n+ convolution5X5Filter\nApplies a convolution 5 x 5 filter to the RGBA components image.\n+ convolution7X7Filter\nApplies a convolution 7 x 7 filter to the RGBA color components of an image.\n+ convolution9VerticalFilter\nApplies a convolution-9 vertical filter to the RGBA components of an image.\n+ convolutionRGB3X3Filter\nApplies a convolution 3 x 3 filter to the RGB components of an image.\n+ convolutionRGB5X5Filter\nApplies a convolution 5 x 5 filter to the RGB components of an image.\n+ convolutionRGB7X7Filter\nApplies a convolution 7 x 7 filter to the RGB components of an image.\n+ convolutionRGB9HorizontalFilter\nApplies a convolution 9 x 1 filter to the RGB components of an image.\n+ convolutionRGB9VerticalFilter\nApplies a convolution 1 x 9 filter to the RGB components of an image."
  },
  {
    "title": "vibranceFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228429-vibrancefilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the vibrance filter to an image. The effect adjusts the saturation of the image while preserving skin tone colors.\n\nThe vibrance filter uses the following properties:\n\namount\n\nA float representing the amount to adjust the saturation with the type NSNumber.\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds darkness to the input image:\n\nfunc vibrance(inputImage: CIImage) -> CIImage {\n    let vibranceFilter = CIFilter.vibrance()\n    vibranceFilter.inputImage = inputImage\n    vibranceFilter.amount = 2\n    return vibranceFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "convolution5X5Filter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228300-convolution5x5filter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a 5 x 5 convolution to the RGBA components of an image. The effect uses a 5 x 5 area surrounding an input pixel, the pixel itself, and those within a distance of 2 pixels horizontally and vertically. The effect repeats this for every pixel within the image. The work area is then combined with the weight property vector to produce the processed image. This filter differs from the convolutionRGB5X5Filter filter, which only processes the RGB components.\n\nThe convolution 5 x 5 filter uses the following properties:\n\nbias\n\nA float representing the value that’s added to each output pixel as a NSNumber.\n\nweights\n\nA CIVector representing the convolution kernel.\n\ninputImage\n\nAn image with the type CIImage.\n\nNote\n\nWhen using a nonzero bias value, the output image has an infinite extent. You should crop the output image before attempting to render it.\n\nThe following code creates a filter that blurs the input image:\n\nfunc convolution5X5(inputImage: CIImage) -> CIImage? {\n    let convolutionFilter = CIFilter.convolution5X5()\n    convolutionFilter.inputImage = inputImage\n    let blur: [CGFloat] = [\n         1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1,\n    ].map { $0/25.0 }\n    let kernel = CIVector(values: blur, count: 25)\n    convolutionFilter.weights = kernel\n    convolutionFilter.bias = 0\n    return convolutionFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ convolution3X3Filter\nApplies a convolution 3 x 3 filter to the RGBA components of an image.\n+ convolution7X7Filter\nApplies a convolution 7 x 7 filter to the RGBA color components of an image.\n+ convolution9HorizontalFilter\nApplies a convolution-9 horizontal filter to the RGBA components of an image.\n+ convolution9VerticalFilter\nApplies a convolution-9 vertical filter to the RGBA components of an image.\n+ convolutionRGB3X3Filter\nApplies a convolution 3 x 3 filter to the RGB components of an image.\n+ convolutionRGB5X5Filter\nApplies a convolution 5 x 5 filter to the RGB components of an image.\n+ convolutionRGB7X7Filter\nApplies a convolution 7 x 7 filter to the RGB components of an image.\n+ convolutionRGB9HorizontalFilter\nApplies a convolution 9 x 1 filter to the RGB components of an image.\n+ convolutionRGB9VerticalFilter\nApplies a convolution 1 x 9 filter to the RGB components of an image."
  },
  {
    "title": "whitePointAdjustFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228432-whitepointadjustfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the white-point adjust filter to an image. The effect adjusts the white-point of the input image by mapping all shades of gray to shades of the color property.\n\nThe white-point adjust filter uses the following properties:\n\ncolor\n\nThe new white point color with the type of CIColor.\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that adds a red hue to the input image:\n\nfunc whitePoint(inputImage: CIImage) -> CIImage {\n    let whitePointFilter = CIFilter.whitePointAdjust()\n    whitePointFilter.inputImage = inputImage\n    whitePointFilter.color = CIColor(red: 1, green: 0.6, blue: 0.6, alpha: 1)\n    return whitePointFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ sRGBToneCurveToLinearFilter\nConverts the colors in an image from sRGB to linear.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy."
  },
  {
    "title": "convolution3X3Filter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228299-convolution3x3filter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a 3 x 3 convolution to the RGBA components of an image. The effect uses a 3 x 3 area surrounding an input pixel, the pixel itself, and those within a distance of 1 pixel horizontally and vertically. The effect repeats this for every pixel within the image. The work area is then combined with the weight property vector to produce the processed image. This filter differs from the convolutionRGB3X3Filter, which only processes the RGB color components.\n\nThe convolution 3 x 3 filter uses the following properties:\n\nbias\n\nA float representing the value that’s added to each output pixel as a NSNumber.\n\nweights\n\nA CIVector representing the convolution kernel.\n\ninputImage\n\nAn image with the type CIImage.\n\nNote\n\nWhen using a nonzero bias value, the output image has an infinite extent. You should crop the output image before attempting to render it.\n\nThe following code creates a filter that sharpens the input image:\n\nfunc convolution3X3(inputImage: CIImage) -> CIImage? {\n    let convolutionFilter = CIFilter.convolution3X3()\n    convolutionFilter.inputImage = inputImage\n    let kernel = CIVector(values: [\n        0, -2, 0,\n        -2, 9, -2,\n        0, -2, 0\n    ], count: 9)\n    convolutionFilter.weights = kernel\n    convolutionFilter.bias = 0.0\n    return convolutionFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ convolution5X5Filter\nApplies a convolution 5 x 5 filter to the RGBA components image.\n+ convolution7X7Filter\nApplies a convolution 7 x 7 filter to the RGBA color components of an image.\n+ convolution9HorizontalFilter\nApplies a convolution-9 horizontal filter to the RGBA components of an image.\n+ convolution9VerticalFilter\nApplies a convolution-9 vertical filter to the RGBA components of an image.\n+ convolutionRGB3X3Filter\nApplies a convolution 3 x 3 filter to the RGB components of an image.\n+ convolutionRGB5X5Filter\nApplies a convolution 5 x 5 filter to the RGB components of an image.\n+ convolutionRGB7X7Filter\nApplies a convolution 7 x 7 filter to the RGB components of an image.\n+ convolutionRGB9HorizontalFilter\nApplies a convolution 9 x 1 filter to the RGB components of an image.\n+ convolutionRGB9VerticalFilter\nApplies a convolution 1 x 9 filter to the RGB components of an image."
  },
  {
    "title": "sRGBToneCurveToLinearFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228398-srgbtonecurvetolinearfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the sRGB-tone-curve-to-linear filter to an image. The effect converts an image in sRGB space to linear color space.\n\nThe sRGB-tone-curve-to-linear filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nThe following code creates a filter that converts from sRGB to linear color space.\n\nfunc sRGBToLinear(inputImage: CIImage) -> CIImage {\n    let sRGBToLinearFilter = CIFilter.sRGBToneCurveToLinear()\n    sRGBToLinearFilter.inputImage = inputImage\n    return sRGBToLinearFilter.outputImage!\n}\n\n\nSee Also\nFilters\n+ colorAbsoluteDifferenceFilter\nCalculates the absolute difference between each color component in the input images.\n+ colorClampFilter\nAlters the colors in an image based on color components.\n+ colorControlsFilter\nAlters the brightness, contrast, and saturation of an image’s colors.\n+ colorMatrixFilter\nAlters the colors in an image based on vectors provided.\n+ colorPolynomialFilter\nAlters an image’s colors.\n+ colorThresholdFilter\nCompares the red, green, and blue components of the input image to a threshold and sets them to 1 or 0.\n+ colorThresholdOtsuFilter\nCompares the red, green, and blue components of the input image against a threshold calculated using Otsu’s algorithm.\n+ depthToDisparityFilter\nConverts from an image containing depth data to an image containing disparity data.\n+ disparityToDepthFilter\nCreates depth data from an image containing disparity data.\n+ exposureAdjustFilter\nAdjusts an image’s exposure.\n+ gammaAdjustFilter\nAlters an image’s transition between black and white.\n+ hueAdjustFilter\nModifies an image’s hue.\n+ linearToSRGBToneCurveFilter\nAlters an image’s color intensity.\n+ temperatureAndTintFilter\nAlters an image’s temperature and tint.\n+ toneCurveFilter\nAlters an image’s tone curve according to a series of data points.\n+ vibranceFilter\nAdjusts an image’s vibrancy.\n+ whitePointAdjustFilter\nAdjusts the image’s white-point."
  },
  {
    "title": "init(cgLayer:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438065-init",
    "html": "Parameters\nlayer\n\nA CGLayer object. For more information see Quartz 2D Programming Guide and CGLayer.\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithCGLayer:\nCreates and returns an image object from the contents supplied by a CGLayer object."
  },
  {
    "title": "startTask(toRender:to:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2875429-starttask",
    "html": "Parameters\nimage\n\nCIImage to prepare to render.\n\ndestination\n\nThe CIRenderDestination to which to render.\n\nerror\n\nPointer to an error should the render task creation fail.\n\nReturn Value\n\nThe asynchronous CIRenderTask to render the image to the specified destination.\n\nSee Also\nCustomizing Render Destination\nfunc prepareRender(CIImage, from: CGRect, to: CIRenderDestination, at: CGPoint)\nAn optional call to warm up a CIContext so that subsequent calls to render with the same arguments run more efficiently.\nfunc startTask(toClear: CIRenderDestination) -> CIRenderTask\nFills the entire destination with black or clear depending on its alphaMode.\nfunc startTask(toRender: CIImage, from: CGRect, to: CIRenderDestination, at: CGPoint) -> CIRenderTask\nRenders a portion of an image to a point in the destination."
  },
  {
    "title": "init(image:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1624098-init",
    "html": "Parameters\nimage\n\nAn image containing the source data.\n\noptions\n\nA dictionary that contains options for creating an image object. You can supply such options as a pixel format and a color space. See Image Dictionary Keys.\n\nReturn Value\n\nThe initialized image object, or nil if the object could not be initialized.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "init(bitmapImageRep:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1535335-init",
    "html": "Parameters\nbitmapImageRep\n\nAn image representation object containing the bitmap data.\n\nReturn Value\n\nThe initialized image object, or nil if the object could not be initialized.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "matchedToWorkingSpace(from:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645896-matchedtoworkingspace",
    "html": "Parameters\ncolorSpace\n\nThe color space to be converted from. This color space must conform to the CGColorSpaceModel.rgb color space model.\n\nReturn Value\n\nAn image object representing the result of the color matching operation, or nil if the color spaces to be converted are not compatible.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "cropped(to:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437833-cropped",
    "html": "Parameters\nrect\n\nThe rectangle, in image coordinates, to which to crop the image.\n\nReturn Value\n\nAn image object cropped to the specified rectangle.\n\nFigure 1 Cropping an image\n\nDiscussion\n\nDue to Core Image's coordinate system mismatch with UIKit, this filtering approach may yield unexpected results when displayed in a UIImageView with contentMode. Be sure to back it with a cgImage so that it handles contentMode properly.\n\nListing 1 Backing a with a to preserve\nlet context = CIContext()\nlet final = context.createCGImage(ciCroppedImage, from:ciCroppedImage.extent)\n\n\nIf you are displaying or processing your image primarily as a CGImage or UIImage, with no additional Core Image application, consider cropping in Core Graphics using the cropping(to:) function to save processing overhead from conversion of images to CIImage. It makes most sense to use cropped(to:) when you already have CIImage in your pipeline.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "clampedToExtent()",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437628-clampedtoextent",
    "html": "Return Value\n\nAn image object representing the result of the clamp operation.\n\nDiscussion\n\nCalling this method is equivalent to using the CIAffineClamp filter, which creates an image of infinite extent by repeating pixel colors from the edges of the original image.\n\nThis operation can be useful when using the image as input to other filters. When an image has finite extent, Core Image treats the area outside the extent as if it were filled with empty (black, zero alpha) pixels. If you apply a filter that samples from outside the image’s extent, those empty pixels affect the result of the filter.\n\nFor example, applying the CIGaussianBlur filter to an image softens the edges of the blurred image, because the opaque pixels at the edges of the image blur into the transparent pixels outside the image’s extent. Applying a clamp effect before the blur filter avoids edge softening by making the original image opaque in all directions. (However, the blurred image will also have infinite extent. Use the cropped(to:) method to return to the original image’s dimensions while retaining hard edges.)\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "oriented(forExifOrientation:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438223-oriented",
    "html": "Parameters\norientation\n\nAn integer specifying an image orientation according to the EXIF specification. For details, see kCGImagePropertyOrientation.\n\nReturn Value\n\nAn image object representing the result of rotating or mirroring the image to the target orientation.\n\nDiscussion\n\nThis method determines and then applies the transformation needed to reorient the image to the specified orientation. If you plan to also apply other transformations, you can retrieve the transformation this method would use by calling the orientationTransform(forExifOrientation:) method.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "applyingFilter(_:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2915368-applyingfilter",
    "html": "Discussion\n\nA convenience method for applying a single filter to the method receiver and returning the output image. Identical to applyingFilter(_:parameters:) with default parameters.\n\nImportant\n\nThis method, though convenient, is inefficient if used multiple times in succession. Achieve better performance by chaining filters without asking for the outputs of individual filters.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "init(imageProvider:size:_:format:colorSpace:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437868-init",
    "html": "Parameters\np\n\nA data provider that implements the CIImageProvider informal protocol. Core Image maintains a strong reference to this object until the image is deallocated.\n\nwidth\n\nThe width of the image data.\n\nheight\n\nThe height of the image data.\n\nf\n\nA pixel format constant. See Pixel Formats.\n\ncs\n\nThe color space of the image. If this value is nil, the image is not color matched. Pass nil for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables).\n\ndict\n\nA dictionary that specifies image-creation options, either kCIImageProviderTileSize or kCIImageProviderUserInfo. See CIImageProvider for more information on these options.\n\nReturn Value\n\nThe initialized image object.\n\nDiscussion\n\nCore Image does not populate the image until it needs the data.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithImageProvider:size::format:colorSpace:options:\nCreates and returns an image object initialized with data provided by an image provider."
  },
  {
    "title": "providerTileSize",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/1437829-providertilesize",
    "html": "See Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "transformed(by:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438203-transformed",
    "html": "Parameters\nmatrix\n\nAn affine transform.\n\nReturn Value\n\nThe transformed image object.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "providerUserInfo",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageoption/1437989-provideruserinfo",
    "html": "See Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "heifRepresentation(of:format:colorSpace:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2902269-heifrepresentation",
    "html": "Parameters\nimage\n\nThe image object to render.\n\nformat\n\nThe pixel format for the output image.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the CGColorSpaceModel.rgb or CGColorSpaceModel.monochrome model and must be compatible with the specified pixel format.\n\noptions\n\nA dictionary with additional options for export. Supported keys include kCGImageDestinationLossyCompressionQuality, avDepthData, depthImage, and disparityImage.\n\nReturn Value\n\nA data representation of the rendered image in HEIF format, or nil if the image could not be rendered.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the clampedToExtent() method.\n\nSee Also\nRendering Images for Data or File Export\nfunc tiffRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in TIFF format.\nfunc jpegRepresentation(of: CIImage, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in JPEG format.\nfunc pngRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in PNG format.\nfunc writeTIFFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in TIFF format.\nfunc writeJPEGRepresentation(of: CIImage, to: URL, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in JPEG format.\nfunc writePNGRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in PNG format.\nfunc writeHEIFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in HEIF format.\nstatic let avDepthData: CIImageRepresentationOption\noptions dictionary key for image export methods to represent data as AVDepthData.\nstatic let depthImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output depth data.\nstatic let disparityImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "init(data:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438032-init",
    "html": "Parameters\ndata\n\nThe image data. The data you supply must be premultiplied.\n\nd\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nThe initialized image object, or nil if the object could not be initialized.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithData:options:\nCreates and returns an image object initialized with the supplied image data, using the specified options."
  },
  {
    "title": "applyingFilter(_:parameters:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437589-applyingfilter",
    "html": "Parameters\nfilterName\n\nThe name of the filter to apply, as used when creating a CIFilter instance with the init(name:) method.\n\nparams\n\nA dictionary whose key-value pairs are set as input values to the filter. Each key is a constant that specifies the name of an input parameter for the filter, and the corresponding value is the value for that parameter. See Core Image Filter Reference for built-in filters and their allowed parameters.\n\nReturn Value\n\nAn image object representing the result of applying the filter.\n\nDiscussion\n\nCalling this method is equivalent to the following sequence of steps:\n\nCreating a CIFilter instance\n\nSetting the original image as the filter’s inputImage parameter\n\nSetting the remaining filter parameters from the params dictionary\n\nRetrieving the outputImage object from the filter\n\nImportant\n\nThis method, though convenient, is inefficient if used multiple times in succession. Achieve better performance by chaining filters without asking for the outputs of individual filters.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "init(ioSurface:plane:format:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437670-init",
    "html": "Parameters\nsurface\n\nAn IOSurface object.\n\nplane\n\nThe index of the data plane in the IOSurface object containing bitmap data for initializing the image.\n\nformat\n\nA pixel format constant. See Pixel Formats.\n\noptions\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nAn image object initialized with the data from the IOSurface.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface."
  },
  {
    "title": "init(data:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437925-init",
    "html": "Parameters\ndata\n\nThe image data. The data you supply must be premultiplied.\n\nReturn Value\n\nThe initialized image object, or nil if the object could not be initialized.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithData:\nCreates and returns an image object initialized with the supplied image data."
  },
  {
    "title": "init(cvPixelBuffer:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438072-init",
    "html": "Parameters\nbuffer\n\nA CVPixelBuffer object.\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithCVPixelBuffer:\nCreates and returns an image object from the contents of CVPixelBuffer object."
  },
  {
    "title": "init(cvPixelBuffer:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438209-init",
    "html": "Parameters\nbuffer\n\nA CVPixelBuffer object.\n\ndict\n\nA dictionary that contains options for creating an image object. (See Image Dictionary Keys.) The pixel format is supplied by the CVPixelBuffer object.\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "apply(extent:arguments:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorkernel/1438110-apply",
    "html": "Parameters\nextent\n\nThe extent of the output image.\n\nargs\n\nAn array of arguments to pass to the kernel routine. The type of each object in the array must be compatible with the corresponding parameter declared in the kernel routine source code. For details, see Core Image Kernel Language Reference.\n\nReturn Value\n\nA new image object describing the result of applying the kernel.\n\nDiscussion\n\nThis method is analogous to the CIFilter method apply(_:arguments:options:), but it does not require construction of a CIFilter object, and it allows you to specify a callback for determining the kernel’s region of interest as a block or closure. As with the similar CIFilter method, calling this method does not execute the kernel code—filters and their kernel code are evaluated only when rendering a final output image."
  },
  {
    "title": "offlineGPUCount()",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437817-offlinegpucount",
    "html": "Return Value\n\nThe number of offline GPU devices.\n\nDiscussion\n\nIf this count is greater than zero, the system has attached GPU devices that are not currently driving a display. You can use these devices for Core Image rendering by creating a context with the init(forOfflineGPUAt:) orinit(forOfflineGPUAt:colorSpace:options:sharedContext:) method.\n\nSee Also\nManaging Resources\nfunc clearCaches()\nFrees any cached data, such as temporary images, associated with the context and runs the garbage collector.\nfunc reclaimResources()\nRuns the garbage collector to reclaim any resources that the context no longer requires.\nvar workingColorSpace: CGColorSpace?\nThe working color space of the Core Image context.\nvar workingFormat: CIFormat\nThe working pixel format of the Core Image context."
  },
  {
    "title": "draw(_:in:from:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437786-draw",
    "html": "Parameters\nim\n\nA Core Image image object.\n\ndest\n\nThe rectangle in the context destination to draw into. The image is scaled to fill the destination rectangle.\n\nsrc\n\nThe subregion of the image that you want to draw into the context, with the origin and target size defined by the dest parameter. This rectangle is always in pixel dimensions.\n\nDiscussion\n\nIn iOS, this method draws the CIImage object into a renderbuffer for the OpenGL ES context. Use this method only if the CIContext object is created with contextWithEAGLContext: and if you are rendering to a CAEAGLayer. This method is asynchronous for apps linked against the iOS 6 or later SDK.\n\nIn macOS, you need to be aware of whether the CIContext object is created with a CGContextRef or a CGLContext object. If you create the CIContext object with a CGContextRef, the dimensions of the destination rectangle are in points. If you create the CIContext object with a CGLContext object, the dimensions are in pixels.\n\nSee Also\nDrawing Images\nfunc draw(CIImage, at: CGPoint, from: CGRect)\nRenders a region of an image to a point in the context destination."
  },
  {
    "title": "clearCaches()",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437790-clearcaches",
    "html": "Discussion\n\nYou can use this method to remove textures from the texture cache that reference deleted images.\n\nSee Also\nManaging Resources\nfunc reclaimResources()\nRuns the garbage collector to reclaim any resources that the context no longer requires.\nclass func offlineGPUCount() -> UInt32\nReturns the number of GPUs not currently driving a display.\nvar workingColorSpace: CGColorSpace?\nThe working color space of the Core Image context.\nvar workingFormat: CIFormat\nThe working pixel format of the Core Image context."
  },
  {
    "title": "inputImageMaximumSize()",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1620425-inputimagemaximumsize",
    "html": "Discussion\n\nSome contexts limit the maximum size of an image that can be rendered into them. For example, the maximum size might reflect a limitation in the underlying graphics hardware.\n\nSee Also\nDetermining the Allowed Extents for Images Used by a Context\nfunc outputImageMaximumSize() -> CGSize\nReturns the maximum size allowed for any image created by the context."
  },
  {
    "title": "oriented(_:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2919727-oriented",
    "html": "Discussion\n\nReturns a new image representing the original image transformed for the given CGImagePropertyOrientation.\n\nSee Also\nWorking with Orientation\nfunc orientationTransform(for: CGImagePropertyOrientation) -> CGAffineTransform\nThe affine transform for changing the image to the given orientation."
  },
  {
    "title": "samplingLinear()",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2867346-samplinglinear",
    "html": "Discussion\n\nReturns the image sampled using bilinear interpolation.\n\nSee Also\nSampling the Image\nfunc samplingNearest() -> CIImage\nSamples the image using nearest-neighbor and returns the result."
  },
  {
    "title": "autoAdjustmentFilters()",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645889-autoadjustmentfilters",
    "html": "Return Value\n\nAn array of CIFilter instances preconfigured for correcting deficiencies in the supplied image.\n\nSee Also\nGetting Autoadjustment Filters\nfunc autoAdjustmentFilters(options: [CIImageAutoAdjustmentOption : Any]?) -> [CIFilter]\nReturns a subset of automatically selected and configured filters for adjusting the image.\nAutoadjustment Keys\nConstants used as keys in the options dictionary for the autoAdjustmentFilters(options:) method."
  },
  {
    "title": "Autoadjustment Keys",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/autoadjustment_keys",
    "html": "Topics\nConstants\nstatic let enhance: CIImageAutoAdjustmentOption\nA key used to specify whether to return enhancement filters.\nstatic let redEye: CIImageAutoAdjustmentOption\nA key used to specify whether to return a red eye filter.\nstatic let features: CIImageAutoAdjustmentOption\nA key used to specify an array of features that you want to apply enhancement and red eye filters to.\nstatic let crop: CIImageAutoAdjustmentOption\nA key used to specify whether to return a filter that crops the image to focus on detected features.\nstatic let level: CIImageAutoAdjustmentOption\nA key used to specify whether to return a filter that rotates the image to keep a level perspective.\nSee Also\nGetting Autoadjustment Filters\nfunc autoAdjustmentFilters() -> [CIFilter]\nReturns all possible automatically selected and configured filters for adjusting the image.\nfunc autoAdjustmentFilters(options: [CIImageAutoAdjustmentOption : Any]?) -> [CIFilter]\nReturns a subset of automatically selected and configured filters for adjusting the image."
  },
  {
    "title": "regionOfInterest(for:in:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437994-regionofinterest",
    "html": "Parameters\nim\n\nAnother image that is part of the filter chain that generates the image.\n\nr\n\nA rectangle in the image’s coordinate space.\n\nReturn Value\n\nA rectangle in the coordinate space of the input image (the im parameter).\n\nDiscussion\n\nThe region of interest is the rectangle containing pixel data in a source image (the im parameter) necessary to produce a corresponding rectangle in the output image. If the image is not the output of a filter (or of a chain or graph of several CIFilter objects), or the image in the im parameter is not an input to that filter, the rectangle returned is the same as that in the r parameter.\n\nFor example,\n\nIf the image is the output of a filter that doubles the size of its input image, the rectangle returned will be half the size of that in the r parameter. (Upscaling causes every pixel in the input image to correspond to multiple pixels in the output image.)\n\nIf the image is the output of a blur filter, the rectangle returned will be slightly larger than that in the r parameter. (In a blur filter, each pixel in the output image is produced using information from the corresponding pixel and those immediately surrounding it in the input image.)"
  },
  {
    "title": "autoAdjustmentFilters(options:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437792-autoadjustmentfilters",
    "html": "Parameters\noptions\n\nYou can control which filters are returned by supplying one or more of the keys described in Autoadjustment Keys.\n\nThe options dictionary can also contain a CIDetectorImageOrientation key. Because some autoadjustment filters rely on face detection, you should specify an image orientation if you want to enable these filters for an image containing face whose orientation does not match that of the image.\n\nReturn Value\n\nAn array of CIFilter instances preconfigured for correcting deficiencies in the supplied image.\n\nSee Also\nGetting Autoadjustment Filters\nfunc autoAdjustmentFilters() -> [CIFilter]\nReturns all possible automatically selected and configured filters for adjusting the image.\nAutoadjustment Keys\nConstants used as keys in the options dictionary for the autoAdjustmentFilters(options:) method."
  },
  {
    "title": "orientationTransform(for:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2919726-orientationtransform",
    "html": "Discussion\n\nReturns a CGAffineTransform for the CGImagePropertyOrientation value to apply to the image.\n\nSee Also\nWorking with Orientation\nfunc oriented(CGImagePropertyOrientation) -> CIImage\nTransforms the original image by a given CGImagePropertyOrientation and returns the result."
  },
  {
    "title": "samplingNearest()",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/2867429-samplingnearest",
    "html": "Discussion\n\nReturns the image sampled using nearest-neighbor.\n\nSee Also\nSampling the Image\nfunc samplingLinear() -> CIImage\nSamples the image using bilinear interpolation and returns the result."
  },
  {
    "title": "draw(in:from:operation:fraction:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1534407-draw",
    "html": "Parameters\ndstRect\n\nThe rectangle in which to draw the image.\n\nsrcRect\n\nThe source rectangle specifying the portion of the image you want to draw. The coordinates of this rectangle must be specified using the image's own coordinate system.\n\nop\n\nThe compositing operation to use when drawing the image. For details, see NSCompositingOperation.\n\ndelta\n\nThe opacity of the image, specified as a value from 0.0 to 1.0. Specifying a value of 0.0 draws the image as fully transparent while a value of 1.0 draws the image as fully opaque. Values greater than 1.0 are interpreted as 1.0.\n\nDiscussion\n\nIf the srcRect and dstRect rectangles have different sizes, the source portion of the image is scaled to fit the specified destination rectangle. The image is otherwise positioned and oriented using the current coordinate system.\n\nSee Also\nDrawing Images\nfunc draw(at: NSPoint, from: NSRect, operation: NSCompositingOperation, fraction: CGFloat)\nDraws all or part of the image at the specified point in the current coordinate system."
  },
  {
    "title": "draw(at:from:operation:fraction:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1534432-draw",
    "html": "Parameters\npoint\n\nThe location in the current coordinate system at which to draw the image.\n\nsrcRect\n\nThe source rectangle specifying the portion of the image you want to draw. The coordinates of this rectangle must be specified using the image's own coordinate system.\n\nop\n\nThe compositing operation to use when drawing the image. For details, see NSCompositingOperation.\n\ndelta\n\nThe opacity of the image, specified as a value from 0.0 to 1.0. Specifying a value of 0.0 draws the image as fully transparent while a value of 1.0 draws the image as fully opaque. Values greater than 1.0 are interpreted as 1.0.\n\nDiscussion\n\nThe image content is drawn at its current resolution and is not scaled unless the CTM of the current coordinate system itself contains a scaling factor. The image is otherwise positioned and oriented using the current coordinate system.\n\nSee Also\nDrawing Images\nfunc draw(in: NSRect, from: NSRect, operation: NSCompositingOperation, fraction: CGFloat)\nDraws all or part of the image in the specified rectangle in the current coordinate system"
  },
  {
    "title": "falseColorFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228325-falsecolorfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies the false color filter to an image. The effect maps the luminance to a color ramp from color0 to color1. People use this effect to process astronomical and other scientific data, such as ultraviolet and X-ray images.\n\nThe false color filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncolor0\n\nA CIColor representing the first color to use for the color ramp.\n\ncolor1\n\nA CIColor representing the second color to use for the color ramp.\n\nThe following code creates a filter that replaces the colors of the input image resulting in blue and yellow colors:\n\nfunc falseColor(inputImage: CIImage) -> CIImage {\n    let falseColorFilter = CIFilter.falseColor()\n    falseColorFilter.inputImage = inputImage\n    falseColorFilter.color0 = CIColor(red: 1, green: 1, blue: 0)\n    falseColorFilter.color1 = CIColor(red: 0, green: 0, blue: 1)\n    return falseColorFilter.outputImage!\n}\n\n\nSee Also\nColor Effect Filters\n+ colorCrossPolynomialFilter\nAdjusts an image’s color by applying polynomial cross-products.\n+ colorCubeFilter\nAdjusts an image’s pixels using a three-dimensional color table.\n+ colorCubeWithColorSpaceFilter\nAdjusts an image’s pixels using a three-dimensional color table in specified color space.\n+ colorCubesMixedWithMaskFilter\nAlters an image’s pixels using a three-dimensional color tables and a mask image.\n+ colorCurvesFilter\nAdjusts an image’s color curves.\n+ colorInvertFilter\nInverts an image’s colors.\n+ colorMapFilter\nPerforms a transformation of the input image colors to colors from a gradient image.\n+ colorMonochromeFilter\nAdjusts an image’s colors to shades of a single color.\n+ colorPosterizeFilter\nFlattens an image’s colors.\n+ convertLabToRGBFilter\nConverts an image from CIELAB to RGB color space.\n+ convertRGBtoLabFilter\nConverts an image from RGB to CIELAB color space.\n+ ditherFilter\nApplies randomized noise to produce a processed look.\n+ documentEnhancerFilter\nAdjusts an image’s shadows and contrast.\n+ LabDeltaE\nCompares an image’s color values.\n+ maskToAlphaFilter\nConverts an image to a white image with an alpha component.\n+ maximumComponentFilter\nCreates a maximum RGB grayscale image.\n+ minimumComponentFilter\nCreates a minimum RGB grayscale image.\n+ paletteCentroidFilter\nCalculates the location of an image’s colors.\n+ palettizeFilter\nReplaces colors with colors from a palette image.\n+ photoEffectChromeFilter\nExaggerates an image’s colors.\n+ photoEffectFadeFilter\nDiminishes an image’s colors.\n+ photoEffectInstantFilter\nDesaturates an image’s colors.\n+ photoEffectMonoFilter\nAdjust an image’s colors to black and white.\n+ photoEffectNoirFilter\nAdjusts an image’s colors to black and white and intensifies the contrast.\n+ photoEffectProcessFilter\nLowers the contrast of the input image.\n+ photoEffectTonalFilter\nAdjusts an image’s colors to black and white.\n+ photoEffectTransferFilter\nBrightens an image’s colors.\n+ sepiaToneFilter\nAdjusts an image’s colors to shades of brown.\n+ thermalFilter\nAlters the image to make it look like it was taken by a thermal camera.\n+ vignetteFilter\nGradually darkens an image’s edges.\n+ vignetteEffectFilter\nGradually darkens a specified area of an image.\n+ xRayFilter\nAlters an image to make it look like an X-ray image."
  },
  {
    "title": "RAW Image Options",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/raw_image_options",
    "html": "Overview\n\nYou can also use the key kCIInputEVKey for RAW images.\n\nTopics\nConstants\nstatic let decoderVersion: CIRAWFilterOption\nA key for the version number of the method to be used for decoding. A newly initialized object defaults to the newest available decoder version for the given image type. You can request an alternative, older version to maintain compatibility with older releases. Must be one of the values listed for the supportedDecoderVersions key, otherwise a nil output image is generated. The associated value must be an NSNumber object that specifies an integer value in range of 0 to the current decoder version. When you request a specific version of the decoder, Core Image produces an image that is visually the same across different versions of the operating system. Core Image, however, does not guarantee that the same bits are produced across different versions of the operating system. That’s because the rounding behavior of floating-point arithmetic can vary due to differences in compilers or hardware. Note that this option has no effect if the image used for initialization is not RAW.\nDeprecated\nstatic let supportedDecoderVersions: CIRAWFilterOption\nA key for the supported decoder versions. The associated value is an NSArray object that contains all supported decoder versions for the given image type, sorted in increasingly newer order. Each entry is an NSDictionary object that contains key-value pairs. All entries represent a valid version identifier that can be passed as the kCIDecoderVersion value for the key kCIDecoderMethodKey. Version values are read-only; attempting to set this value raises an exception. Currently, the only defined key is @\"version\" which has as its value an NSString that uniquely describing a given decoder version. This string might not be suitable for user interface display..\nDeprecated\nstatic let boostAmount: CIRAWFilterOption\nA key for the amount of boost to apply to an image. The associated value is a floating-point value packaged as an NSNumber object. The value must be in the range of 0...1. A value of 0 indicates no boost, that is, a linear response. The default value is 1, which indicates full boost.\nDeprecated\nstatic let neutralChromaticityX: CIRAWFilterOption\nThe x value of the chromaticity. The associated value is a floating-point value packaged as an NSNumber object. You can query this value to get the current x value for neutral x, y.\nDeprecated\nstatic let neutralChromaticityY: CIRAWFilterOption\nThe y value of the chromaticity. The associated value is a floating-point value packaged as an NSNumber object. You can query this value to get the current y value for neutral x, y.\nDeprecated\nstatic let neutralTemperature: CIRAWFilterOption\nA key for neutral temperature. The associated value is a floating-point value packaged as an NSNumber object. You can query this value to get the current temperature value.\nDeprecated\nstatic let neutralTint: CIRAWFilterOption\nA key for the neutral tint. The associated value is a floating-point value packaged as an NSNumber object. Use this key to set or fetch the temperature and tint values. You can query this value to get the current tint value.\nDeprecated\nstatic let neutralLocation: CIRAWFilterOption\nA key for the neutral position. Use this key to set the location in geometric coordinates of the unrotated output image that should be used as neutral. You cannot query this value; it is undefined for reading. The associated value is a two-element CIVector object that specifies the location (x, y).\nDeprecated\nstatic let scaleFactor: CIRAWFilterOption\nA key for the scale factor. The associated value is a floating-point value packaged as an NSNumber object that specifies the desired scale factor at which the image will be drawn. Setting this value can greatly improve the drawing performance. A value of 1 is the identity. In some cases, if you change the scale factor and enable draft mode, performance can decrease. See allowDraftMode.\nDeprecated\nstatic let allowDraftMode: CIRAWFilterOption\nA key for allowing draft mode. The associated value is a Boolean value packaged as an NSNumber object. It’s best not to use draft mode if the image needs to be drawn without draft mode at a later time, because changing the value from true to false is an expensive operation. If the optional scale factor is smaller than a certain value, additionally setting draft mode can improve image decoding speed without any perceivable loss of quality. However, turning on draft mode does not have any effect if the scale factor is not below this threshold.\nDeprecated\nstatic let ignoreImageOrientation: CIRAWFilterOption\nA key for specifying whether to ignore the image orientation. The associated value is a Boolean value packaged as an NSNumber object. The default value is false. An image is usually loaded in its proper orientation, as long as the associated metadata records its orientation. For special purposes you might want to load the image in its physical orientation. The exact meaning of \"physical orientation” is dependent on the specific image.\nDeprecated\nstatic let imageOrientation: CIRAWFilterOption\nA key for the image orientation. The associated value is an integer value packaged as an NSNumber object. Valid values are in range 1...8 and follow the EXIF specification. The value is disregarded when the kCIIgnoreImageOrientationKey flag is set. You can change the orientation of the image by overriding this value. By changing this value you can rotate an image in 90-degree increments.\nDeprecated\nstatic let enableSharpening: CIRAWFilterOption\nA key for the sharpening state. The associated value must be an NSNumber object that specifies a BOOL value (true or false). The default is true. This option has no effect if the image used for initialization is not RAW.\nDeprecated\nstatic let enableChromaticNoiseTracking: CIRAWFilterOption\nA key for progressive chromatic noise tracking (based on ISO and exposure time). The associated value must be an NSNumber object that specifies a BOOL value (true or false). The default is true. This option has no effect if the image used for initialization is not RAW.\nDeprecated\nstatic let noiseReductionAmount: CIRAWFilterOption\nA key for the amount to reduce noise in the image. The associated value must be an NSNumber object that specifies a floating-point value between 0.0 and 1.0. The value has no effect if the image used for initialization is not RAW.\nDeprecated\nstatic let enableVendorLensCorrection: CIRAWFilterOption\nA key for whether to automatically correct for image distortion from known lenses.\nDeprecated\nstatic let luminanceNoiseReductionAmount: CIRAWFilterOption\nA key for the amount of noise reduction to apply to luminance data in the image.\nDeprecated\nstatic let colorNoiseReductionAmount: CIRAWFilterOption\nA key for the amount of noise reduction to apply to color data in the image.\nDeprecated\nstatic let noiseReductionSharpnessAmount: CIRAWFilterOption\nA key for the amount of sharpness enhancement to apply during noise reduction.\nDeprecated\nstatic let noiseReductionContrastAmount: CIRAWFilterOption\nA key for the amount of contrast enhancement to apply during noise reduction.\nDeprecated\nstatic let noiseReductionDetailAmount: CIRAWFilterOption\nA key for the amount of detail enhancement to apply during noise reduction.\nDeprecated\nstatic let boostShadowAmount: CIRAWFilterOption\nA key for the amount to boost the shadow areas of the image. The associated value must be an NSNumber object that specifies floating-point value. The value has no effect if the image used for initialization is not RAW.\nDeprecated\nlet kCIInputBiasKey: String\nA key for the simple bias value to use along with the exposure adjustment (kCIInputEVKey). The associated value must be an NSNumber object that specifies floating-point value. The value has no effect if the image used for initialization is not RAW.\nstatic let linearSpaceFilter: CIRAWFilterOption\nA key for the filter to apply to the image while it is temporarily in a linear color space as part of RAW image processing. The associated value must be a CIFilter object.\nDeprecated\nstatic let outputNativeSize: CIRAWFilterOption\nA key for the full native size of the original, non-transformed RAW image. The associated value is a CIVector object whose X and Y values are the image’s width and height. This key is read-only.\nDeprecated\nstatic let activeKeys: CIRAWFilterOption\nA key for the set of input keys available for use. The associated value is an NSSet object containing the set of input keys which may be used to affect the output image. (Depending on the input image type and the decoder version, some input keys may be unavailable.) This key is read-only.\nDeprecated"
  },
  {
    "title": "User Interface Control Options",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/user_interface_control_options",
    "html": "Overview\n\nYou can use these constants to specify the controls that you want associated with each user scenario. For example, for a filter that has many input parameters you can choose a small set of input parameters that the typical consumer can control and set the other input parameters to default values. For the same filter, however, you can choose to allow professional customers to control all the input parameters.\n\nTopics\nConstants\nlet kCIUIParameterSet: String\nThe set of input parameters to use. The associated value can be kCIUISetBasic, kCIUISetIntermediate, kCIUISetAdvanced, or kCIUISetDevelopment.\nlet kCIUISetBasic: String\nControls that are appropriate for a basic user scenario, that is, the minimum of settings to control the filter.\nlet kCIUISetIntermediate: String\nControls that are appropriate for an intermediate user scenario.\nlet kCIUISetAdvanced: String\nControls that are appropriate for an advanced user scenario.\nlet kCIUISetDevelopment: String\nControls that should be visible only for development purposes."
  },
  {
    "title": "Options for Applying a Filter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/options_for_applying_a_filter",
    "html": "Overview\n\nUse these constants only when creating a custom filter for which you are writing the kernel. For more information, see Core Image Programming Guide. The example on creating a custom filter shows how to use these options.\n\nTopics\nConstants\nlet kCIApplyOptionExtent: String\nThe size of the produced image. The associated value is a four-element array (NSArray) that specifies the x-value of the rectangle origin, the y-value of the rectangle origin, and the width and height.\nlet kCIApplyOptionDefinition: String\nThe domain of definition (DOD) of the produced image. The associated value is either a Core Image filter shape or a four-element array (NSArray) that specifies a rectangle.\nlet kCIApplyOptionUserInfo: String\nInformation needed by a callback. The associated value is an object that Core Image will pass to any callbacks invoked for that filter.\nlet kCIApplyOptionColorSpace: String\nThe color space of the produced image. The associated value must be an RGB CGColorSpace object. If not specified, the output of the kernel is in the working color space of the Core Image context used to render the image."
  },
  {
    "title": "Vector Quantity Attributes",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/vector_quantity_attributes",
    "html": "Topics\nConstants\nlet kCIAttributeTypePosition: String\nA two-dimensional location in the working coordinate space. (A 2-element vector type.)\nlet kCIAttributeTypeOffset: String\nAn offset. (A 2-element vector type.)\nlet kCIAttributeTypePosition3: String\nA three-dimensional location in the working coordinate space. (A 3-element vector type.)\nlet kCIAttributeTypeRectangle: String\nA Core Image vector that specifies the x and y values of the rectangle origin, and the width (w) and height (h) of the rectangle. The vector takes the form [x, y, w, h]. (A 4-element vector type.)"
  },
  {
    "title": "render(_:to:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437853-render",
    "html": "Parameters\nimage\n\nA Core Image image object.\n\nbuffer\n\nThe destination pixel buffer.\n\nSee Also\nRendering Images\nfunc createCGImage(CIImage, from: CGRect) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?, deferred: Bool) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object with deferred rendering.\nfunc createCGLayer(with: CGSize, info: CFDictionary?) -> CGLayer?\nCreates a CGLayer object from the provided parameters.\nDeprecated\nfunc render(CIImage, toBitmap: UnsafeMutableRawPointer, rowBytes: Int, bounds: CGRect, format: CIFormat, colorSpace: CGColorSpace?)\nRenders to the given bitmap.\nfunc render(CIImage, to: CVPixelBuffer, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into a pixel buffer.\nfunc render(CIImage, to: IOSurfaceRef, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into an IOSurface object.\nfunc render(CIImage, to: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?, bounds: CGRect, colorSpace: CGColorSpace)\nRenders a region of an image to a Metal texture."
  },
  {
    "title": "init(cglContext:pixelFormat:colorSpace:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1438137-init",
    "html": "Parameters\nctx\n\nA CGL context obtained by calling the CGL function CGLCreateContext(_:_:_:).\n\npf\n\nA CGL pixel format object either obtained from the system or created by calling a CGL function such as CGLChoosePixelFormat(_:_:_:). This parameter must be the same pixel format object used to create the CGL context. The pixel format object must be valid for the lifetime of the Core Image context. Don’t release the pixel format object until after you release the Core Image context.\n\nspace\n\nA color space object encapsulating color space information that is used to specify how color values are interpreted.\n\noptions\n\nA dictionary that contains options for creating a CIContext object. You can pass any of the keys defined in Context Options along with the appropriate value.\n\nDiscussion\n\nAfter calling this method, Core Image draws content into the surface (drawable object) attached to the CGL context. A CGL context is a macOS OpenGL context. For more information, see OpenGL Programming Guide for Mac.\n\nWhen you create a CIContext object using a CGL context, all OpenGL states set for the CGL context affect rendering to that context. That means that coordinate and viewport transformations set on the CGL context, as well as the vertex color, affect drawing to that context.\n\nFor best results, follow these guidelines when you use Core Image to render into an OpenGL context:\n\nEnsure that a single unit in the coordinate space of the OpenGL context represents a single pixel in the output device.\n\nThe Core Image coordinate space has the origin in the bottom-left corner of the screen. You should configure the OpenGL context in the same way.\n\nThe OpenGL context blending state is respected by Core Image. If the image you want to render contains translucent pixels, it’s best to enable blending using a blend function with the parameters GL_ONE, GL_ONE_MINUS_SRC_ALPHA, as shown in the following code example.\n\nCore Image manages its own internal OpenGL context that shares resources with the OpenGL context you specify. To enable resource sharing, use the following code:\n\nlet attr = [\n    NSOpenGLPFAAccelerated,\n    NSOpenGLPFANoRecovery,\n    NSOpenGLPFAColorSize, 32,\n    0\n    ].map {NSOpenGLPixelFormatAttribute($0)}\nlet pf = NSOpenGLPixelFormat(attributes: attr)!\nlet myCIContext = CIContext(CGLContext: CGLGetCurrentContext(),\n                            pixelFormat: pf.CGLPixelFormatObj,\n                            colorSpace: CGColorSpaceCreateDeviceRGB(),\n                            options: [:])\n\nSee Also\nCreating a Context for GPU-Based Rendering with OpenGL\ninit(eaglContext: EAGLContext)\nCreates a Core Image context from an EAGL context.\nDeprecated\ninit(eaglContext: EAGLContext, options: [CIContextOption : Any]?)\nCreates a Core Image context from an EAGL context using the specified options.\nDeprecated\ninit?(forOfflineGPUAt: UInt32)\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display.\nDeprecated\ninit?(forOfflineGPUAt: UInt32, colorSpace: CGColorSpace?, options: [CIContextOption : Any]?, sharedContext: CGLContextObj?)\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display, with the specified options.\nDeprecated\nRelated Documentation\ninit(cgContext: CGContext, options: [CIContextOption : Any]?)\nCreates a Core Image context from a Quartz context, using the specified options."
  },
  {
    "title": "localizedReferenceDocumentation(forFilterName:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437642-localizedreferencedocumentation",
    "html": "Parameters\nfilterName\n\nThe filter name.\n\nReturn Value\n\nA URL that specifies the location of the localized documentation, or nil if the filter does not provide localized reference documentation.\n\nDiscussion\n\nThe URL can be a local file or a remote document on a web server. Because filters created prior to OS X v10.5 could return nil, you should be make sure that your code handles this case gracefully.\n\nSee Also\nGetting Localized Information for Registered Filters\nclass func localizedName(forFilterName: String) -> String?\nReturns the localized name for the specified filter name.\nclass func localizedName(forCategory: String) -> String\nReturns the localized name for the specified filter category.\nclass func localizedDescription(forFilterName: String) -> String?\nReturns the localized description of a filter for display in the user interface."
  },
  {
    "title": "localizedName(forFilterName:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437697-localizedname",
    "html": "Parameters\nfilterName\n\nA filter name.\n\nReturn Value\n\nThe localized name for the filter.\n\nSee Also\nGetting Localized Information for Registered Filters\nclass func localizedName(forCategory: String) -> String\nReturns the localized name for the specified filter category.\nclass func localizedDescription(forFilterName: String) -> String?\nReturns the localized description of a filter for display in the user interface.\nclass func localizedReferenceDocumentation(forFilterName: String) -> URL?\nReturns the location of the localized reference documentation that describes the filter."
  },
  {
    "title": "init(image:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1624119-init",
    "html": "Parameters\nimage\n\nAn image containing the source data.\n\nReturn Value\n\nThe initialized image object, or nil if the object could not be initialized.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "outputImage",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438169-outputimage",
    "html": "See Also\nGetting Filter Parameters and Attributes\nvar name: String\nA name associated with a filter.\nvar isEnabled: Bool\nA Boolean value that determines whether the filter is enabled. Animatable.\nvar attributes: [String : Any]\nA dictionary of key-value pairs that describe the filter.\nvar inputKeys: [String]\nThe names of all input parameters to the filter.\nvar outputKeys: [String]\nThe names of all output parameters from the filter."
  },
  {
    "title": "init(cgImage:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437764-init",
    "html": "Parameters\nimage\n\nA Quartz 2D image (CGImage) object. For more information, see Quartz 2D Programming Guide and CGImage.\n\noptions\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithCGImage:options:\nCreates and returns an image object from a Quartz 2D image using the specified options."
  },
  {
    "title": "init(bitmapData:bytesPerRow:size:format:colorSpace:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437857-init",
    "html": "Parameters\nd\n\nThe bitmap data to use for the image. The data you supply must be premultiplied.\n\nbpr\n\nThe number of bytes per row.\n\nsize\n\nThe size of the image data.\n\nf\n\nA pixel format constant. See Pixel Formats.\n\nc\n\nThe color space that the image is defined in. It must be a Quartz 2D color space (CGColorSpace). Pass nil for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables).\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithBitmapData:bytesPerRow:size:format:colorSpace:\nCreates and returns an image object from bitmap data."
  },
  {
    "title": "init(cgImage:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437986-init",
    "html": "Parameters\nimage\n\nA Quartz 2D image (CGImage) object. For more information, see Quartz 2D Programming Guide and CGImage.\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithCGImage:\nCreates and returns an image object from a Quartz 2D image."
  },
  {
    "title": "init(color:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437947-init",
    "html": "Parameters\ncolor\n\nA color object.\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithColor:\nCreates and returns an image of infinite extent whose entire content is the specified color."
  },
  {
    "title": "empty()",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438023-empty",
    "html": "Return Value\n\nAn image object.\n\nSee Also\nRelated Documentation\nCore Image Programming Guide"
  },
  {
    "title": "startTask(toClear:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2875450-starttask",
    "html": "Parameters\ndestination\n\nThe CIRenderDestination to clear.\n\nerror\n\nPointer to an error object should the task fail.\n\nReturn Value\n\nThe asynchronous CIRenderTask for clearing the destination.\n\nDiscussion\n\nIf the destination's alphaMode is CIRenderDestinationAlphaMode.none, this command fills the entire destination with black (0, 0, 0, 1).\n\nIf the destination's alphaMode is CIRenderDestinationAlphaMode.premultiplied or CIRenderDestinationAlphaMode.unpremultiplied, this command fills the entire destination with clear (0, 0, 0, 0).\n\nSee Also\nCustomizing Render Destination\nfunc prepareRender(CIImage, from: CGRect, to: CIRenderDestination, at: CGPoint)\nAn optional call to warm up a CIContext so that subsequent calls to render with the same arguments run more efficiently.\nfunc startTask(toRender: CIImage, from: CGRect, to: CIRenderDestination, at: CGPoint) -> CIRenderTask\nRenders a portion of an image to a point in the destination.\nfunc startTask(toRender: CIImage, to: CIRenderDestination) -> CIRenderTask\nRenders an image to a destination so that point (0, 0) of the image maps to point (0, 0) of the destination."
  },
  {
    "title": "prepareRender(_:from:to:at:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2875428-preparerender",
    "html": "Parameters\nimage\n\nCIImage to prepare to render.\n\nfromRect\n\nA CGRect defining the region to render.\n\ndestination\n\nThe CIRenderDestination to which you are preparing to render.\n\natPoint\n\nThe CGPoint at which you are preparing to render.\n\nerror\n\nPointer to an error should preparation to render fail.\n\nReturn Value\n\nReturns true if preparation succeeded.\n\nDiscussion\n\nBy making this call, the Core Image framework ensures that any needed kernels are compiled, and any intermediate buffers are allocated and marked volatile up front.\n\nSee Also\nCustomizing Render Destination\nfunc startTask(toClear: CIRenderDestination) -> CIRenderTask\nFills the entire destination with black or clear depending on its alphaMode.\nfunc startTask(toRender: CIImage, from: CGRect, to: CIRenderDestination, at: CGPoint) -> CIRenderTask\nRenders a portion of an image to a point in the destination.\nfunc startTask(toRender: CIImage, to: CIRenderDestination) -> CIRenderTask\nRenders an image to a destination so that point (0, 0) of the image maps to point (0, 0) of the destination."
  },
  {
    "title": "priorityRequestLow",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption/1620424-priorityrequestlow",
    "html": "Discussion\n\nThe value for this key is an NSNumber object containing a Boolean value. If this value is true, use of the Core Image context from a background thread takes lower priority than GPU usage from the main thread, allowing your app to perform Core Image rendering without disturbing the frame rate of UI animations."
  },
  {
    "title": "outputPremultiplied",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption/1642216-outputpremultiplied",
    "html": "Discussion\n\nThe value for this key is an NSNumber object containing a Boolean value. The default value of true produces premultiplied output."
  },
  {
    "title": "workingColorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption/1437728-workingcolorspace",
    "html": "Discussion\n\nBy default, Core Image assumes that processing nodes are 128 bits-per-pixel, linear light, premultiplied RGBA floating-point values that use the GenericRGB color space. You can specify a different working color space by providing a Quartz 2D CGColorSpace object (CGColorSpace). Note that the working color space must be RGB based. If you have YUV data as input (or other data that is not RGB based), you can use ColorSync functions to convert to the working color space. (See Quartz 2D Programming Guide for information on creating and using CGColorSpace objects.)\n\nTo request that Core Image perform no color management, specify the NSNull object as the value for this key. Use this option for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables)."
  },
  {
    "title": "highQualityDownsample",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption/1437699-highqualitydownsample",
    "html": "Discussion\n\nThe value for this key is an NSNumber object containing a Boolean value. A value of true (the default in macOS) results in higher image quality and lower performance. In iOS and tvOS, the default value is false."
  },
  {
    "title": "cacheIntermediates",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption/1642217-cacheintermediates",
    "html": "Discussion\n\nThe value for this key is an NSNumber object containing a Boolean value. If this value is false, the context empties such buffers during and after renders. The default value is true."
  },
  {
    "title": "outputColorSpace",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption/1438052-outputcolorspace",
    "html": "Discussion\n\nBy default, Core Image uses the GenericRGB color space, which leaves color matching to the system. You can specify a different output color space by providing a Quartz 2D CGColorSpace object. (See Quartz 2D Programming Guide for information on creating and using CGColorSpace objects.)\n\nTo request that Core Image perform no color management, specify the NSNull object as the value for this key. Use this option for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables)."
  },
  {
    "title": "avDepthData",
    "url": "https://developer.apple.com/documentation/coreimage/ciimagerepresentationoption/2902265-avdepthdata",
    "html": "See Also\nRendering Images for Data or File Export\nfunc tiffRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in TIFF format.\nfunc jpegRepresentation(of: CIImage, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in JPEG format.\nfunc pngRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in PNG format.\nfunc heifRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in HEIF format.\nfunc writeTIFFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in TIFF format.\nfunc writeJPEGRepresentation(of: CIImage, to: URL, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in JPEG format.\nfunc writePNGRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in PNG format.\nfunc writeHEIFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in HEIF format.\nstatic let depthImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output depth data.\nstatic let disparityImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "depthImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciimagerepresentationoption/2902268-depthimage",
    "html": "See Also\nRendering Images for Data or File Export\nfunc tiffRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in TIFF format.\nfunc jpegRepresentation(of: CIImage, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in JPEG format.\nfunc pngRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in PNG format.\nfunc heifRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in HEIF format.\nfunc writeTIFFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in TIFF format.\nfunc writeJPEGRepresentation(of: CIImage, to: URL, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in JPEG format.\nfunc writePNGRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in PNG format.\nfunc writeHEIFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in HEIF format.\nstatic let avDepthData: CIImageRepresentationOption\noptions dictionary key for image export methods to represent data as AVDepthData.\nstatic let disparityImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "writeHEIFRepresentation(of:to:format:colorSpace:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2902266-writeheifrepresentation",
    "html": "Parameters\nimage\n\nThe image object to render.\n\nurl\n\nThe file URL at which to write the output HEIF file.\n\nformat\n\nThe pixel format for the output image.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the CGColorSpaceModel.rgb or CGColorSpaceModel.monochrome model and must be compatible with the specified pixel format.\n\noptions\n\nA dictionary with additional options for export.\n\nReturn Value\n\nIf true, file export succeeded. If false, examine the errorPtr parameter for possible failure reasons.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the clampedToExtent() method.\n\nSee Also\nRendering Images for Data or File Export\nfunc tiffRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in TIFF format.\nfunc jpegRepresentation(of: CIImage, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in JPEG format.\nfunc pngRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in PNG format.\nfunc heifRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in HEIF format.\nfunc writeTIFFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in TIFF format.\nfunc writeJPEGRepresentation(of: CIImage, to: URL, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in JPEG format.\nfunc writePNGRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in PNG format.\nstatic let avDepthData: CIImageRepresentationOption\noptions dictionary key for image export methods to represent data as AVDepthData.\nstatic let depthImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output depth data.\nstatic let disparityImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "writePNGRepresentation(of:to:format:colorSpace:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2866197-writepngrepresentation",
    "html": "Parameters\nimage\n\nThe image object to render.\n\nurl\n\nThe file URL at which to write the output PNG file.\n\nformat\n\nThe pixel format for the output image.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the CGColorSpaceModel.rgb or CGColorSpaceModel.monochrome model and must be compatible with the specified pixel format.\n\noptions\n\nA dictionary with additional options for export.\n\nReturn Value\n\nIf true, file export succeeded. If false, examine the errorPtr parameter for possible failure reasons.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the clampedToExtent() method.\n\nSee Also\nRendering Images for Data or File Export\nfunc tiffRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in TIFF format.\nfunc jpegRepresentation(of: CIImage, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in JPEG format.\nfunc pngRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in PNG format.\nfunc heifRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in HEIF format.\nfunc writeTIFFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in TIFF format.\nfunc writeJPEGRepresentation(of: CIImage, to: URL, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in JPEG format.\nfunc writeHEIFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in HEIF format.\nstatic let avDepthData: CIImageRepresentationOption\noptions dictionary key for image export methods to represent data as AVDepthData.\nstatic let depthImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output depth data.\nstatic let disparityImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "applyingGaussianBlur(sigma:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645897-applyinggaussianblur",
    "html": "Parameters\nsigma\n\nThe radius, in pixels, of the blur effect to apply.\n\nReturn Value\n\nAn image object representing the result of the operation.\n\nDiscussion\n\nCalling this method is equivalent to using the CIGaussianBlur filter with the specified radius. To use other blur effects, create a CIFilter object using one of the built-in filters from the CICategoryBlur category. For details, see Core Image Filter Reference.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "composited(over:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437837-composited",
    "html": "Parameters\ndest\n\nAn image to serve as the destination of the compositing operation.\n\nReturn Value\n\nAn image object representing the result of the compositing operation.\n\nDiscussion\n\nCalling this method is equivalent to using the CISourceOverCompositing filter. To use other compositing operations and blending modes, create a CIFilter object using one of the built-in filters from the CICategoryCompositeOperation category. For details, see Core Image Filter Reference.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "unpremultiplyingAlpha()",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645892-unpremultiplyingalpha",
    "html": "Return Value\n\nAn image object representing the result of the operation.\n\nDiscussion\n\nPremultiplied alpha speeds up the rendering of images, but some custom filter routines can be expressed more efficiently with non-premultiplied RGB values. Use this method if you need to apply such a filter to an image that has premultiplied alpha.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "settingAlphaOne(in:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645891-settingalphaone",
    "html": "Parameters\nextent\n\nThe rectangular area in the image to be set to full opacity.\n\nReturn Value\n\nAn image object representing the result of the operation.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "matchedFromWorkingSpace(to:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645898-matchedfromworkingspace",
    "html": "Parameters\ncolorSpace\n\nThe color space to be converted to. This color space must conform to the CGColorSpaceModel.rgb color space model.\n\nReturn Value\n\nAn image object representing the result of the color matching operation, or nil if the color spaces to be converted are not compatible.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "premultiplyingAlpha()",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645894-premultiplyingalpha",
    "html": "Return Value\n\nAn image object representing the result of the operation.\n\nDiscussion\n\nPremultiplied alpha speeds up the rendering of images, so Core Image filters require that input image data be premultiplied. If you have an image without premultiplied alpha that you want to feed into a filter, use this method before applying the filter.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "clamped(to:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1645893-clamped",
    "html": "Parameters\nrect\n\nThe rectangle, in image coordinates, to which to crop the image.\n\nReturn Value\n\nAn image object representing the result of the clamp operation.\n\nDiscussion\n\nCalling this method is equivalent to cropping the image (with the cropped(to:) method or the CICrop filter), then using the clampedToExtent() method (or the CIAffineClamp filter), which creates an image of infinite extent by repeating pixel colors from the edges of the cropped image.\n\nSee Also\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate."
  },
  {
    "title": "CIVector",
    "url": "https://developer.apple.com/documentation/coreimage/civector",
    "html": "Topics\nInitializing a Vector\ninit(values: UnsafePointer<CGFloat>, count: Int)\nInitializes a vector with the provided values.\ninit(x: CGFloat)\nInitializes the first position of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat)\nInitializes the first two positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat)\nInitializes the first three positions of a vector with the provided values.\ninit(x: CGFloat, y: CGFloat, z: CGFloat, w: CGFloat)\nInitializes four positions of a vector with the provided values.\ninit(string: String)\nInitializes a vector with values provided in a string representation.\ninit(cgAffineTransform: CGAffineTransform)\nInitializes a vector that is initialized with values provided by a CGAffineTransform structure.\ninit(cgPoint: CGPoint)\nInitializes a vector that is initialized with values provided by a CGPoint structure.\ninit(cgRect: CGRect)\nInitializes a vector that is initialized with values provided by a CGRect structure.\nGetting Values From a Vector\nfunc value(at: Int) -> CGFloat\nReturns a value from a specific position in the vector.\nvar count: Int\nThe number of items in the vector.\nvar x: CGFloat\nThe value located in the first position in the vector.\nvar y: CGFloat\nThe value located in the second position in the vector.\nvar z: CGFloat\nThe value located in the third position in the vector.\nvar w: CGFloat\nThe value located in the fourth position in the vector.\nvar stringRepresentation: String\nThe string representation of the vector.\nvar cgAffineTransformValue: CGAffineTransform\nThe values in the vector represented as an affine transform.\nvar cgPointValue: CGPoint\nThe values in the vector as a Core Graphics point structure.\nvar cgRectValue: CGRect\nThe values in the vector as a Core Graphics rectangle structure.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nSee Also\nFilter Parameters\nclass CIColor\nThe component values defining a color in a specific color space."
  },
  {
    "title": "init(name:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438255-init",
    "html": "Parameters\nname\n\nThe name of the filter. You must make sure the name is spelled correctly, otherwise your app will run but not produce any output images. For that reason, you should check for the existence of the filter after calling this method.\n\nReturn Value\n\nA CIFilter object whose input values are undefined.\n\nDiscussion\n\nIn macOS, after creating a filter with this method you must call setDefaults() or set parameters individually by calling setValue(_:forKey:). In iOS, the filter’s parameters are automatically set to default values.\n\nSee Also\nCreating a Filter\ninit?(name: String, parameters: [String : Any]?)\nCreates a CIFilter object for a specific kind of filter and initializes the input values.\nRelated Documentation\nImage Unit Tutorial\n+ filterWithName:keysAndValues:\nCreates a object for a specific kind of filter and initializes the input values with a nil-terminated list of arguments.\nCore Image Filter Reference\nCore Image Programming Guide"
  },
  {
    "title": "init(texture:size:flipped:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437880-init",
    "html": "Parameters\nname\n\nAn OpenGL texture. Because CIImage objects are immutable, the texture must remain unchanged for the life of the image object. See the discussion for more information.\n\nsize\n\nThe dimensions of the texture.\n\nflag\n\ntrue to have Core Image flip the coordinates of the texture vertically to convert between OpenGL and Core Image coordinate systems.\n\noptions\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nThe initialized image object.\n\nDiscussion\n\nWhen using a texture to create a CIImage object, the texture must be valid in the Core Image context (CIContext) that you draw the CIImage object into. This means that one of the following must be true:\n\nThe texture must be created using the CGLContext object that the CIContext is based on.\n\nThe context that the texture was created in must be shared with the CGLContext that the CIContextis based on.\n\nNote that textures do not have a retain and release mechanism. This means that your application must make sure that the texture exists for the life cycle of the image. When you no longer need the image, you can delete the texture.\n\nCore Image ignores the texture filtering and wrap modes (GL_TEXTURE_FILTER and GL_TEXTURE_WRAP) that you set through OpenGL. The filter and wrap modes are overridden by what the CISampler object specifies when you apply a filter to the CIImage object.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithTexture:size:flipped:options:\nCreates and returns an image object initialized with data supplied by an OpenGL texture."
  },
  {
    "title": "init(texture:size:flipped:colorSpace:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438015-init",
    "html": "Parameters\nname\n\nAn OpenGL texture. Because CIImage objects are immutable, the texture must remain unchanged for the life of the image object. See the discussion for more information.\n\nsize\n\nThe dimensions of the texture.\n\nflag\n\ntrue to have Core Image flip the coordinates of the texture vertically to convert between OpenGL and Core Image coordinate systems.\n\ncs\n\nThe color space that the image is defined in. This must be a Quartz color space (CGColorSpace). If the colorSpace value is nil, the image is not color matched. Pass nil for images that don’t contain color data (such as elevation maps, normal vector maps, and sampled function tables).\n\nReturn Value\n\nThe initialized image object.\n\nDiscussion\n\nWhen you use a texture to create a CIImage object, the texture must be valid in the Core Image context (CIContext) that you draw the CIImage object into. This means that one of the following must be true:\n\nThe texture must be created using the CGLContext object that the CIContext is based on.\n\nThe context that the texture was created in must be shared with the CGLContext that the CIContextis based on.\n\nNote that textures do not have a retain and release mechanism. This means that your application must make sure that the texture exists for the life cycle of the image. When you no longer need the image, you can delete the texture.\n\nCore Image ignores the texture filtering and wrap modes (GL_TEXTURE_FILTER and GL_TEXTURE_WRAP) that you set through OpenGL. The filter and wrap modes are overridden by what the CISampler object specifies when you apply a filter to the CIImage object.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithTexture:size:flipped:colorSpace:\nCreates and returns an image object initialized with data supplied by an OpenGL texture."
  },
  {
    "title": "init(ioSurface:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438181-init",
    "html": "Parameters\nsurface\n\nAn IOSurface object.\n\noptions\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nAn image object initialized with the data from the IOSurface.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "init(ioSurface:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438030-init",
    "html": "Parameters\nsurface\n\nAn IOSurface object.\n\nReturn Value\n\nAn image object initialized with the data from the IOSurface object.\n\nDiscussion\n\nAn IOSurface object is a framebuffer object that is suitable for sharing across process boundaries. You can use it to allow your app to move complex image decompression and drawing logic into a separate process for the purpose of increasing security.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "init(mtlTexture:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437890-init",
    "html": "Parameters\ntexture\n\nThe Metal texture from which to use image data.\n\noptions\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nThe initialized image object, or nil if the image could not be initialized.\n\nDiscussion\n\nTo render the image using Metal, use this image with a Metal-based CIContext object created with the init(mtlDevice:) method, and call the render(_:to:commandBuffer:bounds:colorSpace:) method to create an output image in another Metal texture object.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated"
  },
  {
    "title": "init(cvImageBuffer:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437617-init",
    "html": "Parameters\nimageBuffer\n\nA CVImageBuffer object in a supported pixel format constant. For more information, see Core Video Programming Guide and Core Video.\n\ndict\n\nA dictionary that contains options for creating an image object. (See Image Dictionary Keys.) The pixel format is supplied by the CVImageBuffer object.)\n\nReturn Value\n\nThe initialized image object.\n\nDiscussion\n\nThe imageBuffer parameter must be in one of the following formats:\n\nkCVPixelFormatType_32ARGB\n\nkCVPixelFormatType_422YpCbCr8\n\nkCVPixelFormatType_32BGRA\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithCVImageBuffer:options:\nCreates and returns an image object from the contents of CVImageBuffer object, using the specified options."
  },
  {
    "title": "init(contentsOf:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437867-init",
    "html": "Parameters\nurl\n\nThe location of the image file to read.\n\nd\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nThe initialized image object, or nil if the object could not be initialized.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithContentsOfURL:options:\nCreates and returns an image object from the contents of a file, using the specified options."
  },
  {
    "title": "init(cvImageBuffer:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1438012-init",
    "html": "Parameters\nimageBuffer\n\nA CVImageBuffer object in a supported pixel format constant. For more information, see Core Video Programming Guide and Core Video.\n\nReturn Value\n\nThe initialized image object.\n\nDiscussion\n\nThe imageBuffer parameter must be in one of the following formats:\n\nkCVPixelFormatType_32ARGB\n\nkCVPixelFormatType_422YpCbCr8\n\nkCVPixelFormatType_32BGRA\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithCVImageBuffer:\nCreates and returns an image object from the contents of CVImageBuffer object."
  },
  {
    "title": "init(contentsOf:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437908-init",
    "html": "Parameters\nurl\n\nThe location of the image file to read.\n\nReturn Value\n\nThe initialized image object, or nil if the object could not be initialized.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithContentsOfURL:\nCreates and returns an image object from the contents of a file."
  },
  {
    "title": "init(cgLayer:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage/1437687-init",
    "html": "Parameters\nlayer\n\nA CGLayer object. For more information see Quartz 2D Programming Guide and CGLayer.\n\nd\n\nA dictionary specifying image options. (See Image Dictionary Keys.)\n\nReturn Value\n\nThe initialized image object.\n\nSee Also\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nRelated Documentation\n+ imageWithCGLayer:options:\nCreates and returns an image object from the contents supplied by a CGLayer object, using the specified options."
  },
  {
    "title": "CIQRCodeDescriptor",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodedescriptor",
    "html": "Overview\n\nISO/IEC 18004 defines versions from 1 to 40, where a higher symbol version indicates a larger data-carrying capacity.\n\nQR codes can encode text, vCard contact information, or Uniform Resource Identifiers (URI).\n\nTopics\nCreating a Descriptor\ninit?(payload: Data, symbolVersion: Int, maskPattern: UInt8, errorCorrectionLevel: CIQRCodeDescriptor.ErrorCorrectionLevel)\nInitializes a descriptor that can be used as input to the CIBarcodeGenerator filter.\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload containing the data encoded in the QR code.\nvar symbolVersion: Int\nThe version of the QR code.\nvar maskPattern: UInt8\nThe QR code's mask pattern.\nvar errorCorrectionLevel: CIQRCodeDescriptor.ErrorCorrectionLevel\nThe QR code error correction level.\nError Correction Constants\nenum CIQRCodeDescriptor.ErrorCorrectionLevel\nConstants that indicate the percentage of the symbol dedicated to error correction.\nRelationships\nInherits From\nCIBarcodeDescriptor\nSee Also\nBarcode Descriptions\nclass CIBarcodeDescriptor\nAn abstract base class that represents a machine readable code's attributes.\nclass CIAztecCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents an Aztec code symbol.\nclass CIPDF417CodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a PDF 417 symbol.\nclass CIDataMatrixCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a Data Matrix code symbol."
  },
  {
    "title": "CIImageAccumulator",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageaccumulator",
    "html": "Overview\n\nThe CIImageAccumulator class enables feedback-based image processing for such things as iterative painting operations or fluid dynamics simulations. You use CIImageAccumulator objects in conjunction with other Core Image classes, such as CIFilter, CIImage, CIVector, and CIContext, to take advantage of the built-in Core Image filters when processing images.\n\nTopics\nInitializing an Image Accumulator\ninit?(extent: CGRect, format: CIFormat)\nInitializes an image accumulator with the specified extent and pixel format.\ninit?(extent: CGRect, format: CIFormat, colorSpace: CGColorSpace)\nInitializes an image accumulator with the specified extent, pixel format, and color space.\nSetting an Image\nfunc setImage(CIImage)\nSets the contents of the image accumulator to the contents of the specified image object.\nfunc setImage(CIImage, dirtyRect: CGRect)\nUpdates an image accumulator with a subregion of an image object.\nObtaining Data From an Image Accumulator\nvar extent: CGRect\nThe extent of the image associated with the image accumulator.\nvar format: CIFormat\nThe pixel format of the image accumulator.\nfunc image() -> CIImage\nReturns the current contents of the image accumulator.\nResetting an Accumulator\nfunc clear()\nResets the accumulator, discarding any pending updates and the current content.\nRelationships\nInherits From\nNSObject"
  },
  {
    "title": "init(source:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorkernel/1438143-init",
    "html": "Parameters\nstring\n\nA program in the Core Image Kernel Language that contains a single routine marked using the kernel keyword.\n\nReturn Value\n\nA new color kernel object, or nil if the specified source code does not contain a valid color kernel routine.\n\nDiscussion\n\nThis method is similar to the init(source:) method of the superclass CIKernel, but creates only color kernels. Use this method when you want to ensure that the type of kernel object returned (if any) is always CIColorKernel."
  },
  {
    "title": "draw(_:at:from:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1473521-draw",
    "html": "Deprecated\n\nInstead use draw(_:in:from:).\n\nParameters\nim\n\nA Core Image image object.\n\np\n\nThe point in the context destination to draw to.\n\nsrc\n\nThe region of the image to draw.\n\nDiscussion\n\nThis method because it is ambiguous as to the units of the dimensions and won’t work as expected in a high-resolution environment which is why you should use drawImage:inRect:fromRect: instead.\n\nOn iOS platforms, this method draws the image onto a render buffer for the OpenGL ES context. Use this method only if the CIContext object is created with contextWithEAGLContext:, and hence, you are rendering to a CAEAGLLayer.\n\nSee Also\nDrawing Images\nfunc draw(CIImage, in: CGRect, from: CGRect)\nRenders a region of an image to a rectangle in the context destination."
  },
  {
    "title": "render(_:to:bounds:colorSpace:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437835-render",
    "html": "Parameters\nimage\n\nA Core Image image object.\n\nbuffer\n\nThe destination pixel buffer.\n\nr\n\nThe rectangle in the destination pixel buffer to draw into.\n\ncs\n\nThe color space of the destination pixel buffer.\n\nSee Also\nRendering Images\nfunc createCGImage(CIImage, from: CGRect) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?, deferred: Bool) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object with deferred rendering.\nfunc createCGLayer(with: CGSize, info: CFDictionary?) -> CGLayer?\nCreates a CGLayer object from the provided parameters.\nDeprecated\nfunc render(CIImage, toBitmap: UnsafeMutableRawPointer, rowBytes: Int, bounds: CGRect, format: CIFormat, colorSpace: CGColorSpace?)\nRenders to the given bitmap.\nfunc render(CIImage, to: CVPixelBuffer)\nRenders an image into a pixel buffer.\nfunc render(CIImage, to: IOSurfaceRef, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into an IOSurface object.\nfunc render(CIImage, to: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?, bounds: CGRect, colorSpace: CGColorSpace)\nRenders a region of an image to a Metal texture."
  },
  {
    "title": "render(_:to:bounds:colorSpace:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437778-render",
    "html": "Parameters\nimage\n\nA Core Image image object.\n\nsurface\n\nThe destination IOSurface object.\n\nr\n\nThe rectangle in the destination IOSurface object to draw into.\n\ncs\n\nThe color space of the destination IOSurface object.\n\nSee Also\nRendering Images\nfunc createCGImage(CIImage, from: CGRect) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?, deferred: Bool) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object with deferred rendering.\nfunc createCGLayer(with: CGSize, info: CFDictionary?) -> CGLayer?\nCreates a CGLayer object from the provided parameters.\nDeprecated\nfunc render(CIImage, toBitmap: UnsafeMutableRawPointer, rowBytes: Int, bounds: CGRect, format: CIFormat, colorSpace: CGColorSpace?)\nRenders to the given bitmap.\nfunc render(CIImage, to: CVPixelBuffer)\nRenders an image into a pixel buffer.\nfunc render(CIImage, to: CVPixelBuffer, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into a pixel buffer.\nfunc render(CIImage, to: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?, bounds: CGRect, colorSpace: CGColorSpace)\nRenders a region of an image to a Metal texture."
  },
  {
    "title": "render(_:to:commandBuffer:bounds:colorSpace:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1438026-render",
    "html": "Parameters\nimage\n\nA Core Image image object.\n\ntexture\n\nThe destination Metal texture object.\n\ncommandBuffer\n\nThe Metal command buffer into which to schedule Core Image rendering tasks.\n\nbounds\n\nThe rectangle in the destination texture to draw into.\n\ncolorSpace\n\nThe color space of the destination texture.\n\nDiscussion\n\nIf you specify nil for the commandBuffer parameter, Core Image manages its own Metal command buffer. To combine Core Image rendering with other Metal rendering tasks—for example, to use Core Image filters on textures whose content is generated by a Metal render-to-texture operation, or to use Core Image output later in the same Metal rendering pass—pass the same MTLCommandBuffer object you use for those tasks.\n\nRendering to a Metal texture requires a Metal-based context created with the init(mtlDevice:) or init(mtlDevice:options:) method. Calling this method on any other context raises an exception. This method renders only to Metal textures whose texture type is MTLTextureType.type2D and whose sampleCount value is 1.\n\nSee Also\nRendering Images\nfunc createCGImage(CIImage, from: CGRect) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?, deferred: Bool) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object with deferred rendering.\nfunc createCGLayer(with: CGSize, info: CFDictionary?) -> CGLayer?\nCreates a CGLayer object from the provided parameters.\nDeprecated\nfunc render(CIImage, toBitmap: UnsafeMutableRawPointer, rowBytes: Int, bounds: CGRect, format: CIFormat, colorSpace: CGColorSpace?)\nRenders to the given bitmap.\nfunc render(CIImage, to: CVPixelBuffer)\nRenders an image into a pixel buffer.\nfunc render(CIImage, to: CVPixelBuffer, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into a pixel buffer.\nfunc render(CIImage, to: IOSurfaceRef, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into an IOSurface object."
  },
  {
    "title": "render(_:toBitmap:rowBytes:bounds:format:colorSpace:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437897-render",
    "html": "Parameters\nim\n\nA Core Image image object.\n\ndata\n\nStorage for the bitmap data.\n\nrb\n\nThe bytes per row.\n\nr\n\nThe bounds of the bitmap data.\n\nf\n\nThe format of the bitmap data.\n\ncs\n\nThe color space for the data. Pass NULL if you want to use the output color space of the context.\n\nSee Also\nRendering Images\nfunc createCGImage(CIImage, from: CGRect) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?, deferred: Bool) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object with deferred rendering.\nfunc createCGLayer(with: CGSize, info: CFDictionary?) -> CGLayer?\nCreates a CGLayer object from the provided parameters.\nDeprecated\nfunc render(CIImage, to: CVPixelBuffer)\nRenders an image into a pixel buffer.\nfunc render(CIImage, to: CVPixelBuffer, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into a pixel buffer.\nfunc render(CIImage, to: IOSurfaceRef, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into an IOSurface object.\nfunc render(CIImage, to: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?, bounds: CGRect, colorSpace: CGColorSpace)\nRenders a region of an image to a Metal texture."
  },
  {
    "title": "Filter Parameter Keys",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/filter_parameter_keys",
    "html": "Overview\n\nThese keys represent some of the most commonly used input parameters. A filter can use other kinds of input parameters.\n\nTopics\nConstants\nlet kCIOutputImageKey: String\nA key for the CIImage object produced by a filter.\nlet kCIInputBackgroundImageKey: String\nA key for the CIImage object to use as a background image.\nlet kCIInputImageKey: String\nA key for the CIImage object to use as an input image. For filters that also use a background image, this key refers to the foreground image.\nlet kCIInputTimeKey: String\nA key for z scalar value (NSNumber) that specifies a time.\nlet kCIInputDepthImageKey: String\nA key for an image with depth values.\nlet kCIInputDisparityImageKey: String\nA key for an image with disparity values.\nlet kCIInputTransformKey: String\nA key for an NSAffineTransform object that specifies a transformation to apply.\nlet kCIInputScaleKey: String\nA key for a scalar value (NSNumber) that specifies the amount of the effect.\nlet kCIInputAspectRatioKey: String\nA key for a scalar value (NSNumber) that specifies a ratio.\nlet kCIInputCenterKey: String\nA key for a CIVector object that specifies the center of the area, as x and y- coordinates, to be filtered.\nlet kCIInputRadiusKey: String\nA key for a scalar value (NSNumber) that specifies the distance from the center of an effect.\nlet kCIInputAngleKey: String\nA key for a scalar value (NSNumber) that specifies an angle.\nlet kCIInputRefractionKey: String\nA key for a scalar value (NSNumber) that specifies the index of refraction of the material (such as glass) used in the effect.\nlet kCIInputWidthKey: String\nA key for a scalar value (NSNumber) that specifies the width of the effect.\nlet kCIInputSharpnessKey: String\nA key for a scalar value (NSNumber) that specifies the amount of sharpening to apply.\nlet kCIInputIntensityKey: String\nA key for a scalar value (NSNumber) that specifies an intensity value.\nlet kCIInputEVKey: String\nA key for a scalar value (NSNumber) that specifies how many F-stops brighter or darker the image should be.\nlet kCIInputSaturationKey: String\nA key for a scalar value (NSNumber) that specifies the amount to adjust the saturation.\nlet kCIInputColorKey: String\nA key for a CIColor object that specifies a color value.\nlet kCIInputBrightnessKey: String\nA key for a scalar value (NSNumber) that specifies a brightness level.\nlet kCIInputContrastKey: String\nA key for a scalar value (NSNumber) that specifies a contrast level.\nlet kCIInputWeightsKey: String\nA key for a CIVector object that describes a weight matrix for use with a convolution filter.\nlet kCIInputGradientImageKey: String\nA key for a CIImage object that specifies an environment map with alpha. Typically, this image contains highlight and shadow.\nlet kCIInputMaskImageKey: String\nA key for a CIImage object to use as a mask.\nlet kCIInputShadingImageKey: String\nA key for a CIImage object that specifies an environment map with alpha values. Typically this image contains highlight and shadow.\nlet kCIInputTargetImageKey: String\nA key for a CIImage object that is the target image for a transition.\nlet kCIInputExtentKey: String\nA key for a CIVector object that specifies a rectangle that defines the extent of the effect.\nlet kCIInputVersionKey: String\nA key for an NSNumber object that specifies a version number.\nstatic let baselineExposure: CIRAWFilterOption\nA key for an NSNumber object containing a float that expresses the amount of baseline exposure applied to an image.\nDeprecated\nstatic let disableGamutMap: CIRAWFilterOption\nA key for an NSNumber object containing a Boolean value that determines whether or not to disable gamut mapping.\nDeprecated\nstatic let moireAmount: CIRAWFilterOption\nA key for an NSNumber object containing a double that expresses the amount of moiré reduction applied to an image.\nDeprecated"
  },
  {
    "title": "init()",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1642212-init",
    "html": "Return Value\n\nAn initialized Core Image context.\n\nDiscussion\n\nIf you create a context without specifying a rendering destination, Core Image automatically chooses and internally manages a rendering destination based on the current device’s capabilities. You cannot use a context without an explicit destination for the methods listed in Drawing Images. Instead, use the methods listed in Rendering Images.\n\nTo specify additional options for the context, use the init(options:) initializer instead.\n\nSee Also\nCreating a Context Without Specifying a Destination\ninit(options: [CIContextOption : Any]?)\nInitializes a context without a specific rendering destination, using the specified options."
  },
  {
    "title": "createCGImage(_:from:format:colorSpace:deferred:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1642211-createcgimage",
    "html": "Parameters\nimage\n\nA Core Image image object.\n\nfromRect\n\nThe region of the image to render.\n\nformat\n\nThe pixel format of the image.\n\ncolorSpace\n\nThe color space for the output image. This color space must conform to either the CGColorSpaceModel.rgb or CGColorSpaceModel.monochrome model and be compatible with the specified pixel format.\n\ndeferred\n\nIf true, Core Image delays rendering until the output Quartz 2D image is itself to be rendered. If false, Core Image renders to the output Quartz 2D image immediately.\n\nReturn Value\n\nA Quartz 2D image. You are responsible for releasing the returned image when you no longer need it.\n\nDiscussion\n\nRenders a region of an image into a temporary buffer using the context, and then creates and returns a Quartz 2D image with the results.\n\nSee Also\nRendering Images\nfunc createCGImage(CIImage, from: CGRect) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGLayer(with: CGSize, info: CFDictionary?) -> CGLayer?\nCreates a CGLayer object from the provided parameters.\nDeprecated\nfunc render(CIImage, toBitmap: UnsafeMutableRawPointer, rowBytes: Int, bounds: CGRect, format: CIFormat, colorSpace: CGColorSpace?)\nRenders to the given bitmap.\nfunc render(CIImage, to: CVPixelBuffer)\nRenders an image into a pixel buffer.\nfunc render(CIImage, to: CVPixelBuffer, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into a pixel buffer.\nfunc render(CIImage, to: IOSurfaceRef, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into an IOSurface object.\nfunc render(CIImage, to: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?, bounds: CGRect, colorSpace: CGColorSpace)\nRenders a region of an image to a Metal texture."
  },
  {
    "title": "init(cgContext:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437864-init",
    "html": "Parameters\nctx\n\nA Quartz graphics context (CGContext object) either obtained from the system or created using a Quartz function such as init(data:width:height:bitsPerComponent:bytesPerRow:space:bitmapInfo:). See Quartz 2D Programming Guide for information on creating Quartz graphics contexts.\n\ndict\n\nA dictionary that contains color space information. You can pass any of the keys defined in Context Options along with the appropriate value.\n\nDiscussion\n\nAfter calling this method, Core Image draws content to the specified Quartz graphics context.\n\nWhen you create a CIContext object using a Quartz graphics context, any transformations that are already set on the Quartz graphics context affect drawing to that context.\n\nNote\n\nTo obtain a Core Image context for the current AppKit drawing context in macOS, use the NSGraphicsContext ciContext property."
  },
  {
    "title": "init(options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1438261-init",
    "html": "Parameters\noptions\n\nA dictionary containing options for the context. For applicable keys and values, see Context Options.\n\nReturn Value\n\nAn initialized Core Image context.\n\nDiscussion\n\nIf you create a context without specifying a rendering destination, Core Image automatically chooses and internally manages a rendering destination based on the current device’s capabilities and your settings in the options dictionary. You cannot use a context without an explicit destination for the methods listed in Drawing Images. Instead, use the methods listed in Rendering Images.\n\nThe options dictionary defines behaviors for the context, such as color space and rendering quality. For example, to create a CPU-based context, use the useSoftwareRenderer key.\n\nSee Also\nCreating a Context Without Specifying a Destination\ninit()\nInitializes a context without a specific rendering destination, using default options."
  },
  {
    "title": "User Interface Options",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/user_interface_options",
    "html": "Topics\nConstants\nlet IKUISizeFlavor: String\nA key for the size of the controls in a filter view. The associated value can be IKUISizeMini, IKUISizeSmall, or IKUISizeRegular.\nlet IKUISizeMini: String\nControls whose size is NSMiniControlSize.\nlet IKUISizeSmall: String\nControls whose size is NSSmallControlSize.\nlet IKUISizeRegular: String\nControls whose size is NSRegularControlSize.\nlet IKUImaxSize: String\nControls whose dimensions are the maximum allowable for the filter view. A width or height of 0 indicates that dimension of the view is not restricted. If the size requested is too small, the filter is expected to return a view as small as possible. It is up to the client to verify that the returned view fits into the context.\nlet IKUIFlavorAllowFallback: String\nSubstitute controls of another size. The associated value is a Boolean value. If the filter cannot provide a view for the requested size and a fallback is allowed, the filter can use controls of a different size."
  },
  {
    "title": "Color Effect Filters",
    "url": "https://developer.apple.com/documentation/coreimage/color_effect_filters",
    "html": "Overview\n\ncolorCrossPolynomial()\n\ncolorCube()\n\ncolorCubesMixedWithMask()\n\ncolorCubeWithColorSpace()\n\ncolorCurves()\n\ncolorInvert()\n\ncolorMap()\n\ncolorMonochrome()\n\ncolorPosterize()\n\nconvertLabToRGBFilter\n\nconvertRGBtoLabFilter\n\ndither()\n\ndocumentEnhancer()\n\nfalseColor()\n\nlabDeltaE()\n\nmaskToAlpha()\n\nmaximumComponent()\n\nminimumComponent()\n\npaletteCentroid()\n\npalettize()\n\nphotoEffectChrome()\n\nphotoEffectFade()\n\nphotoEffectInstant()\n\nphotoEffectMono()\n\nphotoEffectNoir()\n\nphotoEffectProcess()\n\nphotoEffectTonal()\n\nphotoEffectTransfer()\n\nsepiaTone()\n\nthermal()\n\nvignette()\n\nvignetteEffect()\n\nxRay()\n\nSee Also\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nDistortion Filters\nApply distortion to images.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nGradient Filters\nGenerate linear and radial gradients.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nReduction Filters\nReduction Filters\nSharpening Filters\nApply sharpening to images.\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTile Effect Filters\nProduce tiled images from source images.\nTransition Filters\nTransition between two images by using effects including page curl and swipe."
  },
  {
    "title": "Image Attribute Keys",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/image_attribute_keys",
    "html": "Topics\nConstants\nlet kCIAttributeTypeImage: String\nA CIImage object.\nlet kCIAttributeTypeTransform: String\nAn CGAffineTransform is associated with attribute."
  },
  {
    "title": "apply(_:arguments:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438077-apply",
    "html": "Parameters\nk\n\nA CIKernel object that contains a kernel function.\n\nargs\n\nThe arguments that are type compatible with the function signature of the kernel function.\n\ndict\n\nA dictionary that contains options (key-value pairs) to control how the kernel function is evaluated.\n\nReturn Value\n\nThe CIImage object produced by a filter.\n\nDiscussion\n\nIf you are implementing a custom filter, this method needs to be called from within the outputImage method in order to apply your kernel function to the CIImage object. You can pass any of the keys defined in Options for Applying a Filter, along with appropriate values, into the options dictionary.\n\nSee Also\nRelated Documentation\n- apply:\nProduces a object by applying a kernel function."
  },
  {
    "title": "CIFalseColor",
    "url": "https://developer.apple.com/documentation/coreimage/cifalsecolor",
    "html": "Topics\nInstance Properties\ncolor0\nThe first color to use for the color ramp.\n\nRequired\n\ncolor1\nThe second color to use for the color ramp.\n\nRequired\n\ninputImage\nThe image to use as an input image.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ falseColorFilter\nReplaces an image’s colors with specified colors."
  },
  {
    "title": "filterArray(fromSerializedXMP:inputImageExtent:error:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438237-filterarray",
    "html": "Parameters\nxmpData\n\nThe XMP data created previously by calling serializedXMP(from:inputImageExtent:).\n\nextent\n\nThe extent of the image from which the XMP data was extracted.\n\noutError\n\nThe address of an NSError object for receiving errors, otherwise nil.\n\nSee Also\nSerializing and Deserializing Filters\nclass func serializedXMP(from: [CIFilter], inputImageExtent: CGRect) -> Data?\nSerializes filter parameters into XMP form that is suitable for embedding in an image.\nDeprecated"
  },
  {
    "title": "view(forUIConfiguration:excludedKeys:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1427521-view",
    "html": "Parameters\ninUIConfiguration\n\nA dictionary that contains values for the IKUISizeFlavor and kCIUIParameterSet keys. For allowed values for the IKUISizeFlavor key, see User Interface Options. For allowed values for the kCIUIParameterSet key, see User Interface Control Options.\n\ninKeys\n\nAn array of the input keys for which you do not want to provide a user interface. Pass nil if you want all input keys to be represented in the user interface.\n\nReturn Value\n\nAn IKFilterUIView object.\n\nDiscussion\n\nCalling this method to receive a view for a filter causes the CIFilter class to invoke the provideView(forUIConfiguration:excludedKeys:) method. If you override provideView(forUIConfiguration:excludedKeys:) the user interface is created by your filter subclass. Otherwise, Core Image automatically generates the user interface based on the filter keys and attributes.\n\nYour app can retrieve a view whose control sizes complement the size of user interface elements already used in the application. It is also possible to choose which filter input parameters appear in the view. Consumer applications, for example, may want to show a small, basic set of input parameters whereas professional applications may want to provide access to all input parameters.\n\nWhen you request a user interface for a parameter set, all keys for that set and below are included. For example, the advanced set consists of all parameters in the basic, intermediate and advanced sets. The development set should contain parameters that are either experimental or for debugging purposes. You should use them only during the development of filters and client applications, and not in a shipping product.\n\nThe controls in the view use bindings to set the values of the filter. See Cocoa Bindings Programming Topics if you are unfamiliar with bindings."
  },
  {
    "title": "localizedDescription(forFilterName:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437591-localizeddescription",
    "html": "Parameters\nfilterName\n\nThe filter name.\n\nReturn Value\n\nThe localized description of the filter.\n\nSee Also\nGetting Localized Information for Registered Filters\nclass func localizedName(forFilterName: String) -> String?\nReturns the localized name for the specified filter name.\nclass func localizedName(forCategory: String) -> String\nReturns the localized name for the specified filter category.\nclass func localizedReferenceDocumentation(forFilterName: String) -> URL?\nReturns the location of the localized reference documentation that describes the filter."
  },
  {
    "title": "localizedName(forCategory:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438057-localizedname",
    "html": "Parameters\ncategory\n\nA filter category.\n\nReturn Value\n\nThe localized name for the filter category.\n\nSee Also\nGetting Localized Information for Registered Filters\nclass func localizedName(forFilterName: String) -> String?\nReturns the localized name for the specified filter name.\nclass func localizedDescription(forFilterName: String) -> String?\nReturns the localized description of a filter for display in the user interface.\nclass func localizedReferenceDocumentation(forFilterName: String) -> URL?\nReturns the location of the localized reference documentation that describes the filter."
  },
  {
    "title": "serializedXMP(from:inputImageExtent:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438006-serializedxmp",
    "html": "Parameters\nfilters\n\nThe array of filters to serialize. See Discussion for the filters that can be serialized.\n\nextent\n\nThe extent of the input image to the filter.\n\nDiscussion\n\nAt this time the only filters classes that can be serialized using this method are, CIAffineTransform, CICrop, and the filters returned by the CIImage methods autoAdjustmentFilters() and autoAdjustmentFilters(options:). The parameters of other filter classes will not be serialized.\n\nSee Also\nSerializing and Deserializing Filters\nclass func filterArray(fromSerializedXMP: Data, inputImageExtent: CGRect, error: NSErrorPointer) -> [CIFilter]\nReturns an array of filter objects de-serialized from XMP data.\nDeprecated"
  },
  {
    "title": "CIColor",
    "url": "https://developer.apple.com/documentation/coreimage/cicolor",
    "html": "Overview\n\nYou use CIColor objects in conjunction with other Core Image classes, such as CIFilter, CIContext, and CIImage, to take advantage of the built-in Core Image filters when processing images.\n\nA color space defines a one-, two-, three-, or four-dimensional environment whose color components represent intensity values. A color component is also referred to as a color channel. An RGB color space, for example, is a three-dimensional color space whose stimuli are the red, green, and blue intensities that make up a given color. Regardless of the color space, in Core Image, color values range from 0.0 to 1.0, with 0.0 representing an absence of that component (0 percent) and 1.0 representing 100 percent.\n\nColors also have an alpha component, which represents the opacity of the color, with 0.0 meaning completely transparent and 1.0 meaning completely opaque. If a color does not have an explicit alpha component, Core Image paints the color as if the alpha component equals 1.0. You always provide unpremultiplied color components to Core Image, and Core Image then provides unpremultiplied color components to you. Core Image premultiplies each color component with the alpha value in order to optimize calculations. For more information on premultiplied alpha values, see Core Image Programming Guide.\n\nTopics\nInitializing Color Objects\ninit(cgColor: CGColor)\nInitializes a Core Image color object with a Core Graphics color.\ninit(color: UIColor)\nInitializes a Core Image color object using a UIKit (or AppKit) color object.\ninit(red: CGFloat, green: CGFloat, blue: CGFloat, alpha: CGFloat)\nInitializes a Core Image color object with the specified red, green, blue, and alpha component values.\ninit?(red: CGFloat, green: CGFloat, blue: CGFloat, colorSpace: CGColorSpace)\nInitializes a Core Image color object with the specified red, green, and blue component values as measured in the specified color space.\ninit?(red: CGFloat, green: CGFloat, blue: CGFloat, alpha: CGFloat, colorSpace: CGColorSpace)\nInitializes a Core Image color object with the specified red, green, blue, and alpha component values as measured in the specified color space.\nCreating Color Objects\ninit(red: CGFloat, green: CGFloat, blue: CGFloat)\nCreates a color object using the specified RGB color component values\ninit(string: String)\nCreates a color object using the RGBA color component values specified by a string.\nGetting Color Components\nvar colorSpace: CGColorSpace\nThe Quartz 2D color space associated with the color.\nvar components: UnsafePointer<CGFloat>\nThe color components of the color.\nvar numberOfComponents: Int\nReturns the number of color components in the color.\nvar red: CGFloat\nThe unpremultiplied red component of the color.\nvar green: CGFloat\nThe unpremultiplied green component of the color.\nvar blue: CGFloat\nThe unpremultiplied blue component of the color.\nvar alpha: CGFloat\nThe alpha value of the color.\nvar stringRepresentation: String\nA formatted string that specifies the components of the color.\nCreating a CIColor Object with Preset Components\nclass var black: CIColor\nReturns a color object whose RGB values are all 0.0 and whose alpha value is 1.0.\nclass var blue: CIColor\nReturns a color object whose RGB values are 0.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var clear: CIColor\nReturns a color object whose RGB and alpha values are all 0.0.\nclass var cyan: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 1.0 and whose alpha value is 1.0.\nclass var gray: CIColor\nReturns a color object whose RGB values are all 0.5 and whose alpha value is 1.0.\nclass var green: CIColor\nReturns a color object whose RGB values are 0.0, 1.0, and 0.0 and whose alpha value is 1.0.\nclass var magenta: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 1.0 and whose alpha value is 1.0.\nclass var red: CIColor\nReturns a color object whose RGB values are 1.0, 0.0, and 0.0 and whose alpha value is 1.0.\nclass var white: CIColor\nReturns a color object whose RGB values are all 1.0 and whose alpha value is 1.0.\nclass var yellow: CIColor\nReturns a color object whose RGB values are 1.0, 1.0, and 0.0 and whose alpha value is 1.0.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nSee Also\nFilter Parameters\nclass CIVector\nA container for coordinate values, direction vectors, matrices, and other non-scalar values, typically used in Core Image for filter parameters."
  },
  {
    "title": "setDefaults()",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437902-setdefaults",
    "html": "Discussion\n\nInput values whose default values are not defined are left unchanged."
  },
  {
    "title": "workingFormat",
    "url": "https://developer.apple.com/documentation/coreimage/cicontextoption/1437788-workingformat",
    "html": "Discussion\n\nThe value for this key is an NSNumber object containing a CIFormat value. The default working format is RGBA8 for CPU rendering and RGBAf for GPU rendering. For greater color precision, GPU rendering also supports the RGBAh format, but this format requires twice as much memory and can be used only with color management enabled."
  },
  {
    "title": "CIFilterConstructor",
    "url": "https://developer.apple.com/documentation/coreimage/cifilterconstructor",
    "html": "Overview\n\nObjects implementing this protocol are called filter constructors—they produce new instances of CIFilter subclasses when filters are requested by name. You can create a filter constructor to provide new, custom filters that other Core Image clients can discover using the CIFilter class. Normally, you create and register custom filters by packaging them as Image Units (see Packaging and Loading Image Units), but you can use this protocol to provide new filters within your app that are compositions of existing filters.\n\nTo provide custom filters using this protocol, you must:\n\nCreate your custom filters as CIFilter subclasses.\n\nCreate a class that implements this protocol to vend instances of the appropriate CIFilter subclasses when requested.\n\nCall the CIFilter class method registerName(_:constructor:classAttributes:) for each custom filter, providing the filter’s name, an instance of your filter constructor class, and information about the filter’s attributes.\n\nTopics\nProviding Filter Objects\nfunc filter(withName: String) -> CIFilter?\nReturns a filter object specified by name.\n\nRequired\n\nRelationships\nConforming Types\nCIFilterGenerator\nSee Also\nImage Units\nclass CIPlugIn\nThe mechanism for loading image units in macOS.\nclass CIFilterGenerator\nAn object that creates and configures chains of individual image filters.\nprotocol CIPlugInRegistration\nThe interface for loading Core Image image units."
  },
  {
    "title": "CIRAWFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cirawfilter",
    "html": "Overview\n\nUse this class to generate a CIImage object based on the configuration parameters you provide.\n\nYou can use this object in conjunction with other Core Image classes—such as CIFilter and CIContext—to take advantage of the built-in Core Image filters when processing images or writing custom filters.\n\nYou can also query this object to find out about the supported camera models, decoders, and filters.\n\nTopics\nCreating a filter\ninit?(cvPixelBuffer: CVPixelBuffer, properties: [AnyHashable : Any])\nCreates a RAW filter from the pixel buffer and its properties that you specify.\ninit?(imageData: Data, identifierHint: String?)\nCreates a RAW filter from the image data and type hint that you specify.\ninit?(imageURL: URL)\nCreates a RAW filter from the image at the URL location that you specify.\nInspecting supported camera models, decoders, and filters\nclass var supportedCameraModels: [String]\nAn array containing the names of all supported camera models.\nvar supportedDecoderVersions: [CIRAWDecoderVersion]\nAn array of all supported decoder versions for the given image type.\nvar isColorNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports color noise reduction adjustments.\nvar isContrastSupported: Bool\nA Boolean that indicates if the current image supports contrast adjustments.\nvar isDetailSupported: Bool\nA Boolean that indicates if the current image supports detail enhancement adjustments.\nvar isLensCorrectionSupported: Bool\nA Boolean that indicates if you can enable lens correction for the current image.\nvar isLocalToneMapSupported: Bool\nA Boolean that indicates if the current image supports local tone curve adjustments.\nvar isLuminanceNoiseReductionSupported: Bool\nA Boolean that indicates if the current image supports luminance noise reduction adjustments.\nvar isMoireReductionSupported: Bool\nA Boolean that indicates if the current image supports moire artifact reduction adjustments.\nvar isSharpnessSupported: Bool\nA Boolean that indicates if the current image supports sharpness adjustments.\nvar nativeSize: CGSize\nThe full native size of the unscaled image.\nConfiguring a filter\nvar baselineExposure: Float\nA value that indicates the baseline exposure to apply to the image.\nvar boostAmount: Float\nA value that indicates the amount of global tone curve to apply to the image.\nvar boostShadowAmount: Float\nA value that indicates the amount to boost the shadow areas of the image.\nvar colorNoiseReductionAmount: Float\nA value that indicates the amount of chroma noise reduction to apply to the image.\nvar contrastAmount: Float\nA value that indicates the amount of local contrast to apply to the edges of the image.\nvar decoderVersion: CIRAWDecoderVersion\nA value that indicates the decoder version to use.\nvar detailAmount: Float\nA value that indicates the amount of detail enhancement to apply to the edges of the image.\nvar exposure: Float\nA value that indicates the amount of exposure to apply to the image.\nvar extendedDynamicRangeAmount: Float\nA value that indicates the amount of extended dynamic range (EDR) to apply to the image.\nvar isDraftModeEnabled: Bool\nA Boolean that indicates whether to enable draft mode.\nvar isGamutMappingEnabled: Bool\nA Boolean that indicates whether to enable gamut mapping.\nvar isLensCorrectionEnabled: Bool\nA Boolean that indicates whether to enable lens correction.\nvar linearSpaceFilter: CIFilter?\nAn optional filter you can apply to the RAW image while it’s in linear space.\nvar localToneMapAmount: Float\nA value that indicates the amount of local tone curve to apply to the image.\nvar luminanceNoiseReductionAmount: Float\nA value that indicates the amount of luminance noise reduction to apply to the image.\nvar moireReductionAmount: Float\nA value that indicates the amount of moire artifact reduction to apply to high frequency areas of the image.\nvar neutralChromaticity: CGPoint\nA value that indicates the amount of white balance based on chromaticity values to apply to the image.\nvar neutralLocation: CGPoint\nA value that indicates the amount of white balance based on pixel coordinates to apply to the image.\nvar neutralTemperature: Float\nA value that indicates the amount of white balance based on temperature values to apply to the image.\nvar neutralTint: Float\nA value that indicates the amount of white balance based on tint values to apply to the image.\nvar orientation: CGImagePropertyOrientation\nA value that indicates the orientation of the image.\nvar portraitEffectsMatte: CIImage?\nAn optional auxiliary image that represents the portrait effects matte of the image.\nvar previewImage: CIImage?\nAn optional auxiliary image that represents a preview of the original image.\nvar properties: [AnyHashable : Any]\nA dictionary that contains properties of the image source.\nvar scaleFactor: Float\nA value that indicates the desired scale factor to draw the output image.\nvar semanticSegmentationGlassesMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation glasses matte of the image.\nvar semanticSegmentationHairMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation hair matte of the image.\nvar semanticSegmentationSkinMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation skin matte of the image.\nvar semanticSegmentationSkyMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation sky matte of the image.\nvar semanticSegmentationTeethMatte: CIImage?\nAn optional auxiliary image that represents the semantic segmentation teeth matte of the image.\nvar shadowBias: Float\nA value that indicates the amount to subtract from the shadows in the image.\nvar sharpnessAmount: Float\nA value that indicates the amount of sharpness to apply to the edges of the image.\nRelationships\nInherits From\nCIFilter"
  },
  {
    "title": "init(imageData:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437879-init",
    "html": "Deprecated\n\nUse init(imageData:identifierHint:) instead.\n\nParameters\ndata\n\nThe RAW image data to initialize the object with.\n\noptions\n\nAn options dictionary.\n\nReturn Value\n\nA CIFilter object.\n\nDiscussion\n\nYou can pass any of the keys defined in RAW Image Options along with the appropriate value in options. You should provide a source type identifier hint key (kCGImageSourceTypeIdentifierHint) and the appropriate source type value to help the decoder determine the file type. Otherwise it’s possible to obtain incorrect results.\n\nThe first step when working with RAW images in Core Image is to process the image using either init(imageData:options:) or init(imageURL:options:). These initializers create a CIFilter object with an outputImage which is a CIImage representation of the supplied RAW image. You can process After calling this method, the CIFilter object returns a CIImage object that’s properly processed similar to images retrieved using the outputImage key.\n\nImportant\n\nCore Image doesn’t process the supplied RAW image until the filter’s outputImage is rendered. For this reason, if you supply this initializer with a RAW image of an unsupported format, the filter object will be initialized but its outputImage will be nil.\n\nSee Also\nCreating a Filter from a RAW Image\ninit!(cvPixelBuffer: CVPixelBuffer!, properties: [AnyHashable : Any]!, options: [CIRAWFilterOption : Any]!)\nCreates a filter from a Core Video pixel buffer.\nDeprecated\ninit!(imageURL: URL!, options: [CIRAWFilterOption : Any]!)\nCreates a filter that allows the processing of RAW images.\nDeprecated"
  },
  {
    "title": "outputKeys",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438122-outputkeys",
    "html": "See Also\nGetting Filter Parameters and Attributes\nvar name: String\nA name associated with a filter.\nvar isEnabled: Bool\nA Boolean value that determines whether the filter is enabled. Animatable.\nvar attributes: [String : Any]\nA dictionary of key-value pairs that describe the filter.\nvar inputKeys: [String]\nThe names of all input parameters to the filter.\nvar outputImage: CIImage?\nReturns a CIImage object that encapsulates the operations configured in the filter."
  },
  {
    "title": "Core Image Constants",
    "url": "https://developer.apple.com/documentation/coreimage/core_image_constants",
    "html": "Topics\nConstants\nvar COREIMAGE_SUPPORTS_IOSURFACE: Int32\nvar COREIMAGE_SUPPORTS_OPENGLES: Int32\nlet kCIInputAmountKey: String\nlet kCIInputMatteImageKey: String"
  },
  {
    "title": "CIFilterGenerator",
    "url": "https://developer.apple.com/documentation/coreimage/cifiltergenerator",
    "html": "Overview\n\nThe CIFilterGenerator class provides methods for creating a CIFilter object by chaining together existing CIFilter objects to create complex effects. (A filter chain refers to the CIFilter objects that are connected in the CIFilterGenerator object.) The complex effect can be encapsulated as a CIFilterGenerator object and saved as a file so that it can be used again. The filter generator file contains an archived instance of all the CIFilter objects that are chained together.\n\nAny filter generator files that you copy to /Library/Graphics/Image Units/ are loaded when any of the loading methods provided by the CIPlugIn class are invoked. A CIFilterGenerator object is registered by its filename or, if present, by a class attribute that you supply in its description.\n\nYou can create a CIFilterGenerator object programmatically, using the methods provided by the CIFilterGenerator class, or by using the editor view provided by Core Image.\n\nTopics\nInitializing a Filter Generator Object\ninit?(contentsOf: URL)\nInitializes a filter generator object with the contents of a filter generator file.\nConnecting and Disconnecting Objects\nfunc connect(Any, withKey: String?, to: Any, withKey: String)\nAdds an object to the filter chain.\nfunc disconnectObject(Any, withKey: String, to: Any, withKey: String)\nRemoves the connection between two objects in the filter chain.\nManaging Exported Keys\nvar exportedKeys: [AnyHashable : Any]\nReturns an array of the exported keys.\nfunc exportKey(String, from: Any, withName: String?)\nExports an input or output key of an object in the filter chain.\nfunc removeExportedKey(String)\nRemoves a key that was previously exported.\nfunc setAttributes([AnyHashable : Any], forExportedKey: String)\nSets a dictionary of attributes for an exported key.\nSetting and Getting Class Attributes\nvar classAttributes: [AnyHashable : Any]\nThe class attributes associated with the filter.\nArchiving a Filter Generator Object\nfunc write(to: URL, atomically: Bool) -> Bool\nArchives a filter generator object to a filter generator file.\nRegistering a Filter Chain\nfunc registerFilterName(String)\nRegisters the name associated with a filter chain.\nCreating a Filter from a Filter Chain\nfunc filter() -> CIFilter\nCreates a filter object based on the filter chain.\nConstants\nExported Keys\nKeys for the exported parameters of a filter generator object.\nRelationships\nInherits From\nNSObject\nConforms To\nCIFilterConstructor\nNSCopying\nNSSecureCoding\nSee Also\nImage Units\nclass CIPlugIn\nThe mechanism for loading image units in macOS.\nprotocol CIPlugInRegistration\nThe interface for loading Core Image image units.\nprotocol CIFilterConstructor\nA general interface for objects that produce CIFilter instances."
  },
  {
    "title": "CIPDF417CodeDescriptor",
    "url": "https://developer.apple.com/documentation/coreimage/cipdf417codedescriptor",
    "html": "Overview\n\nPDF417 is a stacked linear barcode symbol format used predominantly in transport, ID cards, and inventory management. Each pattern in the code comprises 4 bars and spaces, 17 units long.\n\nTopics\nCreating a Descriptor\ninit?(payload: Data, isCompact: Bool, rowCount: Int, columnCount: Int)\nInitializes a descriptor that can be used as input to the CIBarcodeGenerator filter.\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload containing the data encoded in the PDF417 code.\nvar isCompact: Bool\nA boolean value telling if the PDF417 code is compact.\nvar rowCount: Int\nThe number of rows in the PDF417 code.\nvar columnCount: Int\nThe number of columns in the PDF417 code.\nRelationships\nInherits From\nCIBarcodeDescriptor\nSee Also\nBarcode Descriptions\nclass CIBarcodeDescriptor\nAn abstract base class that represents a machine readable code's attributes.\nclass CIQRCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a square QR code symbol.\nclass CIAztecCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents an Aztec code symbol.\nclass CIDataMatrixCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a Data Matrix code symbol."
  },
  {
    "title": "CIAztecCodeDescriptor",
    "url": "https://developer.apple.com/documentation/coreimage/ciazteccodedescriptor",
    "html": "Overview\n\nAztec code is a format of 2D barcode published as ISO/IEC 24778:2008 standard. It encodes data in concentric square rings around a central bullseye pattern.\n\nTopics\nCreating a Descriptor\ninit?(payload: Data, isCompact: Bool, layerCount: Int, dataCodewordCount: Int)\nInitializes a descriptor that can be used as input to the CIBarcodeGenerator filter.\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload containing the data encoded in the Aztec code.\nvar isCompact: Bool\nA Boolean value telling if the Aztec code is compact.\nvar layerCount: Int\nThe number of layers embedded in the Aztec code.\nvar dataCodewordCount: Int\nThe number of data codewords in the Aztec code.\nRelationships\nInherits From\nCIBarcodeDescriptor\nSee Also\nBarcode Descriptions\nclass CIBarcodeDescriptor\nAn abstract base class that represents a machine readable code's attributes.\nclass CIQRCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a square QR code symbol.\nclass CIPDF417CodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a PDF 417 symbol.\nclass CIDataMatrixCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a Data Matrix code symbol."
  },
  {
    "title": "CIPlugInRegistration",
    "url": "https://developer.apple.com/documentation/coreimage/cipluginregistration",
    "html": "Overview\n\nThe principal class of an image unit—a loadable bundle containing custom Core Image filters for macOS—must support this protocol.\n\nTopics\nInitializing Plug-ins\nfunc load(UnsafeMutableRawPointer!) -> Bool\nLoads and initializes an image unit, performing custom tasks as needed.\n\nRequired\n\nSee Also\nImage Units\nclass CIPlugIn\nThe mechanism for loading image units in macOS.\nclass CIFilterGenerator\nAn object that creates and configures chains of individual image filters.\nprotocol CIFilterConstructor\nA general interface for objects that produce CIFilter instances."
  },
  {
    "title": "init(cvPixelBuffer:properties:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/2138288-init",
    "html": "Deprecated\n\nUse init(cvPixelBuffer:properties:) instead.\n\nParameters\npixelBuffer\n\nCVPixelBufferRef with one of the following RAW pixel format types:\n\nkCVPixelFormatType_14Bayer_GRBG\n\nkCVPixelFormatType_14Bayer_RGGB\n\nkCVPixelFormatType_14Bayer_BGGR\n\nkCVPixelFormatType_14Bayer_GBRG\n\nproperties\n\nA properties dictionary. Defines the properties of the pixel buffer.\n\noptions\n\nAn options dictionary. You can pass any of the keys defined in RAW Image Options.\n\nReturn Value\n\nA CIFilter object.\n\nDiscussion\n\nThe first step when working with RAW images in Core Image is to process the image using either init(imageData:options:) or init(imageURL:options:). These initializers create a CIFilter object with an outputImage which is a CIImage representation of the supplied RAW image.\n\nImportant\n\nCore Image doesn't process the supplied RAW image until the filter's outputImage is rendered. For this reason, if you supply this initializer with a RAW image of an unsupported format, the filter object will be initialized but its outputImage will be nil.\n\nSee Also\nCreating a Filter from a RAW Image\ninit!(imageData: Data!, options: [CIRAWFilterOption : Any]!)\nCreates a filter that allows the processing of RAW images.\nDeprecated\ninit!(imageURL: URL!, options: [CIRAWFilterOption : Any]!)\nCreates a filter that allows the processing of RAW images.\nDeprecated"
  },
  {
    "title": "init(name:parameters:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437894-init",
    "html": "Parameters\nname\n\nThe name of the filter. You must make sure the name is spelled correctly, otherwise your app will run but not produce any output images. For that reason, you should check for the existence of the filter after calling this method.\n\nparams\n\nA list of key-value pairs to set as input values to the filter. Each key is a constant that specifies the name of an input parameter for the filter, and the corresponding value is the value for that parameter. See Core Image Filter Reference for built-in filters and their allowed parameters.\n\nReturn Value\n\nA CIFilter object whose input values are initialized.\n\nDiscussion\n\nUse this method to quickly create and configure a CIFilter instance, as in the example below.\n\nCIFilter *f = [CIFilter filterWithName: @\"CIColorControls\"\n                   withInputParameters: @{\n                             @\"inputImage\"      : inImage,\n                             @\"inputSaturation\" : @0.5,\n                             @\"inputBrightness\" : @1.2,\n                             @\"inputContrast\"   : @1.3\n                                         }];\n\nSee Also\nCreating a Filter\ninit?(name: String)\nCreates a CIFilter object for a specific kind of filter."
  },
  {
    "title": "CIBarcodeDescriptor",
    "url": "https://developer.apple.com/documentation/coreimage/cibarcodedescriptor",
    "html": "Overview\n\nSubclasses encapsulate the formal specification and fields specific to a code type. Each subclass is sufficient to recreate the unique symbol exactly as seen or used with a custom parser.\n\nListing 1 Creating a CIImage from a CIBarcodeDescriptor\n- (CIImage*) imageFromBarcodeDescriptor:(CIBarcodeDescriptor*)descriptor\n{\n    NSDictionary* inputParams = @{\n                                  @\"inputBarcodeDescriptor\" : descriptor\n                                  };\n    CIFilter* barcodeCreationFilter = [CIFilter filterWithName:@\"CIBarcodeGenerator\" withInputParameters:inputParams];\n    CIImage* outputImage = barcodeCreationFilter.outputImage;\n    return outputImage;\n}\n\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nSee Also\nBarcode Descriptions\nclass CIQRCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a square QR code symbol.\nclass CIAztecCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents an Aztec code symbol.\nclass CIPDF417CodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a PDF 417 symbol.\nclass CIDataMatrixCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a Data Matrix code symbol."
  },
  {
    "title": "shadingImage",
    "url": "https://developer.apple.com/documentation/coreimage/cirippletransition/3228695-shadingimage",
    "html": "Required"
  },
  {
    "title": "color1",
    "url": "https://developer.apple.com/documentation/coreimage/cilineargradient/3228543-color1",
    "html": "Required"
  },
  {
    "title": "CIRenderTask",
    "url": "https://developer.apple.com/documentation/coreimage/cirendertask",
    "html": "Overview\n\nA CIRenderTask object appears in Xcode Quick Look as a graph.\n\nTopics\nInstance Methods\nfunc waitUntilCompleted() -> CIRenderInfo\nWaits until the CIRenderTask finishes and returns.\nRelationships\nInherits From\nNSObject\nSee Also\nCustom Render Destination\nGenerating an animation with a Core Image Render Destination\nAnimate a filtered image to a Metal view in a SwiftUI app using a Core Image Render Destination.\nclass CIRenderDestination\nA specification for configuring all attributes of a render task's destination and issuing asynchronous render tasks.\nclass CIRenderInfo\nAn encapsulation of a render task's timing, passes, and pixels processed."
  },
  {
    "title": "apply(extent:roiCallback:image:arguments:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciwarpkernel/1437798-apply",
    "html": "Parameters\nextent\n\nThe extent of the output image.\n\ncallback\n\nA block or closure that computes the region of interest for a given rectangle of destination image pixels. See CIKernelROICallback.\n\nimage\n\nThe input image to be processed by the warp kernel.\n\nargs\n\nAn array of arguments to pass to the kernel routine. The type of each object in the array must be compatible with the corresponding parameter declared in the kernel routine source code. For details, see Core Image Kernel Language Reference.\n\nReturn Value\n\nA new image object describing the result of applying the kernel.\n\nDiscussion\n\nThis method is analogous to the CIFilter method apply(_:arguments:options:), but it does not require construction of a CIFilter object, and it allows you to specify a callback for determining the kernel’s region of interest as a block or closure. As with the similar CIFilter method, calling this method does not execute the kernel code—filters and their kernel code are evaluated only when rendering a final output image.\n\nWhen applying a filter kernel, the region of interest (ROI) is the area of source image pixels that must be processed to produce a given area of destination image pixels. For a more detailed definition, see The Region of Interest. Core Image calls your callback block or closure to determine the ROI when rendering the filter output. Core Image automatically splits large images into smaller tiles for rendering, so your callback may be called multiple times."
  },
  {
    "title": "init(source:)",
    "url": "https://developer.apple.com/documentation/coreimage/ciwarpkernel/1438278-init",
    "html": "Parameters\nstring\n\nA program in the Core Image Kernel Language that contains a single routine marked using the kernel keyword.\n\nReturn Value\n\nA new warp kernel object, or nil if the specified source code does not contain a valid warp kernel routine.\n\nDiscussion\n\nThis method is similar to the init(source:) method of the superclass CIKernel, but creates only warp kernels. Use this method when you want to ensure that the type of kernel object returned (if any) is always CIWarpKernel."
  },
  {
    "title": "CILinearGradient",
    "url": "https://developer.apple.com/documentation/coreimage/cilineargradient",
    "html": "Topics\nInstance Properties\ncolor0\nThe first color to use in the gradient.\n\nRequired\n\ncolor1\nThe second color to use in the gradient.\n\nRequired\n\npoint0\nThe starting position of the gradient.\n\nRequired\n\npoint1\nThe ending position of the gradient.\n\nRequired\n\nRelationships\nInherits From\nCIFilter\nSee Also\nRelated Documentation\n+ linearGradientFilter\nGenerates a color gradient that varies along a linear axis between two defined endpoints."
  },
  {
    "title": "CIKernel",
    "url": "https://developer.apple.com/documentation/coreimage/cikernel",
    "html": "Overview\n\nNote\n\nIf your custom filter uses both color and geometry information, but does not require processing both at the same time, you can improve performance by separating your image processing code: use a CIColorKernel object for the color processing step and a CIWarpKernel object for the geometry processing step.\n\nThe kernel language routine for a general-purpose filter kernel has the following characteristics:\n\nIts return type is vec4 (Core Image Kernel Language) or float4 (Metal Shading Language); that is, it returns a pixel color for the output image.\n\nIt may use zero or more input images. Each input image is represented by a parameter of type sampler.\n\nA kernel routine typically produces its output by calculating source image coordinates (using the destCoord and samplerTransform functions or the samplerTransform function), samples from the source images (using the sample function), and computes a final pixel color (output using the return keyword). For example, the Metal Shading Language source below implements a filter that passes through its input image unchanged.\n\n#include <CoreImage/CoreImage.h>\n \nextern \"C\" {\n    namespace coreimage {\n        float4 do_nothing(sampler src) {\n            return src.sample(src.coord());\n        }\n    }\n}\n\n\nThe equivalent code in Core Image Kernel Language is:\n\nkernel vec4 do_nothing(sampler image) {\n    vec2 dc = destCoord();\n    return sample(image, samplerTransform(image, dc));\n}\n\n\nThe Core Image Kernel Language is a dialect of the OpenGL Shading Language. See Core Image Kernel Language Reference and Core Image Programming Guide for more details.\n\nTopics\nCreating a Kernel Using Core Image Kernel Language\ninit?(source: String)\nCreates a single kernel object.\nDeprecated\nclass func makeKernels(source: String) -> [CIKernel]?\nCreates and returns and array of CIKernel objects.\nDeprecated\nCreating a Kernel Using Metal Shading Language\ninit(functionName: String, fromMetalLibraryData: Data)\nCreates a single kernel object using a Metal Shading Language (MSL) kernel function.\ninit(functionName: String, fromMetalLibraryData: Data, outputPixelFormat: CIFormat)\nCreates a single kernel object using a Metal Shading Language kernel function with optional pixel format.\nGetting a Kernel Name\nvar name: String\nThe name of the kernel routine.\nIdentifying the Region of Interest for the Kernel\nfunc setROISelector(Selector)\nSets the selector Core Image uses to query the region of interest for image processing with the kernel.\nApplying a Kernel to Filter an Image\nfunc apply(extent: CGRect, roiCallback: CIKernelROICallback, arguments: [Any]) -> CIImage?\nCreates a new image using the kernel and specified arguments.\ntypealias CIKernelROICallback\nThe signature for a block that computes the region of interest (ROI) for a given area of destination image pixels. Core Image calls this block when applying the kernel. You specify this block when using the apply(extent:roiCallback:arguments:) method.\nType Methods\nclass func kernelNames(fromMetalLibraryData: Data) -> [String]\nclass func kernels(withMetalString: String) -> [CIKernel]\nRelationships\nInherits From\nNSObject\nSee Also\nCustom Filters\nWriting Custom Kernels\nWrite your own custom kernels in either the Core Image Kernel Language or the Metal Shading Language.\nclass CIColorKernel\nA GPU-based image processing routine that processes only the color information in images, used to create custom Core Image filters.\nclass CIWarpKernel\nA GPU-based image processing routine that processes only the geometry information in an image, used to create custom Core Image filters.\nclass CIBlendKernel\nA GPU-based image processing routine that is optimized for blending two images.\nclass CISampler\nAn object that retrieves pixel samples for processing by a filter kernel."
  },
  {
    "title": "Writing Custom Kernels",
    "url": "https://developer.apple.com/documentation/coreimage/writing_custom_kernels",
    "html": "Overview\n\nThe Core Image Kernel Language is a shading language optimized for writing custom kernels for use in apps leveraging Core Image. You can add custom image processing routines to a Core Image pipeline.\n\nYou can also write your own kernels in the Metal Shading Language. The following flowchart shows how you decide which language to use for writing custom kernels:\n\nSource code written in Core Image Kernel Language should contain one or more image processing routines and may optionally contain other functions that are called by these routines. The source code is parsed and validated when the code is passed to Core Image's CIKernel creation APIs. When rendering, Core Image can concatenate kernel functions used within an image graph and construct optimized shader programs. See Core Image Kernel Language Reference for a list of supported data types, functions, and language features.\n\nAlternatively, you can write custom kernels in the Metal Shading Language. If you intend to use Metal-only language features and support exclusively Metal-supported devices, then writing custom kernels in Metal Shading Language can reduce compile-time cost while providing code consistency across your Metal app. See Metal Shading Language for Core Image Kernels for a list of supported data types, functions, and language features.\n\nSee Also\nCustom Filters\nclass CIKernel\nA GPU-based image processing routine used to create custom Core Image filters.\nclass CIColorKernel\nA GPU-based image processing routine that processes only the color information in images, used to create custom Core Image filters.\nclass CIWarpKernel\nA GPU-based image processing routine that processes only the geometry information in an image, used to create custom Core Image filters.\nclass CIBlendKernel\nA GPU-based image processing routine that is optimized for blending two images.\nclass CISampler\nAn object that retrieves pixel samples for processing by a filter kernel."
  },
  {
    "title": "color0",
    "url": "https://developer.apple.com/documentation/coreimage/cilineargradient/3228542-color0",
    "html": "Required"
  },
  {
    "title": "lineScreenFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228348-linescreenfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a line screen filter to an image. The effect generates a monochrome image containing a series of lines creating detail. The halftone effect is a set of lines, dots, or circles that contain detail. When viewing the image from a distance, the markings blend together, creating the illusion of continuous lines and shapes. Print media commonly uses this effect.\n\nThe line screen filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nangle\n\nA float representing the angle of the pattern as an NSNumber.\n\nwidth\n\nA float representing the distance between lines in the pattern as an NSNumber.\n\nsharpness\n\nA float representing the sharpness of the pattern as an NSNumber.\n\nThe following code creates a filter that creates a monochrome image containing small lines of detail on a black background:\n\nfunc line(inputImage: CIImage) -> CIImage {\n    let lineScreen = CIFilter.lineScreen()\n    lineScreen.inputImage = inputImage\n    lineScreen.center = CGPoint(x: 2016, y: 1512)\n    lineScreen.angle = 1\n    lineScreen.width = 35\n    lineScreen.sharpness = 0.7\n    return lineScreen.outputImage!\n}\n\n\nSee Also\nFilters\n+ circularScreenFilter\nAdds a circular overlay to an image.\n+ CMYKHalftone\nAdds a series of colorful dots to an image.\n+ dotScreenFilter\nCreates a monochrome image with a series of dots to add detail.\n+ hatchedScreenFilter\nCreates a monochrome image with a series of lines to add detail."
  },
  {
    "title": "Selectively Focusing on an Image",
    "url": "https://developer.apple.com/documentation/coreimage/selectively_focusing_on_an_image",
    "html": "Overview\n\nYou can selectively blur areas of an image using maskedVariableBlur() filter.\n\nFigure 1 Applying a masked variable blur effect to selectively focus on a portion of the scene\n\nYou specify the region to blur by applying a mask image; the shape and values of the mask image determine the location and strength of the blurring. Creating this effect involves the following steps:\n\nTo focus on a strip across the image, create two linear gradients representing the portions of the image to blur.\n\nTo focus on a circular region in the image, create a radial gradient centered on the region to keep sharp.\n\nComposite the gradients into a mask.\n\nApply Core Image’s maskedVariableBlur() filter to the original image, inputting the created mask.\n\nFocusing on a Strip of the Image\n\nYou can build your mask in any shape, but the general strategy remains the same: leave the mask transparent where you want the original image to stay sharp and focused. This section covers focusing on a horizontal strip.\n\nTo build a mask that leaves out a stripe, create linear gradients from a single color, such as green or gray. Our goal is to build the mask shown in Figure 2.\n\nFigure 2 CILinearGradient-generated linear mask\n\nThe linear gradients cause the blur to taper smoothly as it approaches the focused stripe of the image. The Core Image CIFilter named linearGradient() generates filters of the desired color. The linear gradient has four parameters:\n\npoint0\n\nA CGPoint representing the starting position of the gradient.\n\npoint1\n\nA CGPoint representing the ending position of the gradient.\n\ncolor0\n\nA CIColor representing the first color to use in the gradient.\n\ncolor1\n\nA CIColor representing the sexond color to use in the gradient.\n\nCompute the start and stop points of the gradient as fractions of the image height, as obtained through extent. For this particular mask and example image, focus on the area near the middle, in the second quarter of the image. Set the linear gradient’s point0 and point1 to reflect the region through which the gradient tapers.\n\nlet h = inputImage.extent.size.height\nlet topGradient = CIFilter.linearGradient()\ntopGradient.point0 = CGPoint(x:0, y:0.85 * h)\ntopGradient.color0 = CIColor.green\ntopGradient.point1 = CGPoint(x:0, y:0.6 * h)\ntopGradient.color1 = CIColor(red:0, green:1, blue:0, alpha:0)\n\n\nThe lower gradient should complement the upper gradient, so that their combined coverage delineates the entire area to blur. Express the starting inputPoint0 y-value at 0.35 of the image height and taper to 0.6, where the top gradient began. This leaves a gap in the combined mask through which the sharp image will show.\n\nlet bottomGradient = CIFilter.linearGradient()\nbottomGradient.point0 = CGPoint(x:0, y:0.35 * h)\nbottomGradient.color0 = CIColor.green\nbottomGradient.point1 = CGPoint(x:0, y:0.6 * h)\nbottomGradient.color1 = CIColor(red:0, green:1, blue:0, alpha:0)\n\n\nCreating a Mask by Compositing Linear Gradients\n\nTo create a mask that dilineates where and how strong a blur to apply, combine the two linear gradients.\n\nFigure 3 Add two mutually exclusive linear gradients to create a mask\n\nSince the gradients themselves are CIFilter objects, compositing them is as simple as concatenating their filter outputs to a compositing filter. Use the built-in CIFilter named additionCompositing() to composite two images additively.\n\nlet gradientMask = CIFilter.additionCompositing()\ngradientMask.inputImage = topGradient.outputImage\ngradientMask.backgroundImage = bottomGradient.outputImage\n\n\nThe resulting mask is now ready to be applied as part of the maskedVariableBlur() filter.\n\nFocusing on a Circular Region\n\nIn order to focus on a circular region of an image, you can create a Core Image radialGradient() filter.\n\nThe filter takes four parameters:\n\ncenter\n\nA CGPoint representing the center of the effect as x and y coordinates.\n\ncolor0\n\nA CIColor representing the first color to use in the gradient.\n\ncolor1\n\nA CIColor representing the second color to use in the gradient.\n\nradius0\n\nA float representing the radius of the starting circle to use in the gradient.\n\nradius1\n\nA float representing the radius of the ending circle to use in the gradient.\n\nSet the center to a CGPoint pointing to the center of the region you want to leave unblurred.\n\nSet the radius0 to a fraction of the image’s dimension, like 0.2*h in this example. You can tweak this parameter to determine the size of the sharp region.\n\nSet the radius1 to a larger fraction of the image’s dimension, like 0.3*h in this example. Tweaking this parameter changes the extent of the blur’s tapering effect; a larger value makes the blur more gradual, whereas a smaller value makes the image transition more abruptly from sharp (at inputRadius0) to blur (at inputRadius1).\n\nAs with the linear gradients, set color0 to transparency, and color1 to a solid opaque color, to indicate the blur’s gradation from nonexistent to full.\n\nlet h = inputImage.extent.size.height\nlet w = inputImage.extent.size.width\nlet radialMask = CIFilter.radialGradient()\nradialMask.center = CGPoint(x:0.55 * w, y:0.6 * h)\nradialMask.radius0 = Float(0.2 * h)\nradialMask.radius1 = Float(0.3 * h)\nradialMask.color0 = CIColor(red:0, green:1, blue:0, alpha:0)\nradialMask.color1 = CIColor(red:0, green:1, blue:0, alpha:1)\n\n\nThis yields a circular mask to use with the maskedVariableBlur() filter.\n\nFigure 4 CIRadialGradient-generated circular transparency mask\n\nMasking the Blurred Image to Apply Selective Focus\n\nThe final step is applying your choice of mask with the input image. The maskedVariableBlur() built-in CIFilter accomplishes this task with the following input parameters:\n\ninputImage\n\nSet to the original, unprocessed CIImage.\n\nradius\n\nA float representing the area of effect.\n\nmask\n\nAn image that masks an area on the input image with the type CIImage\n\nlet maskedVariableBlur = CIFilter.maskedVariableBlur()\nmaskedVariableBlur.inputImage = inputImage\nmaskedVariableBlur.mask = radialMask.outputImage\nmaskedVariableBlur.radius = 10\nlet selectivelyFocusedCIImage = maskedVariableBlur.outputImage\n\n\nThe resulting image shows the original image with portions blurred out according to the mask applied. The linear gradient mask results in an output image focused on a strip, and the radial gradient mask results in an output image focused on a circular region.\n\nFigure 5 CIMaskedVariableBlur with linear gradient mask applied to a walnut image, resulting in focus on a strip\n\nFigure 6 CIMaskedVariableBlur with circular gradient mask applied to a walnut image, resulting in focus on a circle\n\nSee Also\nFilter Recipes\nApplying a Chroma Key Effect\nReplace a color in one image with the background from another.\nCustomizing Image Transitions\nTransition between images in creative ways using Core Image filters.\nSimulating Scratchy Analog Film\nDegrade the quality of an image to make it look like dated, scratchy analog film."
  },
  {
    "title": "hatchedScreenFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228336-hatchedscreenfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a hatched screen filter to an image. The effect generates a monochrome image containing a series of lines in hatched pattern to create detail. The halftone effect is a set of lines, dots, or circles that contain detail. When viewing the image from a distance, the markings blend together, creating the illusion of continuous lines and shapes. The effect is often used in print media for more efficient printing.\n\nThe hatched screen filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nangle\n\nA float representing the angle of the pattern as an NSNumber.\n\nwidth\n\nA float representing the distance between lines in the pattern as an NSNumber.\n\nsharpness\n\nA float representing the sharpness of the pattern as an NSNumber.\n\nThe following code creates a filter that produces a monochrome image containing lines of detail on a black background:\n\nfunc hatched(inputImage: CIImage) -> CIImage {\n    let hatchedScreen = CIFilter.hatchedScreen()\n    hatchedScreen.inputImage = inputImage\n    hatchedScreen.center = CGPoint(x: 2016, y: 1512)\n    hatchedScreen.angle = 10\n    hatchedScreen.width = 35\n    hatchedScreen.sharpness = 0.7\n    return hatchedScreen.outputImage!\n}\n\n\nSee Also\nFilters\n+ circularScreenFilter\nAdds a circular overlay to an image.\n+ CMYKHalftone\nAdds a series of colorful dots to an image.\n+ dotScreenFilter\nCreates a monochrome image with a series of dots to add detail.\n+ lineScreenFilter\nCreates a monochrome image with a series of small lines to add detail."
  },
  {
    "title": "Applying a Chroma Key Effect",
    "url": "https://developer.apple.com/documentation/coreimage/applying_a_chroma_key_effect",
    "html": "Overview\n\nUse the chroma key effect, also known as bluescreening or greenscreening, to replace the background of an image by setting a color or range of colors to transparent.\n\nFigure 1 Applying a chroma key effect to swap background images\n\nYou apply this technique in three steps:\n\nCreate a cube map for the colorCube() filter to determine which colors to set transparent.\n\nApply the colorCube() filter to the image to make pixels transparent.\n\nUse the sourceOverCompositing() filter to place the image over the background image.\n\nCreate a Cube Map\n\nA color cube is a 3D color-lookup table that uses the R, G, and B values from the image to lookup a color. To filter out green from the image, create a color map with the green portion set to transparent pixels.\n\nA simple way to construct a color map with these characteristics is to model colors using an HSV (hue-saturation-value) representation. HSV represents hue as an angle around the central axis, as in a color wheel. In order to make a chroma key color from the source image transparent, set its lookup table value to 0 when its hue is in the correct color range.\n\nFigure 2 Color wheel showing the hue values to filter out of a source image with green background\n\nThe value for green in the source image falls within the slice beginning at 108° (108/360 = 0.3) and ending at 144° (144/360 = 0.4). You’ll set transparency to 0 for this range in the color cube.\n\nTo create the color cube, iterate across all values of red, green, and blue, entering a value of 0 for combinations that the filter wiill set to transparent. Refer to the numbered list for details on each step to the routine.\n\nfunc chromaKeyFilter(fromHue: CGFloat, toHue: CGFloat) -> CIColorCube {\n    // 1\n    let size = 64\n    var cubeRGB = [Float]()\n        \n    // 2\n    for z in 0 ..< size {\n        let blue = CGFloat(z) / CGFloat(size-1)\n        for y in 0 ..< size {\n            let green = CGFloat(y) / CGFloat(size-1)\n            for x in 0 ..< size {\n                let red = CGFloat(x) / CGFloat(size-1)\n                    \n                // 3\n                let hue = getHue(red: red, green: green, blue: blue)\n                let alpha: CGFloat = (hue >= fromHue && hue <= toHue) ? 0: 1\n                    \n                // 4\n                cubeRGB.append(Float(red * alpha))\n                cubeRGB.append(Float(green * alpha))\n                cubeRGB.append(Float(blue * alpha))\n                cubeRGB.append(Float(alpha))\n            }\n        }\n    }\n\n\n    // 5\n    let colorCubeFilter = CIFilter.colorCube()\n    colorCubeFilter.cubeDimension = Float(size)\n    colorCubeFilter.cubeData = Data(bytes: cubeRGB, count: cubeRGB.count * 4)\n    return colorCubeFilter\n}\n\n\nAllocate memory. The color cube has three dimensions, each with four elements of data (RGBA).\n\nUse a for-loop to iterate through each color combination of red, green, and blue, simulating a color gradient.\n\nConvert RGB to HSV, as in Listing 2. Even though the color cube exists in RGB color space, it’s easier to isolate and remove color based on hue. Input 0 for green hues to indicate complete removal; use 1 for other hues to leave those colors intact. To specify green as a hue value, convert its angle in the hue pie chart to a range of 0 to 1. The green in the sample image has hue between 0.3 (108 out of 360 degrees) and 0.4 (144 out of 360 degrees). Your shade of green may differ, so adjust the range accordingly.\n\nThe colorCube() filter requires premultiplied alpha values, meaning that the values in the lookup table have their transparency baked into their stored entries rather than applied when accessed.\n\nCreate a colorCube() filter with the cube data.\n\nNote\n\nThe framework doesn’t have built-in direct conversion between color spaces, but you can access the hue of a UIColor created with RGB values. Create a UIColor from the raw RGB values and then read the hue from it.\n\nListing 2 Converting RGB to HSV\nfunc getHue(red: CGFloat, green: CGFloat, blue: CGFloat) -> CGFloat {\n    let color = UIColor(red: red, green: green, blue: blue, alpha: 1)\n    var hue: CGFloat = 0\n    color.getHue(&hue, saturation: nil, brightness: nil, alpha: nil)\n    return hue\n}\n\n\nRemove Green from the Source Image\n\nApply the color cube filter to a foreground image by setting its inputImage parameter and then accessing the outputImage.\n\nlet chromaCIFilter = chromaKeyFilter(fromHue: 0.3, toHue: 0.4)\nchromaCIFilter.inputImage = foregroundCIImage\nlet sourceCIImageWithoutBackground = chromaCIFilter.outputImage\n\n\nThe output image contains the foreground with all green pixels made transparent.\n\nThe filter passes through each pixel in the input image, looks up its color in the color cube, and replaces the source color with the color in the color cube at the nearest position.\n\nComposite over a Background Image\n\nChain a sourceOverCompositing() filter to the color cube filter to composite a background image to the greenscreened output. The transparency in the colorcube-filtered image allows the composited background image to show through.\n\nlet compositor = CIFilter.sourceOverCompositing()\ncompositor.inputImage = sourceCIImageWithoutBackground\ncompositor.backgroundImage = backgroundCIImage\nlet compositedCIImage = compositor.outputImage\n\n\nThe foreground of the source image now appears in front of the background landscape without any trace of the green screen.\n\nSee Also\nFilter Recipes\nSelectively Focusing on an Image\nFocus on a part of an image by applying Gaussian blur and gradient masks.\nCustomizing Image Transitions\nTransition between images in creative ways using Core Image filters.\nSimulating Scratchy Analog Film\nDegrade the quality of an image to make it look like dated, scratchy analog film."
  },
  {
    "title": "writeTIFFRepresentation(of:to:format:colorSpace:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1642213-writetiffrepresentation",
    "html": "Parameters\nimage\n\nThe image object to render.\n\nurl\n\nThe file URL at which to write the output TIFF file.\n\nformat\n\nThe pixel format for the output image.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the CGColorSpaceModel.rgb or CGColorSpaceModel.monochrome model and must be compatible with the specified pixel format.\n\noptions\n\nA dictionary with additional options for export.\n\nerrorPtr\n\nOn input, a pointer to an error object. If an error occurs, this pointer is set to an object containing error information.\n\nReturn Value\n\nIf true, file export succeeded. If false, examine the errorPtr parameter for possible failure reasons.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the clampedToExtent() method.\n\nSee Also\nRendering Images for Data or File Export\nfunc tiffRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in TIFF format.\nfunc jpegRepresentation(of: CIImage, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in JPEG format.\nfunc pngRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in PNG format.\nfunc heifRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in HEIF format.\nfunc writeJPEGRepresentation(of: CIImage, to: URL, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in JPEG format.\nfunc writePNGRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in PNG format.\nfunc writeHEIFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in HEIF format.\nstatic let avDepthData: CIImageRepresentationOption\noptions dictionary key for image export methods to represent data as AVDepthData.\nstatic let depthImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output depth data.\nstatic let disparityImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "dotScreenFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228318-dotscreenfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a dot screen filter to an image. The effect generates a monochrome image containing a series of dots creating detail. The halftone effect is a set of lines, dots, or circles that contain detail. When viewing the image from a distance, the markings blend together, creating the illusion of continuous lines and shapes. Print media commonly uses this effect.\n\nThe dot screen filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nwidth\n\nA float representing the distance between dots in the pattern as an NSNumber.\n\nsharpness\n\nA float representing the sharpness of the pattern as an NSNumber.\n\nThe following code creates a filter that produces an image containing monochrome dots of detail on a black background:\n\nfunc dot (inputImage: CIImage) -> CIImage {\n    let dotScreen = CIFilter.dotScreen()\n    dotScreen.inputImage = inputImage\n    dotScreen.angle = 0\n    dotScreen.center = CGPoint(x: 2016, y: 1512)\n    dotScreen.width = 35\n    dotScreen.sharpness = 0.7\n    return dotScreen.outputImage!\n}\n\n\nSee Also\nFilters\n+ circularScreenFilter\nAdds a circular overlay to an image.\n+ CMYKHalftone\nAdds a series of colorful dots to an image.\n+ hatchedScreenFilter\nCreates a monochrome image with a series of lines to add detail.\n+ lineScreenFilter\nCreates a monochrome image with a series of small lines to add detail."
  },
  {
    "title": "circularScreenFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228280-circularscreenfilter",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a circular screen filter to an image. The effect generates a monochrome image containing a series of circular rings. The halftone effect is a set of lines, dots, or circles that contain detail. When viewing the image from a distance, the markings blend together, creating the illusion of continuous lines and shapes. Print media commonly uses this effect.\n\nThe circular screen filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\nwidth\n\nA float representing the distance between each circle in the pattern as an NSNumber.\n\nsharpness\n\nA float representing the sharpness of the circles in the pattern as an NSNumber.\n\nThe following code creates a filter that results in a monochrome image with a large circular pattern overlaying the image:\n\nfunc circular(inputImage: CIImage) -> CIImage {\n    let circularHalftone = CIFilter.circularScreen()\n    circularHalftone.inputImage = inputImage\n    circularHalftone.center = CGPoint(x: 2016, y: 1512)\n    circularHalftone.width = 35\n    circularHalftone.sharpness = 0.70\n    return circularHalftone.outputImage!\n}\n\n\nSee Also\nFilters\n+ CMYKHalftone\nAdds a series of colorful dots to an image.\n+ dotScreenFilter\nCreates a monochrome image with a series of dots to add detail.\n+ hatchedScreenFilter\nCreates a monochrome image with a series of lines to add detail.\n+ lineScreenFilter\nCreates a monochrome image with a series of small lines to add detail."
  },
  {
    "title": "CMYKHalftone",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/3228259-cmykhalftone",
    "html": "Return Value\n\nThe modified image.\n\nDiscussion\n\nThis method applies a CMYK halftone filter to an image. The effect generates an image containing a series of dots. The dots contain only cyan, magenta, yellow, and black colors. Halftone effect is a set of lines, dots, or circles that contain detail. When viewing the image from a distance, the markings blend together creating the illusion of continuous lines and shapes. Print media commonly uses this effect.\n\nThe CMYK halftone filter uses the following properties:\n\ninputImage\n\nAn image with the type CIImage.\n\nangle\n\nA float representing the angle of the pattern as an NSNumber.\n\nwidth\n\nA float representing the distance between dots in the pattern as an NSNumber.\n\nsharpness\n\nA float representing the sharpness of the pattern as an NSNumber.\n\ncenter\n\nA set of coordinates marking the center of the image as a CGPoint.\n\ngrayComponentReplacement\n\nA float representing the grey component to be replaced as an NSNumber.\n\nunderColorRemoval\n\nA float representing the under-color removal value as an NSNumber.\n\nThe following code produces an image with visible dots and less color:\n\nfunc cmyk(inputImage: CIImage) -> CIImage {\n    let cmykHalftone = CIFilter.cmykHalftone()\n    cmykHalftone.inputImage = inputImage\n    cmykHalftone.angle = 1\n    cmykHalftone.width = 35\n    cmykHalftone.sharpness = 0.7\n    cmykHalftone.center = CGPoint(x: 2016, y: 1512)\n    cmykHalftone.grayComponentReplacement = 1\n    cmykHalftone.underColorRemoval = 0.1\n    return cmykHalftone.outputImage!\n}\n\n\nSee Also\nFilters\n+ circularScreenFilter\nAdds a circular overlay to an image.\n+ dotScreenFilter\nCreates a monochrome image with a series of dots to add detail.\n+ hatchedScreenFilter\nCreates a monochrome image with a series of lines to add detail.\n+ lineScreenFilter\nCreates a monochrome image with a series of small lines to add detail."
  },
  {
    "title": "writeJPEGRepresentation(of:to:colorSpace:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1642218-writejpegrepresentation",
    "html": "Parameters\nimage\n\nThe image object to render.\n\nurl\n\nThe file URL at which to write the output JPEG file.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the CGColorSpaceModel.rgb or CGColorSpaceModel.monochrome model and must be compatible with the specified pixel format.\n\noptions\n\nA dictionary with additional options for export. Use the kCGImageDestinationLossyCompressionQuality key to specify JPEG compression level.\n\nerrorPtr\n\nOn input, a pointer to an error object. If an error occurs, this pointer is set to an object containing error information.\n\nReturn Value\n\nIf true, file export succeeded. If false, examine the errorPtr parameter for possible failure reasons.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the clampedToExtent() method.\n\nSee Also\nRendering Images for Data or File Export\nfunc tiffRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in TIFF format.\nfunc jpegRepresentation(of: CIImage, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in JPEG format.\nfunc pngRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in PNG format.\nfunc heifRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in HEIF format.\nfunc writeTIFFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in TIFF format.\nfunc writePNGRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in PNG format.\nfunc writeHEIFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in HEIF format.\nstatic let avDepthData: CIImageRepresentationOption\noptions dictionary key for image export methods to represent data as AVDepthData.\nstatic let depthImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output depth data.\nstatic let disparityImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "jpegRepresentation(of:colorSpace:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1642214-jpegrepresentation",
    "html": "Parameters\nimage\n\nThe image object to render.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the CGColorSpaceModel.rgb or CGColorSpaceModel.monochrome model and must be compatible with the specified pixel format.\n\noptions\n\nA dictionary with additional options for export. Use the kCGImageDestinationLossyCompressionQuality key to specify JPEG compression level. Other supported keys include avDepthData, depthImage, and disparityImage.\n\nReturn Value\n\nA data representation of the rendered image in JPEG format, or nil if the image could not be rendered.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the clampedToExtent() method.\n\nSee Also\nRendering Images for Data or File Export\nfunc tiffRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in TIFF format.\nfunc pngRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in PNG format.\nfunc heifRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in HEIF format.\nfunc writeTIFFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in TIFF format.\nfunc writeJPEGRepresentation(of: CIImage, to: URL, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in JPEG format.\nfunc writePNGRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in PNG format.\nfunc writeHEIFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in HEIF format.\nstatic let avDepthData: CIImageRepresentationOption\noptions dictionary key for image export methods to represent data as AVDepthData.\nstatic let depthImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output depth data.\nstatic let disparityImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "pngRepresentation(of:format:colorSpace:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/2866196-pngrepresentation",
    "html": "Parameters\nimage\n\nThe image object to render.\n\nformat\n\nThe pixel format for the output image.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the CGColorSpaceModel.rgb or CGColorSpaceModel.monochrome model and must be compatible with the specified pixel format.\n\noptions\n\nNo options keys are supported at this time.\n\nReturn Value\n\nA data representation of the rendered image in PNG format, or nil if the image could not be rendered.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the clampedToExtent() method.\n\nSee Also\nRendering Images for Data or File Export\nfunc tiffRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in TIFF format.\nfunc jpegRepresentation(of: CIImage, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in JPEG format.\nfunc heifRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in HEIF format.\nfunc writeTIFFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in TIFF format.\nfunc writeJPEGRepresentation(of: CIImage, to: URL, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in JPEG format.\nfunc writePNGRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in PNG format.\nfunc writeHEIFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in HEIF format.\nstatic let avDepthData: CIImageRepresentationOption\noptions dictionary key for image export methods to represent data as AVDepthData.\nstatic let depthImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output depth data.\nstatic let disparityImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "tiffRepresentation(of:format:colorSpace:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1642220-tiffrepresentation",
    "html": "Parameters\nimage\n\nThe image object to render.\n\nformat\n\nThe pixel format for the output image.\n\ncolorSpace\n\nThe color space in which to render the output image. This color space must conform to either the CGColorSpaceModel.rgb or CGColorSpaceModel.monochrome model and must be compatible with the specified pixel format.\n\noptions\n\nA dictionary with additional options for export.\n\nReturn Value\n\nA data representation of the rendered image in TIFF format, or nil if the image could not be rendered.\n\nDiscussion\n\nTo render an image for export, the image’s contents must not be empty and its extent dimensions must be finite. To export after applying a filter whose output has infinite extent, see the clampedToExtent() method.\n\nSee Also\nRendering Images for Data or File Export\nfunc jpegRepresentation(of: CIImage, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in JPEG format.\nfunc pngRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in PNG format.\nfunc heifRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in HEIF format.\nfunc writeTIFFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in TIFF format.\nfunc writeJPEGRepresentation(of: CIImage, to: URL, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in JPEG format.\nfunc writePNGRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in PNG format.\nfunc writeHEIFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in HEIF format.\nstatic let avDepthData: CIImageRepresentationOption\noptions dictionary key for image export methods to represent data as AVDepthData.\nstatic let depthImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output depth data.\nstatic let disparityImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output disparity data."
  },
  {
    "title": "workingFormat",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1642215-workingformat",
    "html": "Discussion\n\nThe working format determines the pixel format that Core Image uses to create intermediate buffers for executing filter kernels. Core Image automatically converts to and from the source and destination pixel formats of input images and output contexts. You specify a working pixel format using the workingFormat key in the options dictionary when creating a Core Image context.\n\nSee Also\nManaging Resources\nfunc clearCaches()\nFrees any cached data, such as temporary images, associated with the context and runs the garbage collector.\nfunc reclaimResources()\nRuns the garbage collector to reclaim any resources that the context no longer requires.\nclass func offlineGPUCount() -> UInt32\nReturns the number of GPUs not currently driving a display.\nvar workingColorSpace: CGColorSpace?\nThe working color space of the Core Image context."
  },
  {
    "title": "Geometry Adjustment Filters",
    "url": "https://developer.apple.com/documentation/coreimage/geometry_adjustment_filters",
    "html": "Overview\n\nbicubicScaleTransform()\n\nedgePreserveUpsample()\n\nkeystoneCorrectionCombined()\n\nkeystoneCorrectionHorizontal()\n\nkeystoneCorrectionVertical()\n\nlanczosScaleTransform()\n\nperspectiveCorrection()\n\nperspectiveRotate()\n\nperspectiveTransform()\n\nperspectiveTransformWithExtent()\n\nstraighten()\n\nSee Also\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nDistortion Filters\nApply distortion to images.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGradient Filters\nGenerate linear and radial gradients.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nReduction Filters\nReduction Filters\nSharpening Filters\nApply sharpening to images.\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTile Effect Filters\nProduce tiled images from source images.\nTransition Filters\nTransition between two images by using effects including page curl and swipe."
  },
  {
    "title": "Composite Operations",
    "url": "https://developer.apple.com/documentation/coreimage/composite_operations",
    "html": "Overview\n\nadditionCompositing()\n\ncolorBlendMode()\n\ncolorBurnBlendMode()\n\ncolorDodgeBlendMode()\n\ndarkenBlendMode()\n\ndifferenceBlendMode()\n\ndivideBlendMode()\n\nexclusionBlendMode()\n\nhardLightBlendMode()\n\nhueBlendMode()\n\nlightenBlendMode()\n\nlinearBurnBlendMode()\n\nlinearDodgeBlendMode()\n\nlinearLightBlendModeFilter\n\nluminosityBlendMode()\n\nmaximumCompositing()\n\nminimumCompositing()\n\nmultiplyBlendMode()\n\nmultiplyCompositing()\n\noverlayBlendMode()\n\npinLightBlendMode()\n\nsaturationBlendMode()\n\nscreenBlendMode()\n\nsoftLightBlendMode()\n\nsourceAtopCompositing()\n\nsourceInCompositing()\n\nsourceOutCompositing()\n\nsourceOverCompositing()\n\nsubtractBlendMode()\n\nSee Also\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nDistortion Filters\nApply distortion to images.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nGradient Filters\nGenerate linear and radial gradients.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nReduction Filters\nReduction Filters\nSharpening Filters\nApply sharpening to images.\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTile Effect Filters\nProduce tiled images from source images.\nTransition Filters\nTransition between two images by using effects including page curl and swipe."
  },
  {
    "title": "createCGImage(_:from:format:colorSpace:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437978-createcgimage",
    "html": "Parameters\nimage\n\nA Core Image image object.\n\nfromRect\n\nThe region of the image to render.\n\nformat\n\nThe pixel format of the image.\n\ncolorSpace\n\nThe color space for the output image. This color space must conform to either the CGColorSpaceModel.rgb or CGColorSpaceModel.monochrome model and be compatible with the specified pixel format.\n\nReturn Value\n\nA Quartz 2D image. You are responsible for releasing the returned image when you no longer need it.\n\nDiscussion\n\nRenders a region of an image into a temporary buffer using the context, and then creates and returns a Quartz 2D image with the results.\n\nSee Also\nRendering Images\nfunc createCGImage(CIImage, from: CGRect) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?, deferred: Bool) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object with deferred rendering.\nfunc createCGLayer(with: CGSize, info: CFDictionary?) -> CGLayer?\nCreates a CGLayer object from the provided parameters.\nDeprecated\nfunc render(CIImage, toBitmap: UnsafeMutableRawPointer, rowBytes: Int, bounds: CGRect, format: CIFormat, colorSpace: CGColorSpace?)\nRenders to the given bitmap.\nfunc render(CIImage, to: CVPixelBuffer)\nRenders an image into a pixel buffer.\nfunc render(CIImage, to: CVPixelBuffer, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into a pixel buffer.\nfunc render(CIImage, to: IOSurfaceRef, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into an IOSurface object.\nfunc render(CIImage, to: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?, bounds: CGRect, colorSpace: CGColorSpace)\nRenders a region of an image to a Metal texture."
  },
  {
    "title": "init(mtlDevice:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437711-init",
    "html": "Parameters\ndevice\n\nThe Metal device object to use for rendering.\n\noptions\n\nA dictionary that contains options for creating a CIContext object. You can pass any of the keys defined in Context Options along with the appropriate value.\n\nReturn Value\n\nA Core Image context.\n\nDiscussion\n\nUse this method to choose a specific Metal device for rendering when a system contains multiple Metal devices. To create a Metal-based context using the system’s default Metal device, use the init(options:) method.\n\nSee Also\nCreating a Context for GPU-Based Rendering with Metal\ninit(mtlDevice: any MTLDevice)\nCreates a Core Image context using the specified Metal device."
  },
  {
    "title": "init(mtlDevice:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437609-init",
    "html": "Parameters\ndevice\n\nThe Metal device object to use for rendering.\n\nReturn Value\n\nA Core Image context.\n\nDiscussion\n\nUse this method to choose a specific Metal device for rendering when a system contains multiple Metal devices. To create a Metal-based context using the system’s default Metal device, use the init(options:) method.\n\nSee Also\nCreating a Context for GPU-Based Rendering with Metal\ninit(mtlDevice: any MTLDevice, options: [CIContextOption : Any]?)\nCreates a Core Image context using the specified Metal device and options."
  },
  {
    "title": "init(forOfflineGPUAt:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437772-init",
    "html": "Parameters\nindex\n\nThe index of the offline GPU with which to create the context; a number between zero and the value returned by the offlineGPUCount() method.\n\nReturn Value\n\nA Core Image context.\n\nDiscussion\n\nGPU devices that are not currently being used to drive a display can be used for Core Image rendering. Use the offlineGPUCount() method to determine whether any such GPUs are available.\n\nTo create a Metal-based Core Image context using an offline GPU, use the MTLCopyAllDevices() function to list Metal devices, then choose a device without a display to pass to the init(mtlDevice:) method.\n\nSee Also\nCreating a Context for GPU-Based Rendering with OpenGL\ninit(cglContext: CGLContextObj, pixelFormat: CGLPixelFormatObj?, colorSpace: CGColorSpace?, options: [CIContextOption : Any]?)\nCreates a Core Image context from a CGL context, using the specified options, color space, and pixel format object.\nDeprecated\ninit(eaglContext: EAGLContext)\nCreates a Core Image context from an EAGL context.\nDeprecated\ninit(eaglContext: EAGLContext, options: [CIContextOption : Any]?)\nCreates a Core Image context from an EAGL context using the specified options.\nDeprecated\ninit?(forOfflineGPUAt: UInt32, colorSpace: CGColorSpace?, options: [CIContextOption : Any]?, sharedContext: CGLContextObj?)\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display, with the specified options.\nDeprecated"
  },
  {
    "title": "init(eaglContext:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1620419-init",
    "html": "Parameters\neaglContext\n\nThe EAGL context to render to.\n\nReturn Value\n\nA Core Image context that targets OpenGL ES.\n\nDiscussion\n\nThe OpenGL ES context must support OpenGL ES 2.0. All drawing performed using the methods listed in Drawing Images is rendered directly into the context.\n\nSee Also\nCreating a Context for GPU-Based Rendering with OpenGL\ninit(cglContext: CGLContextObj, pixelFormat: CGLPixelFormatObj?, colorSpace: CGColorSpace?, options: [CIContextOption : Any]?)\nCreates a Core Image context from a CGL context, using the specified options, color space, and pixel format object.\nDeprecated\ninit(eaglContext: EAGLContext, options: [CIContextOption : Any]?)\nCreates a Core Image context from an EAGL context using the specified options.\nDeprecated\ninit?(forOfflineGPUAt: UInt32)\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display.\nDeprecated\ninit?(forOfflineGPUAt: UInt32, colorSpace: CGColorSpace?, options: [CIContextOption : Any]?, sharedContext: CGLContextObj?)\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display, with the specified options.\nDeprecated"
  },
  {
    "title": "init(eaglContext:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1620362-init",
    "html": "Parameters\neaglContext\n\nThe EAGL context to render to.\n\noptions\n\nA dictionary that contains options for creating a CIContext object. You can pass any of the keys defined in Context Options along with the appropriate value.\n\nReturn Value\n\nA Core Image context that targets OpenGL ES.\n\nDiscussion\n\nThe OpenGL ES context must support OpenGL ES 2.0. All drawing performed using the methods listed in Drawing Images is rendered directly into the context.\n\nSee Also\nCreating a Context for GPU-Based Rendering with OpenGL\ninit(cglContext: CGLContextObj, pixelFormat: CGLPixelFormatObj?, colorSpace: CGColorSpace?, options: [CIContextOption : Any]?)\nCreates a Core Image context from a CGL context, using the specified options, color space, and pixel format object.\nDeprecated\ninit(eaglContext: EAGLContext)\nCreates a Core Image context from an EAGL context.\nDeprecated\ninit?(forOfflineGPUAt: UInt32)\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display.\nDeprecated\ninit?(forOfflineGPUAt: UInt32, colorSpace: CGColorSpace?, options: [CIContextOption : Any]?, sharedContext: CGLContextObj?)\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display, with the specified options.\nDeprecated"
  },
  {
    "title": "init(forOfflineGPUAt:colorSpace:options:sharedContext:)",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext/1437758-init",
    "html": "Parameters\nindex\n\nThe index of the offline GPU with which to create the context; a number between zero and the value returned by the offlineGPUCount() method.\n\ncolorSpace\n\nA color space object encapsulating color space information that is used to specify how color values are interpreted.\n\noptions\n\nA dictionary that contains options for creating a CIContext object. You can pass any of the keys defined in Context Options along with the appropriate value.\n\nsharedContext\n\nA CGL context with which to share OpenGL resources, obtained by calling the CGL function CGLCreateContext(_:_:_:). Pass NULL to use a context that does not share OpenGL resources.\n\nReturn Value\n\nA Core Image context.\n\nDiscussion\n\nGPU devices that are not currently being used to drive a display can be used for Core Image rendering. Use the offlineGPUCount() method to determine whether any such GPUs are available.\n\nTo create a Metal-based Core Image context using an offline GPU, use the MTLCopyAllDevices() function to list Metal devices, then choose a device without a display to pass to the init(mtlDevice:) method.\n\nSee Also\nCreating a Context for GPU-Based Rendering with OpenGL\ninit(cglContext: CGLContextObj, pixelFormat: CGLPixelFormatObj?, colorSpace: CGColorSpace?, options: [CIContextOption : Any]?)\nCreates a Core Image context from a CGL context, using the specified options, color space, and pixel format object.\nDeprecated\ninit(eaglContext: EAGLContext)\nCreates a Core Image context from an EAGL context.\nDeprecated\ninit(eaglContext: EAGLContext, options: [CIContextOption : Any]?)\nCreates a Core Image context from an EAGL context using the specified options.\nDeprecated\ninit?(forOfflineGPUAt: UInt32)\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display.\nDeprecated"
  },
  {
    "title": "Core Image Data Types",
    "url": "https://developer.apple.com/documentation/coreimage/core_image_data_types",
    "html": "Topics\nData Types\nstruct CIContextOption\nstruct CIImageAutoAdjustmentOption\nstruct CIImageOption\nstruct CIImageRepresentationOption\nstruct CIRAWDecoderVersion\nstruct CIRAWFilterOption\nDeprecated"
  },
  {
    "title": "Filter Attribute Keys",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/filter_attribute_keys",
    "html": "Overview\n\nAttribute keys are used for the attribute dictionary of a filter. Most entries in the attribute dictionary are optional. The attribute kCIAttributeFilterName is mandatory. For a parameter, the attribute kCIAttributeClass is mandatory because it specifies the class name of the filter.\n\nA parameter of type NSNumber does not necessarily need the attributes kCIAttributeMin and kCIAttributeMax. These attributes are not present when the parameter has no upper or lower bounds. For example, the Gaussian blur filter has a radius parameter with a minimum of 0 but no maximum value to indicate that all nonnegative values are valid.\n\nTopics\nConstants\nlet kCIAttributeFilterName: String\nThe filter name, specified as an NSString object.\nlet kCIAttributeFilterDisplayName: String\nThe localized version of the filter name that is displayed in the user interface.\nlet kCIAttributeDescription: String\nThe localized description of the filter. This description should inform the user what the filter does and be short enough to display in the user interface for the filter. It is not intended to be technically detailed.\nlet kCIAttributeFilterAvailable_Mac: String\nThe macOS version in which the filter first became available, specified as an NSString object.\nlet kCIAttributeFilterAvailable_iOS: String\nThe iOS version in which the filter first became available, specified as an NSString object.\nlet kCIAttributeReferenceDocumentation: String\nThe localized reference documentation for the filter. The reference should provide developers with technical details.\nlet kCIAttributeFilterCategories: String\nAn array of filter category keys that specifies all the categories in which the filter is a member.\nlet kCIAttributeClass: String\nThe class name of the filter.\nlet kCIAttributeType: String\nOne of the attribute types described in Data Type Attributes.\nlet kCIAttributeMin: String\nThe minimum value for a filter parameter, specified as a floating-point value.\nlet kCIAttributeMax: String\nThe maximum value for a filter parameter, specified as a floating-point value.\nlet kCIAttributeSliderMin: String\nThe minimum value, specified as a floating-point value, to use for a slider that controls input values for a filter parameter.\nlet kCIAttributeSliderMax: String\nThe maximum value, specified as a floating-point value, to use for a slider that controls input values for a filter parameter.\nlet kCIAttributeDefault: String\nThe default value, specified as a floating-point value, for a filter parameter.\nlet kCIAttributeIdentity: String\nIf supplied as a value for a parameter, the parameter has no effect on the input image.\nlet kCIAttributeName: String\nThe name of the attribute.\nlet kCIAttributeDisplayName: String\nThe localized display name of the attribute."
  },
  {
    "title": "Filter Category Keys",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/filter_category_keys",
    "html": "Topics\nConstants\nlet kCICategoryDistortionEffect: String\nA filter that reshapes an image by altering its geometry to create a 3D effect. Using distortion filters, you can displace portions of an image, apply lens effects, make a bulge in an image, and perform other operation to achieve an artistic effect.\nlet kCICategoryGeometryAdjustment: String\nA filter that changes the geometry of an image. Some of these filters are used to warp an image to achieve an artistic effects, but these filters can also be used to correct problems in the source image. For example, you can apply an affine transform to straighten an image that is rotated with respect to the horizon.\nlet kCICategoryCompositeOperation: String\nA filter operates on two image sources, using the color values of one image to operate on the other. Composite filters perform computations such as computing maximum values, minimum values, and multiplying values between input images. You can use compositing filters to add effects to an image, crop an image, and achieve a variety of other effects.\nlet kCICategoryHalftoneEffect: String\nA filter that simulates a variety of halftone screens, to mimic the halftone process used in print media. The output of these filters has the familiar “newspaper” look of the various dot patterns. Filters are typically named after the pattern created by the virtual halftone screen, such as circular screen or hatched screen.\nlet kCICategoryColorAdjustment: String\nA filter that changes color values. Color adjustment filters are used to eliminate color casts, adjust hue, and correct brightness and contrast. Color adjustment filters do not perform color management; ColorSync performs color management. You can use Quartz 2D to specify the color space associated with an image. For more information, see Color Management Overview and Quartz 2D Programming Guide.\nlet kCICategoryColorEffect: String\nA filter that modifies the color of an image to achieve an artistic effect. Examples of color effect filters include filters that change a color image to a sepia image or a monochrome image or that produces such effects as posterizing.\nlet kCICategoryTransition: String\nA filter that provides a bridge between two or more images by applying a motion effect that defines how the pixels of a source image yield to that of the destination image.\nlet kCICategoryTileEffect: String\nA filter that typically applies an effect to an image and then create smaller versions of the image (tiles), which are then laid out to create a pattern that’s infinite in extent.\nlet kCICategoryGenerator: String\nA filter that generates a pattern, such as a solid color, a checkerboard, or a star shine. The generated output is typically used as input to another filter.\nlet kCICategoryReduction: String\nA filter that reduces image data. These filters are used to solve image analysis problems.\nlet kCICategoryGradient: String\nA filter that generates a fill whose color varies smoothly. Exactly how color varies depends on the type of gradient—linear, radial, or Gaussian.\nlet kCICategoryStylize: String\nA filter that makes a photographic image look as if it was painted or sketched. These filters are typically used alone or in combination with other filters to achieve artistic effects.\nlet kCICategorySharpen: String\nA filter that sharpens images, increasing the contrast between the edges in an image. Examples of sharpen filters are unsharp mask and sharpen luminance.\nlet kCICategoryBlur: String\nA filter that softens images, decreasing the contrast between the edges in an image. Examples of blur filters are Gaussian blur and zoom blur.\nlet kCICategoryVideo: String\nA filter that works on video images.\nlet kCICategoryStillImage: String\nA filter that works on still images.\nlet kCICategoryInterlaced: String\nA filter that works on interlaced images.\nlet kCICategoryNonSquarePixels: String\nA filter that works on non-square pixels.\nlet kCICategoryHighDynamicRange: String\nA filter that works on high dynamic range pixels.\nlet kCICategoryBuiltIn: String\nA filter provided by Core Image. This distinguishes built-in filters from plug-in filters.\nlet kCICategoryFilterGenerator: String\nA filter created by chaining several filters together and then packaged as a CIFilterGenerator object."
  },
  {
    "title": "Data Type Attributes",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/data_type_attributes",
    "html": "Topics\nConstants\nlet kCIAttributeTypeTime: String\nA parametric time for transitions, specified as a floating-point value in the range of 0.0 to 1.0.\nlet kCIAttributeTypeScalar: String\nA scalar value.\nlet kCIAttributeTypeDistance: String\nA distance.\nlet kCIAttributeTypeAngle: String\nAn angle.\nlet kCIAttributeTypeBoolean: String\nA Boolean value.\nlet kCIAttributeTypeInteger: String\nAn integer value.\nlet kCIAttributeTypeCount: String\nA positive integer value."
  },
  {
    "title": "Color Attribute Keys",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/color_attribute_keys",
    "html": "Topics\nConstants\nlet kCIAttributeTypeOpaqueColor: String\nA Core Image color (CIColor object) that specifies red, green, and blue component values. Use this key for colors with no alpha component. If the key is not present, Core Image assumes color with alpha.\nlet kCIAttributeTypeGradient: String\nAn n-by-1 gradient image used to describe a color ramp.\nlet kCIAttributeTypeColor: String\nA Core Image color (CIColor object) that specifies red, green, and blue component values."
  },
  {
    "title": "Blur Filters",
    "url": "https://developer.apple.com/documentation/coreimage/blur_filters",
    "html": "Overview\n\nbokehBlur()\n\nboxBlur()\n\ndiscBlur()\n\ngaussianBlur()\n\nmaskedVariableBlur()\n\nmedian()\n\nmorphologyGradient()\n\nmorphologyMaximum()\n\nmorphologyMinimum()\n\nmorphologyRectangleMaximum()\n\nmorphologyRectangleMinimum()\n\nmotionBlur()\n\nnoiseReduction()\n\nzoomBlur()\n\nSee Also\nFilter Catalog\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nDistortion Filters\nApply distortion to images.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nGradient Filters\nGenerate linear and radial gradients.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nReduction Filters\nReduction Filters\nSharpening Filters\nApply sharpening to images.\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTile Effect Filters\nProduce tiled images from source images.\nTransition Filters\nTransition between two images by using effects including page curl and swipe."
  },
  {
    "title": "isEnabled",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438276-isenabled",
    "html": "Discussion\n\nThe filter is applied to its input when this property is set to true (the default).\n\nUse this property in conjunction with the name property when attaching filters to Core Animation layers and accessing or animating filter properties through key-value animations. Core Animation can animate this property on a layer.\n\nSee Also\nGetting Filter Parameters and Attributes\nvar name: String\nA name associated with a filter.\nvar attributes: [String : Any]\nA dictionary of key-value pairs that describe the filter.\nvar inputKeys: [String]\nThe names of all input parameters to the filter.\nvar outputKeys: [String]\nThe names of all output parameters from the filter.\nvar outputImage: CIImage?\nReturns a CIImage object that encapsulates the operations configured in the filter."
  },
  {
    "title": "CIFilterProtocol",
    "url": "https://developer.apple.com/documentation/coreimage/cifilterprotocol",
    "html": "Topics\nInstance Properties\nvar outputImage: CIImage?\nA CIImage object that encapsulates the operations configured in the filter.\n\nRequired\n\nType Methods\nstatic func customAttributes() -> [String : Any]?\nReturns a dictionary that contains key-value pairs that describe the filter."
  },
  {
    "title": "filterNames(inCategories:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437595-filternames",
    "html": "Parameters\ncategories\n\nOne or more of the filter category keys defined in Filter Category Keys. Pass nil to get all filters in all categories.\n\nReturn Value\n\nAn array that contains all published filter names that match all the categories specified by the categories argument.\n\nDiscussion\n\nWhen you pass more than one filter category, this method returns the intersection of the filters in the categories. For example, if you pass the categories kCICategoryBuiltIn and kCICategoryColorAdjustment, you obtain all the filters that are members of both the built-in and color adjustment categories. But if you pass in kCICategoryGenerator and kCICategoryStylize, you will not get any filters returned to you because there are no filters that are members of both the generator and stylize categories. If you want to obtain all stylize and generator filters, you must call the filterNamesInCategories: method for each category separately and then merge the results.\n\nSee Also\nAccessing Registered Filters\nclass func filterNames(inCategory: String?) -> [String]\nReturns an array of all published filter names in the specified category."
  },
  {
    "title": "init(imageURL:options:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438096-init",
    "html": "Deprecated\n\nUse init(imageURL:) instead.\n\nParameters\nurl\n\nThe location of a RAW image file.\n\noptions\n\nAn options dictionary. You can pass any of the keys defined in RAW Image Options.\n\nReturn Value\n\nA CIFilter object.\n\nDiscussion\n\nThe first step when working with RAW images in Core Image is to process the image using either init(imageData:options:) or init(imageURL:options:). These initializers create a CIFilter object with an outputImage which is a CIImage representation of the supplied RAW image.\n\nThe newly created filter object allows you fine control over the image processing that isn’t available when working with processed images such a JPEG. The following listing shows how to create a Core Image filter based on a URL named imageURL. The image is processed so that its neutral temperature is set to 2,000 Kelvin (giving a blue tint) and its baseline exposure doubled. Finally, a Core Image vignette filter is applied to the processed image in the same way it would be with any other source image:\n\nListing 1 Processing a RAW image.\nlet rawFilter = CIFilter(imageURL: imageURL, options: nil)\nrawFilter?.setValue(2000,    \n                    forKey: kCIInputNeutralTemperatureKey)\nif let baselineExposure = rawFilter?.value(forKey: kCIInputBaselineExposureKey) as? NSNumber {    \n    rawFilter?.setValue(baselineExposure.doubleValue * 2.5,                        forKey: kCIInputBaselineExposureKey)\n}\nlet vignettedImage = rawFilter?.outputImage?.applyingFilter(    \n    \"CIVignette\",    \n    withInputParameters: [kCIInputIntensityKey: 5])\nif let outputImage = vignettedImage {    \n    imageView.image = UIImage(ciImage: outputImage)\n}\n\n\nImportant\n\nCore Image doesn’t process the supplied RAW image until the filter’s outputImage is rendered. For this reason, if you supply this initializer with a RAW image of an unsupported format, the filter object will be initialized but its outputImage will be nil.\n\nSee Also\nCreating a Filter from a RAW Image\ninit!(cvPixelBuffer: CVPixelBuffer!, properties: [AnyHashable : Any]!, options: [CIRAWFilterOption : Any]!)\nCreates a filter from a Core Video pixel buffer.\nDeprecated\ninit!(imageData: Data!, options: [CIRAWFilterOption : Any]!)\nCreates a filter that allows the processing of RAW images.\nDeprecated"
  },
  {
    "title": "inputKeys",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438013-inputkeys",
    "html": "See Also\nGetting Filter Parameters and Attributes\nvar name: String\nA name associated with a filter.\nvar isEnabled: Bool\nA Boolean value that determines whether the filter is enabled. Animatable.\nvar attributes: [String : Any]\nA dictionary of key-value pairs that describe the filter.\nvar outputKeys: [String]\nThe names of all output parameters from the filter.\nvar outputImage: CIImage?\nReturns a CIImage object that encapsulates the operations configured in the filter."
  },
  {
    "title": "filterNames(inCategory:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1438145-filternames",
    "html": "Parameters\ncategory\n\nA string object that specifies one of the filter categories defined in Filter Category Keys.\n\nReturn Value\n\nAn array that contains all published names of the filter in a category.\n\nSee Also\nAccessing Registered Filters\nclass func filterNames(inCategories: [String]?) -> [String]\nReturns an array of all published filter names that match all the specified categories."
  },
  {
    "title": "name",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437997-name",
    "html": "Discussion\n\nYou use a filter’s name to construct key paths to its attributes when the filter is attached to a Core Animation layer. For example, if a CALayer object has an attached CIFilter instance whose name is myExposureFilter, you can refer to attributes of the filter using a key path such as filters.myExposureFilter.inputEV. Layer animations may also access filter attributes via these key paths.\n\nCore Animation can animate this property on a layer.\n\nThe default value for this property is nil.\n\nSee Also\nGetting Filter Parameters and Attributes\nvar isEnabled: Bool\nA Boolean value that determines whether the filter is enabled. Animatable.\nvar attributes: [String : Any]\nA dictionary of key-value pairs that describe the filter.\nvar inputKeys: [String]\nThe names of all input parameters to the filter.\nvar outputKeys: [String]\nThe names of all output parameters from the filter.\nvar outputImage: CIImage?\nReturns a CIImage object that encapsulates the operations configured in the filter."
  },
  {
    "title": "registerName(_:constructor:classAttributes:)",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437889-registername",
    "html": "Parameters\nname\n\nA string object that specifies the name of the filter you want to publish.\n\nanObject\n\nA constructor object that implements the filterWithName method.\n\nattributes\n\nA dictionary that contains the class display name and filter categories attributes along with the appropriate value for each attributes. That is, the kCIAttributeFilterDisplayName attribute and a string that specifies the display name, and the kCIAttributeFilterCategories and an array that specifies the categories to which the filter belongs (such as kCICategoryStillImage and kCICategoryDistortionEffect). All other attributes for the filter should be returned by the custom attributes method implement by the filter.\n\nDiscussion\n\nIn most cases you don’t need to use this method because the preferred way to register a custom filter that you write is to package it as an image unit. You do not need to use this method for a filter packaged as an image unit because you register your filter using the CIPlugInRegistration protocol. (See Core Image Programming Guide for additional details.)"
  },
  {
    "title": "attributes",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter/1437661-attributes",
    "html": "Discussion\n\nThis dictionary contains two kinds of key-value pairs:\n\nKeys listed in Filter Attribute Keys describe the filter, providing information such as a human-readable name and categories you can use to organize filters in your app’s UI.\n\nOther keys, including those listed in Filter Parameter Keys and those starting with “input” or “output”, describe parameters that control the filter’s behavior. For each parameter key, the corresponding value is another dictionary describing possible values for that parameter, such as the value class and minimum/maximum values. You can use this information to build a UI to control the filter.\n\nFor example, the attributes dictionary for the CIColorControls filter contains the following information:\n\nCIColorControls:\n{\n    CIAttributeFilterCategories = (\n        CICategoryColorAdjustment,\n        CICategoryVideo,\n        CICategoryStillImage,\n        CICategoryInterlaced,\n        CICategoryNonSquarePixels,\n        CICategoryBuiltIn\n    );\n    CIAttributeFilterDisplayName = \"Color Controls\";\n    CIAttributeFilterName = CIColorControls;\n    inputBrightness = {\n        CIAttributeClass = NSNumber;\n        CIAttributeDefault = 0;\n        CIAttributeIdentity = 0;\n        CIAttributeMin = -1;\n        CIAttributeSliderMax = 1;\n        CIAttributeSliderMin = -1;\n        CIAttributeType = CIAttributeTypeScalar;\n    };\n    inputContrast = {\n        CIAttributeClass = NSNumber;\n        CIAttributeDefault = 1;\n        CIAttributeIdentity = 1;\n        CIAttributeMin = 0.25;\n        CIAttributeSliderMax = 4;\n        CIAttributeSliderMin = 0.25;\n        CIAttributeType = CIAttributeTypeScalar;\n    };\n    inputImage = {CIAttributeClass = CIImage; };\n    inputSaturation = {\n        CIAttributeClass = NSNumber;\n        CIAttributeDefault = 1;\n        CIAttributeIdentity = 1;\n        CIAttributeMin = 0;\n        CIAttributeSliderMax = 3;\n        CIAttributeSliderMin = 0;\n        CIAttributeType = CIAttributeTypeScalar;\n    };\n    outputImage = {CIAttributeClass = CIImage; };\n}\n\nSee Also\nGetting Filter Parameters and Attributes\nvar name: String\nA name associated with a filter.\nvar isEnabled: Bool\nA Boolean value that determines whether the filter is enabled. Animatable.\nvar inputKeys: [String]\nThe names of all input parameters to the filter.\nvar outputKeys: [String]\nThe names of all output parameters from the filter.\nvar outputImage: CIImage?\nReturns a CIImage object that encapsulates the operations configured in the filter."
  },
  {
    "title": "CIFaceFeature",
    "url": "https://developer.apple.com/documentation/coreimage/cifacefeature",
    "html": "Overview\n\nThe properties of a CIFaceFeature object provide information about the face’s eyes and mouth. A face object in a video can also have properties that track its location over time—tracking ID and frame count.\n\nTopics\nLocating Faces\nvar bounds: CGRect\nA rectangle indicating the position and dimensions of the face in image coordinates.\nvar hasFaceAngle: Bool\nA Boolean value that indicates whether information about face rotation is available.\nvar faceAngle: Float\nThe rotation of the face.\nIdentifying Facial Features\nvar hasLeftEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s left eye.\nvar hasRightEyePosition: Bool\nA Boolean value that indicates whether the detector found the face’s right eye.\nvar hasMouthPosition: Bool\nA Boolean value that indicates whether the detector found the face’s mouth.\nvar leftEyePosition: CGPoint\nThe coordinates of the left eye, in image coordinates.\nvar rightEyePosition: CGPoint\nThe coordinates of the right eye, in image coordinates\nvar mouthPosition: CGPoint\nThe coordinates of the mouth, in image coordinates\nvar hasSmile: Bool\nA Boolean value that indicates whether a smile is detected in the face.\nvar leftEyeClosed: Bool\nA Boolean value that indicates whether a closed left eye is detected in the face.\nvar rightEyeClosed: Bool\nA Boolean value that indicates whether a closed right eye is detected in the face.\nTracking Distinct Faces in Video\nvar hasTrackingID: Bool\nA Boolean value that indicates whether the face object has a tracking ID.\nvar trackingID: Int32\nThe tracking identifier of the face object.\nvar hasTrackingFrameCount: Bool\nA Boolean value that indicates the face object has a tracking frame count.\nvar trackingFrameCount: Int32\nThe tracking frame count of the face.\nRelationships\nInherits From\nCIFeature\nSee Also\nImage Feature Detection\nclass CIDetector\nAn image processor that identifies notable features (such as faces and barcodes) in a still image or video.\nclass CIFeature\nThe abstract superclass for objects representing notable features detected in an image.\nclass CIRectangleFeature\nInformation about a rectangular region detected in a still or video image.\nclass CITextFeature\nInformation about a region likely to contain text detected in a still or video image.\nclass CIQRCodeFeature\nInformation about a Quick Response code (a kind of 2D barcode) detected in a still or video image."
  },
  {
    "title": "CIDataMatrixCodeDescriptor",
    "url": "https://developer.apple.com/documentation/coreimage/cidatamatrixcodedescriptor",
    "html": "Overview\n\nData Matrix codes are two-dimensional barcodes comprising black and white cells arranged in a square or rectangular matrix pattern. They can encode text or numeric data.\n\nTopics\nCreating a Descriptor\ninit?(payload: Data, rowCount: Int, columnCount: Int, eccVersion: CIDataMatrixCodeDescriptor.ECCVersion)\nInitializes a descriptor that can be used as input to the CIBarcodeGenerator filter.\nExamining a Descriptor\nvar errorCorrectedPayload: Data\nThe error-corrected payload that comprises the Data Matrix code symbol.\nvar rowCount: Int\nThe number of module rows.\nvar columnCount: Int\nThe number of module columns.\nvar eccVersion: CIDataMatrixCodeDescriptor.ECCVersion\nThe Data Matrix code ECC version.\nError Correction Constants\nenum CIDataMatrixCodeDescriptor.ECCVersion\nConstants concerning Data Matrix code ECC version.\nRelationships\nInherits From\nCIBarcodeDescriptor\nSee Also\nBarcode Descriptions\nclass CIBarcodeDescriptor\nAn abstract base class that represents a machine readable code's attributes.\nclass CIQRCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a square QR code symbol.\nclass CIAztecCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents an Aztec code symbol.\nclass CIPDF417CodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a PDF 417 symbol."
  },
  {
    "title": "CITextFeature",
    "url": "https://developer.apple.com/documentation/coreimage/citextfeature",
    "html": "Overview\n\nThe properties of a CITextFeature object identify its corners in image coordinates. Use this class to locate areas of text within an image—for example, to extract and perspective-correct those portions of the image before performing your own optical character recognition or other processing tasks.\n\nTo detect rectangles in an image or video, choose the CIDetectorTypeText type when initializing a CIDetector object, and use the CIDetectorImageOrientation option to specify the desired orientation for finding upright text.\n\nTopics\nLocating a Detected Feature\nvar bounds: CGRect\nA rectangle indicating the position and extent of the feature in image coordinates.\nLocating Features Within a Detected Region\nvar subFeatures: [Any]?\nAn array containing additional features detected within the feature.\nIdentifying the Corners of a Detected Text Region\nvar bottomLeft: CGPoint\nThe lower-left corner of the detected text region, in image coordinates.\nvar bottomRight: CGPoint\nThe lower-right corner of the detected text region, in image coordinates.\nvar topLeft: CGPoint\nThe upper-left corner of the detected text region, in image coordinates.\nvar topRight: CGPoint\nThe upper-right corner of the detected text region, in image coordinates.\nRelationships\nInherits From\nCIFeature\nSee Also\nImage Feature Detection\nclass CIDetector\nAn image processor that identifies notable features (such as faces and barcodes) in a still image or video.\nclass CIFeature\nThe abstract superclass for objects representing notable features detected in an image.\nclass CIFaceFeature\nInformation about a face detected in a still or video image.\nclass CIRectangleFeature\nInformation about a rectangular region detected in a still or video image.\nclass CIQRCodeFeature\nInformation about a Quick Response code (a kind of 2D barcode) detected in a still or video image."
  },
  {
    "title": "CIQRCodeFeature",
    "url": "https://developer.apple.com/documentation/coreimage/ciqrcodefeature",
    "html": "Overview\n\nA QR code is a two-dimensional barcode using the ISO/IEC 18004:2006 standard. The properties of a CIQRCodeFeature object identify the corners of the barcode in the image perspective and provide the decoded message.\n\nTo detect QR codes in an image or video, choose the CIDetectorTypeQRCode type when initializing a CIDetector object.\n\nTopics\nLocating a Detected Feature\nvar bounds: CGRect\nA rectangle indicating the position and extent of the feature in image coordinates.\nDecoding a Detected Barcode\nvar messageString: String?\nThe string decoded from the detected barcode.\nvar symbolDescriptor: CIQRCodeDescriptor?\nAn abstract representation of a QR Code symbol.\nIdentifying the Corners of a Detected Barcode\nvar bottomLeft: CGPoint\nThe lower-left corner of the detected barcode, in image coordinates.\nvar bottomRight: CGPoint\nThe lower-right corner of the detected barcode, in image coordinates.\nvar topLeft: CGPoint\nThe upper-left corner of the detected barcode, in image coordinates.\nvar topRight: CGPoint\nThe upper-right corner of the detected barcode, in image coordinates.\nRelationships\nInherits From\nCIFeature\nConforms To\nNSCopying\nNSSecureCoding\nSee Also\nImage Feature Detection\nclass CIDetector\nAn image processor that identifies notable features (such as faces and barcodes) in a still image or video.\nclass CIFeature\nThe abstract superclass for objects representing notable features detected in an image.\nclass CIFaceFeature\nInformation about a face detected in a still or video image.\nclass CIRectangleFeature\nInformation about a rectangular region detected in a still or video image.\nclass CITextFeature\nInformation about a region likely to contain text detected in a still or video image."
  },
  {
    "title": "CIPlugIn",
    "url": "https://developer.apple.com/documentation/coreimage/ciplugin",
    "html": "Overview\n\nAn image unit is an image processing bundle that contains one or more Core Image filters. The .plugin extension indicates one or more filters that are packaged as an image unit.\n\nTopics\nLoading Plug-ins\nclass func loadAllPlugIns()\nScans directories for files that have the .plugin extension and then loads the image units.\nDeprecated\nclass func loadNonExecutablePlugIns()\nScans directories for files that have the .plugin extension and then loads only those filters that are marked by the image unit as non-executable filters.\nclass func load(URL!, allowExecutableCode: Bool)\nLoads filters from an image unit that have the appropriate executable status.\nDeprecated\nType Methods\nclass func loadNonExecutablePlugIn(URL!)\nRelationships\nInherits From\nNSObject\nSee Also\nImage Units\nclass CIFilterGenerator\nAn object that creates and configures chains of individual image filters.\nprotocol CIPlugInRegistration\nThe interface for loading Core Image image units.\nprotocol CIFilterConstructor\nA general interface for objects that produce CIFilter instances."
  },
  {
    "title": "CIDetector",
    "url": "https://developer.apple.com/documentation/coreimage/cidetector",
    "html": "Overview\n\nA CIDetector object uses image processing to search for and identify notable features (faces, rectangles, and barcodes) in a still image or video. Detected features are represented by CIFeature objects that provide more information about each feature.\n\nThis class can maintain many state variables that can impact performance. So for best performance, reuse CIDetector instances instead of creating new ones.\n\nTopics\nCreating a Detector Object\ninit?(ofType: String, context: CIContext?, options: [String : Any]?)\nCreates and returns a configured detector.\nUsing a Detector Object to Find Features\nfunc features(in: CIImage) -> [CIFeature]\nSearches for features in an image.\nfunc features(in: CIImage, options: [String : Any]?) -> [CIFeature]\nSearches for features in an image based on the specified image orientation.\nConstants\nDetector Types\nStrings used to declare the detector for which you are interested.\nDetector Configuration Keys\nKeys used in the options dictionary to configure a detector.\nDetector Accuracy Options\nValue options used to specify the desired accuracy of the detector.\nFeature Detection Keys\nKeys used in the options dictionary for features(in:options:).\nRelationships\nInherits From\nNSObject\nSee Also\nImage Feature Detection\nclass CIFeature\nThe abstract superclass for objects representing notable features detected in an image.\nclass CIFaceFeature\nInformation about a face detected in a still or video image.\nclass CIRectangleFeature\nInformation about a rectangular region detected in a still or video image.\nclass CITextFeature\nInformation about a region likely to contain text detected in a still or video image.\nclass CIQRCodeFeature\nInformation about a Quick Response code (a kind of 2D barcode) detected in a still or video image."
  },
  {
    "title": "CIRectangleFeature",
    "url": "https://developer.apple.com/documentation/coreimage/cirectanglefeature",
    "html": "Overview\n\nA detected rectangle feature is not necessarily rectangular in the plane of the image; rather, the feature identifies a shape that may be rectangular in space but which appears in perspective in the image—for example, a paper or book on a desk. The properties of a CIRectangleFeature object identify its corners in image coordinates.\n\nFor example, you can use rectangle feature detection together with the CIPerspectiveCorrection filter to detect rectangular objects in an image or video and transform them to their original orientation.\n\nTo detect rectangles in an image or video, choose the CIDetectorTypeRectangle type when initializing a CIDetector object, and use the CIDetectorAspectRatio and CIDetectorFocalLength options to specify the approximate shape of rectangular features to search for. The detector returns at most one rectangle feature, the most prominent found in the image.\n\nTopics\nLocating a Detected Feature\nvar bounds: CGRect\nA rectangle indicating the position and extent of the feature in image coordinates.\nIdentifying the Corners of a Detected Rectangle\nvar bottomLeft: CGPoint\nThe lower-left corner of the detected rectangle, in image coordinates.\nvar bottomRight: CGPoint\nThe lower-right corner of the detected rectangle, in image coordinates.\nvar topLeft: CGPoint\nThe upper-left corner of the detected rectangle, in image coordinates.\nvar topRight: CGPoint\nThe upper-right corner of the detected rectangle, in image coordinates.\nRelationships\nInherits From\nCIFeature\nSee Also\nImage Feature Detection\nclass CIDetector\nAn image processor that identifies notable features (such as faces and barcodes) in a still image or video.\nclass CIFeature\nThe abstract superclass for objects representing notable features detected in an image.\nclass CIFaceFeature\nInformation about a face detected in a still or video image.\nclass CITextFeature\nInformation about a region likely to contain text detected in a still or video image.\nclass CIQRCodeFeature\nInformation about a Quick Response code (a kind of 2D barcode) detected in a still or video image."
  },
  {
    "title": "CIFeature",
    "url": "https://developer.apple.com/documentation/coreimage/cifeature",
    "html": "Overview\n\nA CIFeature object represents a portion of an image that a detector believes matches its criteria. Subclasses of CIFeature typically hold additional information specific to the detector that discovered the feature.\n\nTopics\nFeature Properties\nvar bounds: CGRect\nThe rectangle that holds discovered feature.\nvar type: String\nThe type of feature that was discovered.\nFeature Types\nlet CIFeatureTypeFace: String\nThe discovered feature is a person’s face.\nlet CIFeatureTypeRectangle: String\nThe discovered feature is a rectangular object, though it might appear in perspective in the image.\nlet CIFeatureTypeQRCode: String\nThe discovered feature is a Quick Response code (2D barcode).\nlet CIFeatureTypeText: String\nThe discovered feature is a region likely to contain upright text.\nRelationships\nInherits From\nNSObject\nSee Also\nImage Feature Detection\nclass CIDetector\nAn image processor that identifies notable features (such as faces and barcodes) in a still image or video.\nclass CIFaceFeature\nInformation about a face detected in a still or video image.\nclass CIRectangleFeature\nInformation about a rectangular region detected in a still or video image.\nclass CITextFeature\nInformation about a region likely to contain text detected in a still or video image.\nclass CIQRCodeFeature\nInformation about a Quick Response code (a kind of 2D barcode) detected in a still or video image."
  },
  {
    "title": "CIRenderInfo",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderinfo",
    "html": "Overview\n\nA CIRenderInfo object allows Xcode Quick Look to visualize the render graph with detailed timing information.\n\nTopics\nInstance Properties\nvar kernelExecutionTime: TimeInterval\nThe amount of time a render spent executing kernels.\nvar passCount: Int\nThe number of passes the render took.\nvar pixelsProcessed: Int\nThe number of pixels the render produced executing kernels.\nvar kernelCompileTime: TimeInterval\nRelationships\nInherits From\nNSObject\nSee Also\nCustom Render Destination\nGenerating an animation with a Core Image Render Destination\nAnimate a filtered image to a Metal view in a SwiftUI app using a Core Image Render Destination.\nclass CIRenderDestination\nA specification for configuring all attributes of a render task's destination and issuing asynchronous render tasks.\nclass CIRenderTask\nA single render task issued in conjunction with CIRenderDestination."
  },
  {
    "title": "CIRenderDestination",
    "url": "https://developer.apple.com/documentation/coreimage/cirenderdestination",
    "html": "Overview\n\nThe CIRenderDestination class provides an API for specifying a render task destination's properties, such as buffer format, alpha mode, clamping behavior, blending, and color space, properties formerly tied to CIContext.\n\nYou can create a CIRenderDestination object for each surface or buffer to which you must render. You can also render multiple times to a single destination with different settings such as colorspace and blend mode by mutating a single CIRenderDestination object between renders.\n\nRenders issued to a CIRenderDestination return to the caller as soon as the CPU has issued the task, rather than after the GPU has performed the task, so you can start render tasks on subsequent frames without waiting for previous renders to finish. If the render fails, a CIRenderTask will return immediately.\n\nTopics\nCreating a Render Destination\ninit(pixelBuffer: CVPixelBuffer)\nCreates a render destination based on a Core Video pixel buffer.\ninit(ioSurface: IOSurface)\nCreates a render destination based on an IOSurface object.\ninit(mtlTexture: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?)\nCreates a render destination based on a Metal texture.\ninit(width: Int, height: Int, pixelFormat: MTLPixelFormat, commandBuffer: (any MTLCommandBuffer)?, mtlTextureProvider: (() -> any MTLTexture)?)\nCreates a render destination based on a Metal texture with specified pixel format.\ninit(glTexture: UInt32, target: UInt32, width: Int, height: Int)\nCreates a render destination based on an OpenGL texture.\ninit(bitmapData: UnsafeMutableRawPointer, width: Int, height: Int, bytesPerRow: Int, format: CIFormat)\nCreates a render destination based on a client-managed buffer.\nCustomizing Rendering\nvar alphaMode: CIRenderDestinationAlphaMode\nThe render destination's representation of alpha (transparency) values.\nenum CIRenderDestinationAlphaMode\nDifferent ways of representing alpha.\nvar blendKernel: CIBlendKernel?\nThe destination's blend kernel.\nvar blendsInDestinationColorSpace: Bool\nIndicator of whether to blend in the destination's color space.\nvar colorSpace: CGColorSpace?\nThe destination's color space.\nvar width: Int\nThe render destination's row width.\nvar height: Int\nThe render destination's buffer height.\nvar isClamped: Bool\nIndicator of whether or not the destination clamps.\nvar isDithered: Bool\nIndicator of whether or not the destination dithers.\nvar isFlipped: Bool\nIndicator of whether the destination is flipped.\nRelationships\nInherits From\nNSObject\nSee Also\nCustom Render Destination\nGenerating an animation with a Core Image Render Destination\nAnimate a filtered image to a Metal view in a SwiftUI app using a Core Image Render Destination.\nclass CIRenderInfo\nAn encapsulation of a render task's timing, passes, and pixels processed.\nclass CIRenderTask\nA single render task issued in conjunction with CIRenderDestination."
  },
  {
    "title": "CIImageProcessorOutput",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessoroutput",
    "html": "Overview\n\nYour app does not define classes that adopt this protocol; Core Image provides an object of this type when applying a custom image processor you create with a CIImageProcessorKernel subclass.\n\nIn your image processor class' process(with:arguments:output:) method, use an appropriate property of the provided CIImageProcessorOutput object to return processed pixel data to Core Image. For example, if you process the image using a Metal shader, bind the metalTexture property as an attachment in a render pass or as an output texture in a compute pass. Or, if you process the image using a CPU-based routine, write processed pixel data to memory using the the baseAddress pointer. You must provide rendered output to one (and only one) of the properties listed in Providing Output Image Data.\n\nTo access input pixel data in your image processor block, see the CIImageProcessorInput class.\n\nTopics\nProviding Output Image Data\nvar baseAddress: UnsafeMutableRawPointer\nA pointer to CPU memory at which to write output pixel data.\n\nRequired\n\nvar metalTexture: (any MTLTexture)?\nA Metal texture to which you can write output pixel data.\n\nRequired\n\nvar pixelBuffer: CVPixelBuffer?\nA CoreVideo pixel buffer to which you can write output pixel data.\n\nRequired\n\nvar surface: IOSurfaceRef\nAn IOSurface object to which you can write output pixel data.\n\nRequired\n\nGetting Supplemental Information for Image Processing\nvar region: CGRect\nThe rectangular region of the output image that your processor must provide.\n\nRequired\n\nvar metalCommandBuffer: (any MTLCommandBuffer)?\nA command buffer to use for image processing using Metal.\n\nRequired\n\nvar bytesPerRow: Int\nThe number of bytes per row of pixels for the output image.\n\nRequired\n\nvar format: CIFormat\nThe per-pixel data format expected of the output image.\n\nRequired\n\nInstance Properties\nvar digest: UInt64\n\nRequired\n\nSee Also\nCustom Image Processors\nclass CIImageProcessorKernel\nThe abstract class you extend to create custom image processors that can integrate with Core Image workflows.\nprotocol CIImageProcessorInput\nA container of image data and information for use in a custom image processor."
  },
  {
    "title": "CIBlendKernel",
    "url": "https://developer.apple.com/documentation/coreimage/ciblendkernel",
    "html": "Overview\n\nThe blend kernel function has the following characteristics:\n\nIt has two arguments of type __sample (Core Image Kernel Language) or sample_t (Metal Shading Language), representing the foreground and background images.\n\nIts return type is vec4 (Core Image Kernel Language) or float4 (Metal Shading Language); that is, it returns a pixel color for the output image.\n\nA blend kernel routine receives as input single-pixel colors (one sampled from each input image) and computes a final pixel color (output using the return keyword). For example, the Metal Shading Language source below implements a filter that returns the average of its two input images.\n\n#include <CoreImage/CoreImage.h>\n \nfloat4 averageBlend(sample_t foreground, sample_t background) {\n    return (foreground + background) / 2.0;\n}\n\n\nGenerally, the extent of the output image is the union of the extents of the foreground and background images.\n\nTopics\nCreating a Kernel\ninit?(source: String)\nCreates a custom blend kernel from a program string.\nDeprecated\nApplying a Kernel to Filter an Image\nfunc apply(foreground: CIImage, background: CIImage) -> CIImage?\nCreates a new image using the blend kernel and specified foreground and background images.\nBuiltin Blend Kernels\nclass var clear: CIBlendKernel\nA blend kernel that returns a clear color.\nclass var color: CIBlendKernel\nA blend kernel that uses the luminance values of the background with the hue and saturation values of the foreground image.\nclass var colorBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples.\nclass var colorDodge: CIBlendKernel\nA blend kernel that brightens the background image samples to reflect the foreground image samples.\nclass var componentAdd: CIBlendKernel\nA blend kernel that adds color components to achieve a brightening effect.\nclass var componentMax: CIBlendKernel\nA blend kernel that creates an image using the maximum values of two input images.\nclass var componentMin: CIBlendKernel\nA blend kernel that creates an image using the minimum values of two input images.\nclass var componentMultiply: CIBlendKernel\nA blend kernel that multiplies the color components of its input images.\nclass var darken: CIBlendKernel\nA blend kernel that creates an image using the darker values of two input images.\nclass var darkerColor: CIBlendKernel\nA blend kernel that creates an image using the darker color of two input images.\nclass var destination: CIBlendKernel\nA blend kernel that returns the background input image.\nclass var destinationAtop: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of the foreground.\nclass var destinationIn: CIBlendKernel\nA blend kernel that places the background over the foreground and crops based on the visibility of both.\nclass var destinationOut: CIBlendKernel\nA blend kernel that uses the background image to define what to take out of the foreground image.\nclass var destinationOver: CIBlendKernel\nA blend kernel that places the background image over the input foreground image.\nclass var difference: CIBlendKernel\nA blend kernel that creates an image using the difference between the background and foreground images.\nclass var divide: CIBlendKernel\nA blend kernel that divides the background image sample color with the foreground image sample color.\nclass var exclusion: CIBlendKernel\nA blend kernel that produces an effect similar to difference blending but with lower contrast.\nclass var exclusiveOr: CIBlendKernel\nA blend kernel that returns either the foreground or background image if the other contains a clear color.\nclass var hardLight: CIBlendKernel\nA blend kernel that either multiplies or screens colors, depending on the source image sample color.\nclass var hardMix: CIBlendKernel\nA blend kernel that adds two images together, setting each color channel value to either 0 or 1.\nclass var hue: CIBlendKernel\nA blend kernel that uses the luminance and saturation values of the background image with the hue of the foreground image.\nclass var lighten: CIBlendKernel\nA blend kernel that creates an image using the lighter values of two input images.\nclass var lighterColor: CIBlendKernel\nA blend kernel that creates an image using the lighter color of two input images.\nclass var linearBurn: CIBlendKernel\nA blend kernel that darkens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearDodge: CIBlendKernel\nA blend kernel that lightens the background image samples to reflect the foreground image samples while also increasing contrast.\nclass var linearLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing brightness, depending on the blend color.\nclass var luminosity: CIBlendKernel\nA blend kernel that uses the hue and saturation of the background image with the luminance of the foreground image.\nclass var multiply: CIBlendKernel\nA blend kernel that multiplies the background image sample color with the foreground image sample color.\nclass var overlay: CIBlendKernel\nA blend kernel that either multiplies or screens the foreground image samples with the background image samples, depending on the background color.\nclass var pinLight: CIBlendKernel\nA blend kernel that conditionally replaces background image samples with source image samples depending on the brightness of the source image samples.\nclass var saturation: CIBlendKernel\nA blend kernel that uses the luminance and hue values of the background image with the saturation of the foreground image.\nclass var screen: CIBlendKernel\nA blend kernel that multiplies the inverse of the foreground image samples with the inverse of the background image samples.\nclass var softLight: CIBlendKernel\nA blend kernel that either darkens or lightens colors, depending on the foreground image sample color.\nclass var source: CIBlendKernel\nA blend kernel that returns the foreground input image.\nclass var sourceAtop: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of the background.\nclass var sourceIn: CIBlendKernel\nA blend kernel that places the foreground over the background and crops based on the visibility of both.\nclass var sourceOut: CIBlendKernel\nA blend kernel that uses the foreground image to define what to take out of the background image.\nclass var sourceOver: CIBlendKernel\nA blend kernel that places the foreground image over the input background image.\nclass var subtract: CIBlendKernel\nA blend kernel that subtracts the background image sample color from the foreground image sample color.\nclass var vividLight: CIBlendKernel\nA blend kernel that burns or dodges colors by changing contrast, depending on the blend color.\nInstance Methods\nfunc apply(foreground: CIImage, background: CIImage, colorSpace: CGColorSpace) -> CIImage?\nRelationships\nInherits From\nCIColorKernel\nSee Also\nCustom Filters\nWriting Custom Kernels\nWrite your own custom kernels in either the Core Image Kernel Language or the Metal Shading Language.\nclass CIKernel\nA GPU-based image processing routine used to create custom Core Image filters.\nclass CIColorKernel\nA GPU-based image processing routine that processes only the color information in images, used to create custom Core Image filters.\nclass CIWarpKernel\nA GPU-based image processing routine that processes only the geometry information in an image, used to create custom Core Image filters.\nclass CISampler\nAn object that retrieves pixel samples for processing by a filter kernel."
  },
  {
    "title": "CIImageProcessorKernel",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorkernel",
    "html": "Overview\n\nUnlike the CIKernel class and its other subclasses that allow you to create new image-processing effects with the Core Image Kernel Language, the CIImageProcessorKernel class provides direct access to the underlying bitmap image data for a step in the Core Image processing pipeline. As such, you can create subclasses of this class to integrate other image-processing technologies—such as Metal compute shaders, Metal Performance Shaders, Accelerate vImage operations, or your own CPU-based image-processing routines—with a Core Image filter chain.\n\nYour custom image processing operation is invoked by your subclassed image processor kernel's process(with:arguments:output:) method. The method can accept zero, one or more inputs: kernels that generate imagery (such as a noise or pattern generator) need no inputs, while kernels that composite source images together require multiple inputs. The arguments dictionary allows the caller to pass in additional parameter values (such as the radius of a blur) and the output contains the destination for your image processing code to write to.\n\nThe following code shows how you can subclass CIImageProcessorKernel to apply the Metal Performance Shader MPSImageThresholdBinary kernel to a CIImage:\n\nListing 1 Creating a Threshold Image Processing Kernel\nclass ThresholdImageProcessorKernel: CIImageProcessorKernel {\nstatic let device = MTLCreateSystemDefaultDevice()        \noverride class func process(with inputs: [CIImageProcessorInput]?, arguments: [String : Any]?, output: CIImageProcessorOutput) throws {                \n    guard            \n        let device = device,            \n        let commandBuffer = output.metalCommandBuffer,            \n        let input = inputs?.first,            \n        let sourceTexture = input.metalTexture,            \n        let destinationTexture = output.metalTexture,            \n        let thresholdValue = arguments?[\"thresholdValue\"] as? Float else  {                \n            return        \n        }                \n    \n    let threshold = MPSImageThresholdBinary(\n        device: device,                                                \n        thresholdValue: thresholdValue,                                               \n        maximumValue: 1.0,                                                \n        linearGrayColorTransform: nil)                \n    \n    threshold.encode(\n        commandBuffer: commandBuffer,                         \n        sourceTexture: sourceTexture,                         \n        destinationTexture: destinationTexture)    \n    }\n}\n\n\nTo apply to kernel to an image, the calling side invokes the image processor's apply(withExtent:inputs:arguments:) method. The following code generates a new CIImage object named result which contains a thresholded version of the source image, inputImage.\n\nlet result = try? ThresholdImageProcessorKernel.apply( \n    withExtent: inputImage.extent,            \n    inputs: [inputImage],            \n    arguments: [\"thresholdValue\": 0.25])\n\n\nImportant\n\nCore Image will concatenate filters in a network into as fewer kernels as possible, avoiding the creation of intermediate buffers. However, it is unable to do this with image processor kernels. To get the best performance, you should only use CIImageProcessorKernel objects when your image processing algorithms can't be expressed as Core Image Kernel Language.\n\nSubclassing Notes\n\nThe CIImageProcessorKernel class is abstract; to create a custom image processor, you define a subclass of this class.\n\nYou do not directly create instances of a custom CIImageProcessorKernel subclass. Image processors must not carry or use state specific to any single invocation of the processor, so all methods (and accessors for readonly properties) of an image processor kernel class are class methods.\n\nYour subclass should override at least the process(with:arguments:output:) method to perform its image processing.\n\nIf your image processor needs to work with a larger or smaller region of interest in the input image than each corresponding region of the output image (for example, a blur filter, which samples several input pixels for each output pixel), you should also override the roi(forInput:arguments:outputRect:) method.\n\nYou can also override the formatForInput(at:) method and outputFormat property getter to customize the input and output pixel formats for your processor (for example, as part of a multi-step workflow where you extract a single channel from an RGBA image, apply an effect to that channel only, then recombine the channels).\n\nUsing a Custom Image Processor\n\nTo apply your custom image processor class to filter one or more images, call the apply(withExtent:inputs:arguments:) class method. (Do not override this method.)\n\nTopics\nType Properties\nclass var outputFormat: CIFormat\nThe processor's output pixel format.\nclass var outputIsOpaque: Bool\nBoolean determining whether or not processor outputs an opaque image.\nclass var synchronizeInputs: Bool\nTells whether or not processor input should be synchronized for CPU access.\nType Methods\nclass func apply(withExtent: CGRect, inputs: [CIImage]?, arguments: [String : Any]?) -> CIImage\nMethod to override when applying a custom image processor kernel to an image and returning the result.\nclass func formatForInput(at: Int32) -> CIFormat\nMethod to override for returning the image processing kernel's input pixel format.\nclass func process(with: [any CIImageProcessorInput]?, arguments: [String : Any]?, output: any CIImageProcessorOutput)\nMethod to override for customizing the kernel's image processing.\nclass func roi(forInput: Int32, arguments: [String : Any]?, outputRect: CGRect) -> CGRect\nMethod to override for determining specific region of input image required to process in rendering a specified region of the output image.\nclass func roiTileArray(forInput: Int32, arguments: [String : Any]?, outputRect: CGRect) -> [CIVector]\nRelationships\nInherits From\nNSObject\nSee Also\nCustom Image Processors\nprotocol CIImageProcessorInput\nA container of image data and information for use in a custom image processor.\nprotocol CIImageProcessorOutput\nA container for writing image data and information produced by a custom image processor."
  },
  {
    "title": "CIImageProcessorInput",
    "url": "https://developer.apple.com/documentation/coreimage/ciimageprocessorinput",
    "html": "Overview\n\nYour app does not define classes that adopt this protocol; Core Image provides an object of this type when applying a custom image processor you create with a CIImageProcessorKernel subclass.\n\nIn your image processor class' process(with:arguments:output:) method, use the provided CIImageProcessorInput object to access the image data and supporting information to perform your custom image processing routine. For example, if you process the image using a Metal shader, use the metalTexture property to bind the image as an input texture. Or, if you process the image using a CPU-based routine, use the baseAddress property to access pixel data in memory.\n\nTo finish setting up or performing your image processing routine, use the provided CIImageProcessorOutput object to return processed pixel data to Core Image.\n\nTopics\nAccessing Input Image Data\nvar baseAddress: UnsafeRawPointer\nA pointer to the image data in CPU memory to be processed.\n\nRequired\n\nvar metalTexture: (any MTLTexture)?\nA Metal texture containing the image data to be processed.\n\nRequired\n\nvar pixelBuffer: CVPixelBuffer?\nA CoreVideo pixel buffer containing the image data to be processed.\n\nRequired\n\nvar surface: IOSurfaceRef\nAn IOSurface object containing the image data to be processed.\n\nRequired\n\nGetting Supplemental Information for Image Processing\nvar region: CGRect\nThe area within the input image to be processed.\n\nRequired\n\nvar bytesPerRow: Int\nThe number of bytes per row of pixels in the input image data.\n\nRequired\n\nvar format: CIFormat\nThe per-pixel data format of the image to be processed.\n\nRequired\n\nInstance Properties\nvar digest: UInt64\n\nRequired\n\nvar roiTileCount: Int\n\nRequired\n\nvar roiTileIndex: Int\n\nRequired\n\nSee Also\nCustom Image Processors\nclass CIImageProcessorKernel\nThe abstract class you extend to create custom image processors that can integrate with Core Image workflows.\nprotocol CIImageProcessorOutput\nA container for writing image data and information produced by a custom image processor."
  },
  {
    "title": "CISampler",
    "url": "https://developer.apple.com/documentation/coreimage/cisampler",
    "html": "Overview\n\nThe CISampler class retrieves samples of images for processing by a CIKernel object. A CISampler object defines a coordinate transform, and modes for interpolation and wrapping. You use CISampler objects in conjunction with other Core Image classes, such as CIFilter, CIKernel, and CIFilterShape, to create custom filters.\n\nTopics\nInitializing a Sampler\ninit(image: CIImage)\nInitializes a sampler with an image object.\ninit(image: CIImage, options: [AnyHashable : Any]?)\nInitializes the sampler with an image object using options specified in a dictionary.\nGetting Information About the Sampler Object\nvar definition: CIFilterShape\nThe domain of definition (DOD) of the sampler\nvar extent: CGRect\nThe rectangle that specifies the extent of the sampler\nConstants\nSampler Option Keys\nKeys for creating a sampler.\nSampler Option Values\nValues for sampler option keys.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nSee Also\nCustom Filters\nWriting Custom Kernels\nWrite your own custom kernels in either the Core Image Kernel Language or the Metal Shading Language.\nclass CIKernel\nA GPU-based image processing routine used to create custom Core Image filters.\nclass CIColorKernel\nA GPU-based image processing routine that processes only the color information in images, used to create custom Core Image filters.\nclass CIWarpKernel\nA GPU-based image processing routine that processes only the geometry information in an image, used to create custom Core Image filters.\nclass CIBlendKernel\nA GPU-based image processing routine that is optimized for blending two images."
  },
  {
    "title": "Simulating Scratchy Analog Film",
    "url": "https://developer.apple.com/documentation/coreimage/simulating_scratchy_analog_film",
    "html": "Overview\n\nThe sepiaTone() filter changes the tint of an image to a reddish-brownish hue resembling old analog photographs. You can enhance the effect by applying random specks and scratches.\n\nFigure 1 Combine filtered white noise with dark scratches to the CISepiaTone filter to create an old analog film effect\n\nThe following steps leverage built-in Core Image filters to tint and texture an image to look as if it were analog film:\n\nApply the sepiaTone() filter.\n\nCreate randomly varying white specks to simulate grain.\n\nCreate randomly varying dark scratches to simulate scratchy film.\n\nComposite the speckle image and scratches onto the sepia-toned image.\n\nApply the Sepia Tone Filter to the Original Image\n\nTint the original image by applying the sepiaTone() filter.\n\nlet sepiaFilter = CIFilter.sepiaTone()\nsepiaFilter.inputImage = inputImage\nsepiaFilter.intensity = 1.0\nlet sepiaCIImage = sepiaFilter.outputImage\n\n\nSimulate Grain by Creating Randomly Varying Specks\n\nYou can use the output of the randomGenerator() filter to generate images containing random noise. Even though the noise pattern isn’t customizable in size, you can extend and crop it to fit the image.\n\nNote\n\nThe image output from randomGenerator() is always the same; even if you reseed your random number generator, the image output from this filter is always the same 512x512 pattern. However, it’s suitable for giving the appearance of randomness. For truly random noise generation, see GameplayKit.\n\nThe filter takes no inputs.\n\nlet colorNoise = CIFilter.randomGenerator()\nlet noiseImage = colorNoise.outputImage\n\n\nNext, apply a whitening effect by chaining the noise output to a colorMatrix() filter. This built-in filter multiplies the noise color values individually and applies a bias to each component. For white grain, apply whitening to the y-component of RGB and no bias.\n\nlet whitenVector = CIVector(x: 0, y: 1, z: 0, w: 0)\nlet fineGrain = CIVector(x:0, y:0.005, z:0, w:0)\nlet zeroVector = CIVector(x: 0, y: 0, z: 0, w: 0)\nlet whiteningFilter = CIFilter.colorMatrix()\nwhiteningFilter.inputImage = noiseImage\nwhiteningFilter.rVector = whitenVector\nwhiteningFilter.gVector = whitenVector\nwhiteningFilter.bVector = whitenVector\nwhiteningFilter.aVector = fineGrain\nwhiteningFilter.biasVector = zeroVector\nlet whiteSpecks = whiteningFilter.outputImage\n\n\nThe whiteSpecks resulting from this filter have the appearance of spotty grain when viewed as an image.\n\nFigure 2 White speckle grain created from a whitening CIColorMatrix applied to CIRandomGenerator noise\n\nCreate the grainy image by compositing the whitened noise as input over the sepia-toned source image using the sourceOverCompositing() filter.\n\nlet speckCompositor = CIFilter.sourceOverCompositing()\nspeckCompositor.inputImage = whiteSpecks\nspeckCompositor.backgroundImage = sepiaCIImage\nlet speckledImage = speckCompositor.outputImage\n\n\nSimulate Scratch by Scaling Randomly Varying Noise\n\nThe process for applying random-looking scratches is the same as the technique used in the white grain: color the output of the randomGenerator() filter.\n\nTo make the speckle resemble scratches, scale the random noise output vertically by applying a scaling CGAffineTransform.\n\nlet verticalScale = CGAffineTransform(scaleX: 1.5, y: 25)\nlet transformedNoise = noiseImage.transformed(by: verticalScale)\n\n\nPreviously, you whitened the speckle image by applying the CIColorMatrix filter evenly across all color components. For the dark scratches, instead focus on only the red component, setting the other vector inputs to zero. This time, instead of multiplying the green, blue, and alpha channels, add bias (0, 1, 1, 1).\n\nlet darkenVector = CIVector(x: 4, y: 0, z: 0, w: 0)\nlet darkenBias = CIVector(x: 0, y: 1, z: 1, w: 1)\nlet darkeningFilter = CIFilter.colorMatrix()\ndarkeningFilter.inputImage = noiseImage\ndarkeningFilter.rVector = darkenVector\ndarkeningFilter.gVector = zeroVector\ndarkeningFilter.bVector = zeroVector\ndarkeningFilter.aVector = zeroVector\ndarkeningFilter.biasVector = darkenBias\nlet randomScratches = darkeningFilter.outputImage.outputImage\n\n\nThe resulting scratches are cyan, so grayscale them using the minimumComponent() filter, which takes the minimum of the RGB values to produce a grayscale image.\n\nlet grayscaleFilter = CIFilter.minimumComponent()\ngrayscaleFilter.inputImage = randomScratches\nlet darkScratches = grayscaleFilter.outputImage\n\n\nThe grayscale filter produces random lines that resemble dark scratches.\n\nFigure 3 Dark scratches created from a darkening CIColorMatrix applied to CIRandomGenerator noise\n\nComposite the Specks and Scratches to the Sepia Image\n\nNow that the components are set, you can add the scratches to the grainy sepia image produced earlier. However, unlike the grainy texture, the scratches impact the image multiplicatively. Instead of the sourceOverCompositing() filter, which composites source over background, use the multiplyCompositing() filter to compose the scratches multiplicatively. Set the scratched image as the filter’s input image, and tab the speckle-composited sepia image as the input background image.\n\nlet oldFilmCompositor = CIFilter.multiplyCompositing()\noldFilmCompositor.inputImage = darkScratches\noldFilmCompositor.backgroundImage = speckledImage\nlet oldFilmImage = oldFilmCompositor.outputImage\n\n\nSince the noise images had different dimensions than the source image, crop the composited result to the original image size to remove excess beyond the original extent.\n\nlet finalImage = oldFilmImage.cropped(to: inputImage.extent)\n\n\nThe cropped image represents the final result: a sepia-toned image with simulated grain and scratches composited to give it an analog film appearance.\n\nFigure 4 Final result of CIMultiplyCompositing the source image with grain and scratches.\n\nSee Also\nFilter Recipes\nApplying a Chroma Key Effect\nReplace a color in one image with the background from another.\nSelectively Focusing on an Image\nFocus on a part of an image by applying Gaussian blur and gradient masks.\nCustomizing Image Transitions\nTransition between images in creative ways using Core Image filters."
  },
  {
    "title": "Customizing Image Transitions",
    "url": "https://developer.apple.com/documentation/coreimage/customizing_image_transitions",
    "html": "Overview\n\nYou can add visual effects to an image transition by chaining together Core Image CIFilter objects in the category CICategoryTransition. Each filter from this category represents a single transition effect.\n\nFor example, you can combine an effect that dissolves an image and one that pixelates it as a transition to a second image. This particular transition chain comprises three steps:\n\nCreate a dissolveTransition() transition filter with time as an input parameter.\n\nCreate a pixellate() transition filter with time as an input parameter.\n\nInitiate the transition by adding a time step to your run loop.\n\nFigure 1 Combine and to create a custom image transition.\n\nLoad Source and Target Images\n\nFilters in the transition category require your program to load both source and target images in order to transform the source into the destination.\n\nlet sourceURL = URL(fileURLWithPath: \"\\(Bundle.main.bundlePath)/YourSourceImage.JPG\")\nlet sourceCIImage = CIImage(contentsOf: sourceURL)\n        \nlet destinationURL = URL(fileURLWithPath: \"\\(Bundle.main.bundlePath)/YourTargetImage.JPG\")\nlet destinationCIImage = CIImage(contentsOf: destinationURL)\n\n\nCreate a Time-Dependent Dissolve Transition\n\nThe key difference of transition filters from their normal filter chain counterparts is the dependence on time. After creating a CIFilter from the Transition Filters category, you set the value of the time parameter to a float between 0.0 and 1.0 to indicate how far along the transition has progressed.\n\nWrite each transition filter to accept time as an input parameter, and reapply the filter at a regular interval to transform the image from its source state to the target state.\n\nimport simd\n\n\nfunc dissolveFilter(_ inputImage: CIImage,\n                    to targetImage: CIImage,\n                    time: TimeInterval) -> CIImage? {\n    let time = simd_smoothstep(0, 1, time)        \n    let filter = CIFilter.dissolveTransition()\n    filter.inputImage = inputImage\n    filter.targetImage = targetImage\n    filter.time = Float(time)\n    return filter.outputImage\n}\n\n\nYou don’t need to pass time linearly from 0.0 to 1.0. In fact, you can advance the transition at a variable rate by modulating the time variable with a function, such as simd_smoothstep, which is a smooth ramp function clamped between two values, imbuing the dissolve effect with an ease-in ease-out feel.\n\nFigure 2 simd_smoothstep(0, 1, time): a smooth ramp between 0 and 1\n\nCreate a Time-Dependent Pixelate Transition\n\nLike the dissolve transition, you can write the pixelate transition filter as a time-dependent function as well.\n\nimport simd\n\n\nfunc pixelateFilter(_ inputImage: CIImage, time: TimeInterval) -> CIImage? {\n    let scale = simd_smoothstep(1, 0, fabs(time))\n    let filter = CIFilter.pixellate()\n    filter.inputImage = inputImage\n    filter.scale = 100 * Float(scale)\n    return filter.outputImage\n}\n\n\nAs with the dissolve filter, you can modify the speed and acceleration of the transition by changing the way time varies between 0.0 and 1.0. In this case, unlike the dissolveTransition() filter, the pixellate() filter accepts a scale, which you can vary over a smoothened triangle function: simd_smoothstep(1, 0, abs(time)).\n\nFigure 3 simd_smoothstep(1, 0, abs(time)): a smoothened triangle ramp that goes from 0 to 1 to 0\n\nThis function puts the peak of the pixelation at the middle of the transition: the pixels start and end small, closely approximating the source image, but as the transition reaches its halfway point, the pixels scale to their largest size, effectively blocking out the moment farthest from source and target.\n\nStep Time with a Display Link\n\nIn writing the filter functions to accept a time parameter, you parametrized the transition effect moving from source to target. Now, you must move time forward when you want to perform the transition.\n\nAdding a CADisplayLink to your run loop gives you a way to refresh an image every time a screen redraw occurs, so you can execute on a reliably regular time interval. In the case of a transition, you need only perform the following steps:\n\nCreate the display link to call an update function.\n\nAdd to your app’s main run loop to begin the transition. Start time at 0.0 and track time through the update function.\n\nIn the update function, update the transition filters’ inputTime value and refresh the filtered image. Since this example chains two filters for a simultaneous effect, update both filters.\n\nIn the update function, remove the link once time has expired.\n\nNote\n\nAdding a Timer may seem like a logical strategy for stepping time, but the display link fires with greater precision in sync with screen redraws.\n\nCreate the Display Link to Call an Update Function\ndisplayLink = CADisplayLink(target: self, \n                            selector: #selector(stepTime))\n\n\nKeeping the display link around beyond function scope allows you to remove it when the transition completes.\n\nAdd the Display Link to Begin the Transition\n\nTo begin the transition effect, add the CADisplayLink to your program’s main run loop, so it can execute each time step and redraw the transitioning CIImage.\n\nfunc beginTransition() {\n        \n    time = 0\n    dt = 0.005\n        \n    displayLink = CADisplayLink(target: self,\n                                selector: #selector(stepTime))\n    displayLink.add(to: RunLoop.current,\n                    forMode: RunLoopMode.defaultRunLoopMode)\n}\n\n\nWrite the Transition Update Function\n\nThe CADisplayLink should call a time-stepping function on each pass through the run loop. Inside this function, recompute the filtered image with that frame's time variable.\n\n@objc\nfunc stepTime() {\n    \n   time += dt\n        \n   // End transition after 1 second\n   if time > 1 {\n       displayLink.remove(from:RunLoop.main, forMode:RunLoopMode.defaultRunLoopMode)\n   } else {\n       guard let dissolvedCIImage = dissolveFilter(sourceCIImage,\n                                                   to:finalCIImage,\n                                                   time:time) else {\n                                                      return\n       }\n       guard let pixelatedCIImage = pixelateFilter(dissolvedCIImage,\n                                                   time:time) else {\n                                                      return\n       }\n       // imageView and ciContext are properties of the class.\n       showCIImage(pixelatedCIImage, in:imageView, context:ciContext)\n   }\n}\n\n\nAs a convenience, the following helper function shows a CIImage in a UIImageView.\n\nfunc showCIImage(_ ciImage: CIImage,\n                 in imageView: UIImageView,\n                 context: CIContext) {\n    \n    guard let cgImage = context.createCGImage(ciImage,\n                                              from: ciImage.extent) else {\n                                                 return\n    }\n    let uiImage = UIImage(CGImage:cgImage)\n        \n    imageView.image = uiImage\n}\n\n\nExplore Other Transition Visual Effects\n\nThe Core Image framework provides many distinct visual effects through its built-in catalog of filters. You can substitute a different transition effect for the dissolve and pixelation effects.\n\nSee filters under the Transition Filters collection for other effects to try.\n\nFor example, the copyMachineTransition() filter passes a scanning light over the source image as it transforms into the target image.\n\nFigure 4 CICopyMachineTransition from a beach at sunset to a beach at daytime\n\nThe pageCurlWithShadowTransition() filter simulates the turn of a page, peeling the source image toward the right to reveal the target image underneath. You can include a separate image on the back of the flipped page.\n\nFigure 5 CIPageCurlWithShadowTransition from a beach at daytime with rainbow in the sky to a beach at sunset, with a flower image on the back of the page\n\nThe barsSwipeTransition() slices the source image into vertical bars that sequentially slide off the page, revealing the target image underneath.\n\nFigure 6 CIBarsSwipeTransition from a beach at sunset to a beach at daytime with rainbow in the sky\n\nYou can apply transitions such as accordion folding, flash photography, disintegration, and watery rippling. Substitute the dissolve and pixellate filters with others from the same category, and tweak the time or scale parameter to customize the effect to fit your app.\n\nSee Also\nFilter Recipes\nApplying a Chroma Key Effect\nReplace a color in one image with the background from another.\nSelectively Focusing on an Image\nFocus on a part of an image by applying Gaussian blur and gradient masks.\nSimulating Scratchy Analog Film\nDegrade the quality of an image to make it look like dated, scratchy analog film."
  },
  {
    "title": "Transition Filters",
    "url": "https://developer.apple.com/documentation/coreimage/transition_filters",
    "html": "Overview\n\naccordionFoldTransition()\n\nbarsSwipeTransition()\n\ncopyMachineTransition()\n\ndisintegrateWithMaskTransition()\n\ndissolveTransition()\n\nflashTransition()\n\nmodTransition()\n\npageCurlTransition()\n\npageCurlWithShadowTransition()\n\nrippleTransition()\n\nswipeTransition()\n\nSee Also\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nDistortion Filters\nApply distortion to images.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nGradient Filters\nGenerate linear and radial gradients.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nReduction Filters\nReduction Filters\nSharpening Filters\nApply sharpening to images.\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTile Effect Filters\nProduce tiled images from source images."
  },
  {
    "title": "Stylizing Filters",
    "url": "https://developer.apple.com/documentation/coreimage/stylizing_filters",
    "html": "Overview\n\nblendWithAlphaMask()\n\nblendWithBlueMask()\n\nblendWithMask()\n\nblendWithRedMask()\n\nbloom()\n\ncannyEdgeDetectorFilter\n\ncomicEffect()\n\ncoreMLModel()\n\ncrystallize()\n\ndepthOfField()\n\nedges()\n\nedgeWork()\n\ngaborGradients()\n\ngloom()\n\nheightFieldFromMask()\n\nhexagonalPixellate()\n\nhighlightShadowAdjust()\n\nlineOverlay()\n\nmix()\n\npersonSegmentationFilter\n\npixellate()\n\npointillize()\n\nsaliencyMap()\n\nshadedMaterial()\n\nsobelGradientsFilter\n\nspotColor()\n\nSee Also\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nDistortion Filters\nApply distortion to images.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nGradient Filters\nGenerate linear and radial gradients.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nReduction Filters\nReduction Filters\nSharpening Filters\nApply sharpening to images.\nTile Effect Filters\nProduce tiled images from source images.\nTransition Filters\nTransition between two images by using effects including page curl and swipe."
  },
  {
    "title": "Tile Effect Filters",
    "url": "https://developer.apple.com/documentation/coreimage/tile_effect_filters",
    "html": "Overview\n\naffineClamp()\n\naffineTile()\n\neightfoldReflectedTile()\n\nfourfoldReflectedTile()\n\nfourfoldRotatedTile()\n\nfourfoldTranslatedTile()\n\nglideReflectedTile()\n\nkaleidoscope()\n\nopTile()\n\nparallelogramTile()\n\nperspectiveTile()\n\nsixfoldReflectedTile()\n\nsixfoldRotatedTile()\n\ntriangleKaleidoscope()\n\ntriangleTile()\n\ntwelvefoldReflectedTile()\n\nSee Also\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nDistortion Filters\nApply distortion to images.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nGradient Filters\nGenerate linear and radial gradients.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nReduction Filters\nReduction Filters\nSharpening Filters\nApply sharpening to images.\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTransition Filters\nTransition between two images by using effects including page curl and swipe."
  },
  {
    "title": "Gradient Filters",
    "url": "https://developer.apple.com/documentation/coreimage/gradient_filters",
    "html": "Overview\n\ngaussianGradient()\n\nhueSaturationValueGradient()\n\nlinearGradient()\n\nradialGradient()\n\nsmoothLinearGradient()\n\nSee Also\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nDistortion Filters\nApply distortion to images.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nReduction Filters\nReduction Filters\nSharpening Filters\nApply sharpening to images.\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTile Effect Filters\nProduce tiled images from source images.\nTransition Filters\nTransition between two images by using effects including page curl and swipe."
  },
  {
    "title": "Sharpening Filters",
    "url": "https://developer.apple.com/documentation/coreimage/sharpening_filters",
    "html": "Overview\n\nsharpenLuminance()\n\nunsharpMask()\n\nSee Also\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nDistortion Filters\nApply distortion to images.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nGradient Filters\nGenerate linear and radial gradients.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nReduction Filters\nReduction Filters\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTile Effect Filters\nProduce tiled images from source images.\nTransition Filters\nTransition between two images by using effects including page curl and swipe."
  },
  {
    "title": "Reduction Filters",
    "url": "https://developer.apple.com/documentation/coreimage/reduction_filters",
    "html": "Overview\n\nareaAverage()\n\nareaHistogram()\n\nareaLogarithmicHistogramFilter\n\nareaMaximum()\n\nareaMaximumAlpha()\n\nareaMinimum()\n\nareaMinimumAlpha()\n\nareaMinMax()\n\nareaMinMaxRed()\n\ncolumnAverage()\n\nhistogramDisplay()\n\nkMeans()\n\nrowAverage()\n\nSee Also\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nDistortion Filters\nApply distortion to images.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nGradient Filters\nGenerate linear and radial gradients.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nSharpening Filters\nApply sharpening to images.\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTile Effect Filters\nProduce tiled images from source images.\nTransition Filters\nTransition between two images by using effects including page curl and swipe."
  },
  {
    "title": "Generator Filters",
    "url": "https://developer.apple.com/documentation/coreimage/generator_filters",
    "html": "Overview\n\nattributedTextImageGenerator()\n\naztecCodeGenerator()\n\nbarcodeGenerator()\n\nblurredRectangleGeneratorFilter\n\ncheckerboardGenerator()\n\ncode128BarcodeGenerator()\n\nlenticularHaloGenerator()\n\nmeshGenerator()\n\npdf417BarcodeGenerator()\n\nqrCodeGenerator()\n\nrandomGenerator()\n\nroundedRectangleGenerator()\n\nroundedRectangleStrokeGeneratorFilter\n\nstarShineGenerator()\n\nstripesGenerator()\n\nsunbeamsGenerator()\n\ntextImageGenerator()\n\nSee Also\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nDistortion Filters\nApply distortion to images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nGradient Filters\nGenerate linear and radial gradients.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nReduction Filters\nReduction Filters\nSharpening Filters\nApply sharpening to images.\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTile Effect Filters\nProduce tiled images from source images.\nTransition Filters\nTransition between two images by using effects including page curl and swipe."
  },
  {
    "title": "Distortion Filters",
    "url": "https://developer.apple.com/documentation/coreimage/distortion_filters",
    "html": "Overview\n\nbumpDistortionFilter\n\nbumpDistortionLinearFilter\n\ncircleSplashDistortionFilter\n\ncircularWrapFilter\n\ndisplacementDistortionFilter\n\ndrosteFilter\n\nglassDistortionFilter\n\nglassLozengeFilter\n\nholeDistortionFilter\n\nlightTunnelFilter\n\nninePartStretchedFilter\n\nninePartTiledFilter\n\npinchDistortionFilter\n\nstretchCropFilter\n\ntorusLensDistortionFilter\n\ntwirlDistortionFilter\n\nvortexDistortionFilter\n\nSee Also\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nGradient Filters\nGenerate linear and radial gradients.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nReduction Filters\nReduction Filters\nSharpening Filters\nApply sharpening to images.\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTile Effect Filters\nProduce tiled images from source images.\nTransition Filters\nTransition between two images by using effects including page curl and swipe."
  },
  {
    "title": "Convolution Filters",
    "url": "https://developer.apple.com/documentation/coreimage/convolution_filters",
    "html": "Overview\n\nconvolution3X3()\n\nconvolution5X5()\n\nconvolution7X7()\n\nconvolution9Horizontal()\n\nconvolution9Vertical()\n\nconvolutionRGB3X3Filter\n\nconvolutionRGB5X5Filter\n\nconvolutionRGB7X7Filter\n\nconvolutionRGB9HorizontalFilter\n\nconvolutionRGB9VerticalFilter\n\nSee Also\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nDistortion Filters\nApply distortion to images.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nGradient Filters\nGenerate linear and radial gradients.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nReduction Filters\nReduction Filters\nSharpening Filters\nApply sharpening to images.\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTile Effect Filters\nProduce tiled images from source images.\nTransition Filters\nTransition between two images by using effects including page curl and swipe."
  },
  {
    "title": "Color Adjustment Filters",
    "url": "https://developer.apple.com/documentation/coreimage/color_adjustment_filters",
    "html": "Overview\n\ncolorAbsoluteDifference()\n\ncolorClamp()\n\ncolorControls()\n\ncolorMatrix()\n\ncolorPolynomial()\n\ncolorThreshold()\n\ncolorThresholdOtsuFilter\n\ndepthToDisparity()\n\ndisparityToDepth()\n\nexposureAdjust()\n\ngammaAdjust()\n\nhueAdjust()\n\nlinearToSRGBToneCurve()\n\nsRGBToneCurveToLinear()\n\ntemperatureAndTint()\n\ntoneCurve()\n\nvibrance()\n\nwhitePointAdjust()\n\nSee Also\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nDistortion Filters\nApply distortion to images.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nGradient Filters\nGenerate linear and radial gradients.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nReduction Filters\nReduction Filters\nSharpening Filters\nApply sharpening to images.\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTile Effect Filters\nProduce tiled images from source images.\nTransition Filters\nTransition between two images by using effects including page curl and swipe."
  },
  {
    "title": "CIImage",
    "url": "https://developer.apple.com/documentation/coreimage/ciimage",
    "html": "Overview\n\nYou use CIImage objects in conjunction with other Core Image classes—such as CIFilter, CIContext, CIVector, and CIColor—to take advantage of the built-in Core Image filters when processing images. You can create CIImage objects with data supplied from a variety of sources, including Quartz 2D images, Core Video image buffers (CVImageBuffer), URL-based objects, and NSData objects.\n\nAlthough a CIImage object has image data associated with it, it is not an image. You can think of a CIImage object as an image “recipe.” A CIImage object has all the information necessary to produce an image, but Core Image doesn’t actually render an image until it is told to do so. This lazy evaluation allows Core Image to operate as efficiently as possible.\n\nCIContext and CIImage objects are immutable, which means each can be shared safely among threads. Multiple threads can use the same GPU or CPU CIContext object to render CIImage objects. However, this is not the case for CIFilter objects, which are mutable. A CIFilter object cannot be shared safely among threads. If you app is multithreaded, each thread must create its own CIFilter objects. Otherwise, your app could behave unexpectedly.\n\nCore Image also provides auto-adjustment methods. These methods analyze an image for common deficiencies and return a set of filters to correct those deficiencies. The filters are preset with values for improving image quality by altering values for skin tones, saturation, contrast, and shadows and for removing red-eye or other artifacts caused by flash. (See Getting Autoadjustment Filters.)\n\nFor a discussion of all the methods you can use to create CIImage objects on iOS and macOS, see Core Image Programming Guide.\n\nTopics\nCreating an Image\nclass func empty() -> CIImage\nCreates and returns an empty image object.\nInitializing an Image\ninit(color: CIColor)\nInitializes an image of infinite extent whose entire content is the specified color.\ninit(bitmapData: Data, bytesPerRow: Int, size: CGSize, format: CIFormat, colorSpace: CGColorSpace?)\nInitializes an image object with bitmap data.\ninit(cgImage: CGImage)\nInitializes an image object with a Quartz 2D image.\ninit(cgImage: CGImage, options: [CIImageOption : Any]?)\nInitializes an image object with a Quartz 2D image, using the specified options.\ninit?(bitmapImageRep: NSBitmapImageRep)\nInitializes an image object with the specified bitmap image representation.\ninit?(image: UIImage)\nInitializes an image object with the specified UIKit image object.\ninit?(image: UIImage, options: [CIImageOption : Any]?)\nInitializes an image object with the specified UIKit image object, using the specified options.\ninit(cgLayer: CGLayer)\nInitializes an image object from the contents supplied by a CGLayer object.\nDeprecated\ninit(cgLayer: CGLayer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents supplied by a CGLayer object, using the specified options.\nDeprecated\ninit?(contentsOf: URL)\nInitializes an image object by reading an image from a URL.\ninit?(contentsOf: URL, options: [CIImageOption : Any]?)\nInitializes an image object by reading an image from a URL, using the specified options.\ninit(cvImageBuffer: CVImageBuffer)\nInitializes an image object from the contents of a Core Video image buffer.\ninit(cvImageBuffer: CVImageBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video image buffer, using the specified options.\ninit(cvPixelBuffer: CVPixelBuffer)\nInitializes an image object from the contents of a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, options: [CIImageOption : Any]?)\nInitializes an image object from the contents of a Core Video pixel buffer using the specified options.\ninit?(data: Data)\nInitializes an image object with the supplied image data.\ninit?(data: Data, options: [CIImageOption : Any]?)\nInitializes an image object with the supplied image data, using the specified options.\ninit(imageProvider: Any, size: Int, Int, format: CIFormat, colorSpace: CGColorSpace?, options: [CIImageOption : Any]?)\nInitializes an image object with data provided by an image provider, using the specified options.\nstatic let providerTileSize: CIImageOption\nA key for the image tiles size. The associated value is an NSArray that containsNSNumber objects for the dimensions of the image tiles requested from the image provider.\nstatic let providerUserInfo: CIImageOption\nA key for data needed by the image provider. The associated value is an object that contains the needed data.\ninit(texture: UInt32, size: CGSize, flipped: Bool, colorSpace: CGColorSpace?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit(texture: UInt32, size: CGSize, flipped: Bool, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by an OpenGL texture.\nDeprecated\ninit?(mtlTexture: any MTLTexture, options: [CIImageOption : Any]?)\nInitializes an image object with data supplied by a Metal texture.\ninit(ioSurface: IOSurfaceRef)\nInitializes an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, options: [CIImageOption : Any]?)\nInitializes, using the specified options, an image with the contents of an IOSurface.\ninit(ioSurface: IOSurfaceRef, plane: Int, format: CIFormat, options: [CIImageOption : Any]?)\nInitializes, using the specified format and options, an image with the contents of a specific data plane in an IOSurface.\nDeprecated\nCreating an Image by Modifying an Existing Image\nfunc applyingFilter(String, parameters: [String : Any]) -> CIImage\nReturns a new image created by applying a filter to the original image with the specified name and parameters.\nfunc applyingFilter(String) -> CIImage\nApplies the filter to an image and returns the output.\nfunc transformed(by: CGAffineTransform) -> CIImage\nReturns a new image that represents the original image after applying an affine transform.\nfunc cropped(to: CGRect) -> CIImage\nReturns a new image with a cropped portion of the original image.\nfunc oriented(forExifOrientation: Int32) -> CIImage\nReturns a new image created by transforming the original image to the specified EXIF orientation.\nfunc clampedToExtent() -> CIImage\nReturns a new image created by making the pixel colors along its edges extend infinitely in all directions.\nfunc clamped(to: CGRect) -> CIImage\nReturns a new image created by cropping to a specified area, then making the pixel colors along the edges of the cropped image extend infinitely in all directions.\nfunc composited(over: CIImage) -> CIImage\nReturns a new image created by compositing the original image over the specified destination image.\nfunc matchedToWorkingSpace(from: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the specified color space to the context’s working color space.\nfunc matchedFromWorkingSpace(to: CGColorSpace) -> CIImage?\nReturns a new image created by color matching from the context’s working color space to the specified color space.\nfunc premultiplyingAlpha() -> CIImage\nReturns a new image created by multiplying the image’s RGB values by its alpha values.\nfunc unpremultiplyingAlpha() -> CIImage\nReturns a new image created by dividing the image’s RGB values by its alpha values.\nfunc settingAlphaOne(in: CGRect) -> CIImage\nReturns a new image created by setting all alpha values to 1.0 within the specified rectangle and to 0.0 outside of that area.\nfunc applyingGaussianBlur(sigma: Double) -> CIImage\nReturns a new image created by applying a Gaussian Blur filter to the image.\nfunc settingProperties([AnyHashable : Any]) -> CIImage\nReturns a new image created by adding the specified metadata properties to the image.\nfunc insertingIntermediate() -> CIImage\nReturns a new image created by inserting an intermediate.\nfunc insertingIntermediate(cache: Bool) -> CIImage\nReturns a new image created by inserting a cacheable intermediate.\nGetting Image Information\nvar definition: CIFilterShape\nReturns a filter shape object that represents the domain of definition of the image.\nvar extent: CGRect\nA rectangle that specifies the extent of the image.\nvar properties: [String : Any]\nA dictionary containing metadata about the image.\nvar url: URL?\nThe URL from which the image was loaded.\nvar colorSpace: CGColorSpace?\nThe color space of the image.\nfunc orientationTransform(forExifOrientation: Int32) -> CGAffineTransform\nReturns the transformation needed to reorient the image to the specified orientation.\nDrawing Images\nfunc draw(at: NSPoint, from: NSRect, operation: NSCompositingOperation, fraction: CGFloat)\nDraws all or part of the image at the specified point in the current coordinate system.\nfunc draw(in: NSRect, from: NSRect, operation: NSCompositingOperation, fraction: CGFloat)\nDraws all or part of the image in the specified rectangle in the current coordinate system\nGetting Autoadjustment Filters\nfunc autoAdjustmentFilters() -> [CIFilter]\nReturns all possible automatically selected and configured filters for adjusting the image.\nfunc autoAdjustmentFilters(options: [CIImageAutoAdjustmentOption : Any]?) -> [CIFilter]\nReturns a subset of automatically selected and configured filters for adjusting the image.\nAutoadjustment Keys\nConstants used as keys in the options dictionary for the autoAdjustmentFilters(options:) method.\nWorking with Filter Regions of Interest\nfunc regionOfInterest(for: CIImage, in: CGRect) -> CGRect\nReturns the region of interest for the filter chain that generates the image.\nWorking with Orientation\nfunc oriented(CGImagePropertyOrientation) -> CIImage\nTransforms the original image by a given CGImagePropertyOrientation and returns the result.\nfunc orientationTransform(for: CGImagePropertyOrientation) -> CGAffineTransform\nThe affine transform for changing the image to the given orientation.\nSampling the Image\nfunc samplingNearest() -> CIImage\nSamples the image using nearest-neighbor and returns the result.\nfunc samplingLinear() -> CIImage\nSamples the image using bilinear interpolation and returns the result.\nAccessing Original Image Content\nvar cgImage: CGImage?\nThe CoreGraphics image object this image was created from, if applicable.\nvar pixelBuffer: CVPixelBuffer?\nThe CoreVideo pixel buffer this image was created from, if applicable.\nvar depthData: AVDepthData?\nAVDepthData representation of the depth image.\nvar portraitEffectsMatte: AVPortraitEffectsMatte?\nAVPortraitEffectsMatte representation of portrait effects.\nImage Dictionary Keys\nConstants used as keys in the options dictionary when initializing an image.\nstatic let colorSpace: CIImageOption\nThe key for a color space.\nstatic let properties: CIImageOption\nThe key for image metadata properties.\nstatic let applyOrientationProperty: CIImageOption\nThe key for transforming an image according to orientation metadata.\nstatic let textureTarget: CIImageOption\nThe key for an OpenGL texture target.\nDeprecated\nstatic let textureFormat: CIImageOption\nThe key for an OpenGL texture format.\nDeprecated\nstatic let nearestSampling: CIImageOption\nThe key into the properties dictionary to indicate whether to use nearest-neighbor sampling.\nstatic let auxiliaryDepth: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary depth image.\nstatic let auxiliaryDisparity: CIImageOption\nThe key into the properties dictionary indicating whether to return an auxiliary disparity image.\nstatic let auxiliaryPortraitEffectsMatte: CIImageOption\nThe key into the properties dictionary indicating whether to return auxiliary portrait effects matte.\nInitializers\ninit(cgImageSource: CGImageSource, index: Int, options: [CIImageOption : Any]?)\ninit?(depthData: AVDepthData)\ninit?(depthData: AVDepthData, options: [String : Any]?)\ninit?(portaitEffectsMatte: AVPortraitEffectsMatte)\ninit?(portaitEffectsMatte: AVPortraitEffectsMatte, options: [CIImageOption : Any]?)\ninit?(semanticSegmentationMatte: AVSemanticSegmentationMatte)\ninit?(semanticSegmentationMatte: AVSemanticSegmentationMatte, options: [CIImageOption : Any]?)\nInstance Properties\nvar semanticSegmentationMatte: AVSemanticSegmentationMatte?\nType Properties\nclass var black: CIImage\nclass var blue: CIImage\nclass var clear: CIImage\nclass var cyan: CIImage\nclass var gray: CIImage\nclass var green: CIImage\nclass var magenta: CIImage\nclass var red: CIImage\nclass var white: CIImage\nclass var yellow: CIImage\nInstance Methods\nfunc convertingLabToWorkingSpace() -> CIImage\nfunc convertingWorkingSpaceToLab() -> CIImage\nfunc transformed(by: CGAffineTransform, highQualityDownsample: Bool) -> CIImage\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nSee Also\nEssentials\nProcessing an Image Using Built-in Filters\nApply effects such as sepia tint, highlight strengthening, and scaling to images.\nclass CIFilter\nAn image processor that produces an image by manipulating one or more input images or by generating new image data.\nclass CIContext\nAn evaluation context for rendering image processing results and performing image analysis.\nBasic Data Types\nColors, vectors, and other types used in applying or creating image filters."
  },
  {
    "title": "CIContext",
    "url": "https://developer.apple.com/documentation/coreimage/cicontext",
    "html": "Overview\n\nThe CIContext class provides an evaluation context for Core Image processing with Quartz 2D, Metal, or OpenGL. You use CIContext objects in conjunction with other Core Image classes, such as CIFilter, CIImage, and CIColor, to process images using Core Image filters. You also use a Core Image context with the CIDetector class to analyze images—for example, to detect faces or barcodes.\n\nCIContext and CIImage objects are immutable, so multiple threads can use the same CIContext object to render CIImage objects. However, CIFilter objects are mutable and thus cannot be shared safely among threads. Each thread must create its own CIFilter objects, but you can pass a filter’s immutable input and output CIImage objects between threads.\n\nTopics\nCreating a Context Without Specifying a Destination\ninit()\nInitializes a context without a specific rendering destination, using default options.\ninit(options: [CIContextOption : Any]?)\nInitializes a context without a specific rendering destination, using the specified options.\nCreating a Context for CPU-Based Rendering\ninit(cgContext: CGContext, options: [CIContextOption : Any]?)\nCreates a Core Image context from a Quartz context, using the specified options.\nCreating a Context for GPU-Based Rendering with OpenGL\ninit(cglContext: CGLContextObj, pixelFormat: CGLPixelFormatObj?, colorSpace: CGColorSpace?, options: [CIContextOption : Any]?)\nCreates a Core Image context from a CGL context, using the specified options, color space, and pixel format object.\nDeprecated\ninit(eaglContext: EAGLContext)\nCreates a Core Image context from an EAGL context.\nDeprecated\ninit(eaglContext: EAGLContext, options: [CIContextOption : Any]?)\nCreates a Core Image context from an EAGL context using the specified options.\nDeprecated\ninit?(forOfflineGPUAt: UInt32)\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display.\nDeprecated\ninit?(forOfflineGPUAt: UInt32, colorSpace: CGColorSpace?, options: [CIContextOption : Any]?, sharedContext: CGLContextObj?)\nCreates an OpenGL-based Core Image context using a GPU that is not currently driving a display, with the specified options.\nDeprecated\nCreating a Context for GPU-Based Rendering with Metal\ninit(mtlDevice: any MTLDevice)\nCreates a Core Image context using the specified Metal device.\ninit(mtlDevice: any MTLDevice, options: [CIContextOption : Any]?)\nCreates a Core Image context using the specified Metal device and options.\nRendering Images\nfunc createCGImage(CIImage, from: CGRect) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object.\nfunc createCGImage(CIImage, from: CGRect, format: CIFormat, colorSpace: CGColorSpace?, deferred: Bool) -> CGImage?\nCreates a Quartz 2D image from a region of a Core Image image object with deferred rendering.\nfunc createCGLayer(with: CGSize, info: CFDictionary?) -> CGLayer?\nCreates a CGLayer object from the provided parameters.\nDeprecated\nfunc render(CIImage, toBitmap: UnsafeMutableRawPointer, rowBytes: Int, bounds: CGRect, format: CIFormat, colorSpace: CGColorSpace?)\nRenders to the given bitmap.\nfunc render(CIImage, to: CVPixelBuffer)\nRenders an image into a pixel buffer.\nfunc render(CIImage, to: CVPixelBuffer, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into a pixel buffer.\nfunc render(CIImage, to: IOSurfaceRef, bounds: CGRect, colorSpace: CGColorSpace?)\nRenders a region of an image into an IOSurface object.\nfunc render(CIImage, to: any MTLTexture, commandBuffer: (any MTLCommandBuffer)?, bounds: CGRect, colorSpace: CGColorSpace)\nRenders a region of an image to a Metal texture.\nDrawing Images\nfunc draw(CIImage, at: CGPoint, from: CGRect)\nRenders a region of an image to a point in the context destination.\nfunc draw(CIImage, in: CGRect, from: CGRect)\nRenders a region of an image to a rectangle in the context destination.\nDetermining the Allowed Extents for Images Used by a Context\nfunc inputImageMaximumSize() -> CGSize\nReturns the maximum size allowed for any image rendered into the context.\nfunc outputImageMaximumSize() -> CGSize\nReturns the maximum size allowed for any image created by the context.\nManaging Resources\nfunc clearCaches()\nFrees any cached data, such as temporary images, associated with the context and runs the garbage collector.\nfunc reclaimResources()\nRuns the garbage collector to reclaim any resources that the context no longer requires.\nclass func offlineGPUCount() -> UInt32\nReturns the number of GPUs not currently driving a display.\nvar workingColorSpace: CGColorSpace?\nThe working color space of the Core Image context.\nvar workingFormat: CIFormat\nThe working pixel format of the Core Image context.\nRendering Images for Data or File Export\nfunc tiffRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in TIFF format.\nfunc jpegRepresentation(of: CIImage, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in JPEG format.\nfunc pngRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in PNG format.\nfunc heifRepresentation(of: CIImage, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data?\nRenders the image and exports the resulting image data in HEIF format.\nfunc writeTIFFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in TIFF format.\nfunc writeJPEGRepresentation(of: CIImage, to: URL, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in JPEG format.\nfunc writePNGRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in PNG format.\nfunc writeHEIFRepresentation(of: CIImage, to: URL, format: CIFormat, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nRenders the image and exports the resulting image data as a file in HEIF format.\nstatic let avDepthData: CIImageRepresentationOption\noptions dictionary key for image export methods to represent data as AVDepthData.\nstatic let depthImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output depth data.\nstatic let disparityImage: CIImageRepresentationOption\noptions dictionary key for image export methods to output disparity data.\nConstants\nKeys to be used in the options dictionary when creating a CIContext object.\nstatic let outputColorSpace: CIContextOption\nA key for the color space to use for images before they are rendered to the context.\nstatic let workingColorSpace: CIContextOption\nA key for the color space to use for image operations.\nstatic let workingFormat: CIContextOption\nAn option for the color format to use for intermediate results when rendering with the context.\nstatic let highQualityDownsample: CIContextOption\nAn option controlling the quality of image downsampling operations performed by the context.\nstatic let outputPremultiplied: CIContextOption\nAn option for whether output rendering by the context produces alpha-premultiplied pixels.\nstatic let cacheIntermediates: CIContextOption\nAn option for whether the context caches the contents of any intermediate pixel buffers it uses during rendering.\nstatic let useSoftwareRenderer: CIContextOption\nA key for enabling software renderer use. If the associated NSNumber object is true, the software renderer is required.\nstatic let priorityRequestLow: CIContextOption\nA key for enabling low-priority GPU use.\nCustomizing Render Destination\nfunc prepareRender(CIImage, from: CGRect, to: CIRenderDestination, at: CGPoint)\nAn optional call to warm up a CIContext so that subsequent calls to render with the same arguments run more efficiently.\nfunc startTask(toClear: CIRenderDestination) -> CIRenderTask\nFills the entire destination with black or clear depending on its alphaMode.\nfunc startTask(toRender: CIImage, from: CGRect, to: CIRenderDestination, at: CGPoint) -> CIRenderTask\nRenders a portion of an image to a point in the destination.\nfunc startTask(toRender: CIImage, to: CIRenderDestination) -> CIRenderTask\nRenders an image to a destination so that point (0, 0) of the image maps to point (0, 0) of the destination.\nInitializers\ninit(mtlCommandQueue: any MTLCommandQueue)\ninit(mtlCommandQueue: any MTLCommandQueue, options: [CIContextOption : Any]?)\nInstance Methods\nfunc depthBlurEffectFilter(for: CIImage, disparityImage: CIImage, portraitEffectsMatte: CIImage?, hairSemanticSegmentation: CIImage?, glassesMatte: CIImage?, gainMap: CIImage?, orientation: CGImagePropertyOrientation, options: [AnyHashable : Any]?) -> CIFilter?\nfunc depthBlurEffectFilter(for: CIImage, disparityImage: CIImage, portraitEffectsMatte: CIImage?, hairSemanticSegmentation: CIImage?, orientation: CGImagePropertyOrientation, options: [AnyHashable : Any]?) -> CIFilter?\nfunc depthBlurEffectFilter(for: CIImage, disparityImage: CIImage, portraitEffectsMatte: CIImage?, orientation: CGImagePropertyOrientation, options: [AnyHashable : Any]?) -> CIFilter?\nfunc depthBlurEffectFilter(forImageData: Data, options: [AnyHashable : Any]?) -> CIFilter?\nfunc depthBlurEffectFilter(forImageURL: URL, options: [AnyHashable : Any]?) -> CIFilter?\nfunc heif10Representation(of: CIImage, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any]) -> Data\nfunc openEXRRepresentation(of: CIImage, options: [CIImageRepresentationOption : Any]) -> Data\nfunc writeHEIF10Representation(of: CIImage, to: URL, colorSpace: CGColorSpace, options: [CIImageRepresentationOption : Any])\nfunc writeOpenEXRRepresentation(of: CIImage, to: URL, options: [CIImageRepresentationOption : Any])\nRelationships\nInherits From\nNSObject\nSee Also\nEssentials\nProcessing an Image Using Built-in Filters\nApply effects such as sepia tint, highlight strengthening, and scaling to images.\nclass CIImage\nA representation of an image to be processed or produced by Core Image filters.\nclass CIFilter\nAn image processor that produces an image by manipulating one or more input images or by generating new image data.\nBasic Data Types\nColors, vectors, and other types used in applying or creating image filters."
  },
  {
    "title": "CIFilter",
    "url": "https://developer.apple.com/documentation/coreimage/cifilter",
    "html": "Overview\n\nThe CIFilter class produces a CIImage object as output. Typically, a filter takes one or more images as input. Some filters, however, generate an image based on other types of input parameters. The parameters of a CIFilter object are set and retrieved through the use of key-value pairs.\n\nYou use the CIFilter object in conjunction with other Core Image classes, such as CIImage, CIContext, and CIColor, to take advantage of the built-in Core Image filters when processing images, creating filter generators, or writing custom filters.\n\nCIFilter objects are mutable, and thus cannot be shared safely among threads. Each thread must create its own CIFilter objects, but you can pass a filter’s immutable input and output CIImage objects between threads.\n\nTo get a quick overview of how to set up and use Core Image filters, see Core Image Programming Guide.\n\nCreate Type-Safe Filters\n\nCore Image provides methods that create type-safe CIFilter instances. Use these filters to avoid run-time errors that can occur when relying on Core Image’s string-based API.\n\nTo use the type-safe API, import CoreImage.CIFilterBuiltins:\n\nimport CoreImage\nimport CoreImage.CIFilterBuiltins\n\n\nThe type-safe approach returns a non-optional filter. Because the returned filter conforms to the relevant protocol—for example, CIFalseColor in the case of falseColor()—the parameters are available as properties. The following creates and applies a false color filter:\n\nfunc falseColor(inputImage: CIImage) -> CIImage? {\n    let falseColorFilter = CIFilter.falseColor()\n    falseColorFilter.color0 = CIColor(red: 1, green: 1, blue: 0)\n    falseColorFilter.color1 = CIColor(red: 0, green: 0, blue: 1)\n    falseColorFilter.inputImage = inputImage\n    return falseColorFilter.outputImage\n}\n\n\nThe false color filter maps luminance to a color ramp of two colors:\n\nSubclassing Notes\n\nYou can subclass CIFilter in order to create custom filter effects:\n\nBy chaining together two or more built-in Core Image filters\n\nBy using an image-processing kernel that you write\n\nRegardless of whether your subclass provides its effect by chaining filters or implementing its own kernel, you should:\n\nDeclare any input parameters as properties whose names are prefixed with input, such as inputImage.\n\nOverride the setDefaults() methods to provide default values for any input parameters you’ve declared.\n\nImplement an outputImage method to create a new CIImage with your filter’s effect.\n\nThe CIFilter class automatically manages input parameters when archiving, copying, and deallocating filters. For this reason, your subclass must obey the following guidelines to ensure proper behavior:\n\nStore input parameters in instance variables whose names are prefixed with input.\n\nDon’t use auto-synthesized instance variables, because their names are automatically prefixed with an underscore. Instead, synthesize the property manually. For example:\n\n@synthesize inputMyParameter;\n\nIf using manual reference counting, don’t release input parameter instance variables in your dealloc method implementation. The dealloc implementation in the CIFilter class uses Key-value coding to automatically set the values of all input parameters to nil.\n\nTopics\nCreating a Filter\ninit?(name: String)\nCreates a CIFilter object for a specific kind of filter.\ninit?(name: String, parameters: [String : Any]?)\nCreates a CIFilter object for a specific kind of filter and initializes the input values.\nCreating a Filter from a RAW Image\ninit!(cvPixelBuffer: CVPixelBuffer!, properties: [AnyHashable : Any]!, options: [CIRAWFilterOption : Any]!)\nCreates a filter from a Core Video pixel buffer.\nDeprecated\ninit!(imageData: Data!, options: [CIRAWFilterOption : Any]!)\nCreates a filter that allows the processing of RAW images.\nDeprecated\ninit!(imageURL: URL!, options: [CIRAWFilterOption : Any]!)\nCreates a filter that allows the processing of RAW images.\nDeprecated\nConfiguring Type-Safe Filters\nConfigure Core Image filters that expose their attributes as properties.\nprotocol CIFilterProtocol\nThe properties you use to configure a Core Image filter.\nAccessing Registered Filters\nclass func filterNames(inCategories: [String]?) -> [String]\nReturns an array of all published filter names that match all the specified categories.\nclass func filterNames(inCategory: String?) -> [String]\nReturns an array of all published filter names in the specified category.\nRegistering a Filter\nclass func registerName(String, constructor: any CIFilterConstructor, classAttributes: [String : Any])\nPublishes a custom filter that is not packaged as an image unit.\nGetting Filter Parameters and Attributes\nvar name: String\nA name associated with a filter.\nvar isEnabled: Bool\nA Boolean value that determines whether the filter is enabled. Animatable.\nvar attributes: [String : Any]\nA dictionary of key-value pairs that describe the filter.\nvar inputKeys: [String]\nThe names of all input parameters to the filter.\nvar outputKeys: [String]\nThe names of all output parameters from the filter.\nvar outputImage: CIImage?\nReturns a CIImage object that encapsulates the operations configured in the filter.\nSetting Default Values\nfunc setDefaults()\nSets all input values for a filter to default values.\nApplying a Filter\nfunc apply(CIKernel, arguments: [Any]?, options: [String : Any]?) -> CIImage?\nProduces a CIImage object by applying arguments to a kernel function and using options to control how the kernel function is evaluated.\nGetting Localized Information for Registered Filters\nclass func localizedName(forFilterName: String) -> String?\nReturns the localized name for the specified filter name.\nclass func localizedName(forCategory: String) -> String\nReturns the localized name for the specified filter category.\nclass func localizedDescription(forFilterName: String) -> String?\nReturns the localized description of a filter for display in the user interface.\nclass func localizedReferenceDocumentation(forFilterName: String) -> URL?\nReturns the location of the localized reference documentation that describes the filter.\nCreating a Configuration View for a Filter\nfunc view(forUIConfiguration: [AnyHashable : Any]!, excludedKeys: [Any]!) -> IKFilterUIView!\nReturns a filter view for the filter.\nSerializing and Deserializing Filters\nclass func serializedXMP(from: [CIFilter], inputImageExtent: CGRect) -> Data?\nSerializes filter parameters into XMP form that is suitable for embedding in an image.\nDeprecated\nclass func filterArray(fromSerializedXMP: Data, inputImageExtent: CGRect, error: NSErrorPointer) -> [CIFilter]\nReturns an array of filter objects de-serialized from XMP data.\nDeprecated\nConstants\nFilter Attribute Keys\nAttributes for a filter and its parameters.\nData Type Attributes\nNumeric data types.\nVector Quantity Attributes\nVector data types.\nColor Attribute Keys\nColor types.\nImage Attribute Keys\nImage Types\nFilter Category Keys\nCategories of filters.\nOptions for Applying a Filter\nOptions that control the application of a custom Core Image filter.\nUser Interface Control Options\nSets of controls for various user scenarios.\nUser Interface Options\nKeys or values for the size of the input parameter controls for a filter view.\nFilter Parameter Keys\nKeys for input parameters to filters.\nRAW Image Options\nOptions for creating a CIFilter object from RAW image data.\nType Methods\nclass func supportedRawCameraModels() -> [String]!\nDeprecated\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nSee Also\nEssentials\nProcessing an Image Using Built-in Filters\nApply effects such as sepia tint, highlight strengthening, and scaling to images.\nclass CIImage\nA representation of an image to be processed or produced by Core Image filters.\nclass CIContext\nAn evaluation context for rendering image processing results and performing image analysis.\nBasic Data Types\nColors, vectors, and other types used in applying or creating image filters."
  },
  {
    "title": "Processing an Image Using Built-in Filters",
    "url": "https://developer.apple.com/documentation/coreimage/processing_an_image_using_built-in_filters",
    "html": "Overview\n\nYou can add effects to images by applying Core Image filters to CIImage objects. Figure 1 shows three filters chained together to achieve a cumulative effect:\n\nApply the sepia filter to tint an image with a reddish-brown hue.\n\nAdd the bloom filter to accentuate highlights.\n\nUse the Lanczos scale filter to scale an image down.\n\nFigure 1 Filtering a waterwheel image using built-in Core Image filters\n\nCreate a Context\n\nCIImage processing occurs in a CIContext object. Creating a CIContext is expensive, so create one during your initial setup and reuse it throughout your app.\n\nListing 1 Creating a context in Core Image\nlet context = CIContext()\n\nLoad an Image to Process\n\nThe next step is to load an image to process. This example loads an image from the project bundle.\n\nListing 2 Loading an image into CIImage\nlet imageURL = URL(fileURLWithPath: \"\\(Bundle.main.bundlePath)/YourImageName.png\")\nlet originalCIImage = CIImage(contentsOf: imageURL)!\nself.imageView.image = UIImage(ciImage:originalCIImage)\n\n\nThe CIImage object isn’t itself a displayable image, but rather image data. To display it, you must convert it to another type, such as UIImage.\n\nApply Built-In Core Image Filters\n\nA CIFilter represents a single operation or recipe for a particular effect. To process a CIImage object, pass it through CIFilter objects. You can subclass CIFilter or draw from the existing library of built-in filters.\n\nNote\n\nThe built-in filters aren’t separate class types with visible properties. You must know their names and input parameters in advance; see Core Image Filter Reference for a list of filters and their effects. Some of the more common input parameter types have associated keys, such as kCIInputImageKey. If you can’t infer the associated key constant, you can use the string literal found in the filter reference.\n\nTint Reddish-Brown with the Sepia Filter\n\nAlthough you can chain filters without separating them into functions, the following example shows how to configure a single CIFilter, the CISepiaTone filter.\n\nListing 3 Applying sepia tint as a\nfunc sepiaFilter(_ input: CIImage, intensity: Double) -> CIImage?\n{\n    let sepiaFilter = CIFilter(name:\"CISepiaTone\")\n    sepiaFilter?.setValue(input, forKey: kCIInputImageKey)\n    sepiaFilter?.setValue(intensity, forKey: kCIInputIntensityKey)\n    return sepiaFilter?.outputImage\n}\n\n\nTo pass the image through the filter, call the sepia filter function.\n\nListing 4 Calling the sepia filter function\nlet sepiaCIImage = sepiaFilter(originalCIImage, intensity:0.9)\n\n\nYou can check the intermediate result at any point in the filter chain by converting from CIImage to a UIImage. You can then assign this UIImage to a UIImageView for display.\n\nListing 5 Displaying intermediate outputs as\nself.imageView.image = UIImage(ciImage:sepiaCIImage!)\n\nStrengthen Highlights with the Bloom Filter\n\nThe bloom filter accentuates the highlights of an image. You can apply it as part of a chain without factoring it into a separate function, but this example encapsulates its functionality into a function.\n\nListing 6 Applying the bloom filter as a\nfunc bloomFilter(_ input:CIImage, intensity: Double, radius: Double) -> CIImage?\n{\n   let bloomFilter = CIFilter(name:\"CIBloom\")\n    bloomFilter?.setValue(input, forKey: kCIInputImageKey)\n    bloom lter?.setValue(intensity, forKey: kCIInputIntensityKey)\n    bloomFilter?.setValue(radius, forKey: kCIInputRadiusKey)\n    return bloomFilter?.outputImage\n}\n\n\nLike the sepia filter, the intensity of the bloom filter’s effect ranges between 0 and 1, with 1 being the most intense effect. The bloom filter has an additional inputRadius parameter to determine how much the glowing regions expand. Experiment with a range to values to fine tune the effect, or assign the input parameter to a control like a UISlider to allow your users to tweak its values.\n\nNote\n\nThe CIGloom filter performs the opposite effect.\n\nTo display the output, convert the CIImage to a UIImage.\n\nListing 7 Calling the bloom filter\nlet bloomCIImage = bloomFilter(sepiaCIImage, intensity:1, radius:10)\n_imageView.image = UIImage(ciImage:bloomCIImage)\n\nScale Image Size with the Lanczos Scale Filter\n\nApply the CILanczosScaleTransform to obtain a high-quality downsampling of the image, preserving the original image’s aspect ratio through the CILanczosScaleTransform filter’s input parameter aspectRatio. For built-in Core Image filters, calculate the aspect ratio as the image’s width over height, as in Listing 8.\n\nListing 8 Computing aspect ratio as height over width\nCGFloat imageWidth = originalUIImage.size.width\nCGFloat imageHeight = originalUIImage.size.height\nletdoesn’tatio = Double(originalImage.size.width) / Double(originalImage.size.height)\nlet scaledCIImage = scaleFilter(bloomCIImage, aspectRatio:aspectRatio, scale:0.5)\n\n\nLike other built-in filters, the CILanczosScale filter also outputs its result as a CIImage.\n\nListing 9 Applying the Lanczos scale transform as a\nfunc scaleFilter(_ input:CIImage, aspectRatio : Double, scale : Double) -> CIImage\n{\n    let scaleFilter = CIFilter(name:\"CILanczosScaleTransform\")!\n    scaleFilter.setValue(input, forKey: kCIInputImageKey)\n    scaleFilter.setValue(scale, forKey: kCIInputScaleKey)\n    scaleFilter.setValue(aspectRatio, forKey: kCIInputAspectRatioKey)\n    return scaleFilter.outputImage!\n}\n\n\nImportant\n\nTo optimize computation, Core Image doesn’t actually render any intermediate CIImage result until you force the CIImage to display its content onscreen, as you might do using UIImageView.\n\nListing 10 Showing the final result in a\nself.imageView.image = UIImage(ciImage:scaledCIImage)\n\n\nNote\n\nCore Image optimizes filtering by reordering the three chained filters and concatenating them into a single image processing kernel, saving computation and rendering cycles.\n\nIn addition to trying out the built-in filters for a fixed effect, you can combine filters in certain Filter Recipes to accomplish tasks such as Applying a Chroma Key Effect, Selectively Focusing on an Image, Customizing Image Transitions, and Simulating Scratchy Analog Film.\n\nSee Also\nEssentials\nclass CIImage\nA representation of an image to be processed or produced by Core Image filters.\nclass CIFilter\nAn image processor that produces an image by manipulating one or more input images or by generating new image data.\nclass CIContext\nAn evaluation context for rendering image processing results and performing image analysis.\nBasic Data Types\nColors, vectors, and other types used in applying or creating image filters."
  },
  {
    "title": "Generating an animation with a Core Image Render Destination",
    "url": "https://developer.apple.com/documentation/coreimage/generating_an_animation_with_a_core_image_render_destination",
    "html": "Overview\n\nThis sample shows how to assemble a SwiftUI app that displays a Metal view with animated images that you generate procedurally from Core Image.\n\nTo accomplish this, the sample sets up a Scene in a WindowGroup with a single content view. The sample’s ContentView adopts the View protocol and initializes a Renderer using a closure to vend a CIImage. It then adds a MetalView, with the instantiated Renderer, to the content body.\n\nThe sample combines view update and state changes to produce the animation:\n\nFor view update, the MetalView structure conforms to the UIViewRepresentable or NSViewRepresentable protocol of the SwiftUI life cycle.\n\nFor state changes, the Renderer is a StateObject conforming to the ObservableObject protocol.\n\nGenerate an animation\n\nThe Renderer class generates an image for an animation frame by conforming to the MetalKit MTKViewDelegate delegate protocol. The protocol’s draw(in:) function commits render destination work to the GPU using a render task in a Metal command buffer.\n\nFor more information about drawing with MetalKit see Using a Render Pipeline to Render Primitives.\n\nMetalKit calls the draw(in:) delegate function of the Renderer automatically.\n\nfinal class Renderer: NSObject, MTKViewDelegate, ObservableObject {\n\n\nAn image-supplying function parameterized by both timestamp and scale factor initializes the Renderer. This function combines checkerboard and hue-adjustment filters to generate animated checkerboard pattern images cropped to a fixed size.\n\n// Create a Metal view with its own renderer.\nlet renderer = Renderer(imageProvider: { (time: CFTimeInterval, scaleFactor: CGFloat, headroom: CGFloat) -> CIImage in\n    \n    var image: CIImage\n    \n    // Animate a shifting red and yellow checkerboard pattern.\n    let pointsShiftPerSecond = 25.0\n    let checkerFilter = CIFilter.checkerboardGenerator()\n    checkerFilter.width = 20.0 * Float(scaleFactor)\n    checkerFilter.color0 = CIColor.red\n    checkerFilter.color1 = CIColor.yellow\n    checkerFilter.center = CGPoint(x: time * pointsShiftPerSecond, y: time * pointsShiftPerSecond)\n    image = checkerFilter.outputImage ?? CIImage.empty()\n    \n    // Animate the hue of the image with time.\n    let colorFilter = CIFilter.hueAdjust()\n    colorFilter.inputImage = image\n    colorFilter.angle = Float(time)\n    image = colorFilter.outputImage ?? CIImage.empty()\n\n\nAfter the sample initializes the Renderer, the Renderer makes a command buffer and gets the currentDrawable.\n\nif let commandBuffer = commandQueue.makeCommandBuffer() {\n    \n    // Add a completion handler that signals `inFlightSemaphore` when Metal and the GPU have fully\n    // finished processing the commands that the app encoded for this frame.\n    // This completion indicates that Metal and the GPU no longer need the dynamic buffers that\n    // Core Image writes to in this frame.\n    // Therefore, the CPU can overwrite the buffer contents without corrupting any rendering operations.\n    let semaphore = inFlightSemaphore\n    commandBuffer.addCompletedHandler { (_ commandBuffer)-> Swift.Void in\n        semaphore.signal()\n    }\n    \n    if let drawable = view.currentDrawable {\n\n\nThe Renderer then configures a CIRenderDestination with the command buffer, currentDrawable, dimensions, and pixel format, along with a closure that returns the texture for the currentDrawable.\n\n// Create a destination the Core Image context uses to render to the drawable's Metal texture.\nlet destination = CIRenderDestination(width: Int(dSize.width),\n                                      height: Int(dSize.height),\n                                      pixelFormat: view.colorPixelFormat,\n                                      commandBuffer: commandBuffer,\n                                      mtlTextureProvider: { () -> MTLTexture in\n    // Core Image calls the texture provider block lazily when starting a task to render to the destination.\n    return drawable.texture\n})\n\n\nThe sample uses the render destination to create an animation frame at a specific timestamp.\n\nFinally, the sample composites the render destination’s centered image on a background and submits work to the GPU to render and present the result.\n\n// Create a displayable image for the current time.\nlet time = CFTimeInterval(CFAbsoluteTimeGetCurrent() - self.startTime)\nvar image = self.imageProvider(time, contentScaleFactor, headroom)\n\n\n// Center the image in the view's visible area.\nlet iRect = image.extent\nlet backBounds = CGRect(x: 0, y: 0, width: dSize.width, height: dSize.height)\nlet shiftX = round((backBounds.size.width + iRect.origin.x - iRect.size.width) * 0.5)\nlet shiftY = round((backBounds.size.height + iRect.origin.y - iRect.size.height) * 0.5)\nimage = image.transformed(by: CGAffineTransform(translationX: shiftX, y: shiftY))\n\n\n// Blend the image over an opaque background image.\n// This is needed if the image is smaller than the view, or if it has transparent pixels.\nimage = image.composited(over: self.opaqueBackground)\n\n\n// Start a task that renders to the texture destination.\n_ = try? self.cicontext.startTask(toRender: image, from: backBounds,\n                                  to: destination, at: CGPoint.zero)\n\n\n// Insert a command to present the drawable when the buffer has been scheduled for execution.\ncommandBuffer.present(drawable)\n\n\n// Commit the command buffer so that the GPU executes the work that the Core Image Render Task issues.\ncommandBuffer.commit()\n\n\nAdd an EDR effect\n\nThe sample adds an EDR effect, a shiny ripple with a bright specular highlight, to the rendered checkerboard animation in three steps:\n\nOpt into EDR support for the view and set an accommodating color space and pixel format.\n\nQuery the EDR headroom for each frame and pass headroom to the image provider closure for the Renderer.\n\nSet the peak specular highlight value to the maximum value of white with respect to the current headroom, or a reasonable default value.\n\nFor more information about adding an EDR effect, see Display EDR content with Core Image, Metal, and SwiftUI.\n\nConfigure the view for EDR support\n\nThe MetalView opts into EDR support setting wantsExtendedDynamicRangeContent to true on the backing CAMetalLayer. When enabled, the layer uses a wide gamut colorspace to render colors beyond SDR range. Similarly, the MTKView sets a wide gamut pixelFormat to render the generated EDR image.\n\nif let layer = view.layer as? CAMetalLayer {\n    // Enable EDR with a color space that supports values greater than SDR.\n    if #available(iOS 16.0, *) {\n        layer.wantsExtendedDynamicRangeContent = true\n    }\n    layer.colorspace = CGColorSpace(name: CGColorSpace.extendedLinearDisplayP3)\n    // Ensure the render view supports pixel values in EDR.\n    view.colorPixelFormat = MTLPixelFormat.rgba16Float\n}\n\n\nQuery EDR headroom\n\nThe Renderer queries the current EDR headroom for each draw call using either maximumPotentialExtendedDynamicRangeColorComponentValue (NSScreen) or currentEDRHeadroom (UIScreen). If EDR headroom is unavailable the sample sets headroom to 1.0 clamping to SDR.\n\n                // Determine EDR headroom and fallback to SDR, as needed.\n                // Note: The headroom must be determined every frame to include changes in environmental lighting conditions.\n                let screen = view.window?.screen\n#if os(iOS)\n                var headroom = CGFloat(1.0)\n                if #available(iOS 16.0, *) {\n                    headroom = screen?.currentEDRHeadroom ?? 1.0\n                }\n#else\n                let headroom = screen?.maximumExtendedDynamicRangeColorComponentValue ?? 1.0\n#endif\n\n\nLeverage EDR headroom\n\nThe sample’s ripple effect takes a gradient shadingImage to shade the contor of the ripple so that it appears to reflect light from the upper-left corner. CILinearGradient generates the gradient shading image between the current maximum RGB white, color0, and a fully transparent clear color, color1.\n\n// Compute a shading image for the ripple effect below.\n// Cast light on the upper-left corner of the shading gradient image.\nlet angle = 135.0 * (.pi / 180.0)\nlet gradient = CIFilter.linearGradient()\n// Create a bright white color for a specular highlight with the current\n// maximum possible pixel component values within headroom\n// or a reasonable alternative.\nlet maxRGB = min(headroom, 8.0)\ngradient.color0 = CIColor(red: maxRGB, green: maxRGB, blue: maxRGB,\n                          colorSpace: CGColorSpace(name: CGColorSpace.extendedLinearSRGB)!)!\ngradient.color1 = CIColor.clear\ngradient.point0 = CGPoint(x: sin(angle) * 90.0 + 100.0,\n                          y: cos(angle) * 90.0 + 100.0)\ngradient.point1 = CGPoint(x: sin(angle) * 85.0 + 100.0,\n                          y: cos(angle) * 85.0 + 100.0)\nlet shading = gradient.outputImage?.cropped(to: CGRect(x: 0, y: 0,\n                                                       width: 200, height: 200))\n\n\n// Add a shiny ripple effect to the image.\nlet ripple = CIFilter.rippleTransition()\nripple.inputImage = image\nripple.targetImage = image\nripple.center = CGPoint(x: 256.0 * scaleFactor,\n                        y: 192.0 * scaleFactor)\nripple.time = Float(fmod(time * 0.25, 1.0))\nripple.shadingImage = shading\nimage = ripple.outputImage ?? CIImage()\n\n\nreturn image.cropped(to: CGRect(x: 0, y: 0,\n                                width: 512.0 * scaleFactor,\n                                height: 384.0 * scaleFactor))\n\n\nSee Also\nCustom Render Destination\nclass CIRenderDestination\nA specification for configuring all attributes of a render task's destination and issuing asynchronous render tasks.\nclass CIRenderInfo\nAn encapsulation of a render task's timing, passes, and pixels processed.\nclass CIRenderTask\nA single render task issued in conjunction with CIRenderDestination."
  },
  {
    "title": "CIWarpKernel",
    "url": "https://developer.apple.com/documentation/coreimage/ciwarpkernel",
    "html": "Overview\n\nThe kernel language routine for a warp kernel has the following characteristics:\n\nIt uses exactly one input image.\n\nIts return type is vec2 (Core Image Kernel Language) or float2 (Metal Shading Language), specifying a position in source image coordinates.\n\nA warp kernel routine requires no input parameters (but can use additional custom parameters you declare). Typically, a warp kernel uses the destination coordinate function to look up the coordinates of the destination pixel currently being rendered, then computes a corresponding position in source image coordinates (output using the return keyword). Core Image then samples from the source image at the returned coordinates to produce a pixel color for the output image. For example, the Metal Shading Language source below implements a filter that passes through its input image unchanged.\n\n#include <CoreImage/CoreImage.h>\n \nextern \"C\" {\n    namespace coreimage {\n        float2 do_nothing(destination dest) {\n            return dest.coord();\n        }\n    }\n}\n\n\nThe equivalent code in Core Image Kernel Language is:\n\nkernel vec2 do_nothing() {\n    return destCoord();\n}\n\n\nThe Core Image Kernel Language is a dialect of the OpenGL Shading Language. See Core Image Kernel Language Reference and Core Image Programming Guide for more details.\n\nTopics\nCreating a Kernel\ninit?(source: String)\nCreates a warp kernel object from the specified kernel source code.\nDeprecated\nApplying a Kernel to Filter an Image\nfunc apply(extent: CGRect, roiCallback: CIKernelROICallback, image: CIImage, arguments: [Any]) -> CIImage?\nCreates a new image using the kernel and the specified input image and arguments.\nRelationships\nInherits From\nCIKernel\nSee Also\nCustom Filters\nWriting Custom Kernels\nWrite your own custom kernels in either the Core Image Kernel Language or the Metal Shading Language.\nclass CIKernel\nA GPU-based image processing routine used to create custom Core Image filters.\nclass CIColorKernel\nA GPU-based image processing routine that processes only the color information in images, used to create custom Core Image filters.\nclass CIBlendKernel\nA GPU-based image processing routine that is optimized for blending two images.\nclass CISampler\nAn object that retrieves pixel samples for processing by a filter kernel."
  },
  {
    "title": "CIColorKernel",
    "url": "https://developer.apple.com/documentation/coreimage/cicolorkernel",
    "html": "Overview\n\nThe kernel language routine for a color kernel has the following characteristics:\n\nIts return type is vec4 (Core Image Kernel Language) or float4 (Metal Shading Language); that is, it returns a pixel color for the output image.\n\nIt may use zero or more input images. Each input image is represented by a parameter of type __sample (Core Image Kernel Language) or sample_t (Metal Shading Language), which can be treated as a single pixel color of type vec4 (Core Image Kernel Language) or float4 (Metal Shading Language);.\n\nA color kernel routine receives as input single-pixel colors (one sampled from each input image) and computes a final pixel color (output using the return keyword). For example, the Metal Shading Language source below implements a filter that passes through its input image unchanged.\n\n#include <CoreImage/CoreImage.h>\n \nextern \"C\" {\n    namespace coreimage {\n        float4 do_nothing(sample_t s) {\n            return s;\n        }\n    }\n}\n\n\nThe equivalent code in Core Image Kernel Language is:\n\nkernel vec4 do_nothing(__sample s) {\n    return s.rgba;\n}\n\n\nThe Core Image Kernel Language is a dialect of the OpenGL Shading Language. See Core Image Kernel Language Reference and Core Image Programming Guide for more details.\n\nTopics\nCreating a Kernel\ninit?(source: String)\nCreates a color kernel object from the specified kernel source code.\nDeprecated\nApplying a Kernel to Filter an Image\nfunc apply(extent: CGRect, arguments: [Any]) -> CIImage?\nCreates a new image using the kernel and specified arguments.\nRelationships\nInherits From\nCIKernel\nSee Also\nCustom Filters\nWriting Custom Kernels\nWrite your own custom kernels in either the Core Image Kernel Language or the Metal Shading Language.\nclass CIKernel\nA GPU-based image processing routine used to create custom Core Image filters.\nclass CIWarpKernel\nA GPU-based image processing routine that processes only the geometry information in an image, used to create custom Core Image filters.\nclass CIBlendKernel\nA GPU-based image processing routine that is optimized for blending two images.\nclass CISampler\nAn object that retrieves pixel samples for processing by a filter kernel."
  },
  {
    "title": "Halftone Effect Filters",
    "url": "https://developer.apple.com/documentation/coreimage/halftone_effect_filters",
    "html": "Overview\n\ncircularScreen()\n\ncmykHalftone()\n\ndotScreen()\n\nhatchedScreen()\n\nlineScreen()\n\nSee Also\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nDistortion Filters\nApply distortion to images.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nGradient Filters\nGenerate linear and radial gradients.\nReduction Filters\nReduction Filters\nSharpening Filters\nApply sharpening to images.\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTile Effect Filters\nProduce tiled images from source images.\nTransition Filters\nTransition between two images by using effects including page curl and swipe."
  },
  {
    "title": "Core Image",
    "url": "https://developer.apple.com/documentation/coreimage",
    "html": "Overview\n\nCore Image is an image processing and analysis technology that provides high-performance processing for still and video images. Use the many built-in image filters to process images and build complex effects by chaining filters. For details, see Core Image Filter Reference.\n\nYou can also create new effects with custom filters and image processors; see Core Image Programming Guide.\n\nTopics\nEssentials\nProcessing an Image Using Built-in Filters\nApply effects such as sepia tint, highlight strengthening, and scaling to images.\nclass CIImage\nA representation of an image to be processed or produced by Core Image filters.\nclass CIFilter\nAn image processor that produces an image by manipulating one or more input images or by generating new image data.\nclass CIContext\nAn evaluation context for rendering image processing results and performing image analysis.\nBasic Data Types\nColors, vectors, and other types used in applying or creating image filters.\nFilter Catalog\nBlur Filters\nApply blurs, simulate motion and zoom effects, reduce noise, and erode and dilate image regions.\nColor Adjustment Filters\nApply color transformations, including exposure, hue, and tint adjustments.\nColor Effect Filters\nApply color effects, including photo effects, dithering, and color maps.\nComposite Operations\nComposite images by using a range of blend modes and compositing operators.\nConvolution Filters\nProduce effects such as blurring, sharpening, edge detection, translation, and embossing.\nDistortion Filters\nApply distortion to images.\nGenerator Filters\nGenerate barcode, geometric, and special-effect images.\nGeometry Adjustment Filters\nTranslate, scale, and rotate images in 2D and 3D.\nGradient Filters\nGenerate linear and radial gradients.\nHalftone Effect Filters\nSimulate monochrome and CMYK halftone screens.\nReduction Filters\nReduction Filters\nSharpening Filters\nApply sharpening to images.\nStylizing Filters\nCreate stylized versions of images by applying effects including pixellation and line overlays.\nTile Effect Filters\nProduce tiled images from source images.\nTransition Filters\nTransition between two images by using effects including page curl and swipe.\nFilter Recipes\nApplying a Chroma Key Effect\nReplace a color in one image with the background from another.\nSelectively Focusing on an Image\nFocus on a part of an image by applying Gaussian blur and gradient masks.\nCustomizing Image Transitions\nTransition between images in creative ways using Core Image filters.\nSimulating Scratchy Analog Film\nDegrade the quality of an image to make it look like dated, scratchy analog film.\nCustom Filters\nUse the Core Image Kernel Language to create universal image processing routines that work in any Core Image context.\nWriting Custom Kernels\nWrite your own custom kernels in either the Core Image Kernel Language or the Metal Shading Language.\nclass CIKernel\nA GPU-based image processing routine used to create custom Core Image filters.\nclass CIColorKernel\nA GPU-based image processing routine that processes only the color information in images, used to create custom Core Image filters.\nclass CIWarpKernel\nA GPU-based image processing routine that processes only the geometry information in an image, used to create custom Core Image filters.\nclass CIBlendKernel\nA GPU-based image processing routine that is optimized for blending two images.\nclass CISampler\nAn object that retrieves pixel samples for processing by a filter kernel.\nCustom Image Processors\nAccess image content directly to use other image processing technologies within a Core Image workflow.\nclass CIImageProcessorKernel\nThe abstract class you extend to create custom image processors that can integrate with Core Image workflows.\nprotocol CIImageProcessorInput\nA container of image data and information for use in a custom image processor.\nprotocol CIImageProcessorOutput\nA container for writing image data and information produced by a custom image processor.\nCustom Render Destination\nGenerating an animation with a Core Image Render Destination\nAnimate a filtered image to a Metal view in a SwiftUI app using a Core Image Render Destination.\nclass CIRenderDestination\nA specification for configuring all attributes of a render task's destination and issuing asynchronous render tasks.\nclass CIRenderInfo\nAn encapsulation of a render task's timing, passes, and pixels processed.\nclass CIRenderTask\nA single render task issued in conjunction with CIRenderDestination.\nFeedback-Based Processing\nclass CIImageAccumulator\nAn object that manages feedback-based image processing for tasks such as painting or fluid simulation.\nBarcode Descriptions\nModel barcode data to create barcode images with Core Image filters or to interpret barcodes detected by Core Image, Vision, or AVFoundation features.\nclass CIBarcodeDescriptor\nAn abstract base class that represents a machine readable code's attributes.\nclass CIQRCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a square QR code symbol.\nclass CIAztecCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents an Aztec code symbol.\nclass CIPDF417CodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a PDF 417 symbol.\nclass CIDataMatrixCodeDescriptor\nA concrete subclass of CIBarcodeDescriptor that represents a Data Matrix code symbol.\nImage Feature Detection\nIn macOS 10.13, iOS 11, and tvOS 11 or later, the Vision framework replaces these classes for identifying and analyzing image features.\nclass CIDetector\nAn image processor that identifies notable features (such as faces and barcodes) in a still image or video.\nclass CIFeature\nThe abstract superclass for objects representing notable features detected in an image.\nclass CIFaceFeature\nInformation about a face detected in a still or video image.\nclass CIRectangleFeature\nInformation about a rectangular region detected in a still or video image.\nclass CITextFeature\nInformation about a region likely to contain text detected in a still or video image.\nclass CIQRCodeFeature\nInformation about a Quick Response code (a kind of 2D barcode) detected in a still or video image.\nImage Units\nIn macOS, create and package custom filters as image units that other apps can load as plug-ins, or discover and load image units published by third-party apps.\nclass CIPlugIn\nThe mechanism for loading image units in macOS.\nclass CIFilterGenerator\nAn object that creates and configures chains of individual image filters.\nprotocol CIPlugInRegistration\nThe interface for loading Core Image image units.\nprotocol CIFilterConstructor\nA general interface for objects that produce CIFilter instances.\nEnumerations\nCore Image Enumerations\nClasses\nclass CIRAWFilter\nA filter subclass that produces an image by manipulating RAW image sensor data from a digital camera or scanner.\nReference\nCore Image Constants\nCore Image Data Types\nSee Also\nRelated Documentation\nCore Image Filter Reference\nCore Image Programming Guide"
  }
]
[
  {
    "title": "Adding 3D content to your app | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionOS/adding-3d-content-to-your-app",
    "html": "Overview\n\nA device with a stereoscopic display lets people experience 3D content in a way that feels more real. Content appears to have real depth, and people can view it from different angles, making it seem like it’s there in front of them.\n\nWhen building an app for visionOS, think about ways you might add depth to your app’s interface. The system provides several ways to display 3D content, including in your existing windows, in a volume, and in an immersive space. Choose the options that work best for your app and the content you offer.\n\nWindow\n\nVolume\n\nImmersive space\n\nAdd depth to traditional 2D windows\n\nWindows are an important part of your app’s interface. With visionOS, apps automatically get materials with the visionOS look and feel, fully resizable windows with spacing tuned for eyes and hands input, and access to highlighting adjustments for your custom controls.\n\nPlay\n\nIncorporate depth effects into your custom views as needed, and use 3D layout options to arrange views in your windows.\n\nApply a shadow(color:radius:x:y:) or visualEffect(_:) modifier to the view.\n\nLift or highlight the view when someone looks at it using a hoverEffect(_:isEnabled:) modifier.\n\nLay out views using a ZStack.\n\nAnimate view-related changes with transform3DEffect(_:).\n\nRotate the view using a rotation3DEffect(_:axis:anchor:anchorZ:perspective:) modifier.\n\nIn addition to giving 2D views more depth, you can also add static 3D models to your 2D windows. The Model3D view loads a USDZ file or other asset type and displays it at its intrinsic size in your window. Use this in places where you already have the model data in your app, or can download it from the network. For example, a shopping app might use this type of view to display a 3D version of a product.\n\nDisplay dynamic 3D scenes using RealityKit\n\nRealityKit is Apple’s technology for building 3D models and scenes that you update dynamically onscreen. In visionOS, use RealityKit and SwiftUI together to seamlessly couple your app’s 2D and 3D content. Load existing USDZ assets or create scenes in Reality Composer Pro that incorporate animation, physics, lighting, sounds, and custom behaviors for your content. To use a Reality Composer Pro project in your app, add the Swift package to your Xcode project and import its module in your Swift file. For more information, see Managing files and folders in your Xcode project.\n\nWhen you’re ready to display 3D content in your interface, use a RealityView. This SwiftUI view serves as a container for your RealityKit content, and lets you update that content using familiar SwiftUI techniques.\n\nThe following example shows a view that uses a RealityView to display a 3D sphere. The code in the view’s closure creates a RealityKit entity for the sphere, applies a texture to the surface of the sphere, and adds the sphere to the view’s content.\n\n struct SphereView: View {\n    var body: some View {\n        RealityView { content in\n            let model = ModelEntity(\n                         mesh: .generateSphere(radius: 0.1),\n                         materials: [SimpleMaterial(color: .white, isMetallic: true)])\n            content.add(model)\n        }\n    }\n}\n\n\nWhen SwiftUI displays your RealityView, it executes your code once to create the entities and other content. Because creating entities is relatively expensive, the view runs your creation code only once. When you want to update the state of your entities, change the state of your view and use an update closure to apply those changes to your content. The following example uses an update closure to change the size of the sphere when the value in the scale property changes:\n\nstruct SphereView: View {\n    var scale = false\n\n\n    var body: some View {\n        RealityView { content in\n            let model = ModelEntity(\n                         mesh: .generateSphere(radius: 0.1),\n                         materials: [SimpleMaterial(color: .white, isMetallic: true)])\n            content.add(model)\n        } update: { content in\n            if let model = content.entities.first {\n                model.transform.scale = scale ? [1.2, 1.2, 1.2] : [1.0, 1.0, 1.0]\n            }\n        }\n    }\n}\n\n\nFor information about how to create content using RealityKit, see RealityKit.\n\nRespond to interactions with RealityKit content\n\nTo handle interactions with the entities of your RealityKit scenes:\n\nAttach a gesture recognizer to your RealityView and add the targetedToAnyEntity() modifier to it.\n\nAttach an InputTargetComponent to the entity or one of its parent entities.\n\nAdd collision shapes to the RealityKit entities that support interactions.\n\nThe targetedToAnyEntity() modifier provides a bridge between the gesture recognizer and your RealityKit content. For example, to recognize when someone drags an entity, specify a DragGesture and add the modifier to it. When the specified gesture occurs on an entity, SwiftUI executes the provided closure.\n\nThe following example adds a tap gesture recognizer to the sphere view from the previous example. The code also adds InputTargetComponent and CollisionComponent components to the shape to allow the interactions to occur. If you omit these components, the view doesn’t detect the interactions with your entity.\n\nstruct SphereView: View {\n    @State private var scale = false\n\n\n    var body: some View {\n        RealityView { content in\n            let model = ModelEntity(\n                mesh: .generateSphere(radius: 0.1),\n                materials: [SimpleMaterial(color: .white, isMetallic: true)])\n\n\n            // Enable interactions on the entity.\n            model.components.set(InputTargetComponent())\n            model.components.set(CollisionComponent(shapes: [.generateSphere(radius: 0.1)]))\n            content.add(model)\n        } update: { content in\n            if let model = content.entities.first {\n                model.transform.scale = scale ? [1.2, 1.2, 1.2] : [1.0, 1.0, 1.0]\n            }\n        }\n        .gesture(TapGesture().targetedToAnyEntity().onEnded { _ in\n            scale.toggle()\n        })\n    }\n}\n\nDisplay 3D content in a volume\n\nA volume is a type of window that grows in three dimensions to match the size of the content it contains. Windows and volumes both accommodate 2D and 3D content, and are alike in many ways. However, windows clip 3D content that extends too far from the window’s surface, so volumes are the better choice for content that is primarily 3D.\n\nTo create a volume, add a WindowGroup scene to your app and set its style to volumetric. This style tells SwiftUI to create a window for 3D content. Include any 2D or 3D views you want in your volume. You can also add a RealityView to build your content using RealityKit. The following example creates a volume with a static 3D model of some balloons stored in the app’s bundle:\n\nstruct MyApp: App {\n    var body: some Scene {\n        WindowGroup {\n            Model3D(\"balloons\")\n        }.windowStyle(style: .volumetric)\n    }\n}\n\n\nWindows and volumes are a convenient way to display bounded 2D and 3D content, but your app doesn’t control the placement of that content in the person’s surroundings. The system sets the initial position of each window and volume at display time. The system also adds a window bar to allow someone to reposition the window or resize it.\n\nFor more information about when to use volumes, see Human Interface Guidelines > Windows.\n\nDisplay 3D content in a person’s surroundings\n\nWhen you need more control over the placement of your app’s content, add that content to an ImmersiveSpace. An immersive space offers an unbounded area for your content, and you control the size and placement of content within the space. After receiving permission from the user, you can also use ARKit with an immersive space to integrate content into their surroundings. For example, you can use ARKit scene reconstruction to obtain a mesh of furniture and nearby objects and have your content interact with that mesh.\n\nAn ImmersiveSpace is a scene type that you create alongside your app’s other scenes. The following example shows an app that contains an immersive space and a window:\n\n@main\nstruct MyImmersiveApp: App {\n    var body: some Scene {\n        WindowGroup() {\n            ContentView()\n        }\n\n\n        ImmersiveSpace(id: \"solarSystem\") {\n            SolarSystemView()\n        }\n    }\n}\n\n\nIf you don’t add a style modifier to your ImmersiveSpace declaration, the system creates that space using the mixed style. This style displays your content together with the passthrough content that shows the person’s surroundings. Other styles let you hide passthrough to varying degrees. Use the immersionStyle(selection:in:) modifier to specify which styles your space supports. If you specify more than one style, you can toggle between the styles using the selection parameter of the modifier.\n\nWarning\n\nBe mindful of how much content you include in immersive scenes that use the mixed style. Content that fills a significant portion of the screen, even if that content is partially transparent, can prevent the person from seeing potential hazards in their surroundings. If you want to immerse the person in your content, configure your space with the full style. For more information, see, Creating fully immersive experiences in your app.\n\nRemember to set the position of items you place in an ImmersiveSpace. Position SwiftUI views using modifiers, and position a RealityKit entity using its transform component. SwiftUI places the origin of a space at a person’s feet initially, but can change this origin in response to other events. For example, the system might shift the origin to accommodate a SharePlay activity that displays your content with Spatial Personas. If you need to position SwiftUI views and RealityKit entities relative to one another, perform any needed coordinate conversions using the methods in the content parameter of RealityView.\n\nTo display your ImmersiveSpace scene, open it using the openImmersiveSpace action, which you obtain from the SwiftUI environment. This action runs asynchronously and uses the provided information to find and initialize your scene. The following example shows a button that opens the space with the solarSystem identifier:\n\nButton(\"Show Solar System\") {\n    Task {\n        let result = await openImmersiveSpace(id: \"solarSystem\")\n        if case .error = result {\n            print(\"An error occurred\")\n        }\n    }\n}\n\n\nWhen an app presents an ImmersiveSpace, the system hides the content of other apps to prevent visual conflicts. The other apps remain hidden while your space is visible but return when you dismiss it. If your app defines multiple spaces, you must dismiss the currently visible space before displaying a different space. If you don’t dismiss the visible space, the system issues a runtime warning when you try to open the other space.\n\nSee Also\nApp construction\nCreating your first visionOS app\nBuild a new visionOS app using SwiftUI and add platform-specific features.\nCreating fully immersive experiences in your app\nBuild fully immersive experiences by combining spaces with content you create using RealityKit or Metal.\nDrawing sharp layer-based content in visionOS\nDeliver text and vector images at multiple resolutions from custom Core Animation layers in visionOS."
  },
  {
    "title": "Bringing your ARKit app to visionOS | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/bringing-your-arkit-app-to-visionos",
    "html": "Overview\n\nIf you use ARKit to create an augmented reality experience on iPhone or iPad, you need to rethink your use of that technology when bringing your app to visionOS. ARKit plays a crucial role in delivering your content to the display in iPadOS and iOS. In visionOS, you use ARKit only to acquire data about the person’s surroundings, and you do so using a different set of APIs.\n\nIn visionOS, you don’t need a special view to display an augmented reality interface. Build windows with your app’s content using SwiftUI or UIKit. When you display those windows, visionOS places them in the person’s surroundings for you. If you want to control the placement of any 2D or 3D content in the person’s surroundings, build your content using SwiftUI and RealityKit.\n\nWhen migrating your app to visionOS, reuse as much of your app’s existing content as you can. visionOS supports most of the same technologies as iOS, so you can reuse project assets, 3D models, and most custom views. Don’t reuse your app’s ARKit code or any code that relies on technologies visionOS doesn’t support.\n\nFor general guidance on how to port apps to visionOS, see Bringing your existing apps to visionOS.\n\nAdopt technologies available in both iOS and visionOS\n\nTo create a single app that runs in both iOS and visionOS, use technologies that are available on both platforms. While ARKit in iOS lets you create your interface using several different technologies, the preferred technologies in visionOS are SwiftUI and RealityKit. If you’re not currently using RealityKit for 3D content, consider switching to it before you start adding visionOS support. If you retain code that uses older technologies in your iOS app, you might need to re-create much of that code using RealityKit when migrating to visionOS.\n\nIf you use Metal to draw your app’s content, you can bring your code to visionOS to create content for 2D views or to create fully immersive experiences. You can’t use Metal to create 3D content that integrates with the person’s surroundings. This restriction prevents apps from sampling pixels of the person’s surroundings, which might contain sensitive information. For information on how to create a fully immersive experience with Metal, see Drawing fully immersive content using Metal.\n\nConvert 3D assets to the USDZ format\n\nThe recommended format for 3D assets in iOS and visionOS is USDZ. This format offers a compact single file for everything, including your models, textures, behaviors, physics, anchoring, and more. If you have assets that don’t use this format, use the Reality Converter tool that comes with Xcode to convert them for your project.\n\nWhen building 3D scenes for visionOS, use Reality Composer Pro to create your scenes that incorporate your USDZ assets. With Reality Composer Pro, you can import your USD files and edit them in place, nondestructively. If your iOS app applies custom materials to your assets, convert those materials to shader graphs in the app.\n\nAlthough you can bring models and materials to your project using USDZ files, you can’t bring custom shaders you wrote using Metal. Replace any custom shader code with MaterialX shaders. Many digital content creation tools support the MaterialX standard, and let you create dynamic shaders and save them with your USDZ files. Reality Composer Pro and RealityKit support MaterialX shaders, and incorporate them with your other USDZ asset content. For more information about MaterialX, see https://materialx.org.\n\nUpdate your interface to support visionOS\n\nIn visionOS, you manage your app’s content, and the system handles the integration of that content with the person’s surroundings. This approach differs from iOS, where you use a special ARKit view to blend your content and the live camera content. Bringing your interface to visionOS therefore means you need to remove this special ARKit view and focus only on your content.\n\nIf you can display your app’s content using SwiftUI or UIKit views, build a window with those views and present it from your visionOS app. If you use other technologies to incorporate 2D or 3D content into the person’s surroundings, make the following substitutions in the visionOS version of your app.\n\nIf you create your AR experience using:\n\n\t\n\nUpdate to:\n\n\n\n\nRealityKit and ARView\n\n\t\n\nRealityKit and RealityView\n\n\n\n\nSceneKit and ARSCNView\n\n\t\n\nRealityKit and RealityView\n\n\n\n\nSpriteKit and ARSKView\n\n\t\n\nRealityKit or SwiftUI\n\nA RealityView is a SwiftUI view that manages the content and animations you create using RealityKit and Reality Composer Pro. You can add a RealityView to any of your app’s windows to display 2D or 3D content. You can also add the view to an ImmersiveSpace scene, which you use to integrate your RealityKit content into the person’s surroundings.\n\nNote\n\nYou can load iOS storyboards into a visionOS app, but you can’t customize your interface for visionOS or include 3D content. If you want to share interface files between iOS and visionOS, adopt SwiftUI views or create your interface programmatically.\n\nFor more information about how to use RealityView and respond to interactions with your content, see Adding 3D content to your app.\n\nReplace your ARKit code\n\nARKit provides different APIs for iOS and visionOS, and the way you use ARKit services on the platforms is also different. In iOS, you must use ARKit to put your content onscreen, and you can also use it to manage interactions between your content and a person’s surroundings. In visionOS, the system puts your content onscreen, so you only use ARKit to manage interactions with the surroundings. Because of this more limited usage, some apps don’t need ARKit at all in visionOS.\n\nThe only time you use ARKit in visionOS is when you need one of the following services:\n\nPlane detection\n\nImage tracking\n\nScene reconstruction\n\nHand tracking\n\nWorld tracking and device-pose prediction\n\nUse plane detection, image tracking, and scene reconstruction to facilitate interactions between your app’s virtual content and real-world items. For example, use plane detection to detect a tabletop on which to place your content. Use world tracking to record anchors that you want to persist between launches of your app. Use hand tracking if your app requires custom hands-based input.\n\nTo start ARKit services in your app, create an ARKitSession object and run it with the data providers for each service. Unlike ARKit in iOS, services in visionOS are independent of one another, and you can start and stop each one at any time. The following example shows how to detect horizontal and vertical planes. Data providers deliver new information using an asynchronous sequence.\n\nlet session = ARKitSession()\nlet planeData = PlaneDetectionProvider(alignments: [.horizontal, .vertical])\n\n\nTask {\n    try await session.run([planeData])\n    \n    for await update in planeData.anchorUpdates {\n        switch update.event {\n        case .added, .updated:\n            // Update plane representation.\n            print(\"Updated planes.\")\n        case .removed:\n            // Indicate plane removal.\n            print(\"Removed plane.\")\n        }\n    }\n}\n\n\nIf you use the world-tracking data provider in visionOS, ARKit automatically persists the anchors you add to your app’s content. You don’t need to persist these anchors yourself.\n\nFor more information about how to use ARKit, see ARKit.\n\nIsolate ARKit features not available in visionOS\n\nIf your app uses ARKit features that aren’t present in visionOS, isolate that code to the iOS version of your app. The following features are available in iOS, but don’t have an equivalent in visionOS:\n\nFace tracking\n\nBody tracking\n\nGeotracking and placing anchors using a latitude and longitude\n\nObject detection\n\nApp Clip Code detection\n\nVideo frame post-processing\n\nAlthough whole body tracking isn’t available in visionOS, you can track the hands of the person wearing the device. Hand gestures are an important way of interacting with content in visionOS. SwiftUI handles common types of interactions like taps and drags, but you can use custom hand tracking for more complex gestures your app supports.\n\nIf you use ARKit raycasting in iOS to detect interactions with objects in the person’s surroundings, you might not need that code in visionOS. SwiftUI and RealityKit handle both direct and indirect interactions with your app’s content in 3D space, eliminating the need for raycasting in many situations. In other situations, you can use the features of ARKit and RealityKit to manage interactions with your content. For example, you might use ARKit hand tracking to determine where someone is pointing in the scene, and use scene reconstruction to build a mesh you can integrate into your RealityKit content.\n\nSee Also\niOS migration and compatibility\nBringing your existing apps to visionOS\nBuild a version of your iPadOS or iOS app using the visionOS SDK, and update your code for platform differences.\nChecking whether your existing app is compatible with visionOS\nDetermine whether your existing iOS or iPadOS app runs as is in visionOS or requires modifications to handle platform differences.\nMaking your existing app compatible with visionOS\nModify your iPadOS or iOS app to run successfully in visionOS."
  },
  {
    "title": "Interacting with your app in the visionOS simulator | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/interacting-with-your-app-in-the-visionos-simulator",
    "html": "Overview\n\nUse Simulator to run apps in visionOS without installing them on a physical device. When you run your app in Simulator, you can see a monoscopic view of your app’s windows and 3D content inside an immersive space. Use your Mac to alter the viewpoint of the app within the space and navigate the app’s interface.\n\nInteract with your app\n\nTo use your Mac’s pointer and keyboard to create gestures, choose “Select to interact with the scene” from the buttons at the bottom-right of a visionOS simulator window. The current gaze position tracks your pointer movements when you hover over content within the space.\n\nUse the following actions to trigger gestures:\n\nGesture\n\n\t\n\nTo simulate\n\n\n\n\nTap\n\n\t\n\nClick.\n\n\n\n\nDouble-tap\n\n\t\n\nDouble-click.\n\n\n\n\nTouch and hold\n\n\t\n\nClick and hold.\n\n\n\n\nDrag (left, right, up, and down)\n\n\t\n\nDrag left, right, up, and down.\n\n\n\n\nDrag (forward and back)\n\n\t\n\nShift-drag up and down.\n\n\n\n\nTwo-handed gestures\n\n\t\n\nPress and hold the Option key to display touch points. Move the pointer while pressing the Option key to change the distance between the touch points. Move the pointer and hold the Shift and Option keys to reposition the touch points.\n\nActivate device buttons using menu items or by clicking the controls in the simulator window toolbar.\n\nNavigate the space\n\nUse your Mac’s pointer and the keyboard to reposition your viewpoint in a visionOS simulator window:\n\nMovement\n\n\t\n\nTo simulate\n\n\n\n\nForward\n\n\t\n\nPress the W key (or Up Arrow key), or perform a pinch gesture moving two fingers away from each other on a trackpad.\n\n\n\n\nBackward\n\n\t\n\nPress the S key (or Down Arrow key), or perform a pinch gesture moving two fingers toward each other on a trackpad.\n\n\n\n\nLeft\n\n\t\n\nPress the A key (or Left Arrow key), or scroll left using a trackpad or Magic Mouse.\n\n\n\n\nRight\n\n\t\n\nPress the D key (or Right Arrow key), or scroll right using a trackpad or Magic Mouse.\n\n\n\n\nUp\n\n\t\n\nPress the E key, or scroll up using a trackpad or Magic Mouse.\n\n\n\n\nDown\n\n\t\n\nPress the Q key, or scroll down using a trackpad or Magic Mouse.\n\nYou can also control the viewpoint with a standard drag. To do so, choose “Drag to pan the camera” from the buttons at the bottom-right of the simulator window to move the viewpoint left, right, up or down and choose “Drag to dolly the camera” to move it forward or backward.\n\nTo change your viewing angle, Control-drag inside a visionOS simulator window. You can choose “Drag to tilt the camera” from the buttons at the bottom-right of the simulator window to use a drag without the Control key.\n\nTo reset the viewpoint and viewing angle for a visionOS simulator, choose the Reset Camera button from the toolbar at the top-right of its window.\n\nNote\n\nTo capture the input from the pointer and keyboard, bypassing navigation control, to direct the input to the simulated device, use the Pointer Capture and Keyboard Capture buttons in the toolbar at the top-right of a visionOS simulator window. Press the Esc (Escape) key to disable capture and restore navigation controls.\n\nWhen moving with a trackpad or Magic Mouse, Simulator respects the natural scrolling setting on macOS.\n\nYou can also use a game controller to control your movement. Use the left stick to move left, right, forward or back. Use R2 and L2 to move up and down. Use the right stick on a game controller to pan around the space.\n\nSwitch between simulated scenes\n\nSimulator provides multiple built-in scenes you can use to simulate passthrough in different surroundings. These include unique room layouts and furniture for different testing scenarios, each available in different lighting conditions.\n\nUse the simulated scene to test:\n\nReadability of your app in varying backgrounds and varying lighting conditions.\n\nDifferent scenarios, including limited and cluttered surroundings, to see how your app adapts to them.\n\nContent layout, positioning, and scale.\n\nSpatial audio and acoustics.\n\nTo change the simulated scene, click the Simulated Scenes button in the toolbar at the top-right of a visionOS simulator window and choose a different scene.\n\nSee Also\nSimulator\nRunning your app in Simulator or on a device\nLaunch your app in a simulated iOS, tvOS, watchOS, or visionOS device, or on a device connected to a Mac."
  },
  {
    "title": "Bringing your existing apps to visionOS | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/bringing-your-app-to-visionos",
    "html": "Overview\n\nIf you have an existing app that runs in iPadOS or iOS, you can build that app against the visionOS SDK to run it on the platform. Apps built specifically for visionOS adopt the standard system appearance, and they look more natural on the platform. Updating your app is also an opportunity to add elements that work well on the platform, such as 3D content and immersive experiences.\n\nIn most cases, all you need to do to support visionOS is update your Xcode project’s settings and recompile your code. Depending on your app, you might need to make additional changes to account for features that are only found in the iOS SDK. While most of the same technologies are available on both platforms, some technologies don’t make sense or require hardware that isn’t present on visionOS devices. For example, people don’t typically use a headset to make contactless payments, so apps that that use the ProximityReader framework must disable those features when running in visionOS.\n\nNote\n\nIf you use ARKit in your iOS app to create an augmented reality experience, you need to make additional changes to support ARKit in visionOS. For information on how to update this type of app, see Bringing your ARKit app to visionOS.\n\nAdd visionOS as a supported destination for your app\n\nThe first step to updating your app is to add visionOS as a supported destination. In your project’s settings, select your app target and navigate to the General tab. In Supported Destinations, click the Add (+) button to add a new destination and select the Apple Vision option. Adding this option lets you build your app specifically for the visionOS SDK.\n\nWhen you add Apple Vision as a destination, Xcode makes some one-time changes to your project’s build settings. After you add the destination, you can modify your project’s build settings and build phases to customize the build behavior specifically for visionOS. For example, you might remove dependencies for the visionOS version of your app, or change the set of source files you want to compile.\n\nFor more information about how to update a target’s configuration, see Customizing the build phases of a target.\n\nClean up code that uses deprecated APIs\n\nFix any deprecation warnings in the iOS version of your code before you build for visionOS. Apple marks APIs as deprecated when they are no longer relevant or a suitable replacement exists. When you compile code that calls deprecated APIs, the compiler generates warnings and often suggests replacements for you to use instead. visionOS removed many deprecated symbols entirely, turning these deprecation warnings into missing-symbol errors on the platform. Make changes in the iOS version of your app to see the original deprecation warning and replacement details.\n\nIn addition to individual symbols, the following frameworks are deprecated in their entirety in both iOS and visionOS. If your app still uses these frameworks, stop using them immediately. The reference documentation for each framework includes information about how to update your code.\n\nAccounts\n\nAddress Book\n\nAddress Book UI\n\nAssets Library\n\nGLKit\n\niAd\n\nNewsstand Kit\n\nNotificationCenter\n\nOpenGL ES\n\nIsolate features that are unavailable in visionOS\n\nThe iOS SDK includes many frameworks that don’t apply to visionOS, either because they use hardware that isn’t available or their features don’t apply to the platform. Move code that uses these frameworks to separate source files whenever possible, and include those files only in the iOS version of your app.\n\nWhen you can’t isolate the code to separate source files, use conditional statements such as the ones below to offer a different code path for visionOS and iOS. The following example shows how to configure conditional statements to execute separate code paths in visionOS and iOS:\n\n#if os(visionOS)\n   // visionOS code\n#elseif os(iOS)\n   // iOS code\n#endif\n\n\nThe following frameworks are available in the iOS SDK but not in the visionOS SDK.\n\n\t\n\n\t\n\n\n\n\nActivityKit\n\n\t\n\nAdSupport\n\n\t\n\nAppClip\n\n\n\n\nAutomatedDeviceEnrollment\n\n\t\n\nBusinessChat\n\n\t\n\nCarKey\n\n\n\n\nCarPlay\n\n\t\n\nCinematic\n\n\t\n\nClockKit\n\n\n\n\nCoreLocationUI\n\n\t\n\nCoreMediaIO\n\n\t\n\nCoreNFC\n\n\n\n\nCoreTelephony\n\n\t\n\nDeviceActivity\n\n\t\n\nDockKit\n\n\n\n\nExposureNotification\n\n\t\n\nFamilyControls\n\n\t\n\nFinanceKit\n\n\n\n\nFinanceKitUI\n\n\t\n\nManagedSettings\n\n\t\n\nManagedSettingsUI\n\n\n\n\nMessages\n\n\t\n\nMLCompute\n\n\t\n\nNearbyInteraction\n\n\n\n\nOpenAL\n\n\t\n\nProximityReader\n\n\t\n\nRoomPlan\n\n\n\n\nSafetyKit\n\n\t\n\nScreenTime\n\n\t\n\nSensorKit\n\n\n\n\nServiceManagement\n\n\t\n\nSocial\n\n\t\n\nTwitter\n\n\n\n\nWidgetKit\n\n\t\n\nWorkoutKit\n\n\t\n\nSome frameworks have behavioral changes that impact your app in visionOS, and some frameworks disable features when the required hardware is unavailable. To help you avoid using APIs for missing features, many frameworks offer APIs to check the availability of those features. Continue to use those APIs and take appropriate actions when the features aren’t available. In other cases, be prepared for the framework code to do nothing or to generate errors when you use it.\n\nARKit. This framework requires you to use different APIs for iOS and visionOS. For more information, see Bringing your ARKit app to visionOS.\n\nAutomaticAssessmentConfiguration. The framework returns an error if you try to start a test in visionOS.\n\nAVFoundation. Capture interfaces aren’t available in visionOS. Use availability checks to determine which services are present.\n\nCallKit. You may continue to offer Voice-over-IP (VoIP) services, but phone number verification, call-blocking, and other cellular-related services are unavailable.\n\nClockKit. The APIs of this framework do nothing in visionOS.\n\nCoreHaptics. visionOS plays audio feedback instead of haptic feedback.\n\nCoreLocation. You can request someone’s location using the standard location service, but most other services are unavailable. Use availability checks to determine which services are present. The Always authorization level is unavailable and automatically becomes When in Use authorization.\n\nCoreMotion. Barometer data is unavailable, but most other sensors are available. Use availability checks to determine which sensors you can use.\n\nHealthKit and HealthKitUI. Health data is unavailable. Use availability checks to determine when information is available.\n\nMapKit. User-tracking features that involve heading information aren’t available.\n\nMediaPlayer. Some APIs are unavailable in visionOS.\n\nMetricKit. You can gather on-device diagnostic logs and generate reports, but you can’t gather metrics.\n\nMusicKit. Some APIs are unavailable in visionOS.\n\nNearbyInteraction. The framework does nothing in visionOS. Use availability checks to determine when services are present.\n\nPushToTalk. Push to Talk services are unavailable. Check for errors when creating a PTChannelManager.\n\nSafariServices. A link that presents a SFSafariViewController now opens a new scene in the Safari app.\n\nUIKit. The system reports a maximum of two simultaneous touch inputs — one for each of the person’s hands. All system gesture recognizers handle these inputs correctly, including for zoom and rotation gestures that require multiple fingers. If you have custom gesture recognizers that require more than two fingers, update them to support only one or two touches in visionOS.\n\nVisionKit. The DataScannerViewController APIs are unavailable, but other features are still available.\n\nWatchConnectivity. The framework supports connections only between an iPhone and Apple Watch. Use availability checks to determine when services are available.\n\nFor additional information about how to isolate code to the iOS version of your app, see Running code on a specific platform or OS version.\n\nUpdate your interface to take advantage of visionOS features\n\nAfter your existing code runs correctly in visionOS, look for ways to improve the experience you offer on the platform. In visionOS, you can display content using more than just windows. Think about ways to incorporate the following elements into your interface:\n\nDepth. Many SwiftUI and UIKit views use visual effects to add depth. Look for similar ways to incorporate depth into your own custom views.\n\n3D content. Think about where you might incorporate 3D models and shapes into your content. Use RealityKit to implement your content, and a RealityView to present that content from your app. See Adding 3D content to your app.\n\nImmersive experiences. Present a space to immerse someone in your app’s content. Spaces let you place content anywhere in a person’s surroundings. You can also create fully immersive experiences that display only your app’s content. See Creating fully immersive experiences in your app.\n\nInteractions with someone’s surroundings. Use ARKit to facilitate interactions between your content and the surroundings. For example, detect planar surfaces to use as anchor points for your content. See ARKit.\n\nIf you built your interface using UIKit, you can still load iOS storyboards into your app, but you can’t customize your interface for visionOS or include 3D content. To include visionOS content in your app, programmatically add your SwiftUI views using UIHostingController or UIViewRepresentable. Alternatively, migrate the relevant parts of your interface to SwiftUI. Moving your interface to SwiftUI gives you less code to maintain and makes it easier to validate that your interface does what you want.\n\nFor information about mixing SwiftUI and UIKit content, see UIKit integration in SwiftUI. For guidance on how best to incorporate depth and 3D elements in your interface, see Human Interface Guidelines.\n\nUpdate your app’s assets\n\nAdd vector-based or high-resolution images to your project specifically to support visionOS. In visionOS, people can view your app’s content at different angles and different distances, so image pixels rarely line up with screen pixels. Vector-based images work best because they maintain their detail and crispness at any size. For bitmap-based images, use high-resolution images (@2x or better) to ensure your images retain detail at different sizes.\n\nFor more information about designing images for your app, see Images in Human Interface Guidelines.\n\nDecide whether to port your app at all\n\nIn some cases, it might not make sense to port your app for visionOS. For example, don’t port the following types of apps:\n\nApps that act as containers for app extensions. This includes apps where the primary purpose is to deliver custom keyboard extensions, device drivers, sticker packs, SMS and MMS message filtering extensions, call directory extensions, or widgets.\n\nMovement-based apps. This includes apps that follow a person’s location changes, such as apps that offer turn-by-turn directions or navigation. It also includes apps that track body movements.\n\nSelfie or photography apps. This includes apps where the primary purpose is to capture images or video from the device’s cameras.\n\nIf your app uses an unsupported feature but can function without it, you can still bring your app to visionOS. Remove features that aren’t available and focus on bringing the rest of your content to the platform. For example, if you have an app that lets people write down notes and take pictures to include with those notes, disable the picture-taking ability in visionOS but let people add text and incorporate images they already have.\n\nSee Also\niOS migration and compatibility\nBringing your ARKit app to visionOS\nUpdate an iPadOS or iOS app that uses ARKit, and provide an equivalent experience in visionOS.\nChecking whether your existing app is compatible with visionOS\nDetermine whether your existing iOS or iPadOS app runs as is in visionOS or requires modifications to handle platform differences.\nMaking your existing app compatible with visionOS\nModify your iPadOS or iOS app to run successfully in visionOS."
  },
  {
    "title": "Analyzing the performance of your visionOS app | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/analyzing-the-performance-of-your-visionos-app",
    "html": "Overview\n\nTo maintain the sense of immersion on Apple Vision Pro, the system attempts to provide the device displays with up-to-date imagery at a constant rate and respond to interactions with minimum latency. Any visual choppiness or delay in responsiveness interferes with the spatial experience. Higher power consumption over extended periods of time, or extreme power consumption over shorter periods of time, can trigger thermal mitigations that also impact the quality of the experience. It’s important to minimize your app’s use of system resources to ensure your app performs well on the platform. Many of the same best practices and optimization procedures you use developing for other Apple platforms apply when developing for visionOS as well. For more information about optimizing your app on other platforms, see Improving your app’s performance.\n\nTo get useful information specific to rendering bottlenecks, high system power use, and other issues that effect the responsiveness of your visionOS app, profile your app with the RealityKit Trace template in Instruments. This template helps you identify:\n\nComplex content or content with frequent updates that cause the render server to miss deadlines and drop frames.\n\nContent and tasks that result in high system power use.\n\nLong running tasks on the the main thread that interfere with efficient processing of input events.\n\nTasks running on other threads that don’t complete in time to sync back to the main thread for view hierarchy updates.\n\nNote\n\nYou can profile using a real device or a simulator, but to get the most accurate and actionable information, use a real device. Software and hardware differences between a simulator on your Mac and a real device prevent you from relying on timing information. Simulated devices are useful for quick iteration and improving performance aspects that aren’t based on time.\n\nOpen a new trace document\n\nTo create a new trace document:\n\nSelect your app’s scheme and a visionOS run destination from the Xcode project window.\n\nChoose Product > Profile.\n\nChoose RealityKit Trace template\n\nSelect the Choose button.\n\nAlternatively, launch Instruments and choose a target app from the template selection dialog.\n\nThe RealityKit Trace template includes the following instruments:\n\nRealityKit Frames\n\nCaptures frame render times and lifespans for frames the visionOS render server generates. This instrument indicates when frames miss rendering deadlines and provides average CPU and GPU render rates.\n\nRealityKit Metrics\n\nCaptures comprehensive timing information from the entire render pipeline including rendering, commits, animations, physics, and spatial systems. This instrument identifies potential bottlenecks in your app’s process or in the render server as a result of your app’s content and indicates areas of moderate and high system power usage that require optimization.\n\nRunloops\n\nCaptures and displays Runloop execution details.\n\nTime Profiler\n\nProfiles running threads on all cores at regular intervals for all processes.\n\nHangs\n\nCaptures and displays periods of time when the main thread is unresponsive.\n\nMetal Application\n\nRecords Metal app events.\n\nConsider adding other instruments to your trace for specific investigations. For example, you can use the Thermal State instrument to record device thermal states to check if thermal pressures are throttling performance.\n\nProfile your workflows\n\nClick the record button at the top left of the window to start capturing profile data. Perform the actions in your app that you want to investigate. When you complete the actions, click the record button again to stop recording.\n\nTo investigate performance issues or analyze system power impact, profile your app in isolation to understand your app’s impact on system performance and ensure you get the most actionable information. For apps that run alongside other apps, profile your app again with those other apps running to understand how people experience your app in conjunction with other apps.\n\nInspect frame rendering performance\n\nTo maintain a smooth visual experience, the system tries to render new frames for the Apple Vision Pro at 90 frames per second (FPS). The system renders at other frame rates depending on the content it displays and the current surroundings. Each frame has a deadline for rendering based on the target frame rate. Not meeting these deadlines results in dropped frames. This creates a poor spatial experience overall. People tend to notice it in the visual performance of Persona and SharePlay experiences, video playback, and scrolling. The RealityKit Frames instrument displays the time spent rendering each frame in the Frames section of its timeline:\n\nWhen you zoom out, you can identify areas with a high number of frame drops or with frames running close to the rendering deadline. The timeline uses green to identify frames that complete rendering before the deadline, orange for frames that complete rendering close to the deadline, and red for frames that don’t complete rendering that the renderer drops. Dropped frames contribute to a poor spatial experience, but frames that complete close to their rendering deadline indicate performance problems too. Hold the Option key and drag to zoom into a frame, or group of frames, to see their lifespan broken down in stages:\n\nThis provides you with insight into which portion of the rendering pipeline to investigate further. This timeline also includes sections that visualize the Average CPU Frame Time and Average GPU Frame Time to indicate the type of processing that computes the frames. A region of the timeline without a frame block indicates a period of time without changes to a person’s surroundings or app updates. The render server avoids computing new frames to send to the compositor during these periods which helps optimize power use.\n\nMonitor system power usage\n\nWhen thermal levels rise to levels that trigger thermal mitigations in the system, performance degrades and negatively impacts the responsiveness of your app. Optimize for power to avoid this negative impact. The timeline for the RealityKit Metrics instrument includes a System Power Impact section to identify areas of high power usage in your app:\n\nIf the timeline displays green, the tool considers your app’s impact on system power low enough to sustain. Regions that display orange or red indicate the system power usage could cause thermal levels to rise and trigger thermal mitigations. This decreases the availability of system resources, which can cause visual interruptions and responsiveness issues.\n\nNote\n\nIf the render server can’t maintain the target frame rate of 90 FPS due to thermal pressure, it might reduce its frame rate in half. When this occurs, all frames in the frames track show up as missing their rendering deadlines. Other factors can cause reduced frame rate, including the complexity and frequency of the content the system is processing. Use the Thermal State instrument to determine if thermal conditions are causing the rate limiting or if it’s due to other factors.\n\nIdentify bottlenecks\n\nThe Bottlenecks section of the timeline for the RealityKit Metrics instrument contains markers that indicate high overhead in your app or the render server that contribute to dropped frames and high system power use. When you encounter either of these issues, check if the timeline identifies bottlenecks you can address. Double-click on any of the markers to display more information in the detail area at the bottom of the instruments window. If the detail area is hidden, choose View > Detail Area > Show Detail Area to reveal it. The render server encounters bottlenecks in either the CPU or GPU. The instrument categorizes bottlenecks by their severity and type.\n\nTo filter the bottlenecks listed in the detail area to a particular time period, drag inside the timeline to select the region. To see an outline view of the bottlenecks organized by severity and type, select Summary: RealityKit Bottlenecks from the menu at the top left of the detail area. Click the arrow button to the right of the severity or type in the outline view to show the list of bottlenecks in that category.\n\nWhen you select a specific bottleneck, the extended detail provides recommendations for you to address the bottleneck – choose View > Show Extended Detail to reveal the extended detail if it’s hidden.\n\nExplore the metrics that relate to bottlenecks\n\nThe trace provides additional information you can use to identify changes to make in your app to address these bottlenecks. Click the expansion arrow for the RealityKit Metrics instrument timeline to reveal graphs specific to each major category of work. Use the metrics associated with these graphs to determine which RealityKit feature has the biggest impact on high CPU frame times in the app process or in the render server. When interpreting these graphs, lower indicates better performance and power. The metrics represent values from all apps running, so profile with just your app running when trying to optimize for these metrics.\n\n3D Render\n\nMetrics related to the cost of 3D RealityKit rendering in the render server. This includes the number of draw calls, triangles, and vertices from all apps.\n\nCore Animation Render\n\nMetrics related to UI content rendering costs in the render server. This includes the total number of render passes, offscreen render passes, and translucent UI meshes from all apps.\n\nEntity Commits\n\nMetrics related to the costs of entity commits in the app and the render server. This includes the number of RealityKit entities shared with the render server from all apps, as well as the number of updates received from all apps over certain intervals.\n\nRealityKit Animations\n\nMetrics related to the cost of RealityKit animations in the app and the render server. This includes the number of skeletal animations, across all apps.\n\nRealityKit Physics\n\nMetrics related to the cost of RealityKit physics simulations, collisions, and hit testing in the app process and render server. This includes the number of rigid body counts and colliders in use, as well as the type of physics shapes that the UI and other 3D content use, across all apps.\n\nSpatial Systems\n\nMetrics related to the costs of spatial algorithms in the render server. This includes the number of custom anchors, across all apps.\n\nTip\n\nThe graphs for some sections combine several individual metrics. The heading indicates this by displaying a graph count. Click on the bottom of the timeline’s heading and drag down to display individual graphs for each metric. For example, the 3D Render Timeline might display 13 Graphs in the heading; expanding that timeline exposes individual graphs for 3D Mesh Draw Calls, 3D Mesh Triangles, 3D Mesh Vertices, and the 10 additional metrics.\n\nThe timeline for your app’s process helps summarize information from the instruments about your process and the work the render server completes for your process.\n\nChoose an option from the pop-up in the timeline header to show different graphs in the timeline:\n\nRunloops\n\nTime each thread spends waiting or busy.\n\nHangs\n\nTime the main thread is unresponsive.\n\nTime Profile\n\nCPU usage and lifecycle status.\n\nRealityKit System Times\n\nOverhead attributed to RealityKit systems.\n\nWhen you select the timeline for your app’s process, you can choose instrument summaries and profile data to display in the detail area from the popup-button at its top-left:\n\nTo filter the information in the detail area by time, select periods of time in the timeline above.\n\nDetect delays on the main thread\n\nSelect Hangs in your app’s process timeline to identify times in the trace that might have interaction delays. Use the RealityKit Metrics and Time Profiler summaries to better understand the work your app is doing. Choose the following options from the detail area pop-up menu:\n\nProfile and Samples\n\nShows information from the Time Profiler instrument to determine what your app is doing during a hang.\n\nSummary\n\nRealityKit System CPU times: Shows minimum, maximum, and average times the CPU spends on various RealityKit system operations.\n\nOptimize any 3D render updates, hit testing, and collision work you find. For more information about addressing hangs in your app, see Improving app responsiveness.\n\nManage audio overhead\n\nUse the Audio Playback section of your process’s timeline to identify areas of high audio overhead. The system defaults to using spatial audio for your app when running on visionOS. It processes information in real time about your position, surroundings, and the current location of audio sources to generate an immersive audio experience. If you include too many concurrent audio sources that require the system to adapt audio sources to their location within a large space, the increased demand on system resources can lead to delays in the audio output.\n\nTo reduce the spatial audio work, limit:\n\nThe number of concurrently playing audio sources\n\nThe number of moving audio sources\n\nThe size of the soundstage\n\nConsider creating a pool of audio players to limit the maximum number of players your app uses. Place players on stationary entities, instead of moving entities, when appropriate. Initializing several audio players at the same time causes a high overhead that affects other aspects of the system, such as rendering performance. Consider the other tasks the system completes during these allocations and space them out over time. For more information, see Create a great spatial playback experience.\n\nSee Also\nXcode and Instruments\nDiagnosing and resolving bugs in your running app\nInspect your app to isolate bugs, locate crashes, identify excess system-resource usage, visualize memory bugs, and investigate problems in its appearance.\nDiagnosing issues in the appearance of a running app\nInspect your running app to investigate issues in the appearance and placement of the content it displays.\nCreating a performance plan for your visionOS app\nIdentify your app’s performance and power goals and create a plan to measure and assess them.\nConfiguring your app icon\nAdd app icon variations to represent your app in places such as Settings, search results, and the App Store."
  },
  {
    "title": "Placing content on detected planes | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/placing-content-on-detected-planes",
    "html": "Overview\n\nFlat surfaces are an ideal place to position content in an app that uses a Full Space in visionOS. They provide a place for virtual 3D content to live alongside a person’s surroundings. Use plane detection in ARKit to detect these kinds of surfaces and filter the available planes based on criteria your app might need, such as the size of the plane, its proximity to someone, or a required plane orientation.\n\nPlay\nUse RealityKit anchor entities for basic plane anchoring\n\nIf you don’t need a specific plane in your app and you’re rendering your app’s 3D content in RealityKit, you can use an AnchorEntity instead. This approach lets you attach 3D content to a plane without prompting the person for world-sensing permission and without any particular knowledge of where that plane is relative to the person.\n\nThe following shows an anchor that you can use to attach entities to a table:\n\nAnchorEntity(.plane(.horizontal, classification: .table, minimumBounds: [0.5, 0.5]))\n\n\nAnchor entities don’t let you choose a specific plane in a person’s surroundings, but rather let you ask for a plane with certain characteristics. When you need more specific plane selection or real-time information about the plane’s position and orientation in the world, use ARKitSession and PlaneDetectionProvider.\n\nConfigure an ARKit session for plane detection\n\nPlane-detection information comes from an ARKitSession that’s configured to use a PlaneDetectionProvider. You can choose to detect horizontal planes, vertical planes, or both. Each plane that ARKit detects comes with a classification, like PlaneAnchor.Classification.table or PlaneAnchor.Classification.floor. You can use these classifications to further refine which kinds of planes your app uses to present content. Plane detection requires ARKitSession.AuthorizationType.worldSensing authorization.\n\nThe following starts a session that detects both horizontal and vertical planes, but filters out planes classified as windows:\n\nlet session = ARKitSession()\nlet planeData = PlaneDetectionProvider(alignments: [.horizontal, .vertical])\n\n\nTask {\n    try await session.run([planeData])\n    \n    for await update in planeData.anchorUpdates {\n        if update.anchor.classification == .window {\n            // Skip planes that are windows.\n            continue\n        }\n        switch update.event {\n        case .added, .updated:\n            await updatePlane(update.anchor)\n        case .removed:\n            await removePlane(update.anchor)\n        }\n        \n    }\n}\n\nCreate and update entities associated with each plane\n\nIf you’re displaying content that needs to appear attached to a particular plane, update your content whenever you receive new information from ARKit. When a plane is no longer available in the person’s surroundings, ARKit sends a removal event. Respond to these events by removing content associated with the plane.\n\nThe following shows plane updates that place a text entity on each plane in a person’s surroundings; the text entity displays the kind of plane ARKit detected:\n\n@MainActor var planeAnchors: [UUID: PlaneAnchor] = [:]\n@MainActor var entityMap: [UUID: Entity] = [:]\n\n\n@MainActor\nfunc updatePlane(_ anchor: PlaneAnchor) {\n    if planeAnchors[anchor.id] == nil {\n        // Add a new entity to represent this plane.\n        let entity = ModelEntity(mesh: .generateText(anchor.classification.description))\n        entityMap[anchor.id] = entity\n        rootEntity.addChild(entity)\n    }\n    \n    entityMap[anchor.id]?.transform = Transform(matrix: anchor.originFromAnchorTransform)\n}\n\n\n@MainActor\nfunc removePlane(_ anchor: PlaneAnchor) {\n    entityMap[anchor.id]?.removeFromParent()\n    entityMap.removeValue(forKey: anchor.id)\n    planeAnchors.removeValue(forKey: anchor.id)\n}\n\nSee Also\nARKit\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nIncorporating real-world surroundings in an immersive experience\nCreate an immersive experience by making your app’s content respond to the local shape of the world.\nTracking specific points in world space\nRetrieve the position and orientation of anchors your app stores in ARKit.\nTracking preregistered images in 3D space\nPlace content based on the current position of a known image in a person’s surroundings."
  },
  {
    "title": "Diorama | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/diorama",
    "html": "Overview\n\nUse Reality Composer Pro to compose, edit, and preview RealityKit content for your visionOS app. In your Reality Composer Pro project, you can create one or more scenes, each of which contains a hierarchy of virtual objects called entities that your app can efficiently load and display.\n\nIn addition to helping you compose entity hierarchies, Reality Composer Pro also gives you the ability to add and configure components — even custom components that you’ve written — to the entities in your scenes.\n\nPlay\n\nYou can also design the visual appearance of entities using Shader Graph, a node-based visual tool for creating RealityKit materials. Shader Graph gives you a tremendous amount of control over the surface details and shape of entities. You can even create animated materials and dynamic materials that change based on the state of your app or user input.\n\nDiorama demonstrates many of RealityKit and Reality Composer Pro’s features. It displays an interactive, virtual topographical trail map, much like the real-world dioramas you find at trailheads and ranger stations in national parks. This virtual map has points of interest you can tap to bring up more detailed information. You can also smoothly transition between two trail maps: Yosemite and Catalina Island.\n\nImport assets for building the scene\n\nYour Reality Composer Pro project must contain assets, which you use to compose scenes for your app. Diorama’s project has several assets, including 3D models like the diorama table, trail map, some birds and clouds that fly over the map, and a number of sounds and images. Reality Composer Pro provides a library of 3D models you can use. Access the library by clicking the Add (+) button on the right side of the toolbar. Selecting objects from the library imports them into your project.\n\nDiorama uses custom assets instead of the available library assets. To use custom assets in your own Reality Composer Pro scenes, import them into your project in one of three ways: by dragging them to Reality Composer Pro’s project browser, using File > Import from the File menu, or copying the assets into the .rkassets bundle inside your project’s Swift package.\n\nNote\n\nAlthough you can still load USDZ files and other assets directly in visionOS, RealityKit compiles assets in your Reality Composer Pro project into a binary format that loads considerably faster than loading from individual files.\n\nCreate scenes containing the app’s entities\n\nA single Reality Composer Pro project can have multiple scenes. A scene is an entity hierarchy stored in the project as a .usda file that you can load and display in a RealityView. You can use Reality Composer’s scenes to build an entire RealityKit scene, or to store reusable entity hierarchies that you can use as building block for composing scenes at runtime — the approach Diorama uses. You can add as many different scenes to your project as you need by selecting File > New > Scene, or pressing ⌘N.\n\nAt the top of the Reality Composer Pro window, there’s a separate tab for every scene that’s currently open. To open a scene, double-click the scene’s .usda file in the project browser. To edit a scene, select its tab, and make changes using the hierarchy viewer, the 3D view, and the inspector.\n\nAdd assets to your scenes\n\nRealityKit can only include entities in a scene, but it can’t use every type of asset that Reality Composer Pro supports as an entity. Reality Composer Pro automatically turns some assets, like 3D models, into an entity when you place them in a scene. It uses other assets indirectly. It uses image files, for example, primarily to define the surface details of model entities.\n\nDiorama uses multiple scenes to group assets together and then, at runtime, combines those scenes into a single immersive experience. For example, the diorama table has its own scene that includes the table, the map surface, and the trail lines. There are separate scenes for the birds that flock over the table, and for the clouds that float above it.\n\nTo add entities to a scene, drag assets from the project browser to the scene’s hierarchy view or 3D view. If the asset you drag is a type that can be represented as an entity, Reality Composer Pro adds it to your scene. You can select any asset in the scene hierarchy or the 3D view and change its location, rotation, and scale using the inspector on the right side of the window or the manipulator in the 3D view.\n\nAdd components to entities\n\nRealityKit follows a design pattern called Entity Component System (ECS). In an ECS app, you store additional data on an entity using components and can implement entity behavior by writing systems that use the data from those components. You can add and configure components to entities in Reality Composer Pro, including both shipped components like PhysicsBodyComponent, and custom components that you write and place in the Sources folder of your Reality Composer Pro Swift package. You can even create new components in Reality Composer Pro and then edit them in Xcode. For more information about ECS, see Understanding RealityKit’s modular architecture.\n\nDiorama uses custom components to identify which transforms are points of interest, to mark the birds so the app can make sure they flock together, and to control the opacity of entities that are specific to just one of the two maps.\n\nTo add a component to an entity, select that entity in the hierarchy view or 3D view. At the bottom right of the inspector window, click on the Add Component button. A list of available components appears and the first item in that list is New Component. This item creates a new component class, and optionally a new system class, and adds the component to the selected entity.\n\nIf you look at the list of components, you see the PointOfInterestComponent that Diorama uses to indicate which transforms are points of interest. If the selected entity doesn’t already contain a PointOfInterestComponent, selecting that adds it to the selected entity. Each entity can only have one component of a specific type. You can edit the values of the existing component in the inspector, which changes what shows up when you tap that point of interest in the app.\n\nUse transforms to mark locations\n\nIn Reality Composer Pro, a transform is an empty entity that marks a point in space. A transform contains a location, rotation, and scale, and its child entities inherit those. But, transforms have no visual representation and do nothing by themselves. Use transforms to mark locations in your scene or organize your entity hierarchy. For example, you might make several entities that need to move together into child entities of the same transform, so you can move them together by moving the parent transform.\n\nDiorama uses transforms with a PointOfInterestComponent to indicate points of interest on the map. When the app runs, those transforms mark the location of the floating placards with the name of the location. Tapping on a placard expands it to show more detailed information. To turn transforms into an interactive view, the app looks for a specific component on transforms called a PointOfInterestComponent. Because a transform contains no data other than location, orientation, and scale, it uses this component to hold the data the app needs to display on the placards. If you open the DioramaAssembled scene in Reality Composer Pro and click on the transform called Cathedral_Rocks, you see the PointOfInterestComponent in the inspector.\n\nLoad a scene at runtime\n\nTo load a Reality Composer Pro scene, use load(named:in:), passing the name of the scene you want to load and the project’s bundle. Reality Composer Pro Swift packages define a constant that provides ready access to its bundle. The constant is the name of the Reality Composer Pro project with “Bundle” appended to the end. In this case, the project is called RealityKitContent, so the constant is called RealityKitContentBundle. Here’s how Diorama loads the map table in the RealityView initializer:\n\nlet entity = try await Entity.load(named: \"DioramaAssembled\", \n                                   in: RealityKitContent.RealityKitContentBundle)\n\n\nThe load(named:in:) function is asynchronous when called from an asynchronous context. Because the content closure of the RealityView initializer is asynchronous, it automatically uses the async version to load the scene. Note that when using it asynchronously, you must call it using the await keyword.\n\nCreate the floating view\n\nDiorama adds a PointOfInterestComponent to a transform to display details about interesting places. Every point of interest’s name appears in a view that floats above its location on the map. When you tap the floating view, it expands to show detailed information, which the app pulls from the PointOfInterestComponent. The app shows these details by creating a SwiftUI view for each point of interest and querying for all entities that have a PointOfInterestComponent using this query declared in ImmersiveView.swift:\n\nstatic let markersQuery = EntityQuery(where: .has(PointOfInterestComponent.self))\n\n\nIn the RealityView initializer, Diorama queries to retrieve the points of interest entities and passes them to a function called createLearnMoreView(for:), which creates the view and saves it for display when it’s tapped.\n\nsubscriptions.append(content.subscribe(to: ComponentEvents.DidAdd.self, componentType: PointOfInterestComponent.self, { event in\n    createLearnMoreView(for: event.entity)\n}))\n\nCreate attachments for points of interest\n\nDiorama displays the information added to a PointOfInterestComponent in a LearnMoreView, which it stores as an attachment. Attachments are SwiftUI views that are also RealityKit entities and that you can place into a RealityKit scene at a specific location. Diorama uses attachments to position the view that floats above each point of interest.\n\nThe app first checks to see if the entity has a component called PointOfInterestRuntimeComponent. If it doesn’t, it creates a new one and adds it to the entity. This new component contains a value you only use at runtime that you don’t need to edit in Reality Composer Pro.\n\nBy putting this value into a separate component and adding it to entities at runtime, Reality Composer Pro never displays it in the inspector. The PointOfInterestRuntimeComponent stores an identifier called an attachment tag, which uniquely identifies an attachment so the app can retrieve and display it at the appropriate time.\n\nstruct PointOfInterestRuntimeComponent: Component {\n    let attachmentTag: ObjectIdentifier\n}\n\n\nNext, Diorama creates a SwiftUI view called a LearnMoreView with the information from the PointOfInterestComponent, tags that view, and stores the tag in the PointOfInterestRuntimeComponent. Finally, it stores the view in an AttachmentProvider, which is a custom class that maintains references to the attachment views so they don’t get deallocated when they’re not in a scene.\n\nlet tag: ObjectIdentifier = entity.id\n\n\nlet view = LearnMoreView(name: pointOfInterest.name,\n                         description: pointOfInterest.description ?? \"\",\n                         imageNames: pointOfInterest.imageNames,\n                         trail: trailEntity,\n                         viewModel: viewModel)\n    .tag(tag)\nentity.components[PointOfInterestRuntimeComponent.self] = PointOfInterestRuntimeComponent(attachmentTag: tag)\n\n\nattachmentsProvider.attachments[tag] = AnyView(view)\n\nDisplay point of interest attachments\n\nAssigning a view to an attachment provider doesn’t actually display that view in the scene. The initializer for RealityView has an optional view builder called attachments that’s used to specify the attachments.\n\nForEach(attachmentsProvider.sortedTagViewPairs, id: \\.tag) { pair in\n    pair.view\n}\n\n\nIn the update closure of the initializer, which RealityKit calls when the contents of the view change, the app queries for entities with a PointOfInterestRuntimeComponent, uses the tag from that component to retrieve the correct attachment for it, and then adds that attachment and places it above its location on the map.\n\nviewModel.rootEntity?.scene?.performQuery(Self.runtimeQuery).forEach { entity in\n\n\n    guard let attachmentEntity = attachments.entity(for: component.attachmentTag) else { return }\n    \n    if let pointOfInterestComponent = entity.components[PointOfInterestComponent.self] {\n        attachmentEntity.components.set(RegionSpecificComponent(region: pointOfInterestComponent.region))\n        attachmentEntity.components.set(OpacityComponent(opacity: 0))\n    }\n    \n    viewModel.rootEntity?.addChild(attachmentEntity)\n    attachmentEntity.setPosition([0, 0.2, 0], relativeTo: entity)\n}\n\nCreate custom materials with Shader Graph\n\nTo switch between the two different topographical maps, Diorama shows a slider that morphs the map between the two locations. To accomplish this, and to draw elevation lines on the map, the FlatTerrain entity in the DioramaAssembled scene uses a Shader Graph material. Shader Graph is a node-based material editor that’s built into Reality Composer Pro. Shader Graph gives you the ability to create dynamic materials that you can change at runtime. Prior to Reality Composer Pro, the only way to implement a dynamic material like this was to create a CustomMaterial and write Metal shaders to implement the necessary logic.\n\nDiorama’s DynamicTerrainMaterialEnhanced does two things. It draws contour lines on the map based on height data stored in displacement map images, and it also offsets the vertices of the flat disk based on the same data. By interpolating between two different height maps, the app achieves a smooth transition between the two different sets of height data.\n\nWhen you build Shader Graph materials, you can give them input parameters called promoted inputs that you set from Swift code. This allows you to implement logic that previously required writing a Metal shader. The materials you build in the editor can affect both the look of an entity using the custom surface output node, which equates to writing Metal code in a fragment shader, or the position of vertices using the geometry modifier output, which equates to Metal code running in a vertex shader.\n\nNode graphs can contain subgraphs, which are similar to functions. They contain reusable sets of nodes with inputs and outputs. Subgraphs contain the logic to draw the contour lines and the logic to offset the vertices. Double-click a subgraph to edit it. For more information about building materials using Shader Graph, see Explore Materials in Reality Composer Pro.\n\nUpdate the Shader Graph material at runtime\n\nTo change the map, DynamicTerrainMaterialEnhanced has a promoted input called Progress. If that parameter is set to 1.0, it displays Catalina Island. If it’s set to 0, it displays Yosemite. Any other number shows a state in transition between the two. When someone manipulates the slider, the app updates that input parameter based on the slider’s value.\n\nImportant\n\nShader Graph material parameters are case-sensitive. If the capitalization is wrong, your code won’t actually update the material.\n\nThe app sets the value of the input parameter in a function called handleMaterial() that the slider’s .onChanged closure calls. That function retrieves the ShaderGraphMaterial from the terrain entity and calls setParameter(name:value:) on it.\n\nprivate func handleMaterial() {\n    guard let terrain = viewModel.rootEntity?.terrain,\n            let terrainMaterial = terrainMaterial else { return }\n    do {\n        var material = terrainMaterial\n        try material.setParameter(name: materialParameterName, value: .float(viewModel.sliderValue))\n        \n        if var component = terrain.modelComponent {\n            component.materials = [material]\n            terrain.components.set(component)\n        }\n        \n        try terrain.update(shaderGraphMaterial: terrainMaterial, { m in\n            try m.setParameter(name: materialParameterName, value: .float(viewModel.sliderValue))\n        })\n    } catch {\n        print(\"problem: \\(error)\")\n    }\n}\n\nSee Also\nRelated samples\nHello World\nUse windows, volumes, and immersive spaces to teach people about the Earth.\nDestination Video\nLeverage 3D video and Spatial Audio to deliver an immersive experience.\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nRelated articles\nAdding 3D content to your app\nAdd depth and dimension to your visionOS app and discover how to incorporate your app’s content into a person’s surroundings.\nUnderstanding RealityKit’s modular architecture\nLearn how everything fits together in RealityKit.\nDesigning RealityKit content with Reality Composer Pro\nDesign RealityKit scenes for your visionOS app.\nImplementing systems for entities in a scene\nApply behaviors and physical effects to the objects and characters in a RealityKit scene with the Entity Component System (ECS).\nRelated videos\nMeet Reality Composer Pro\nExplore materials in Reality Composer Pro\nWork with Reality Composer Pro content in Xcode"
  },
  {
    "title": "Understanding RealityKit’s modular architecture | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/understanding-the-realitykit-modular-architecture",
    "html": "Overview\n\nRealityKit is a 3D framework designed for building apps, games, and other immersive experiences. Although it’s built in an object-oriented language and uses object-oriented design principles, RealityKit’s architecture avoids heavy use of composition — where objects are built by adding instance variables that hold references to other objects — in favor of a modular design based on a paradigm called Entity Component System (ECS) that divides application objects into one of three types.\n\nFollowing the ECS paradigm allows you to re-use the functionality contained in a component in many different entities, even if they have very different inheritance chains. Even if two objects have no common ancestors other than Entity, you can add the same components to both of them and give them the same behavior or functionality.\n\nStart with Entities\n\nEntities are the core actors of RealityKit. Any object that you can put into a scene, whether visible or not, is an entity and must be a descendent of Entity. Entities can be 3D models, shape primitives, lights, or even invisible items like sound emitters or trigger volumes. Add components to entities to let them store additional state relevant to a specific type of functionality. Entities themselves contain relatively few properties: Nearly all entity state is stored on an entity’s components.\n\nRealityKit provides a number of entity types you use to represent different kinds of objects. For example, a ModelEntity represents a 3D model, such as one imported from a .usdz or .reality file. These provided entities are essentially just an Entity with certain components already added to them. Adding a ModelComponent to an instance of Entity, for example, results in an entity with identical functionality to a ModelEntity.\n\nAdd components to entities\n\nComponents are modular building blocks that you add to an entity; they identify which entities a system will act on, and maintain the per-entity state that systems rely on. Components can contain logic, but limit component logic to code that validates its property values or sets its initial state. Use systems for any logic that affects the behavior of entities or that potentially changes their state on every frame. To add accessibility information to an entity, for example, add a AccessibilityComponent to it and populate its fields with the information the accessibility system needs, such as putting the description that VoiceOver reads into its label property.\n\nKeep in mind that an entity can only hold one copy of any particular type of component at a time. So, for example, you can’t add two accessibility components to one entity. If you add an accessibility component to an entity that already has one, the new component replaces the previous one.\n\nCreate systems to implement entity behavior\n\nA System contains code that RealityKit calls on every frame to implement a specific type of entity behavior or to update a particular type of entity state. Systems use components to store their entity-specific state and query for entities to act on by looking for ones with a specific component or combination of components.\n\nFor example, a game might have a damage system that monitors and updates the health of every entity that can be damaged or destroyed. Systems typically work together with one or more components, so that damage system might use a health component to keep track of how much damage each entity has taken and how much each one is able to take before it’s destroyed. It might also interact with other components. For example, an entity might have an armor component that provides protection to the entity, and the damage system would also need to use the state stored in that component.\n\nEvery frame, the damage system queries for entities that have the health component and updates values on those entities’ components based on the current state of the app. If an entity has taken too much damage, the system might trigger a specific animation or remove the entity from the scene.\n\nWriting entity logic in a system avoids duplication of work. Using traditional OOP design patterns, where this type of logic would reside on the entity class, can often result in the same calculations being performed multiple times, once for every entity potentially affected. No matter how many entities the calculation potentially impacts the system only has to do the calculation once.\n\nFor more information on creating systems, see Implementing systems for entities in a scene\n\nSee Also\nRealityKit and Reality Composer Pro\nSwift Splash\nUse RealityKit to create an interactive ride in visionOS.\nDiorama\nDesign scenes for your visionOS app using Reality Composer Pro.\nDesigning RealityKit content with Reality Composer Pro\nDesign RealityKit scenes for your visionOS app.\nCapturing screenshots and video from Apple Vision Pro for 2D viewing\nCreate screenshots and record high-quality video of your visionOS app and its surroundings for app previews."
  },
  {
    "title": "Making your existing app compatible with visionOS | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/making-your-app-compatible-with-visionos",
    "html": "Overview\n\nA compatible iPadOS or iOS app links against the iOS SDK and runs in visionOS. Although visionOS provides a complete set of iOS frameworks for linking, some features of those frameworks might be unavailable due to hardware or usage differences. To ensure your app runs correctly in visionOS, handle any missing features gracefully and provide workarounds wherever possible.\n\nPerform availability and authorization checks before using features\n\nSome frameworks offer APIs to let you determine when framework features are available or whether your app is authorized to use them. Always check these APIs before you try to use the corresponding features, and don’t assume a feature is available because the necessary hardware is present. The device’s configuration also plays a role in determining the results of some availability and authorization checks, and features might not be present when your app runs in Simulator. If an availability or authorization check fails, don’t try to use the associated feature in your app.\n\nThe following frameworks support availability or authorization checks:\n\nActivityKit. Check the areActivitiesEnabled property of ActivityAuthorizationInfo to determine if Live Activities are authorized.\n\nARKit. Check the isSupported property of your configuration object to determine availability of augmented reality features. In visionOS, ARKit views such as ARView are never available, so isolate interface code containing those views to the iOS version of your app.\n\nAVFoundation. Identify what cameras are available using the AVCaptureDevice.DiscoverySession class. Don’t assume the presence of specific cameras.\n\nAutomatic Assessment Configuration. Check for error values when you configure an AEAssessmentSession object.\n\nContacts. Use the CNContactStore class to determine your app’s authorization status.\n\nCore Bluetooth. Use the CBCentralManager and CBPeripheralManager classes to determine feature availability and your app’s authorization status.\n\nCore Haptics. Call the capabilitiesForHardware() method of the haptic engine to determine the available features.\n\nCore Location. Check the properties of CLLocationManager to determine the availability of location services.\n\nCore Motion. Check the properties of CMMotionManager to determine the availability of accelerometers, gyroscopes, magnetometers, and other hardware sensors.\n\nCore NFC. Check the readingAvailable property of your reader session to determine if NFC tag reading is available.\n\nEventKit. Use the EKEventStore class to determine your app’s authorization status.\n\nExposureNotification. Use the ENManager class to determine your app’s authorization status.\n\nHealthKit. Use the HKHealthStore class to determine if health-related data is available.\n\nHomeKit. Check the properties of HMHomeManager to determine your app’s authorization status.\n\nLocal Authentication. Use the LAContext class to determine the authentication policies you can use.\n\nMedia Player. Use the MPMediaLibrary class to determine your app’s authorization status.\n\nNearby Interaction. Check the deviceCapabilities property of your session to determine whether features are available.\n\nPhotoKit. Use the PHPhotoLibrary class to determine your app’s authorization status.\n\nProximityReader. Check the isSupported property of the card reader object to determine if Tap to Pay on iPhone is available.\n\nReplayKit. Check the doc://com.apple.documentation/documentation/replaykit/rpscreenrecorder/1620992-isavailable property of RPScreenRecorder to determine if screen recording support is available.\n\nRoomPlan. Check the isSupported property of the RoomCaptureSession object to determine if LiDAR scanning is available on the device.\n\nSensorKit. Use the SRSensorReader class to determine your app’s authorization status.\n\nSpeech. Use the SFSpeechRecognizer class to determine if speech recognition is available.\n\nUser Notifications. Use the getNotificationSettings(completionHandler:) method of UNUserNotificationCenter to determine your app’s authorization status.\n\nWatchConnectivity. Call the isSupported() method of the WCSession object to determine if the framework is available.\n\nHandle environmental differences appropriately\n\nApple frameworks take a device-agnostic approach whenever possible to minimize issues when you use them on different device types. Apple devices come in a variety of shapes and sizes, and with different sets of features. Rather than build your app for a specific device, make sure it adapts to any device and can gracefully handle differences.\n\nBuild robustness into your app during the design process. Avoid assumptions that might cause your app to break when it runs on a new device, and make sure your app adapts dynamically to different conditions. For example:\n\nDon’t assume the device type or idiom is always iPhone, iPad, or iPod Touch. Avoid decisions based on the current idiom. If you do rely on the current idiom, provide reasonable defaults for unknown idioms.\n\nDesign your app to handle unavailable hardware or features. Specific hardware and features might be unavailable for many different reasons. For example, a feature might be unavailable when your app runs in Simulator. Perform availability checks whenever possible, and handle missing features gracefully.\n\nDesign your windows and views to adapt dynamically. Build your interface to adapt dynamically to any size using SwiftUI or Auto Layout. Assume the size of your app can change dynamically.\n\nDon’t assume the device has a specific number of displays. People can connect iPad and iPhone to an external display, and visionOS devices use two displays to create a stereoscopic version of your app’s content.\n\nDon’t make assumptions based on the available frameworks or symbols. The presence or absence of frameworks or code symbols is an unreliable way to identify a device type, and can change in later software updates.\n\nDon’t assume your app runs in the background. visionOS doesn’t support the location, external accessory, or Bluetooth-peripheral background execution modes.\n\nDon’t assume that background apps are hidden. In visionOS, the windows of background apps remain visible, but are dimmed when no one looks at them. The only time app windows disappear is when one app presents an immersive space.\n\nWhen you make decisions using device details, your app might produce inconsistent or erroneous results on an unknown device type, or it might fail altogether. Find solutions that rely on environmental information, rather than the device type. For example, SwiftUI and UIKit start layout using the app’s window size, which isn’t necessarily the same size as the device’s display.\n\nNote\n\nDevice-specific information is available when you absolutely need it, but validate the information you receive and provide reasonable default behavior for unexpected values.\n\nAudit your interface code\n\nTo minimize disruptions, visionOS runs your compatible iPad or iPhone app in an environment that matches an iPad as much as possible. Windows and views retain the same appearance that they have in iPadOS or iOS, and the system sizes your app’s window to fit an iPad whenever possible.\n\nWhen building your app’s interface, make choices that ensure your app runs well in visionOS too. Adopt the following best practices for your interface-related code:\n\nSupport iPad and iPhone in the same app. Create one app that supports both device types, rather than separate apps for each device. SwiftUI and UIKit support adaptable interfaces, and Xcode provides tools to help you visualize your interface at different supported sizes.\n\nOrganize your interface using scenes. Scenes are a fundamental tool for managing your app’s interface. Use the scene types in SwiftUI and UIKit to assemble and manage the views you display in windows.\n\nAdapt your interface to any size. Design your interface to adapt naturally to different sizes. For an introduction to SwiftUI views and layout, see Declaring a custom view. For information about laying out views in UIKit, see View layout.\n\nDon’t access screen details. visionOS provides reasonable values for UIScreen objects, but don’t use those values to make decisions.\n\nSpecify the supported interface orientations. Add the UISupportedInterfaceOrientations key to your app’s Info.plist file to specify the interface orientations it supports. Support all interface orientations whenever possible. visionOS adds an interface rotation for your app button only when this key is present.\n\nUpdate hover effects in custom views. Hover effects convey the focused view or control in your interface. Standard system views apply hover effects as needed. For custom views and controls, verify that the hover effects look appropriate in visionOS. Add or update the content shape for your hover effects if needed.\n\nAdopt vector-based images when possible. Vector-based images scale well to different sizes while retaining a crisp appearance. If you use bitmap-based assets, make them the exact size you need. Don’t use oversized assets, which require extra work to display at the correct size.\n\nIf you want visionOS to display your app’s interface in a particular orientation at launch, add the UIPreferredDefaultInterfaceOrientation key to your app’s Info.plist file. Set the value of the key to one of the values in your app’s UISupportedInterfaceOrientations key. For example, to specify a preference for a portrait orientation, set the value to UIInterfaceOrientationPortrait. Add ~ipad or ~iphone to the key name to specify device-specific orientation preferences.\n\nRespond gracefully to missing features\n\nIf your app relies on frameworks that behave differently in visionOS, update your code to handle those differences. Availability checks give you a clear indication when you can’t use a feature, but some frameworks might have more subtle behavior. Throughout your code, make sure you respond to unusual situations:\n\nHandle error conditions. If a function throws an exception or returns an error, handle the error. Use error information to adjust your app’s behavior or provide an explanation of why it can’t perform certain operations.\n\nHandle nil or empty values gracefully. Validate objects and return values before you try to use them.\n\nUpdate your interface. Provide appropriate messaging in your interface when a feature is missing, or remove feature-specific views entirely if you can do so cleanly. Don’t leave empty views where the feature was.\n\nFor information about frameworks that behave differently in visionOS, see Checking whether your existing app is compatible with visionOS.\n\nRemove code that uses deprecated APIs\n\nIf your app currently uses deprecated APIs or frameworks, update your code to use appropriate replacements. Deprecated symbols represent outdated features, and in some cases might not do anything when you call them. To prevent potential issues, replace them with modern equivalents to ensure your code behaves as expected.\n\nThe following frameworks are deprecated in their entirety in iPadOS, iOS, and visionOS. If your app still uses these frameworks, move off of them immediately. The reference documentation for each framework includes information about how to update your code.\n\nAccounts\n\nAddress Book\n\nAddress Book UI\n\nAssets Library\n\niAd\n\nNewsstand Kit\n\nNotificationCenter\n\nOpenGL ES\n\nSee Also\niOS migration and compatibility\nBringing your existing apps to visionOS\nBuild a version of your iPadOS or iOS app using the visionOS SDK, and update your code for platform differences.\nBringing your ARKit app to visionOS\nUpdate an iPadOS or iOS app that uses ARKit, and provide an equivalent experience in visionOS.\nChecking whether your existing app is compatible with visionOS\nDetermine whether your existing iOS or iPadOS app runs as is in visionOS or requires modifications to handle platform differences."
  },
  {
    "title": "Tracking preregistered images in 3D space | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/tracking-images-in-3d-space",
    "html": "Overview\n\nUse ARKit’s support for tracking 2D images to place 3D content in a space. ARKit provides updates to the image’s location as it moves relative to the person. If you supply one or more reference images in your app’s asset catalog, people can use a real-world copy of that image to place virtual 3D content in your app. For example, if you design a pack of custom playing cards and provide those assets to people in the form of a real-world deck of playing cards, they can place unique content per card in a fully immersive experience.\n\nThe following example tracks a set of images loaded from an app’s asset catalog:\n\nlet session = ARKitSession()\nlet imageInfo = ImageTrackingProvider(\n    referenceImages: ReferenceImage.loadReferenceImages(inGroupNamed: \"playingcard-photos\")\n)\n\n\nif ImageTrackingProvider.isSupported {\n    Task {\n        try await session.run([imageInfo])\n        for await update in imageInfo.anchorUpdates {\n            updateImage(update.anchor)\n        }\n    }\n}\n\n\nfunc updateImage(_ anchor: ImageAnchor) {\n    if imageAnchors[anchor.id] == nil {\n        // Add a new entity to represent this image.\n        let entity = ModelEntity(mesh: .generateSphere(radius: 0.05))\n        entityMap[anchor.id] = entity\n        rootEntity.addChild(entity)\n    }\n    \n    if anchor.isTracked {\n        entityMap[anchor.id]?.transform = Transform(matrix: anchor.originFromAnchorTransform)\n    }\n}\n\n\nIf you know the real-world dimensions of the images you’re tracking, use the physicalSize property to improve tracking accuracy. The estimatedScaleFactor property provides information about how the scale of the tracked image differs from the expected physical size you provide.\n\nSee Also\nARKit\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nIncorporating real-world surroundings in an immersive experience\nCreate an immersive experience by making your app’s content respond to the local shape of the world.\nPlacing content on detected planes\nDetect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.\nTracking specific points in world space\nRetrieve the position and orientation of anchors your app stores in ARKit."
  },
  {
    "title": "Checking whether your existing app is compatible with visionOS | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/checking-whether-your-app-is-compatible-with-visionos",
    "html": "Overview\n\nvisionOS runs compatible iPad and iPhone apps to provide continuous access to existing content right away. visionOS supports most of the same technologies as iOS, so many apps built to run on iPad or iPhone can run unmodified on visionOS devices. When a compatible app runs in visionOS, it retains the same appearance it had in iPadOS or iOS, and its content appears in a window in the person’s surroundings.\n\nIf you have an app in the iOS App Store, try downloading it and running it on Apple Vision Pro. If you run into issues, use Xcode to identify and fix them. If you built your app using the iOS SDK, Xcode 15 and later automatically adds a Designed for iPad runtime destination to your project. Use this destination to run your app and test its compatibility in visionOS. You can test most of your app’s core functionality in Simulator, but some features are available only on a device.\n\nDetermine whether a missing feature impacts your app\n\nvisionOS contains most of the same technologies as iPadOS and iOS, but there are some differences. In some cases, a feature you use in your app might not be available because of hardware differences or because of differences in how people use a visionOS device. As part of your testing, consider the impact of any missing features on your app’s overall experience. Whenever possible, work around missing features by disabling them or providing alternate ways to access the same content.\n\nThe following features aren’t available in compatible iPad and iPhone apps in visionOS. Use framework APIs to determine when the features are available.\n\nCore Motion services\n\nBarometer and magnetometer data\n\nAll location services except the standard service\n\nHealthKit data\n\nVideo or still-photo capture\n\nCamera features like auto-focus or flash\n\nRear-facing (selfie) cameras\n\nIn some cases, a framework or feature behaves differently when your app runs in visionOS. Be prepared to handle these differences when your app runs in visionOS.\n\nAirPlay. visionOS hides AirPlay sharing buttons in system interfaces, and you can’t use AirPlay features from compatible apps.\n\nApp extensions. visionOS doesn’t load App Clips, device drivers, device activity monitors, keyboard extensions, Messages app extensions, photo-editing app extensions, SMS and call-reporting extensions, or widgets.\n\nApple Watch features. visionOS ignores watchOS apps and WatchKit extensions in your iOS or iPadOS app. The Watch Connectivity framework is unavailable. Face sharing in ClockKit does nothing in visionOS.\n\nAudio and video. visionOS doesn’t support Picture in Picture or AV routing features. Check the availability of video features before using them. Be prepared for audio playback to stop automatically when your app moves to the background.\n\nClassroom features. Starting a test with Automatic Assessment Configuration reports an error.\n\nCellular telephony. Cellular services are unavailable. You can still implement Voice-over-IP (VoIP) services using CallKit and Core Telephony.\n\nDevice management. Calls to the ManagedSettings and ManagedSettingsUI frameworks do nothing.\n\nGame controllers. visionOS delivers game controller events only when someone is looking at the app. To require a game controller as an input device for your app, add the GCRequiresControllerUserInteraction key with the visionOS value to your app’s Info.plist.\n\nHandoff. visionOS doesn’t attempt to hand off user activities to other devices.\n\nHaptics. visionOS plays sounds instead of haptics.\n\nHomeKit. You can’t add accessories using a QR code from a visionOS device.\n\nMetrics. You can use MetricKit to gather on-device diagnostic logs and generate reports, but you can’t gather metrics.\n\nMulti-Touch. The system reports a maximum of two simultaneous touch inputs — one for each of the person’s hands. All system gesture recognizers handle these inputs correctly, including for zoom and rotation gestures that require multiple fingers. If you have custom gesture recognizers that require more than two points of interaction, update them to support only one or two touches in visionOS.\n\nParental controls. Calls to the FamilyControls framework do nothing.\n\nPencilKit. visionOS doesn’t report touches of type UITouch.TouchType.pencil, but it does report other types of touches.\n\nPush to Talk. Calls to the Push to Talk framework do nothing.\n\nSafari Services. Links that present an SFSafariViewController open a Safari scene instead.\n\nScreenTime. Calls to the Screen Time framework do nothing.\n\nSensor-related features. Calls to the SensorKit framework do nothing.\n\nSocial media. Calls to the Social framework do nothing.\n\nSystem interfaces. Authorization prompts, Sign in with Apple prompts, and other system-provided interfaces run asynchronously outside of your app’s process. Because these interfaces don’t run modally in your app, your app might not receive immediate responses.\n\nVehicle features. The system doesn’t call your app’s CarPlay code. Calls you make using CarKey do nothing.\n\nVision. Data scanners do nothing in VisionKit.\n\nThe version of ARKit in iOS is incompatible with the one in visionOS and visionOS can’t display windows that contain ARKit views. For information about how to bring an ARKit app to visionOS, see Bringing your ARKit app to visionOS.\n\nFor details about how to handle missing features in your code, see Making your existing app compatible with visionOS.\n\nTest specific scenarios before uploading your app\n\nThe following App Store features for iOS continue to work when your app runs in visionOS:\n\nIn-app purchases and subscriptions\n\nApp capabilities and entitlements\n\nOn-demand resources\n\nApp thinning\n\nWhen you use app thinning to optimize your app for different devices and operating systems, the App Store selects the resources and content that offer the best fit for visionOS devices. It then removes any other resources to create a streamlined installation of your app. When you export your app from Xcode 15 or later, you can test the thinning support using the visionOS virtual thinning target.\n\nWhen you’re ready to distribute your app, create an archive and export it using the Ad-Hoc or Development distribution method. During the export process, Xcode creates an appropriately signed app for you to distribute to your testers. For more information, see Distributing your app to registered devices.\n\nUpdate your app information in App Store Connect\n\nThe App Store makes compatible iPad and iPhone apps available in visionOS automatically after you sign the updated Apple Developer Program License Agreement. If you don’t want your app to run on Apple Vision Pro, change your app’s availability in App Store Connect.\n\nSelect your app in App Store Connect.\n\nNavigate to the Pricing and Availability information.\n\nDisable the “Make this app available on Apple Vision Pro” option.\n\nWhen you remove your app’s availability for Apple Vision Pro, the App Store stops making your iOS app available for visionOS. People who already downloaded your iOS app can still run it in visionOS, but they can’t download it again. This setting doesn’t affect the version of your app built using the visionOS SDK.\n\nSee Also\niOS migration and compatibility\nBringing your existing apps to visionOS\nBuild a version of your iPadOS or iOS app using the visionOS SDK, and update your code for platform differences.\nBringing your ARKit app to visionOS\nUpdate an iPadOS or iOS app that uses ARKit, and provide an equivalent experience in visionOS.\nMaking your existing app compatible with visionOS\nModify your iPadOS or iOS app to run successfully in visionOS."
  },
  {
    "title": "Positioning and sizing windows | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/positioning-and-sizing-windows",
    "html": "Overview\n\nvisionOS and macOS enable people to move and resize windows. In some cases, your app can use scene modifiers to influence a window’s initial geometry on these platforms, as well as to specify the strategy that the system employs to place minimum and maximum size limitations on a window. This kind of configuration affects both windows and volumes, which are windows with the volumetric window style.\n\nYour ability to configure window size and position is subject to the following constraints:\n\nThe system might be unable to fulfill your request. For example, if you specify a default size that’s outside the range of the window’s resizability, the system clamps the affected dimension to keep it in range.\n\nAlthough you can change the window’s content, you can’t directly manipulate window position or size after the window appears. This ensures that people have full control over their workspace.\n\nDuring state restoration, the system restores windows to their previous position and size.\n\nNote\n\nWindows in iPadOS occupy the full screen, or share the screen with another window in Slide Over or Split View. You can’t programmatically affect window geometry on that platform.\n\nSpecify initial window position\n\nIn macOS, the first time your app opens a window from a particular scene declaration, the system places the window at the center of the screen by default. For scene types that support multiple simultaneous windows, the system offsets each additional window by a small amount to avoid fully obscuring existing windows.\n\nYou can override the default placement of the first window in macOS by applying the defaultPosition(_:) scene modifier to indicate where to place the window relative to the screen bounds. For example, you can request that the system place a new window in the bottom trailing corner of the screen:\n\n@main\nstruct MyApp: App {\n    var body: some Scene {\n        WindowGroup {\n            ContentView()\n        }\n        .defaultPosition(.bottomTrailing)\n    }\n}\n\n\nThe system aligns the point in the window that corresponds to the specified UnitPoint with the point in the screen that corresponds to the same unit point. You can use a built-in unit point, like bottomTrailing in the above example, or define a custom one.\n\nImportant\n\nYou can’t use defaultPosition(_:) in visionOS. The system always places new windows directly in front of people, where they happen to be looking at the moment the window opens. This helps to make people aware of new windows.\n\nSpecify initial window size\n\nYou can indicate a default initial size for a new window that the system creates from a Scene declaration by applying one of the default size scene modifiers, like defaultSize(width:height:). For example, you can request that new windows that a WindowGroup generates occupy 600 points in the x-dimension and 400 points in the y-dimension:\n\n@main\nstruct MyApp: App {\n    var body: some Scene {\n        WindowGroup {\n            ContentView()\n        }\n        .defaultSize(CGSize(width: 600, height: 400))\n    }\n}\n\n\nThe system might clamp the actual size of the window depending on both the window’s content and resizability settings.\n\nSpecify window resizability\n\nBoth macOS and visionOS provide interface controls that enable people to resize windows, within certain limits. For example, people can use the control that appears when they look at the corner of a visionOS window to resize a window on that platform.\n\nPlay\n\nYou can specify how the system limits window resizability. The default resizability for all scenes is automatic. With that strategy, Settings windows use the contentSize strategy, where both the minimum and maximum window size match the respective minimum and maximum sizes of the content that the window contains. Other scene types use contentMinSize by default, which retains the minimum size restriction, but doesn’t limit the maximium size.\n\nYou can specify one of these resizability strategies explicitly by adding the windowResizability(_:) scene modifier to a scene. For example, people can resize windows from the following window group to between 100 and 400 points in both dimensions because the frame modifier imposes those bounds on the content view:\n\n@main\nstruct MyApp: App {\n    var body: some Scene {\n        WindowGroup {\n            ContentView()\n                .frame(\n                    minWidth: 100, maxWidth: 400,\n                    minHeight: 100, maxHeight: 400)\n        }\n        .windowResizability(.contentSize)\n    }\n}\n\n\nYou can take this even further and enforce a specific size for a window with content that has a fixed size.\n\nSpecify a volume size\n\nWhen you create a volume, which is a window with the volumetric style, you can specify the volume’s size using one of the three-dimensional default size modifiers, like defaultSize(width:height:depth:in:). The following code creates a volume that’s one meter on a side:\n\nWindowGroup(id: \"globe\") {\n    Globe()\n}\n.windowStyle(.volumetric)\n.defaultSize(width: 1, height: 1, depth: 1, in: .meters)\n\n\nThe volume maintains this size for its entire lifetime. People can’t change the size of a volume at runtime.\n\nAlthough you can specify a volume’s size in points, it’s typically better to use physical units, like the above code which specifies a size in meters. This is because the system renders a volume with fixed scaling rather than dynamic scaling, unlike a regular window, which means the volume appears more like a physical object than a user interface. For information about the different kinds of scaling, see Spatial layout.\n\nSee Also\nSwiftUI\nHello World\nUse windows, volumes, and immersive spaces to teach people about the Earth.\nPresenting windows and spaces\nOpen and close the scenes that make up your app’s interface."
  },
  {
    "title": "Presenting windows and spaces | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/presenting-windows-and-spaces",
    "html": "Overview\n\nAn app’s scenes, which contain views that people interact with, can take different forms. For example, a scene can fill a window, a tab in a window, or an entire screen. Some scenes can even place views throughout a person’s surroundings. How a scene appears depends on its type, the platform, and the context.\n\nWhen someone launches your app, SwiftUI looks for the first WindowGroup, Window, or DocumentGroup in your app declaration and opens a scene of that type, typically filling a new window or the entire screen, depending on the platform. For example, the following app running in macOS presents a window that contains a MailViewer view:\n\n@main\nstruct MailReader: App {\n    var body: some Scene {\n        WindowGroup(id: \"mail-viewer\") {\n            MailViewer()\n        }\n\n\n        Window(\"Connection Status\", id: \"connection\") {\n            ConnectionStatus()\n        }\n    }\n}\n\n\nIn visionOS, you can alternatively configure your app to open the first ImmersiveSpace that the app declares. In any case, specific platforms and configurations enable you to open more than one scene at a time. Under those conditions, you can use actions that appear in the environment to programmatically open and close the scenes in your app.\n\nCheck for multiple-scene support\n\nIf you share code among different platforms and need to find out at runtime whether the current system supports displaying multiple scenes, read the supportsMultipleWindows environment value. The following code creates a button that’s hidden unless the app supports multiple windows:\n\nstruct NewWindowButton: View {\n    @Environment(\\.supportsMultipleWindows) private var supportsMultipleWindows\n    @Environment(\\.openWindow) private var openWindow\n\n\n    var body: some View {\n        Button(\"Open New Window\") {\n            openWindow(id: \"mail-viewer\")\n        }\n        .opacity(supportsMultipleWindows ? 1 : 0)\n    }\n}\n\n\nThe value that you read depends on both the platform and how you configure your app:\n\nIn macOS, this property returns true for any app that uses the SwiftUI app lifecycle.\n\nIn iPadOS and visionOS, this property returns true for any app that uses the SwiftUI app lifecycle and has the Information Property List key UIApplicationSupportsMultipleScenes set to true, and false otherwise.\n\nFor all other platforms and configurations, the value returns false.\n\nIf your app only ever runs in one of these situations, you can assume the associated behavior and don’t need to check the value.\n\nEnable multiple simultaneous scenes\n\nYou can always present multiple scenes in macOS. To enable an iPadOS or visionOS app to simultaneously display multiple scenes — including ImmersiveSpace scenes in visionOS — add the UIApplicationSupportsMultipleScenes key with a value of true in the UIApplicationSceneManifest dictionary of your app’s Information Property List. Use the Info tab in Xcode for your app’s target to add this key:\n\nApps on other platforms can display only one scene during their lifetime.\n\nOpen windows programmatically\n\nSome platforms provide built-in controls that enable people to open instances of the window-style scenes that your app defines. For example, in macOS people can choose File > New Window from the menu bar to open a new window. SwiftUI also provides ways for you to open new windows programmatically.\n\nTo do this, get the openWindow action from the environment and call it with an identifier, a value, or both to indicate what kind of window to open and optionally what data to open it with. The following view opens a new instance of the previously defined mail viewer window when someone clicks or taps the button:\n\nstruct NewViewerButton: View {\n    @Environment(\\.openWindow) private var openWindow\n\n\n    var body: some View {\n        Button(\"New Mail Viewer\") {\n            openWindow(id: \"mail-viewer\")\n        }\n    }\n}\n\n\nWhen the action runs on a system that supports multiple scenes, SwiftUI looks for a window in the app declaration that has a matching identifier and creates a new scene of that type.\n\nImportant\n\nIf supportsMultipleWindows is false and you try to open a new window, SwiftUI ignores the action and logs a runtime error.\n\nIn addition to opening more instances of an app’s main window, as in the above example, you can also open other window types that your app’s body declares. For example, you can open an instance of the Window that displays connectivity information:\n\nButton(\"Connection Status\") {\n    openWindow(id: \"connection\")\n}\n\nOpen a space programmatically\n\nIn visionOS, you open an immersive space — a scene that you can use to present unbounded content in a person’s surroundings — in much the same way that you open a window, except that you use the openImmersiveSpace action. The action runs asynchronously, so you use the await keyword when you call it, and typically do so from inside a Task:\n\nstruct NewSpaceButton: View {\n    @Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n\n\n    var body: some View {\n        Button(\"View Orbits\") {\n            Task {\n                await openImmersiveSpace(id: \"orbits\")\n            }\n        }\n    }\n}\n\n\nBecause your app operates in a Full Space when you open an ImmersiveSpace scene, you can only open one scene of this type at a time. If you try to open a space when one is already open, the system logs a runtime error.\n\nYour app can display any number of windows together with an immersive space. However, when you open a space from your app, the system hides all windows that belong to other apps. After you dismiss your space, the other apps’ windows reappear. Similarly, the system hides your app’s windows if another app opens an immersive space.\n\nDesignate a space as your app’s main interface\n\nWhen visionOS launches an app, it opens the first window group, window, or document scene that the app’s body declares, just like on other platforms. This is true even if you first declare a space. However, if you want to open your app into an immersive space directly, specify a space as the default scene for your app by adding the UIApplicationPreferredDefaultSceneSessionRole key to your app’s information property list and setting its value to UISceneSessionRoleImmersiveSpaceApplication. In that case, visionOS opens the first space that it finds in your app declaration.\n\nImportant\n\nBe careful not to overwhelm people when starting your app with an immersive space. For design guidance, see Immersive experiences.\n\nClose windows programmatically\n\nPeople can close windows using system controls, like the close button built into the frame around a macOS window. You can also close windows programmatically. Get the dismissWindow action from the environment, and call it using the identifier of the window that you want to dismiss:\n\nprivate struct ContentView: View {\n    @Environment(\\.dismissWindow) private var dismissWindow\n\n\n    var body: some View {\n        Button(\"Done\") {\n            dismissWindow(id: \"connection\")\n        }\n    }\n}\n\n\nIn iPadOS and visionOS, the system ignores the dismiss action if you use it to close a window that’s your app’s only open scene.\n\nClose spaces programmatically\n\nTo close a space, call the dismissImmersiveSpace action. Like the corresponding open space action, the close action operates asynchronously and requires the await keyword:\n\nprivate struct ContentView: View {\n    @Environment(\\.dismissImmersiveSpace) private var dismissImmersiveSpace\n\n\n    var body: some View {\n        Button(\"Done\") {\n            Task {\n                await dismissImmersiveSpace()\n            }\n        }\n    }\n}\n\n\nYou don’t need to specify an identifier for this action, because there can only ever be one space open at a time. Like with windows, you can’t dismiss a space that’s your app’s only open scene.\n\nTransition between a window and a space\n\nBecause you can’t programmatically close the last open window or immersive space in a visionOS app, be sure to open a new scene before closing the old one. Pay particular attention to the sequencing when moving between a window and an immersive space, because the space’s open and dismiss actions run asynchronously.\n\nFor example, consider a chess game that begins by displaying a start button in a window. When someone taps the button, the app dismisses the window and opens an immersive space that presents a chess board. The following button demonstrates proper sequencing by opening the space and then closing the window:\n\nButton(\"Start\") {\n    Task {\n        await openImmersiveSpace(id: \"chessboard\")\n        dismissWindow(id: \"start\") // Runs after the space opens.\n    }\n}\n\n\nIn the above code, it’s important to include the dismissWindow action inside the task, so that it waits until the openImmersiveSpace action completes. If you put the action outside the task — either before or after — it might execute before the asynchronous open action completes, when the window is still the only open scene. In that case, the system opens the space but doesn’t close the window.\n\nSee Also\nSwiftUI\nHello World\nUse windows, volumes, and immersive spaces to teach people about the Earth.\nPositioning and sizing windows\nInfluence the initial geometry of windows that your app presents."
  },
  {
    "title": "Destination Video | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/destination-video",
    "html": "Overview\n\nDestination Video is a multiplatform video-playback app for visionOS, iOS, and tvOS. People get a familiar media-browsing experience navigating the libraryʼs content and playing videos they find interesting. The app provides a similar experience on supported platforms, but leverages unique features of visionOS to create a novel, immersive playback experience.\n\nPlay\nPlay video in an inline player\n\nWhen you select a video in the library, Destination Video presents a view that displays additional details about the item. The view presents controls to play the video and specify whether to include it in your Up Next list. In visionOS, it also displays a video poster along its leading edge. Tapping the view’s Preview button displays an inline preview of the video.\n\nPlay\n\nWhen you present an AVPlayerViewController object’s interface as a child of another view, inline controls display, for example, pause, skip, and seek. Showing standard playback controls in your app provides a familiar UI that automatically adapts its appearance to fit each platform, and is the recommended choice in most cases.\n\nDestination Video uses a simple UI for the inline player view: a single button that toggles state of playback. AVPlayerViewController doesn’t provide this controls style, but the app uses it to display the video content without controls by setting the value of its showsPlaybackControls property to false. It then overlays the custom playback controls it requires. See Destination Video’s InlinePlayerView type for details on how you can implement this.\n\nNote\n\nAVPlayerViewController only supports displaying 2D content when embedded inline.\n\nPlay video in a full-window player\n\nOne of the most exciting features of visionOS is its ability to play 3D video along with Spatial Audio, which adds a deeper level of immersion to the viewing experience. Playing 3D content in your app requires that you display AVPlayerViewController full window. When you present the player this way, the system automatically docks it into the ideal viewing position, and presents streamlined playback controls that keep the person’s focus on the content.\n\nNote\n\nIn iOS or tvOS, you typically present video in a full-screen presentation using the fullScreenCover(isPresented:onDismiss:content:) modifier. This API is available in visionOS; however, the recommended way to present the player for full-window playback is to set it as the root view of your app’s window group.\n\nDestination Video’s ContentView displays the app’s library by default. It observes changes to the player model’s presentation property, which indicates whether the app requests inline or full-window playback. When the presentation state changes to fullWindow, the view redraws the UI to display the player view in place of the library.\n\nstruct ContentView: View {\n    \n    /// The library's selection path.\n    @State private var navigationPath = [Video]()\n    /// A Boolean value that indicates whether the app is currently presenting an immersive space.\n    @State private var isPresentingSpace = false\n    /// The app's player model.\n    @Environment(PlayerModel.self) private var player\n    \n    var body: some View {\n        switch player.presentation {\n        case .fullWindow:\n            // Present the player full window and begin playback.\n            PlayerView()\n                .onAppear {\n                    player.play()\n                }\n        default:\n            // Show the app's content library by default.\n            LibraryView(path: $navigationPath, isPresentingSpace: $isPresentingSpace)\n        }\n    }\n}\n\n\nWhen someone selects the Play Video button on the detail view, the app calls the player model’s loadVideo(_: presentation:) method requesting the fullWindow presentation option.\n\nButton {\n    /// Load the media item for full-window presentation.\n    player.loadVideo(video, presentation: .fullWindow)\n} label: {\n    Label(\"Play Video\", systemImage: \"play.fill\")\n}\n\n\nAfter the player model successfully loads the video content for playback, it updates its presentation value to fullWindow, which causes the app to replace the library with PlayerView.\n\nTo dismiss the full-window player in visionOS, people tap the Back button in the player UI. To handle this action, the app’s PlayerViewControllerDelegate type defines an AVPlayerViewControllerDelegate object that handles the dismissal.\n\nfunc playerViewController(_ playerViewController: AVPlayerViewController,\n                          willEndFullScreenPresentationWithAnimationCoordinator coordinator: UIViewControllerTransitionCoordinator) {\n    // Reset the player model's state.\n    player.reset()\n}\n\n\nWhen the delegate receives this call, it clears the media from the player model and resets the presentation state back to its default value, which results in the Destination Video app redisplaying the library view.\n\nConfigure the Spatial Audio experience\n\nMedia playback apps require common configuration of their capabilities and audio session. In addition to performing the steps outlined in Configuring your app for media playback, Destination Video also adopts new AVAudioSession API to customize a person’s Spatial Audio experience.\n\nAfter the app successfully loads a video for playback, it configures the Spatial Audio experience for the current presentation. For the inline player view, it sets the experience to a small, focused sound stage where the audio originates from the location of the view. When displaying a video full window, it sets the experience to a large, fully immersive sound stage.\n\n/// Configures a person's intended Spatial Audio experience to best fit the presentation.\n/// - Parameter presentation: the requested player presentation.\nprivate func configureAudioExperience(for presentation: Presentation) {\n    #if os(xrOS)\n    do {\n        let experience: AVAudioSessionSpatialExperience\n        switch presentation {\n        case .inline:\n            // Set a small, focused sound stage when watching trailers.\n            experience = .headTracked(soundStageSize: .small, anchoringStrategy: .automatic)\n        case .fullWindow:\n            // Set a large sound stage size when viewing full window.\n            experience = .headTracked(soundStageSize: .large, anchoringStrategy: .automatic)\n        }\n        try AVAudioSession.sharedInstance().setIntendedSpatialExperience(experience)\n    } catch {\n        logger.error(\"Unable to set the intended spatial experience. \\(error.localizedDescription)\")\n    }\n    #endif\n}\n\nPresent an immersive space\n\nBuilding video playback apps for visionOS provides new opportunities to enhance the viewing experience beyond the bounds of the player window. To add a greater level of immersion, the sample presents an immersive space that displays a scene around a person as they watch the video. It defines the immersive space in the DestinationVideo app structure.\n\nstruct DestinationVideo: App {\n    \n    var body: some Scene {\n        // The app's primary window.\n        WindowGroup {\n            ContentView()\n        }\n\n\n        // Defines an immersive space to present a destination in which to watch the video.\n        ImmersiveSpace(for: Destination.self) { $destination in\n            if let destination {\n                DestinationView(destination)\n            }\n        }\n        // Set the immersion style to progressive, so the person can use the Digital Crown to dial in their experience.\n        .immersionStyle(selection: .constant(.progressive), in: .progressive)\n    }\n}\n\n\nThe immersive space presents an instance of DestinationView, which maps a texture to the inside of a sphere that it displays around a person. The app presents it using the .progressive immersion style, which lets someone change their amount of immersion by turning the Digital Crown on the device.\n\nPlay\n\nThe Destination Video app automatically presents the immersive space when a person navigates to a video’s detail view, and dismisses it when they return to the library. To monitor these events, the app observes its navigation path to determine when a navigation event occurs so it can show or dismiss the space.\n\n.onChange(of: navigationPath) {\n    Task {\n        // The selection path becomes empty when the person returns to the main library window.\n        if navigationPath.isEmpty {\n            if isSpaceOpen {\n                // Dismiss the space and return the person to their real-world space.\n                await dismissSpace()\n                isSpaceOpen = false\n            }\n        } else {\n            // The navigationPath has one video, or is empty.\n            guard let video = navigationPath.first else { fatalError() }\n            // Await the request to open the destination and set the state accordingly.\n            switch await openSpace(value: video.destination) {\n            case .opened: isSpaceOpen = true\n            default: isSpaceOpen = false\n            }\n        }\n    }\n}\n\nProvide a shared viewing experience\n\nOne of the best ways to enhance your app’s playback experience is to make that experience shareable with others. You can use the AVFoundation and the Group Activities frameworks to build SharePlay experiences that bring people together even when they can’t be in the same location.\n\nThe Destination Video app creates an experience where people can watch videos with others across devices and platforms. It defines a group activity called VideoWatchingActivity that adopts the GroupActivity protocol. When people have a FaceTime call active and they play a video in the full-window player, it becomes eligible for playback for everyone on the call.\n\nThe app’s VideoWatchingCoordinator actor manages Destination Video’s SharePlay functionality. It observes the activation of new VideoWatchingActivity sessions and when one starts, it sets the GroupSession instance on the player object’s AVPlaybackCoordinator.\n\nprivate var groupSession: GroupSession<VideoWatchingActivity>? {\n    didSet {\n        guard let groupSession else { return }\n        // Set the group session on the AVPlayer object's playback coordinator\n        // so it can synchronize playback with other devices.\n        playbackCoordinator.coordinateWithSession(groupSession)\n    }\n}\n\n\nWith the player configured to use the group session, when the app loads new videos, they become eligible to share with people in the FaceTime call.\n\nSee Also\nRelated samples\nHello World\nUse windows, volumes, and immersive spaces to teach people about the Earth.\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nDiorama\nDesign scenes for your visionOS app using Reality Composer Pro.\nRelated articles\nConfiguring your app for media playback\nConfigure apps to enable standard media playback behavior.\nAdopting the system player interface in visionOS\nProvide an optimized viewing experience for watching 3D video content.\nControlling the transport behavior of a player\nPlay, pause, and seek through a media presentation.\nMonitoring playback progress in your app\nObserve the playback of a media asset to update your app’s user-interface state.\nTrimming and exporting media in visionOS\nDisplay standard controls in your app to edit the timeline of the currently playing media.\nRelated videos\nCreate a great spatial playback experience\nDeliver video content for spatial experiences\nBuild spatial SharePlay experiences"
  },
  {
    "title": "Creating a performance plan for your visionOS app | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/creating-a-performance-plan-for-visionos-app",
    "html": "Overview\n\nPerformance tuning is an important part of the development process, regardless of platform. Performance tuning means making your app run as efficiently as possible, so it does more work in less time and with fewer system resources. Efficiency is especially important on devices that can support multiple apps in an immersive experience. Apps that consume too many resources, can push the device beyond thermal limits. When this occurs, the system takes steps to cool down to a more acceptable level. This can have a noticeable visual impact and be disorienting for the wearer.\n\nAs you start development, set aggressive goals and evaluate progress throughout the development cycle. Automate the collection of performance metrics as much as possible and look at data over time to see if performance is improving or declining. When you detect a significant decrease in performance, take immediate steps to correct it. When you start fine-tuning early in development, you have more time to make needed changes to algorithms and approaches.\n\nFor more information on performance tuning, see Improving your app’s performance.\n\nSet performance and power targets\n\nPerformance isn’t a single metric that you measure and improve. Typically, you choose several metrics and set goals for each of them. For example, consider:\n\nApp launch and load times\n\nMake sure your app launches quickly; this is your first chance to make a good impression.\n\nResponsiveness and latency\n\nYour interface needs to respond quickly to interactions, even while doing other work. Minimize the time it takes to start tasks. For example, make sure audio and video start without noticeable delays.\n\nGraphics rendering\n\nFor an immersive experience with realtime rendering, it’s important to maintain consistently high frame rates. Help maintain these rates by avoiding unnecessary changes that result in more frequent updates to the shared render server. Measure things like update rates, stalls, and hangs in both the render server and your app. Only render the content you need, and optimize the textures and other resources you use during drawing.\n\nPower Usage\n\nWhen the device begins to reach thermal limits, the system reduces CPU or GPU usage and performance degrades over time. Avoid this thermal ceiling by prioritizing and spreading out work, limiting the number of simultaneous threads your app maintains, and turning off hardware-related features like Core Location when you don’t need them.\n\nTask efficiency\n\nMake the app do as much as possible using the smallest amount of hardware resources. Minimize task-based overhead.\n\nMemory footprint and bandwidth\n\nUse as little free memory as possible. Don’t allocate or deallocate memory during critical operations, which might make your app appear slow.\n\nAfter you choose the metrics you want, set realistic goals and prioritize them, so you know which ones matter the most. Performance tuning often involves making tradeoffs between competing goals. For example, if you reduce CPU usage by caching computed data or pre-load assets to improve responsiveness, you increase your app’s memory usage. Make these kinds of tradeoffs carefully, and always measure the results of any changes to learn whether they were successful. In some cases, you might find the sacrifice isn’t worthwhile.\n\nConsider how people will use your app. If your app runs in the Shared Space, consider more conservative targets and goals for system resources. If you expect people to use your app for longer periods of time, factor this extended use into your targets and goals when choosing metrics.\n\nIdentify the code flows and user scenarios to test\n\nAfter you choose the metrics to collect, decide which portions of your app to test. Choose features that are repeatable, measurable, and reliable to test. Repeatable automated tests allow you to compare the results and know the comparisons represent the exact same task. Focus on places where your app executes code, but don’t ignore places where your app hands off data to the system and waits. If your app spends a significant amount of time waiting for information, consider eliminating the requests altogether or batching them to achieve better performance.\n\nFocus your tuning efforts on the parts of your app that people use the most, or that have the most impact on overall system performance, including:\n\nUser-facing workflows\n\nKey algorithms\n\nTask that allocate or deallocate memory\n\nBackground and network-based tasks\n\nCustom Metal shaders\n\nChoose actions that people perform frequently or that correspond to important features. For example, if your app lets someone add a new contact, test the workflow for creating the contact, editing the contact, and saving the results. Test your app with a particular feature enabled and disabled to determine whether the feature is solely responsible for any performance impacts. Choose lightweight workflows such as how your app performs at idle time, and also heavyweight workflows, for example, ones that involve user interactions and your app’s responses. For launch times, gather metrics for both hot and cold launches — that is, when the app is already resident in memory and when it is not.\n\nConsider thermal and environmental factors\n\nConsider how environmental factors impact your app. The characteristics of your physical environment can affect system load and thermals of the device. Consider the effect that ambient room temperature, the presence of other people, and the number and type of real-world objects can have on the your app‘s algorithms. Try to test in different settings to get an idea of whether you need to optimize for these scenarios or not.\n\nUse Xcode’s thermal inducers to mimic the device hitting its thermal limits and consider how your app responds to fair, serious, and critical thermal notifications. You might need to have different performance goals when under thermal pressure, and prioritize optimizing for power or find ways to dynamically lower your app‘s complexity in response to thermal pressure to give a smoother experience, even if latency is a bit higher.\n\nChoose tools to collect performance data\n\nThere are many tools and APIs you can use to collect performance-related data for your visionOS app. Use a variety of tools to make sure you have the data you need:\n\nDebug gauges\n\nMonitor the CPU, memory, disk and network gauges in the Debug navigator to track system resources utilization.\n\nInstruments\n\nProfile your app to gather performance data on most metrics. Instruments lets you profile your app’s code execution, find memory leaks, track memory allocations, analyze file-system or graphics performance, SwiftUI performance, and much more. Use the RealityKit Trace template to monitoring and investigate render server stalls and bottlenecks on visionOS.\n\nXCTest\n\nUse XCTest APIs to collect performance data.\n\nMetricKit\n\nUse MetricKit to gather on-device app diagnostics and generate reports.\n\nOrganizer\n\nReview diagnostic logs for hangs, disk and energy usage, and crashes in the Xcode Organizer.\n\nReality Composer Pro\n\nReview statistics on the contents of your RealityKit scenes. Use this information to optimize your 3D models and textures.\n\nSignposts\n\nAdd signposts to your code to generate timing information you can view in Instruments. For more information, see Recording performance data.\n\nLog messages\n\nInclude log messages to report significant events and relevant data for those events. For more information, see Generating log messages from your code.\n\nTestFlight\n\nGet feedback from testers about their experiences with beta versions of your app. Fill out the Test Information page for your beta version, and request that testers provide feedback about the performance of your app.\n\nProfile on a physical device\n\nIn general, profile and analyze performance on a physical device rather than in Simulator. Even if something works well in Simulator, it might not perform as well on devices for all use cases. Simulator doesn’t support some hardware features and APIs. There are differences in the rendering pipeline for Simulator running on macOS, so rendering performance characteristics will be different. Other pipelines such as input delivery and audio or video playback are also different. There are, however, some insights you can gain profiling in Simulator, such as CPU stalls, that help you spot areas to investigate and address.\n\nBuild automated test cases and run them regularly\n\nXcode comes with tools to help you automate the collection of performance data:\n\nUse the XCTest framework to build test cases to collect performance metrics. XCTest lets you gather several different metrics, including the time it takes to perform operations, the amount of CPU activity that occurs during the test, details about memory or storage use, and more.\n\nUse Instruments to collect metrics for specific interactions with your app. Record those interactions and play them back later to collect a new set of metrics.\n\nWrite custom scripts to gather performance-related data using system command-line tools. Integrate these scripts into your project’s build process to automate their execution.\n\nConfigure Xcode to run test cases each time you build your app, or create a separate target to run test cases or custom scripts on demand. Integrate your performance tests into your Xcode Cloud workflows, or your own custom continuous integration solution.\n\nNote\n\nCollect performance data using a production version of your app to obtain more accurate results. Debug builds contain additional code to support debugging operations and logging. You can collect data from debug builds too, but keep those metrics separate from production-build metrics.\n\nFor information about how to write test cases for your app, see Testing your apps in Xcode. For information about how to automate testing with Xcode Cloud, see Xcode Cloud.\n\nSee Also\nXcode and Instruments\nDiagnosing and resolving bugs in your running app\nInspect your app to isolate bugs, locate crashes, identify excess system-resource usage, visualize memory bugs, and investigate problems in its appearance.\nDiagnosing issues in the appearance of a running app\nInspect your running app to investigate issues in the appearance and placement of the content it displays.\nAnalyzing the performance of your visionOS app\nUse the RealityKit Trace template in Instruments to evaluate and improve the performance of your visionOS app.\nConfiguring your app icon\nAdd app icon variations to represent your app in places such as Settings, search results, and the App Store."
  },
  {
    "title": "Tracking specific points in world space | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/tracking-points-in-world-space",
    "html": "Overview\n\nUse world anchors along with an ARKit session’s WorldTrackingProvider to track points of interest in the world over time, as a person moves while wearing the device, and across device usage sessions. For example, someone might place a 3D object in a specific position on their desk and expect it to come back the next time they use the device.\n\nPlay\n\nARKit keeps track of a unique identifier for each world anchor your app creates and automatically places those anchors back in the space when the person returns to your app in the same location. A world tracking provider also provides the position of the device the person is wearing.\n\nStart an ARKit session with world tracking\n\nUse an ARKitSession configured for world tracking to start receiving updates on the world anchors your app places. The following shows updates to world anchors your app previously registered using the addAnchor(_:) method:\n\nlet session = ARKitSession()\nlet worldInfo = WorldTrackingProvider()\n\n\nTask {\n    try await session.run([worldInfo])\n    \n    for await update in worldInfo.anchorUpdates {\n        switch update.event {\n        case .added, .updated:\n            // Update the app's understanding of this world anchor.\n            print(\"Anchor position updated.\")\n        case .removed:\n            // Remove content related to this anchor.\n            print(\"Anchor position now unknown.\")\n    }\n}\n\n\nImportant\n\nIf a person repositions the current space — for example, by holding down the Digital Crown — world anchor updates begin updating their position relative to the new world origin. For example, a world anchor placed on a table still reports information about the table’s position, but those positions are relative to the updated world origin.\n\nCreate and add world anchors\n\nYou can create world anchors for any point of interest in your app’s world coordinate system once you’ve started a world tracking ARKit session. For example, you might track that a person placed an item at a particular offset from a desk in their space:\n\nlet anchor = WorldAnchor(originFromAnchorTransform: deskPlane.originFromAnchorTransform + offset)\ntry await worldInfo.addAnchor(anchor)\n\n\nOnce you add a world anchor to your app’s tracking provider using the addAnchor(_:) method, the anchorUpdates sequence in the current session and future runs of your app provides updates to the current position of that new world anchor.\n\nPersist world anchors across sessions\n\nThe only information ARKit persists about the world anchors in your app is their UUID — a WorldAnchor instance’s id property — and pose in a particular space. It’s your app’s responsibility to persist additional information, such as the meaning of each anchor. For example, you might save local data about a custom 3D lamp model that a person placed on their desk.\n\nAs a person moves from town-to-town or room-to-room, your app won’t receive all of the world anchor updates from each place someone used your app. Instead, the anchorUpdates sequence only provides world anchors for nearby objects.\n\nTrack the device position in the world\n\nUse the Compositor Services framework and the WorldTrackingProvider class’s queryDeviceAnchor(atTimestamp:) method to get low-latency information about the current and future-predicted pose of the person’s device in world space. For more information, see Drawing fully immersive content using Metal.\n\nSee Also\nARKit\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nIncorporating real-world surroundings in an immersive experience\nCreate an immersive experience by making your app’s content respond to the local shape of the world.\nPlacing content on detected planes\nDetect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.\nTracking preregistered images in 3D space\nPlace content based on the current position of a known image in a person’s surroundings."
  },
  {
    "title": "Swift Splash | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/swift-splash",
    "html": "Overview\n\nApple Vision Pro’s ability to combine virtual content seamlessly with the real world allows for many kinds of interactive virtual experiences. Swift Splash leverages RealityKit and Reality Composer Pro to create a virtual water slide by combining modular slide pieces. When the builder finishes their ride, they can release an adventurous goldfish to try it out.\n\nPlay\n\nSwift Splash uses multiple Reality Composer Scenes to create prepackaged entity hierarchies that represent each of the slide pieces the player connects to construct their ride. It demonstrates how to hide and reveal sections of the entity hierarchy based on the current state of the app. For example, each slide piece contains an animated fish entity that’s hidden until the ride runs and the fish arrives at that particular piece. While Swift Splash is a fun, game-like experience, the core idea of assembling virtual objects out of predefined parts can also be used as the basis for a productivity or creation app.\n\nSwift Splash scenes include Shader Graph materials built in Reality Composer Pro to change the appearance of the ride at runtime. Each piece can be configured to display in one of three materials: metal, wood, or plastic. Other Shader Graph materials create special effects, such as the movement of the water and the flashing lights on the start and end pieces. Even particle effects are included in some of these prepackaged entities, such as the fireworks that play when the goldfish crosses the finish line.\n\nPlay\nPlay\nBuild slide pieces in Reality Composer Pro\n\nSlide pieces are the building blocks of Swift Splash. The Reality Composer project contains a separate scene for each one. In addition to the 3D models that make up the slide piece, each scene contains a number of other entities the app uses to animate and place the slide piece.\n\nIn the hierarchy viewer on the left side of the screenshot above, there are two transform entities called connect_in and connect_out. These transforms mark the points where the slide piece connects to the next or previous piece. Swift Splash uses these transforms to place new pieces at the end of the existing slide, as well as to snap pieces to other slide pieces when you manually move them near each other.\n\nSlide pieces demonstrate the two primary mechanisms Swift Splash uses to find entities at runtime. For some entities, such as connect_in, Swift Splash uses a naming convention and retrieves the entities by name or suffix when it needs to use them. In other cases, such as when names aren’t unique or the retrieving code needs configuration values, Swift Splash uses a custom component to mark and retrieve entities.\n\nFor example, animated entities that appear when the ride runs contain a component called RideAnimationComponent. The app uses this component to determine if the entity is an animation that plays while the ride is running. The component also stores additional state the app needs to implement the ride animation, such as a property called duration that specifies when to start the animations on the next connected slide piece.\n\nRideAnimationComponent also includes a property called isPersistent. Persistent ride animations stay visible at all times but only animate when the ride is running, such as the animated door on the start piece. Nonpersistent ride animations, such as the fish swimming through a slide piece, display only while the ride is running and the fish swims through that particular piece.\n\nAvoid duplicate materials with material references\n\nMany of Swift Splash’s slide pieces use the same materials. For example, the shader graph material that changes pieces from metal to wood to plastic is shared by all but one of the slide pieces. To avoid having duplicate copies of each material, Swift Splash leverages USD material references to share materials between multiple entities in multiple scenes.\n\nThe Reality Composer Pro project contains a separate scene for each shared material, containing only that one material. Other track pieces create references to that material. If you change the original material, it affects all of the entities that reference it. For example, a scene called M_RainbowLights.usda contains the material M_RainbowLights, and both StartPiece.usda and EndPiece.usda reference that material.\n\nParallelize the asset load\n\nTo maximize load speed and make the most efficient use of available compute resources, Swift Splash parallelizes loading scenes from the Reality Composer project using a TaskGroup. The app creates a separate Task for each of the scenes it needs to load.\n\nawait withTaskGroup(of: LoadResult.self) { taskGroup in   \n    // Load the regular slide pieces and ride animations.\n    logger.info(\"Loading slide pieces.\")\n    for piece in pieces {\n        taskGroup.addTask {\n            do {\n                guard let pieceEntity = try await self.loadFromRCPro(named: piece.key.rawValue, \n                                                       fromSceneNamed: piece.sceneName) else {\n                    fatalError(\"Attempted to load piece entity \\(piece.name) but failed.\")\n                }\n                return LoadResult(entity: pieceEntity, key: piece.key.rawValue)\n            } catch {\n                fatalError(\"Attempted to load \\(piece.name) but failed: \\(error.localizedDescription)\")\n            }\n        }\n    }\n    // Continue adding asset load jobs.\n    // ...\n}\n\n\nThe app then uses an async iterator to wait for and receive the results.\n\nfor await result in taskGroup {\n    if let pieceKey = pieces.filter({ piece in\n        piece.key.rawValue == result.key\n    }).first {\n        self.add(template: result.entity, for: pieceKey.key)\n        setupConnectible(entity: result.entity)\n        result.entity.generateCollisionShapes(recursive: true)\n        result.entity.setUpAnimationVisibility()\n    }\n    // ...\n}\n\n\nFor more information on task groups, see Concurrency in The Swift Programming Language.\n\nEach of these loaded pieces acts as a template. When the player adds a new piece of that type, the app clones the piece loaded from Reality Composer Pro and adds the clone to the scene.\n\nSpecify sort ordering for transparent entities\n\nWhen multiple entities have more than one overlapping, nonopaque material, RealityKit’s default depth-sorting can cause it to draw those entities in the wrong order. As a result, some entities may not be visible from certain angles or in certain positions relative to other transparent entities. The default depth sorting is based on the center of the entity’s bounding box, which may result in the incorrect drawing order when there are multiple overlapping materials with any amount of transparency. You can see an example of this by looking at the start piece in Reality Composer Pro, or by watching the video below.\n\nPlay\n\nThe following video demonstrates the problem. If the three boxes are the bounding boxes for three different transparent entities, and the small spheres are the box centers, the sphere that’s closest to the camera changes as the camera moves around the boxes, which changes the order that RealityKit’s default depth sorting algorithm draws them.\n\nPlay\nPlay\n\nSwift Splash assigns a ModelSortGroupComponent to each of the transparent entities to manually specify the relative depth sorting. To fix the transparency issues in the start piece in the video above, Swift Splash instructs RealityKit to draw the opaque parts of the fish first, its transparent goggles second, the water third, the glass globe fourth, and the selection glow shell last. Swift Splash does this by assigning a ModelSortGroupComponent to each of the overlapping entities using the same ModelSortGroup, but with a different order specified.\n\nfileprivate func setEntityDrawOrder(_ entity: Entity, _ sortOrder: Int32, _ sortGroup: ModelSortGroup) {\n    entity.forEachDescendant(withComponent: ModelComponent.self) { modelEntity, model in\n        logger.info(\"Setting sort order of \\(sortOrder) of \\(entity.name), child entity: \\(modelEntity.name)\")\n        let component = ModelSortGroupComponent(group: sortGroup, order: sortOrder)\n        modelEntity.components.set(component)\n    }\n}\n\n\n/// Manually specify sort ordering for the transparent start piece meshes.\nfunc handleStartPieceTransparency(_ startPiece: Entity) {\n    let group = ModelSortGroup()\n    \n    // Opaque fish parts.\n    if let entity = startPiece.findEntity(named: fishIdleAnimModelName) {\n        setEntityDrawOrder(entity, 1, group)\n    }\n    if let entity = startPiece.findEntity(named: fishRideAnimModelName) {\n        setEntityDrawOrder(entity, 2, group)\n    }\n    \n    // Transparent fish parts.\n    if let entity = startPiece.findEntity(named: fishGlassIdleAnimModelName) {\n        setEntityDrawOrder(entity, 3, group)\n    }\n    if let entity = startPiece.findEntity(named: fishGlassRideAnimModelName) {\n        setEntityDrawOrder(entity, 4, group)\n    }\n    \n    // Water.\n    if let entity = startPiece.findEntity(named: sortOrderWaterName) {\n        setEntityDrawOrder(entity, 5, group)\n    }\n    \n    // Glass globe.\n    if let entity = startPiece.findEntity(named: sortOrderGlassGlobeName) {\n        setEntityDrawOrder(entity, 6, group)\n    }\n    \n    // Selection glow.\n    if let entity = startPiece.findEntity(named: startGlowName) {\n        setEntityDrawOrder(entity, 7, group)\n    }\n}\n\nTraverse connected track pieces\n\nThe root entity for all of the individual slide pieces has a ConnectableComponent. This custom component marks the entity as one that can be connected or snapped to other connectable entities. At runtime, the app adds a ConnectableStateComponent to each slide piece it adds. The component stores state information for the track piece that doesn’t need to be edited in Reality Composer Pro. Among the state information that this component stores is a reference to the next and previous piece.\n\nPlay\n\nTo iterate through the entire ride, ignoring any disconnected pieces, the app gets a reference to the start piece and then iterates until nextPiece is nil. This iteration, similar to iterating a linked list, repeats many times throughout the app. One example is the function that calculates the duration of the built ride by iterating through the individual pieces and adding up the duration of their animations.\n\n/// Calculates the duration of the built ride by summing up the individual durations.\npublic func calculateRideDuration() {\n    guard let startPiece = startPiece else { fatalError(\"No start piece found.\") }\n    var nextPiece: Entity? = startPiece\n    var duration: TimeInterval = 0\n    while nextPiece != nil {\n        // Some pieces have more than one ride animation. Use the longest one to calculate duration.\n        var longestAnimation: TimeInterval = 0\n        nextPiece?.forEachDescendant(withComponent: RideAnimationComponent.self) { entity, component in\n            longestAnimation = max(component.duration, longestAnimation)\n        }\n        duration += longestAnimation\n        nextPiece = nextPiece?.connectableStateComponent?.nextPiece\n    }\n    // Remove the part of the animation after the goal post.\n    rideDuration = duration  / animationSpeedMultiplier + 1.0\n}\n\nInteract with the ride\n\nTo build and edit the ride, players interact with Swift Splash in two different ways. They interact with SwiftUI windows to perform certain tasks, such as adding a new piece or deleting an existing piece of the ride. They also manipulate slide pieces using standard visionOS gestures, including taps, double taps, drags, and rotates. The player taps on a piece to select or deselect it. When a player double taps a piece, they select that piece without deselecting any other selected pieces. When someone drags a piece, it moves around the immsersive space, snapping together with other pieces if placed near one. A two-finger rotate gesture spins the selected track piece or pieces on the Z-axis.\n\nPlay\nAdd a ride piece\nPlay\nDelete a ride piece\nPlay\nRotate a ride piece\nPlay\nDrag a ride piece\n\nSwift Splash handles all of these interactions using standard SwiftUI gestures targeted to an entity. To support any of these gestures at any time, the app declares them using SimultaneousGesture. The code for all of the gestures are contained in TrackBuildingView, which controls the app’s immersive space. Here’s how the app defines the rotation gesture:\n\n.simultaneousGesture(\n    RotateGesture()\n        .targetedToAnyEntity()\n        .onChanged({ value in\n            guard appState.phase == .buildingTrack || appState.phase == .placingStartPiece || appState.phase == .draggingStartPiece else { return }\n            handleRotationChanged(value)\n        })\n        .onEnded({ value in\n            guard appState.phase == .buildingTrack || appState.phase == .placingStartPiece || appState.phase == .draggingStartPiece else { return }\n            handleRotationChanged(value, isEnded: true)\n        })\n)\n\n\nBecause multiple tap gestures on the same RealityView execute with a different number of taps, multiple gestures may be called at once. If a player double taps an entity, for example, both the single tap and the double tap gesture code get called, and the app has to determine which one to execute. Swift Splash makes this determination by using a Boolean state variable. If a player single taps, it sets that variable — called shouldSingleTap — to true. Then it waits for a period of time before executing the rest of its code. If shouldSingleTap gets set to false while it’s waiting, the code doesn’t execute. When SwiftSplash detects a double tap gesture, it sets shouldSingleTap to false, preventing the single-tap code from firing when it executes the double-tap code.\n\n.simultaneousGesture(\n    TapGesture()\n        .targetedToAnyEntity()\n        .onEnded({ value in\n            guard appState.phase == .buildingTrack else { return }\n            Task {\n                shouldSingleTap = true\n                try? await Task.sleep(for: .seconds(doubleTapTolerance))\n                if shouldSingleTap {\n\nSee Also\nRelated samples\nHello World\nUse windows, volumes, and immersive spaces to teach people about the Earth.\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nDestination Video\nLeverage 3D video and Spatial Audio to deliver an immersive experience.\nDiorama\nDesign scenes for your visionOS app using Reality Composer Pro.\nRelated articles\nAdding 3D content to your app\nAdd depth and dimension to your visionOS app and discover how to incorporate your app’s content into a person’s surroundings.\nUnderstanding RealityKit’s modular architecture\nLearn how everything fits together in RealityKit.\nDesigning RealityKit content with Reality Composer Pro\nDesign RealityKit scenes for your visionOS app.\nImplementing systems for entities in a scene\nApply behaviors and physical effects to the objects and characters in a RealityKit scene with the Entity Component System (ECS).\nCreating USD files for Apple devices\nGenerate 3D assets that render as expected.\nRelated videos\nMeet Reality Composer Pro\nExplore materials in Reality Composer Pro\nWork with Reality Composer Pro content in Xcode\nBuild spatial experiences with RealityKit\nEnhance your spatial computing app with RealityKit\nBuild great games for spatial computing"
  },
  {
    "title": "Incorporating real-world surroundings in an immersive experience | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/incorporating-real-world-surroundings-in-an-immersive-experience",
    "html": "Overview\n\nScene reconstruction helps bridge the gap between the rendered 3D content in your app and the person’s surroundings. Use scene reconstruction in ARKit to give your app an idea of the shape of the person’s surroundings and to bring your app experience into their world. Immersive experiences — those that use the mixed space style — are best positioned to incorporate this kind of contextual information: scene reconstruction is only available in spaces and isn’t as relevant for the full space style.\n\nIn addition to providing a 3D mesh of the shape of different nearby objects, ARKit gives a classification to each mesh face it detects. For example, it might classify a face of a mesh as being part of an appliance, a piece of furniture, or structural information about the room like the position of walls and floors. The following video shows virtual cubes colliding with the scene reconstruction mesh, which makes the cubes appear to land on a table:\n\nPlay\nConfigure a scene reconstruction session\n\nScene reconstruction requires the ARKitSession.AuthorizationType.worldSensing authorization type and corresponding usage description that you supply in your app’s Info.plist file. The following starts a session and processes updates as ARKit refines its reconstruction of the person’s surroundings:\n\nRealityView { content in\n    content.add(model.setupContentEntity())\n}\n.task {\n    do {\n        if model.dataProvidersAreSupported && model.isReadyToRun {\n            try await model.session.run([model.sceneReconstruction, model.handTracking])\n        } else {\n            await dismissImmersiveSpace()\n        }\n    } catch {\n        logger.error(\"Failed to start session: \\(error)\")\n        await dismissImmersiveSpace()\n        openWindow(id: \"error\")\n    }\n}\n.task {\n    await model.processHandUpdates()\n}\n.task {\n    await model.monitorSessionEvents()\n}\n.task(priority: .low) {\n    await model.processReconstructionUpdates()\n}\n.gesture(SpatialTapGesture().targetedToAnyEntity().onEnded { value in\n    let location3D = value.convert(value.location3D, from: .global, to: .scene)\n    model.addCube(tapLocation: location3D)\n})\n\nAdd real-world interactivity using collision components\n\nYou can make rendered 3D content more lifelike by having it appear to interact physically with objects in the person’s surroundings, like furniture and floors. Use RealityKit’s collision components and physics support to provide these interactions in your app. The generateStaticMesh(from:) method bridges between scene reconstruction and RealityKit’s physics simulation.\n\nWarning\n\nBe mindful of how much content you include in immersive scenes that use the mixed style. Content that fills a significant portion of the screen, even if that content is partially transparent, can prevent the person from seeing potential hazards in their surroundings. If you want to immerse the person in your content, configure your space with the full style. For more information, see Creating fully immersive experiences in your app.\n\nUse low-priority tasks to generate meshes, because generating them is a computationally expensive operation. The following creates a mesh entity with collision shapes using scene reconstruction:\n\nfunc processReconstructionUpdates() async {\n    for await update in sceneReconstruction.anchorUpdates {\n        let meshAnchor = update.anchor\n\n\n        guard let shape = try? await ShapeResource.generateStaticMesh(from: meshAnchor) else { continue }\n        switch update.event {\n        case .added:\n            let entity = ModelEntity()\n            entity.transform = Transform(matrix: meshAnchor.originFromAnchorTransform)\n            entity.collision = CollisionComponent(shapes: [shape], isStatic: true)\n            entity.components.set(InputTargetComponent())\n            \n            entity.physicsBody = PhysicsBodyComponent(mode: .static)\n            \n            meshEntities[meshAnchor.id] = entity\n            contentEntity.addChild(entity)\n        case .updated:\n            guard let entity = meshEntities[meshAnchor.id] else { continue }\n            entity.transform = Transform(matrix: meshAnchor.originFromAnchorTransform)\n            entity.collision?.shapes = [shape]\n        case .removed:\n            meshEntities[meshAnchor.id]?.removeFromParent()\n            meshEntities.removeValue(forKey: meshAnchor.id)\n        }\n    }\n}\n\n\nNote\n\nScene reconstruction meshes only support the PhysicsBodyMode.static physics body component mode.\n\nEach object in the scene reconstruction mesh updates its originFromAnchorTransform information independently and requires a separate static mesh because ARKit subdivides its representation of the world into multiple, distinct sections.\n\nDisplay scene reconstruction meshes during debugging\n\nPeople using an app that leverages scene reconstruction typically don’t need to see a visual rendering of the scene reconstruction mesh. The system already shows passthrough video in an immersive experience. However, temporarily displaying the scene reconstruction mesh can help while you’re developing and debugging your app. In Xcode’s debugging toolbar, click the Enable Visualizations button and select Collision Shapes. Because each element of the scene reconstruction mesh has a collision component, the details of the mesh appear in the debug visualization. For more information, see Diagnosing issues in the appearance of a running app.\n\nSee Also\nARKit\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nPlacing content on detected planes\nDetect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.\nTracking specific points in world space\nRetrieve the position and orientation of anchors your app stores in ARKit.\nTracking preregistered images in 3D space\nPlace content based on the current position of a known image in a person’s surroundings."
  },
  {
    "title": "Happy Beam | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/happybeam",
    "html": "Overview\n\nIn visionOS, you can create fun, dynamic games and apps using several different frameworks to create new kinds of spatial experiences: RealityKit, ARKit, SwiftUI, and Group Activities. This sample introduces Happy Beam, a game where you and your friends can hop on a FaceTime call and play together.\n\nYou’ll learn the mechanics of the game where grumpy clouds float around in the space, and people play by making a heart shape with their hands to project a beam. People aim the beam at the clouds to cheer them up, and a score counter keeps track of how well each player does cheering up the clouds.\n\nPlay\nDesign the game interface in SwiftUI\n\nMost apps in visionOS launch as a window that opens different scene types depending on the needs of the app.\n\nHere you see how Happy Beam presents a fun interface to people by using several SwiftUI views that display a welcome screen, a coaching screen that gives instructions, a scoreboard, and a game-ending screen.\n\nWelcome window\nInstructions\nScoreboard\nEnding window\n\nThe following shows you the primary view in the app that displays each phase of gameplay:\n\nstruct HappyBeam: View {\n    @Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n    @Environment(GameModel.self) var gameModel\n    \n    @State private var session: GroupSession<HeartProjection>? = nil\n    @State private var timer = Timer.publish(every: 1, on: .main, in: .common).autoconnect()\n    @State private var subscriptions = Set<AnyCancellable>()\n    \n    var body: some View {\n        let gameState = GameScreen.from(state: gameModel)\n        VStack {\n            Spacer()\n            Group {\n                switch gameState {\n                case .start:\n                    Start()\n                case .soloPlay:\n                    SoloPlay()\n                case .lobby:\n                    Lobby()\n                case .soloScore:\n                    SoloScore()\n                case .multiPlay:\n                    MultiPlay()\n                case .multiScore:\n                    MultiScore()\n                }\n            }\n            .glassBackgroundEffect(\n                in: RoundedRectangle(\n                    cornerRadius: 32,\n                    style: .continuous\n                )\n            )\n        }\n    }\n}\n\n\nWhen 3D content starts to appear, the game opens an immersive space to present content outside of the main window and in a person’s surroundings.\n\n@main\nstruct HappyBeamApp: App {\n    @State private var gameModel = GameModel()\n    @State private var immersionState: ImmersionStyle = .mixed\n    \n    var body: some SwiftUI.Scene {\n        WindowGroup(\"HappyBeam\", id: \"happyBeamApp\") {\n            HappyBeam()\n                .environmentObject(gameModel)\n        }\n        .windowStyle(.plain)\n        \n        ImmersiveSpace(id: \"happyBeam\") {\n            HappyBeamSpace(gestureModel: HeartGestureModelContainer.heartGestureModel)\n                .environmentObject(gameModel)\n        }\n        .immersionStyle(selection: $immersionState, in: .mixed)\n    }\n}\n\n\nThe HappyBeam container view declares a dependency on openImmersiveSpace:\n\n@Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n\n\nIt later uses that dependency to open the space from the app’s declaration when it’s time to start showing 3D content:\n\nif gameModel.countDown == 0 {\n    Task {\n        await openImmersiveSpace(id: \"happyBeam\")\n    }\n}\n\nDetect a heart gesture with ARKit\n\nThe Happy Beam app recognizes the central heart-shaped hands gesture using ARKit’s support for 3D hand tracking in visionOS. Using hand tracking requires a running session and authorization from the wearer. It uses the NSHandsTrackingUsageDescription user info key to explain to players why the app requests permission for hand tracking.\n\nTask {\n    do {\n        try await session.run([handTrackingProvider])\n    } catch {\n        print(\"ARKitSession error:\", error)\n    }\n}\n\n\nHand-tracking data isn’t available when your app is only displaying a window or volume. Instead, it’s available when you present an immersive space, as in the previous example.\n\nYou can detect gestures using ARKit data with a level of accuracy that depends on your use case and intended experience. For example, Happy Beam could require strict positioning of finger joints to closely resemble a heart shape. Instead, however, it prompts people to make a heart shape and uses a heuristic to indicate when the gesture is close enough.\n\nThe following checks whether a person’s thumbs and index fingers are almost touching:\n\n// Get the position of all joints in world coordinates.\nlet originFromLeftHandThumbKnuckleTransform = matrix_multiply(\n    leftHandAnchor.originFromAnchorTransform, leftHandThumbKnuckle.anchorFromJointTransform\n).columns.3.xyz\nlet originFromLeftHandThumbTipTransform = matrix_multiply(\n    leftHandAnchor.originFromAnchorTransform, leftHandThumbTipPosition.anchorFromJointTransform\n).columns.3.xyz\nlet originFromLeftHandIndexFingerTipTransform = matrix_multiply(\n    leftHandAnchor.originFromAnchorTransform, leftHandIndexFingerTip.anchorFromJointTransform\n).columns.3.xyz\nlet originFromRightHandThumbKnuckleTransform = matrix_multiply(\n    rightHandAnchor.originFromAnchorTransform, rightHandThumbKnuckle.anchorFromJointTransform\n).columns.3.xyz\nlet originFromRightHandThumbTipTransform = matrix_multiply(\n    rightHandAnchor.originFromAnchorTransform, rightHandThumbTipPosition.anchorFromJointTransform\n).columns.3.xyz\nlet originFromRightHandIndexFingerTipTransform = matrix_multiply(\n    rightHandAnchor.originFromAnchorTransform, rightHandIndexFingerTip.anchorFromJointTransform\n).columns.3.xyz\n\n\nlet indexFingersDistance = distance(originFromLeftHandIndexFingerTipTransform, originFromRightHandIndexFingerTipTransform)\nlet thumbsDistance = distance(originFromLeftHandThumbTipTransform, originFromRightHandThumbTipTransform)\n\n\n// Heart gesture detection is true when the distance between the index finger tips centers\n// and the distance between the thumb tip centers is each less than four centimeters.\nlet isHeartShapeGesture = indexFingersDistance < 0.04 && thumbsDistance < 0.04\nif !isHeartShapeGesture {\n    return nil\n}\n\n\n// Compute a position in the middle of the heart gesture.\nlet halfway = (originFromRightHandIndexFingerTipTransform - originFromLeftHandThumbTipTransform) / 2\nlet heartMidpoint = originFromRightHandIndexFingerTipTransform - halfway\n\n\n// Compute the vector from left thumb knuckle to right thumb knuckle and normalize (X axis).\nlet xAxis = normalize(originFromRightHandThumbKnuckleTransform - originFromLeftHandThumbKnuckleTransform)\n\n\n// Compute the vector from right thumb tip to right index finger tip and normalize (Y axis).\nlet yAxis = normalize(originFromRightHandIndexFingerTipTransform - originFromRightHandThumbTipTransform)\n\n\nlet zAxis = normalize(cross(xAxis, yAxis))\n\n\n// Create the final transform for the heart gesture from the three axes and midpoint vector.\nlet heartMidpointWorldTransform = simd_matrix(\n    SIMD4(xAxis.x, xAxis.y, xAxis.z, 0),\n    SIMD4(yAxis.x, yAxis.y, yAxis.z, 0),\n    SIMD4(zAxis.x, zAxis.y, zAxis.z, 0),\n    SIMD4(heartMidpoint.x, heartMidpoint.y, heartMidpoint.z, 1)\n)\nreturn heartMidpointWorldTransform\n\nSupport several kinds of input\n\nTo support accessibility features and general user preferences, include multiple kinds of input in an app that uses hand tracking as one form of input.\n\nHappy Beam supports several kinds of input:\n\nInteractive hands input from ARKit with the custom heart gesture.\n\nDrag gesture input to rotate the stationary beam on its platform.\n\nAccessibility components from RealityKit to support custom actions for cheering up the clouds.\n\nGame Controller support to make control over the beam more interactive from Switch Control.\n\nDisplay 3D content with RealityKit\n\nThe 3D content in the app comes in the form of assets that you can export from Reality Composer Pro. You place each asset in the RealityView that represents your immersive space.\n\nThe following shows how Happy Beam generates clouds when the game starts, as well as materials for the floor-based beam projector. Because the game uses collision detection to keep score — the beam cheers up grumpy clouds when they collide — you make collision shapes for each model that might be involved.\n\n@MainActor\nfunc placeCloud(start: Point3D, end: Point3D, speed: Double) async throws -> Entity {\n    let cloud = await loadFromRealityComposerPro(\n        named: BundleAssets.cloudEntity,\n        fromSceneNamed: BundleAssets.cloudScene\n    )!\n        .clone(recursive: true)\n    \n    cloud.generateCollisionShapes(recursive: true)\n    cloud.components[PhysicsBodyComponent.self] = PhysicsBodyComponent()\n    \n    var accessibilityComponent = AccessibilityComponent()\n    accessibilityComponent.label = \"Cloud\"\n    accessibilityComponent.value = \"Grumpy\"\n    accessibilityComponent.isAccessibilityElement = true\n    accessibilityComponent.traits = [.button, .playsSound]\n    accessibilityComponent.systemActions = [.activate]\n    cloud.components[AccessibilityComponent.self] = accessibilityComponent\n    \n    let animation = cloudMovementAnimations[cloudPathsIndex]\n    \n    cloud.playAnimation(animation, transitionDuration: 1.0, startsPaused: false)\n    cloudAnimate(cloud, kind: .sadBlink, shouldRepeat: false)\n    spaceOrigin.addChild(cloud)\n    \n    return cloud\n}\n\nAdd SharePlay support for multiplayer gaming experiences\n\nYou use the Group Activities framework in visionOS to support SharePlay during a FaceTime call. Happy Beam uses Group Activities to sync the score, active players list, and the position of each player’s projected beam.\n\nNote\n\nDevelopers using the Apple Vision Pro developer kit can test spatial SharePlay experiences on-device by installing the Persona Preview Profile.\n\nUse a reliable channel to send information that’s important to be correct, even if it can be slightly delayed as a result. The following shows how Happy Beam updates the game model’s score state in response to a score message:\n\nsessionInfo.reliableMessenger = GroupSessionMessenger(session: newSession, deliveryMode: .reliable)\n\n\nTask {\n    for await (message, sender) in sessionInfo!.reliableMessenger!.messages(of: ScoreMessage.self) {\n        gameModel.clouds[message.cloudID].isHappy = true\n        gameModel\n            .players\n            .filter { $0.name == sender.source.id.asPlayerName }\n            .first!\n            .score += 1\n    }\n}\n\n\nUse an unreliable messenger for sending data with low-latency requirements. Because the delivery mode is unreliable, some messages might not make it. Happy Beam uses the unreliable mode to send live updates to the position of the beam when each participant in the call chooses the Spatial option in FaceTime.\n\nsessionInfo.messenger = GroupSessionMessenger(session: newSession, deliveryMode: .unreliable)\n\n\nThe following shows how Happy Beam serializes beam data for each message:\n\n// Send each player's beam data during FaceTime calls where players have selected the Spatial option.\nfunc sendBeamPositionUpdate(_ pose: Pose3D) {\n    if let sessionInfo = sessionInfo, let session = sessionInfo.session, let messenger = sessionInfo.messenger {\n        let everyoneElse = session.activeParticipants.subtracting([session.localParticipant])\n        \n        if isShowingBeam, gameModel.isSpatial {\n            messenger.send(BeamMessage(pose: pose), to: .only(everyoneElse)) { error in\n                if let error = error { print(\"Message failure:\", error) }\n            }\n        }\n    }\n}\n\nSee Also\nRelated samples\nIncorporating real-world surroundings in an immersive experience\nCreate an immersive experience by making your app’s content respond to the local shape of the world.\nHello World\nUse windows, volumes, and immersive spaces to teach people about the Earth.\nDestination Video\nLeverage 3D video and Spatial Audio to deliver an immersive experience.\nDiorama\nDesign scenes for your visionOS app using Reality Composer Pro.\nRelated articles\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nPlacing content on detected planes\nDetect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.\nTracking specific points in world space\nRetrieve the position and orientation of anchors your app stores in ARKit.\nTracking preregistered images in 3D space\nPlace content based on the current position of a known image in a person’s surroundings.\nRelated videos\nMeet ARKit for spatial computing\nBuild great games for spatial computing\nCreate accessible spatial experiences\nBuild spatial SharePlay experiences"
  },
  {
    "title": "Setting up access to ARKit data | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/setting-up-access-to-arkit-data",
    "html": "Overview\n\nIn visionOS, ARKit can enable new kinds of experiences that leverage data such as hand tracking and world sensing. The system gates access to this kind of sensitive information. Because people can decline your app’s request to use ARKit data or revoke access later, you need to provide alternative ways to use your app and to handle cases where your app loses access to data.\n\nAdd usage descriptions for ARKit data access\n\nPeople need to know why your app wants to access data from ARKit. Add the following keys to your app’s information property list to provide a user-facing usage description that explains how your app uses the data:\n\nNSHandsTrackingUsageDescription\n\nUse this key if your app uses hand tracking.\n\nNSWorldSensingUsageDescription\n\nUse this key if your app uses image tracking, plane detection, or scene reconstruction.\n\nNote\n\nWorld tracking — unlike world sensing — doesn’t require authorization. For more information, see Tracking specific points in world space.\n\nChoose between up-front or as-needed authorization\n\nYou can choose when someone sees an authorization request to use ARKit data. If you need precise control over when the request appears, call the requestAuthorization(for:) method on ARKitSession to explicitly authorize access at the time you call it. Otherwise, people see an authorization request when you call the run(_:) method. This is an implicit authorization because the timing of the request depends entirely on when you start the session.\n\nOpen a space and run a session\n\nTo help protect people’s privacy, ARKit data is available only when your app presents a Full Space and other apps are hidden. Present one of these space styles before calling the run(_:) method.\n\nThe following shows an app structure that’s set up to use a space with ARKit:\n\n@main\nstruct MyApp: App {\n    @State var session = ARKitSession()\n    @State var immersionState: ImmersionStyle = .mixed\n    var body: some Scene {\n        WindowGroup {\n            ContentView()\n        }\n        ImmersiveSpace(id: \"appSpace\") {\n            MixedImmersionView()\n            .task {\n                let planeData = PlaneDetectionProvider(alignments: [.horizontal])\n                \n                if PlaneDetectionProvider.isSupported {\n                    do {\n                        try await session.run([planeData])\n                        for await update in planeData.anchorUpdates {\n                            // Update app state.\n                        }\n                    } catch {\n                        print(\"ARKit session error \\(error)\")\n                    }\n                }\n            }\n        }\n        .immersionStyle(selection: $immersionState, in: .mixed)\n    }\n}\n\n\nCall openImmersiveSpace from your app’s user interface to create a space, start running an ARKit session, and kick off an immersive experience. The following shows a simple view with a button that opens the space:\n\nstruct ContentView: View {\n    @Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n    \n    var body: some View {\n        Button(\"Start ARKit experience\") {\n            Task {\n                await openImmersiveSpace(id: \"appSpace\")\n            }\n        }\n    }\n}\n\nProvide alternatives for declined and revoked authorizations\n\nSomeone might not want to give your app access to data from ARKit, or they might choose to revoke that access later in Settings. Handle these situations gracefully, and remove or transition content that depends on ARKit data. For example, you might fade out content that you need to remove, or recenter content to an appropriate starting position. If your app uses ARKit data to place content in a person’s surroundings, consider letting people place content using the system-provided interface.\n\nProviding alternatives is especially important if you’re using ARKit for user input. People using accessibility features, trackpads, keyboards, or other forms of input might need a way to use your app without ARKit.\n\nSee Also\nARKit\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nIncorporating real-world surroundings in an immersive experience\nCreate an immersive experience by making your app’s content respond to the local shape of the world.\nPlacing content on detected planes\nDetect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.\nTracking specific points in world space\nRetrieve the position and orientation of anchors your app stores in ARKit.\nTracking preregistered images in 3D space\nPlace content based on the current position of a known image in a person’s surroundings."
  },
  {
    "title": "Capturing screenshots and video from Apple Vision Pro for 2D viewing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/capturing-screenshots-and-video-from-your-apple-vision-pro-for-2d-viewing",
    "html": "Overview\n\nUse screenshots and short videos of your visionOS app to showcase your user interface, highlight functionality, and demonstrate usage. Help people understand what to expect from an immersive experience by recording content from Apple Vision Pro that includes your app and its surroundings.\n\nThe system renders content with spatial effects and optimizations for viewing during immersive experiences. Techniques that improve rendering performance during normal operation — such as foveated rendering, which reduces image quality in the peripheral — don’t translate well to 2D displays. To produce high-resolution content for people to view on 2D displays, the system needs to render without these effects and drop some optimizations. Use Developer Capture in Reality Composer Pro to notify the system to reconfigure rendering, and capture screenshots or high-resolution video, including sound, for up to 60 seconds from Apple Vision Pro.\n\nNote\n\nFor guidance on the screenshots and previews you include in your app’s product page, see Submit your apps to the App Store for Apple Vision Pro.\n\nPair your Apple Vision Pro to Xcode\n\nBefore capturing screenshots and video from your device, pair it with a Mac that has Xcode and the visionOS SDK installed. For instructions on pairing your device, see Running your app in Simulator or on a device.\n\nPrepare to capture your app and its surroundings\n\nSelect a well-lit location that’s free from clutter. Avoid including objects that might distract the audience or get in the way of your app‘s windows and 3D content. Include enough detail in the scene to provide context and anchoring points. Avoid material that you don’t have permission to capture, including people, screens, branded products, logos, artwork, and other intellectual property.\n\nUse the version of your app that you intend to share with your audience. Build and install your app using a release configuration. This configuration enables code optimizations for better runtime performance and disables the generation of debugging information. Debug configurations typically disable code optimizations and might include UI you donʼt intend to share. Don’t use them to record video for previews you intend to share. Build schemes manage the build configuration Xcode uses during build actions, for more information see Customizing the build schemes for a project.\n\nPlan the tasks you intend to capture ahead of time and keep them short and focused. Launch your app and go to the state where you plan to begin the capture. Reduce unnecessary processing overhead on Apple Vision Pro by quitting other apps and avoiding background tasks.\n\nTo capture screenshots or video from a device, select your device from the capture dialog in Reality Composer Pro:\n\nLaunch Reality Composer Pro. Choose Open Developer Tool > Reality Composer Pro from the Xcode menu.\n\nChoose File > Developer Capture to bring up the Developer Capture dialog.\n\nSelect the device to capture from the pop-up menu.\n\nIf you see the message “Preparing, wait for the device to be ready”. You can click the info button that appears to the right of the pop-up menu for more information.\n\nCapture screenshots\n\nTo begin capturing screenshots from Apple Vision Pro, click the button with the still camera icon in the capture dialog. The system begins your capture session:\n\nTo capture a screenshot immediately, without a countdown, press the spacebar. Click the countdown button to capture a screenshot after a 3 second countdown. Continue to keep relevant content centered and in frame for screenshots. The aspect ratio of screenshots crops content that appears at the sides of an experience.\n\nThe status area of the capture dialog displays the time remaining before the system ends the capture session. Click the stop button from your Mac to end the capture session yourself.\n\nCapture video\n\nTo begin capturing video from the device, click the video camera button in the Developer Capture dialog. This begins a countdown. When the countdown reaches 0, the capture session begins. As the capture process happens, the video changes because the system reconfigures to render content for viewing in 2D. You might notice reduced responsiveness from the device during the session as it devotes more processing to render and capture the video.\n\nWhile recording on the device, perform your planned interactions. Keep relevant content centered and in frame. The aspect ratio of the video you capture crops content that appears at the sides of an experience. Keep your head stable, and use slow, steady movement to transition the focus of the device when necessary. When viewing the video you capture in 2D, small head movements appear amplified and might be jarring to the audience.\n\nThe capture session ends when the elapsed time exceeds 60 seconds. You can click the record button again from your Mac to end the session sooner.\n\nNote\n\nTo begin a capture, the device must have a stable connection to your Mac and start at low power and thermal levels to stay below thresholds necessary to achieve consistent frame rates. When capturing multiple sessions, you might need to wait between each session.\n\nReview the captured video file\n\nEach recording session creates a QuickTime Movie file (.mov) and saves it to the desktop of your Mac. The file includes video captured at 30 FPS using 10-bit HEVC in HDTV Rec. 709 color space with system audio recorded in 32-bit floating-point linear PCM.\n\nReview the video to make sure that it includes all the content you planned and it doesn’t include any unexpected elements. Ensure that the transitions and animations are smooth and frame rates are consistent.\n\nUse additional video-editing tools to trim, edit, and apply post-processing, such as stabilization, to the video to create a high-quality preview.\n\nSee Also\nRealityKit and Reality Composer Pro\nSwift Splash\nUse RealityKit to create an interactive ride in visionOS.\nDiorama\nDesign scenes for your visionOS app using Reality Composer Pro.\nUnderstanding RealityKit’s modular architecture\nLearn how everything fits together in RealityKit.\nDesigning RealityKit content with Reality Composer Pro\nDesign RealityKit scenes for your visionOS app."
  },
  {
    "title": "Designing RealityKit content with Reality Composer Pro | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/designing-realitykit-content-with-reality-composer-pro",
    "html": "Overview\n\nUse Reality Composer Pro to visually design, edit, and preview RealityKit content. In Reality Composer Pro, you can create one or more scenes, which act as a container for RealityKit content. Scenes contain hierarchies of entities, which are virtual objects such as 3D models.\n\nIn addition to helping you compose scenes, Reality Composer Pro also gives you the ability to add and configure components — even custom components that you’ve written — to the entities in your scenes and also lets you create complex materials and effects using a node-based material editor called Shader Graph.\n\nLaunch Reality Composer Pro\n\nWhen you create a visionOS project in Xcode, it also contains a default Reality Composer Pro project named RealityKitContent within the Packages folder, which is a Swift package. The RealityKitContent package can include images, 3D models, and other assets like audio and video files. The assets you add to your project go in the RealityKitContent.rkassets bundle, while your source code goes into its Sources directory. The package also contains a file called Package.realitycomposerpro, which is the actual Reality Composer Pro project.\n\nTo launch Reality Composer Pro, double-click the Package.realitycomposerpro file in the Project navigator, or click the Open in Reality Composer Pro button. If your project doesn’t already have a Reality Composer Pro project, you can launch Reality Composer Pro directly by choosing Xcode > Open Developer Tool > Reality Composer Pro.\n\nFor efficiency, store all of your RealityKit assets in Reality Composer Pro projects. Xcode compiles Reality Composer Pro projects into a more efficient format when you build your app.\n\nNote\n\nLoading assets from a .reality file is considerably faster and more resource efficient than loading individual asset files.\n\nOrient yourself in Reality Composer Pro\n\nThe Reality Composer Pro window has several sections. The top-half displays the active scene. If you have multiple scenes, the window shows a tab bar at the top with one tab for each open scene. A scene in Reality Composer Pro is an entity hierarchy stored in a .usda file.\n\nThe left side of the top pane contains the hierarchy browser, which shows a tree representation of the entities in the active scene. You can toggle it using the top-left toolbar button to reveal errors and warnings. The middle pane is the 3D View, which shows a 3D representation of the active scene. The top-right is the inspector, which shows configurable values for the item selected in the 3D view, hierarchy view, or Shader Graph, depending on which has focus.\n\nTip\n\nA Reality Composer Pro scene can represent an entire RealityKit scene, and you can have multiple scenes in your Reality Composer Pro project, each driving a different RealityView in the same app. A scene can also contain a collection of entities to use as a building block. For example, if you had an airplane model, you might build a scene for it that contains its 3D model, a particle effect to make smoke come out its engine, and audio entities or components that represent the various sounds a plane makes. Your app could then load those combined assets and use them together anywhere it needs.\n\nThe bottom half of Reality Composer Pro contains the following four tabs:\n\nProject Browser\n\nDisplays all of the assets in your project.\n\nShader Graph\n\nAn advanced, node-based material editor.\n\nAudio Mixer\n\nA tool for combining sound assets.\n\nStatistics\n\nInformation about the currently open scene, such as the number of entities, vertices, and animations it contains.\n\nReality Composer Pro projects start with a single empty scene called Scene which is stored in a file called Scene.usda. You can create as many additional scenes as you need by choosing File > New > Scene. New scenes open as tabs along the top of the window, and they also appear in the Project Browser as .usda files.\n\nIf you close a scene’s tab and need to re-open it, double-click on the scene’s .usda file in the Project Browser. If you no longer need a scene, delete its .usda file from the Project Browser or remove it from your project’s .rkassets bundle in Xcode.\n\nTo delete a scene:\n\nClose the scene tab by selecting File > Close Tab\n\nSelect the scene’s .usda file in the Project Browser\n\nControl-click the scene’s .usda file the Project Browser.\n\nChoose Delete from the contextual menu.\n\nClick Move to Trash.\n\nThis removes the scene’s .usda and the scene tab at the top of the window.\n\nAdd assets to your project\n\nIn Reality Composer Pro, you design scenes by first importing assets into your project. Then add assets to scenes and move, rotate, and scale them. The Project Browser tab displays all of the asset files in your project. You can add new assets by dragging them to the Project Browser or by choosing File > Import and select the assets to add to your project. To add an asset from the Project Browser to the current scene, drag it to the 3D view in the center of the window, or to the hierarchy view in the top-left of the window.\n\nNote\n\nReality Composer Pro projects can contain assets not used in any scene. Such assets are still compiled into your app and can be loaded at runtime and take full advantage of the efficient loading process for .reality files.\n\nReality Composer Pro can represent many assets as entities, but it can’t represent all assets that way; for example:\n\nUSDZ models do become an entity or entity hierarchy when you add them to a scene.\n\nImage files do not become an entity. Reality Composer Pro only uses image assets indirectly, such as being the source texture for materials you build in Shader Graph. If you drag assets that Reality Composer Pro can’t turn into an entity, nothing happens.\n\nAdd any 3D models, animations, sounds, and image files you need to your project. You can organize your assets into subfolders to make the Project Browser more manageable as your project grows in size.\n\nReality Composer Pro has a library of assets that you can use in your own apps. You can access the library by clicking the Add button (+) in the toolbar. Click the icon of the down-arrow inside a circle next to an asset to download the asset to Reality Composer Pro. When the download finishes, you can double-click or drag the asset into your project.\n\nImportant\n\nReality Composer Pro treats your imported assets as read-only.\n\nChanges you make to assets in a scene only affect that scene’s copy of the asset. The changes you make are stored in the scene’s .usda file, not in the original asset. That means you can work without fear of inadvertently changing other scenes. If you plan to make significant changes to an imported 3D model, such as by replacing its materials with dynamic Shader Graph materials, import the model as a.usdc file instead of as a .usdz file, and then separately import just the supporting assets you need to avoid Xcode compiling assets that you don’t need into your app.\n\nCompose scenes from assets\n\nAll RealityKit entities in a scene exist at a specific position, orientation, and scale, even if that entity has no visual representation. When you click to select an entity in the 3D view or hierarchy view, Reality Composer Pro displays:\n\nA manipulator over the entity in the 3D view.\n\nAny configurable values from the entity’s components in the inspector on the right.\n\nYou can use the manipulator to move, rotate, and scale the selected entity.\n\nTo move the selected entity around the 3D scene, drag the small colored cone that corresponds to the axis you want to move it along. Alternatively, you can drag the entity itself to move it freely relative to your viewing angle.\n\nTo rotate the selected entity, click on the manipulator’s rotation control, which looks like a circle, and drag in a circular motion.\n\nReality Composer Pro’s manipulator only shows one rotation control at a time.\n\nTo rotate an entity on one of the other axes, click the cone corresponding to the axis you want to rotate. For example, if you want to rotate the entity on the X axis, tap the red cone to bring up the red rotation handle for that axis.\n\nTo scale the selected entity uniformly, click the rotation circle and drag away from the entity origin to scale it up, or toward the entity origin to scale it down. Because it scales uniformly, it doesn’t matter which rotation handle Reality Composer Pro is showing.\n\nPlay\n\nNote\n\nIn the manipulator, Red indicates the X axis, Green indicates the Y axis, and Blue indicates the Z axis.\n\nAlternatively, you can make the same changes to the selected entity by typing new values into the transform component of the inspector. The transform component stores the position, rotation, and scale for an entity. The manipulator is just a visual way to change the values on this component.\n\nActivate and deactivate scene entities\n\nReality Composer Pro scenes can get quite complex and sometimes contain overlapping entities, which can be difficult to work with. To simplify a scene, you can deactivate entities to remove them from the 3D view. Deactivate entities by Control-clicking them and selecting Deactivate from the contextual menu. The entity still exists in your project and is shown in the hierarchy view, albeit grayed out and without any child entities. It won’t, however, appear in the 3D view. Xcode doesn’t compile deactivated entities into your app’s bundle, so it’s important to re-activate any entities your app needs before saving your project. To reactivate an entity, Control-click the entity in the hierarchy view and select Activate from the contextual menu.\n\nAdd components to entities\n\nRealityKit follows a design pattern called Entity-Component-System (ECS). In ECS, you store data on an entity using components and then implement entity behavior by writing systems that use the data from those components. You can add and configure components to entities in Reality Composer Pro, including both built-in components like ParticleEmitterComponent, and custom components that you write and place in the Sources folder of your Reality Composer Pro Swift package. You can also create new components in Reality Composer Pro and edit them in Xcode.\n\nFor more information about ECS, see Understanding RealityKit’s modular architecture.\n\nTo add a component to an entity, select that entity in the hierarchy view or 3D view. At the bottom-right of the inspector window, click Add Component. A list of available components appears with New Component at the top. If you select the first item, Reality Composer Pro creates a new component class in the Sources folder, and optionally a new system class. It also adds the component to the selected entity. If you select any other item in the list, it adds that component to the selected entity if it doesn’t already have that component.\n\nCreate or modify entity hierarchies\n\nReality Composer Pro scenes are hierarchies of RealityKit entities. You can change the relationship between entities in the hierarchy browser except for parts of the hierarchy imported from a .usdz file, which Reality Composer Pro treats as a read-only file.\n\nTo change the relationship between entities, or to create a relationship between two currently unrelated entities, use the hierarchy view and drag an entity onto the entity that you want it to be part of. If you want an entity to become a root entity, drag it to the Root transform at the top of the hierarchy view.\n\nModify or create new materials\n\nWhen you import a USDZ model into Reality Composer Pro, it creates a RealityKit material for every physically-based rendering (PBR) material the asset contains. Reality Composer Pro displays materials in the hierarchy view just like it displays entities, except it uses a paintbrush icon. Reality Composer Pro doesn’t display materials in the 3D view.\n\nNote\n\nThe library in Reality Composer Pro contains materials for several common real-world surfaces like metal, wood, and denim that you can import into your project.\n\nIf you select a PBR material in the hierarchy view, you can edit it using the inspector. You can replace images, colors, or values for any of the PBR attributes with another image, color, or value of your choosing. Any changes you make to a material affects any entity that’s bound to that material. You can also create new materials from scratch by clicking the Add button (+) at the bottom of the scene hierarchy and choosing Material.\n\nBuild materials in Shader Graph\n\nPBR materials are great at reproducing real-world surfaces. However, they can’t represent nonrealistic materials like cartoon shaders, and they can’t contain logic. This means that you can’t animate a PBR material or have it react to input from your app.\n\nReality Composer Pro offers a second type of material called a custom material. You can build and edit custom materials using the Shader Graph tab. Shader Graph provides a tremendous amount of control over materials and allows you to do things that would otherwise require writing Metal shaders. For more information on writing Metal shaders, see Metal.\n\nNote\n\nRealityKit doesn’t represent Reality Composer Pro custom materials as an instance of CustomMaterial, as you might expect. Instead, RealityKit represents these materials as ShaderGraphMaterial instances.\n\nThe materials you build in the editor can affect both the look of an entity and its shape. If you build a node graph and connect it to the Custom Surface pin on the output node, that node graph controls the surface appearance of the model and roughly equates to writing Metal code in a fragment shader. If you build a node graph and connect it to the Custom Geometry Modifier output pin, those nodes control the shape of the entity, which equates to Metal code running in a vertex shader.\n\nNodes represent values and operations and serve the same purpose as either a variable or constant, or a function in Metal. If you need the sine of a value, for example, connect the value’s output node to the input pin of a Sin node. Add new nodes to the graph by double-clicking the background of the Shader Graph view or click the New Node button on the right side of the screen.\n\nImportant\n\nSome nodes, like Sin, are universal and can be used with either output pin. Other nodes are specific to either the Custom Surface or Geometry Modifier outputs. If a node name starts with Geometry Modifier, you can only connect it to the Geometry Modifier output pin. If the node’s name starts with “Surface”, you can only connect it to the Custom Surface output pin.\n\nTo unlock the real power of Shader Graph, you need to be able to change values on the material from Swift code. Shader Graph allows you to do this by creating promoted inputs, which are parameters you can set and read from Swift to change your material at runtime. If you have a feature that you want to turn on and off, you might create a Boolean input parameter and have conditional logic based on its value. If you want to smoothly interpolate between two colors, you might create a Float input parameter and use it to control how to interpolate between the two colors. You can Control-click on a constant node and select Promote to turn it into a promoted input. You can also turn a promoted input back into a constant by Control-clicking it and selecting Demote.\n\nIf you don’t have an existing constant to promote, you can create new promoted inputs using the inspector. The New Input button only shows up in the inspector when you select a material in the hierarchy view but have no nodes selected in the Shader Graph tab.\n\nTo change the value of an input parameter from Swift code, use setParameter(name:value:), passing the name of the parameter and the new value. Note that parameter names are case sensitive, so your name string must exactly match what you called the parameter in Shader Graph.\n\nFor examples of Shader Graph use, see Diorama and Happy Beam.\n\nUse references to reuse assets\n\nIf your project has multiple scenes that share assets, you can use references to avoid creating duplicate assets. A reference acts like an alias in Finder — it points to the original asset and functions just as if it were another copy of that asset.\n\nCreate references using the inspector. By default, the references section is hidden for entities and materials that don’t have any references. To add a new reference to an asset or material that doesn’t have one, choose Reality Composer Pro > Settings and uncheck Hide Empty References.\n\nTo add a reference, click the Add button (+) at the bottom of the references section in the inspector, choose the .usda file for the scene that contains the asset, then choose the asset you want to link to. After you do that, the selected entity or material becomes a copy of the one you linked to.\n\nImportant\n\nIf you make changes to a linked asset, those changes will affect every linked reference.\n\nPreview scenes on device\n\nIf you have an Apple Vision Pro connected to your Mac, choose Preview > Play or click the preview button in the Reality Composer Pro toolbar to view your scene on device. The Preview button is the left-most button on the right side of the toolbar — the one with an Apple Vision Pro icon. If you have multiple Apple Vision Pro devices connected, choose which device to use by clicking the pull-down menu next to the Preview button.\n\nLoad Reality Composer Pro scenes in RealityKit\n\nLoading a Reality Composer Pro scene is nearly identical to loading a USDZ asset from your app bundle, except you have to specify the Reality Composer Pro package bundle instead. You typically do this in the make closure of a RealityView initializer. Reality Composer Pro packages define a global constant that points to its bundle, which is named after the project with “Bundle” appended to it. In the default Xcode visionOS template, the Reality Composer Pro project is called RealityKitContent, so the global bundle variable is called realityKitContentBundle:\n\nRealityView { content in\n    if let scene = try? await Entity.load(named: \"Biplane\", \n                                          in: realityKitContentBundle) {\n        myDataModel.add(scene) \n        content.add(scene)\n    }\n} update: { content in\n    // ...\n}\n\n\nNote\n\nThe code above saves a reference to the root node. This isn’t required, but with RealityView, unlike ARView on iOS and macOS, you don’t have ready access to the scene content, so it’s often handy to maintain your own reference to the root entity of your scene in your app’s data model.\n\nWhen RealityKit finishes loading the scene, the scene variable contains the root entity of the scene you specified. Add it to content and RealityKit displays it to the user.\n\nSee Also\nRealityKit and Reality Composer Pro\nSwift Splash\nUse RealityKit to create an interactive ride in visionOS.\nDiorama\nDesign scenes for your visionOS app using Reality Composer Pro.\nUnderstanding RealityKit’s modular architecture\nLearn how everything fits together in RealityKit.\nCapturing screenshots and video from Apple Vision Pro for 2D viewing\nCreate screenshots and record high-quality video of your visionOS app and its surroundings for app previews."
  },
  {
    "title": "Adopting best practices for privacy and user preferences | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/adopting-best-practices-for-privacy",
    "html": "Overview\n\nTo protect user privacy, the system handles camera and sensor inputs without passing the information to apps directly. Instead, the system enables your app to seamlessly interact with a user’s surroundings and to automatically receive input from the user. For example, the system handles the eye- and hand-position data needed to detect interactions with your app’s content. Similarly, the system provides a way to automatically alter a view’s appearance when someone looks at it, without your app ever knowing what the user is looking at.\n\nIn the few cases where you actually need access to hand position or information about the user’s surroundings, the system requires you to obtain authorization from the user first.\n\nImportant\n\nIt’s your responsibility to protect any data your app collects, and to use it in responsible and privacy-preserving ways. Don’t ask for data that you don’t need, be transparent about how you use the data you acquire, and respect the choices of the person whose data it is.\n\nFor information about how to specify the privacy data your app uses, see Describing data use in privacy manifests. For general information about privacy, see Protecting the User’s Privacy.\n\nAdopt the system-provided input mechanisms\n\nOn Apple Vision Pro, people use their eyes and hands to interact with the items they see in front of them. Where they look determines where the system applies focus, and a tap gesture with either hand generates a touch event on that focused item. The system can also detect when someone’s fingers interact with virtual items in the person’s field of vision. When you adopt the standard UIKit and SwiftUI event-handling mechanisms, you get all of these interactions automatically.\n\nFor most apps, the system-provided gesture recognizers are sufficient for responding to interactions. Although you can get the position of someone’s hands with ARKit, doing so isn’t necessary for most apps. Collect hand-position data only when the system doesn’t offer what you need. For example, you might use hand-position data to attach 3D content to the person’s hands. Some other things to remember about hand-position data:\n\nPeople can deny your request for access to hand-position data. Be prepared to handle situations where the data isn’t available.\n\nYou must present an immersive space to access hand data. When you open an immersive space, the system hides other apps.\n\nFor information about how to handle the standard-system events, see the SwiftUI and UIKit documentation.\n\nProvide clear messaging around privacy-sensitive features\n\nThe following ARKit features require you to provide a usage description string in your app’s Info.plist file:\n\nWorld-tracking data\n\nHand-tracking data\n\nOther privacy-sensitive technologies in visionOS also require you to supply usage description strings. For example, you provide usage descriptions for the Core Location features you adopt. These strings communicate why your app needs the data, and how you plan to use the data to help the person using your app. The first time you request authorization to use the technology, the system prompts the person to grant or deny access to your app. The system includes your usage-description string in the dialog it displays.\n\nFor information about requesting access to ARKit data, see ARKit. For guidance on how to craft good messages around privacy-friendly features, see Human Interface Guidelines.\n\nSee Also\nDesign\nDesigning for visionOS\nWhen people wear Apple Vision Pro, they enter an infinite 3D space where they can engage with your app or game while staying connected to their surroundings.\nImproving accessibility support in your visionOS app\nUpdate your code to ensure everyone can access your app’s content in visionOS."
  },
  {
    "title": "Hello World | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/world",
    "html": "Overview\n\nYou can use visionOS scene types and styles to share information in fun and compelling ways. Features like volumes and immersive spaces let you put interactive virtual objects into people’s environments, or put people into a virtual environment.\n\nHello World uses these tools to teach people about the Earth — the planet we call home. The app shows how the Earth’s tilt creates the seasons, how objects move as they orbit the Earth, and how Earth appears from space.\n\nPlay\n\nThe app uses SwiftUI to define its interface, including both 2D and 3D elements. To create, customize, and manage 3D models and effects, it also relies on the RealityKit framework and Reality Composer Pro.\n\nCreate an entry point into the app\n\nHello World constructs the scene that it displays at launch — the first scene that appears in the WorldApp structure — using a WindowGroup:\n\nWindowGroup(\"Hello World\", id: \"modules\") {\n    Modules()\n        .environment(model)\n}\n.windowStyle(.plain)\n\n\nLike other platforms — for example, macOS and iOS — visionOS displays a window group as a familiar-looking window. In visionOS, people can resize and move windows around the Shared Space. Even if your app offers a sophisticated 3D experience, a window is a great starting point for an app because it eases people into the experience. It’s also a good place to provide instructions or controls.\n\nTip\n\nThis particular window group uses the plain window style to maintain control over the glass background effect that visionOS would otherwise automatically add.\n\nPresent different modules using a navigation stack\n\nAfter you watch a brief introductory animation that shows the text Hello World typing in, the Modules view that defines the primary scene’s content presents options to explore different aspects of the world. This view contains a table of contents at the root of a NavigationStack:\n\nNavigationStack(path: $model.navigationPath) {\n    TableOfContents()\n        .navigationDestination(for: Module.self) { module in\n            ModuleDetail(module: module)\n                .navigationTitle(module.eyebrow)\n        }\n}\n\n\nA visionOS navigation stack has the same behavior that it has in other platforms. When it first appears, the stack displays its root view. When someone chooses an embedded NavigationLink, the stack draws a new view and displays a back button in the toolbar. When someone taps the back button, the stack restores the previous view.\n\nThe trailing closure of the navigationDestination(for:destination:) view modifier in the code above displays a view when someone activates a link based on a module input that comes from the corresponding link’s initializer:\n\nNavigationLink(value: module) { /* The link's label. */ }\n\n\nThe possible module values come from a custom Module enumeration:\n\nenum Module: String, Identifiable, CaseIterable, Equatable {\n    case globe, orbit, solar\n    // ...\n}\n\nDisplay an interactive globe in a new scene\n\nThe globe module opens with a few facts about the Earth in the main window next to a decorative, flat image that supports the content. To help people understand even more, the module includes a button titled View Globe that opens a 3D interactive globe in a new window.\n\nTo be able to open multiple scene types, Hello World includes the UIApplicationSceneManifest key in its Information Property List file. The value for this key is a dictionary that includes the UIApplicationSupportsMultipleScenes key with a value of true:\n\n<key>UIApplicationSceneManifest</key>\n<dict>\n    <key>UIApplicationSupportsMultipleScenes</key>\n    <true/>\n    <key>UISceneConfigurations</key>\n    <dict/>\n</dict>\n\nDeclare a volume for the globe\n\nWith the key in place, the app makes use of a second WindowGroup in its App declaration. This new window group uses the Globe view as its content:\n\nWindowGroup(id: Module.globe.name) {\n    Globe()\n        .environment(model)\n}\n.windowStyle(.volumetric)\n.defaultSize(width: 0.6, height: 0.6, depth: 0.6, in: .meters)\n\n\nThis window group creates a volume — which is a container that has three dimensions and behaves like a transparent box — because Hello World uses the volumetric window style scene modifier. People can move this box around the Shared Space like they move other window types, and the content remains fixed inside. The defaultSize(width:height:depth:in:) modifier specifies a size for the volume in meters, including a depth dimension.\n\nPlay\n\nThe Globe view inside the volume contains 3D content, but is still just a SwiftUI view. It contains two elements in a ZStack: a subview that draws a model of the Earth, and another that provides a control panel that people can use to configure the model’s appearance.\n\nOpen and dismiss the globe volume\n\nThe globe module presents a View Globe button that people can tap to display or dismiss the volume, depending on the current state. Hello World achieves this behavior by creating a Toggle with the button style, and embedding it in a custom GlobeToggle view.\n\nstruct GlobeToggle: View {\n    @Environment(ViewModel.self) private var model\n    @Environment(\\.openWindow) private var openWindow\n    @Environment(\\.dismissWindow) private var dismissWindow\n\n\n    var body: some View {\n        @Bindable var model = model\n\n\n        Toggle(Module.globe.callToAction, isOn: $model.isShowingGlobe)\n            .onChange(of: model.isShowingGlobe) { _, isShowing in\n                if isShowing {\n                    openWindow(id: Module.globe.name)\n                } else {\n                    dismissWindow(id: Module.globe.name)\n                }\n            }\n            .toggleStyle(.button)\n    }\n}\n\n\nWhen someone taps the toggle, the isShowingGlobe state changes, and the onChange(of:initial:_:) modifier calls the openWindow or dismissWindow action to open or dismiss the volume, respectively. The view gets these actions from the environment and uses an identifier that matches the volume’s identifier.\n\nDisplay objects that orbit the Earth\n\nYou use windows in visionOS the same way you do in other platforms. But even 2D windows in visionOS provide a small amount of depth you can use to create 3D effects — like elements that appear in front of other elements. Hello World takes advantage of this depth to present small models inline with 2D content.\n\nThe app’s second module, Objects in Orbit, provides information about objects that go around the Earth, like the Moon and artificial satellites. To give a sense of what these objects look like, the module displays 3D models of these items directly inside the window.\n\nHello World loads these models from the asset bundle using a Model3D structure inside a custom ItemView. The view scales and positions the model to fit the available space, and applies optional orientation adjustments:\n\nprivate struct ItemView: View {\n    var item: Item\n    var orientation: SIMD3<Double> = .zero\n\n\n    var body: some View {\n        Model3D(named: item.name, bundle: worldAssetsBundle) { model in\n            model.resizable()\n                .scaledToFit()\n                .rotation3DEffect(\n                    Rotation3D(\n                        eulerAngles: .init(angles: orientation, order: .xyz)\n                    )\n                )\n                .frame(depth: modelDepth)\n                .offset(z: -modelDepth / 2)\n        } placeholder: {\n            ProgressView()\n                .offset(z: -modelDepth * 0.75)\n        }\n    }\n}\n\n\nThe app uses this ItemView once for each model, placing each in an overlay that only becomes visible based on the current selection. For example, the following overlay displays the satellite model with a small amount of tilt in the x-axis and z-axis:\n\n.overlay {\n    ItemView(item: .satellite, orientation: [0.15, 0, 0.15])\n        .opacity(selection == .satellite ? 1 : 0)\n}\n\n\nThe VStack that contains the models also contains a Picker that people use to select a model to view:\n\nPicker(\"Satellite\", selection: $selection) {\n    ForEach(Item.allCases) { item in\n        Text(item.name)\n    }\n}\n.pickerStyle(.segmented)\n\n\nWhen you add 3D effects to a 2D window, keep this guidance in mind:\n\nDon’t overdo it. These kinds of effects add interest, but can unintentionally obscure important controls or information as people view the window from different directions.\n\nEnsure that elements don’t exceed the available depth. Excess depth causes elements to clip. Account for any position or orientation changes that might occur after initial placement.\n\nAvoid models intersecting with the backing glass. Again, account for potential movement after initial placement.\n\nShow Earth’s relationship to its satellites in an immersive space\n\nPeople can visualize how satellites move around the Earth because the app’s orbit module displays the Earth, the Moon, and a communications satellite together as a single system. People can move the system anywhere in their environment or resize it using standard gestures. They can also move themselves around the system to get different perspectives.\n\nNote\n\nTo learn about designing with gestures in visionOS, read Gestures in Human Interface Guidelines.\n\nTo create this visualization, the app displays the Orbit view — which contains a single RealityView that models the entire system — in an ImmersiveSpace scene with the mixed immersion style:\n\nImmersiveSpace(id: Module.orbit.name) {\n    Orbit()\n        .environment(model)\n}\n.immersionStyle(selection: $orbitImmersionStyle, in: .mixed)\n\n\nAs with any secondary scene in a visionOS app, this scene depends on having the UIApplicationSupportsMultipleScenes key in the Information Property List file. The app also opens and closes the space using a toggle view that resembles the one used for the globe:\n\nstruct OrbitToggle: View {\n    @Environment(ViewModel.self) private var model\n    @Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n    @Environment(\\.dismissImmersiveSpace) private var dismissImmersiveSpace\n\n\n    var body: some View {\n        @Bindable var model = model\n\n\n        Toggle(Module.orbit.callToAction, isOn: $model.isShowingOrbit)\n            .onChange(of: model.isShowingOrbit) { _, isShowing in\n                Task {\n                    if isShowing {\n                        await openImmersiveSpace(id: Module.orbit.name)\n                    } else {\n                        await dismissImmersiveSpace()\n                    }\n                }\n            }\n            .toggleStyle(.button)\n    }\n}\n\n\nThere are a few key differences from the version that appears in the section Open and dismiss the globe volume:\n\nOrbitToggle uses openImmersiveSpace and dismissImmersiveSpace from the environment, rather than the window equivalents.\n\nThe dismiss action in this case doesn’t require an identifier, because people can only open one space at a time, even across apps.\n\nThe open and dismiss actions for spaces operate asynchronously, and so they appear inside a Task.\n\nView the solar system from space using full immersion\n\nThe app’s final module gives people a sense of the Earth’s place in the solar system. Like other modules, this one includes information and a decorative image next to a button that leads to another visualization — in this case so people can experience Earth from space.\n\nWhen a person taps the button, the app takes over the entire display and shows stars in all directions. The Earth appears directly in front, the Moon to the right, and the Sun to the left. The main window also shows a small control panel that people can use to exit the fully immersive experience.\n\nTip\n\nPeople can always close the currently open immersive space by pressing the device’s Digital Crown, but it’s typically useful when you provide a built-in mechanism to maintain control of the experience within your app.\n\nThe app uses another immersive space scene for this module, but here with the full immersion style that turns off the passthrough video:\n\nImmersiveSpace(id: Module.solar.name) {\n    SolarSystem()\n        .environment(model)\n}\n.immersionStyle(selection: $solarImmersionStyle, in: .full)\n\n\nThis scene depends on the same UIApplicationSupportsMultipleScenes key that other secondary scenes do, and is activated by a SolarSystemToggle that’s similar to the ones that the app uses for the other scenes:\n\nstruct SolarSystemToggle: View {\n    @Environment(ViewModel.self) private var model\n    @Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n    @Environment(\\.dismissImmersiveSpace) private var dismissImmersiveSpace\n\n\n    var body: some View {\n        Button {\n            Task {\n                if model.isShowingSolar {\n                    await dismissImmersiveSpace()\n                } else {\n                    await openImmersiveSpace(id: Module.solar.name)\n                }\n            }\n        } label: {\n            if model.isShowingSolar {\n                Label(\n                    \"Exit the Solar System\",\n                    systemImage: \"arrow.down.right.and.arrow.up.left\")\n            } else {\n                Text(Module.solar.callToAction)\n            }\n        }\n    }\n}\n\n\nThis control appears in the main window to provide a way to begin the fully immersive experience, and separately in the control panel as a way to exit the experience. Because the app uses this control as two distinct buttons rather than as a toggle in one location, it’s composed of a Button with behavior that changes depending on the app state rather than as a toggle with a button style.\n\nTo reuse the main window for the solar system controls, Hello World places both the navigation stack and the controls in a ZStack, and then sets the opacity of each to ensure that only one appears at a time:\n\nZStack {\n    SolarSystemControls()\n        .opacity(model.isShowingSolar ? 1 : 0)\n\n\n    NavigationStack(path: $model.navigationPath) {\n        // ...\n    }\n    .opacity(model.isShowingSolar ? 0 : 1)\n}\n.animation(.default, value: model.isShowingSolar)\n\nSee Also\nRelated samples\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nDestination Video\nLeverage 3D video and Spatial Audio to deliver an immersive experience.\nDiorama\nDesign scenes for your visionOS app using Reality Composer Pro.\nRelated articles\nCreating your first visionOS app\nBuild a new visionOS app using SwiftUI and add platform-specific features.\nAdding 3D content to your app\nAdd depth and dimension to your visionOS app and discover how to incorporate your app’s content into a person’s surroundings.\nCreating fully immersive experiences in your app\nBuild fully immersive experiences by combining spaces with content you create using RealityKit or Metal.\nPresenting windows and spaces\nOpen and close the scenes that make up your app’s interface.\nPositioning and sizing windows\nInfluence the initial geometry of windows that your app presents.\nRelated videos\nPlatforms State of the Union\nMeet SwiftUI for spatial computing\nGo beyond the window with SwiftUI\nTake SwiftUI to the next dimension\nDevelop your first immersive app\nGet started with building apps for spatial computing"
  },
  {
    "title": "Improving accessibility support in your visionOS app | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/improving-accessibility-support-in-your-app",
    "html": "Overview\n\nvisionOS is an immersive platform that supports people of all abilities. Even though experiences incorporate stunning visual content and hand- and eye-tracking technologies, people can engage with content in other ways. In fact, the platform supports people in many different situations, including those who are blind, have low vision, have limited mobility, or have limb differences. With the help of assistive technologies, people can interact with all of your app’s content.\n\nPlay\n\nDuring development, enable VoiceOver and other assistive features and test your app’s accessibility support. Make sure people can navigate your app’s interface intuitively, and that all of the necessary elements are present. Improve the descriptive information for those elements to communicate their intended purpose. And make sure your app adapts to changing conditions, such as changes to the Dynamic Type setting while your app is running.\n\nDefault font size\n\nIncreased font size\n\nFor general information about supporting accessibility, see Accessibility. For design guidance, see Human Interface Guidelines > Accessibility.\n\nAdd accessibility traits to RealityKit entities\n\nVoiceOver and other assistive technologies rely on the accessibility information that your app’s views and content provide. SwiftUI and UIKit provide default information for the standard system views, but RealityKit doesn’t provide default information for the entities in your scenes.\n\nTo configure the accessibility information for a RealityKit entity, add an instance of AccessibilityComponent to the entity. Use this component to specify the same values you specify for the rest of your app’s views. The following example shows how to create this component and add it to an entity:\n\nvar accessibilityComponent = AccessibilityComponent()\naccessibilityComponent.isAccessibilityElement = true\naccessibilityComponent.traits = [.button]\naccessibilityComponent.label = \"Sports car\"\naccessibilityComponent.value = \"Parked\"\naccessibilityComponent.systemActions = [.activate]\nmyEntity.components[AccessibilityComponent.self] = accessibilityComponent\n\n\nPeople can use VoiceOver to initiate specific types of actions on your entities. Assign a value to the systemActions property of your component if your entity supports the incrementing or decrementing of its value, or supports activation with a gesture other than a standard tap. You don’t need to set a system action if you let people interact with the entity using a standard single-tap gesture.\n\nThe following example uses the content of a RealityView to determine when activation events occur on the view’s entities. After subscribing to the view’s activation events, the code sets up an asynchronous task to handle incoming events. When a new event occurs, the task executes the custom code to handle a collision.\n\nactivationSubscription = content.subscribe(to: AccessibilityEvents.Activate.self, \n                            on: nil, componentType: nil) { activation in\n    Task {\n        try handleCollision(for: activation.entity, gameModel: gameModel)\n    }\n}\n\nAdd support for Direct Gesture mode\n\nWhen VoiceOver is active in visionOS, people use hand gestures to navigate your app’s interface and inspect elements. To prevent your app’s code from interfering with VoiceOver interactions, the system doesn’t deliver hand input to your app during this time. However, a person can perform a special VoiceOver gesture to enable Direct Gesture mode, which leaves VoiceOver enabled but restores hand input to your app.\n\nAdd VoiceOver announcements to your code to communicate the results of meaningful events. VoiceOver speaks these announcements at all times, but they are particularly useful when Direct Gesture mode is on. The following example posts an announcement when a custom gesture causes an interaction with a game piece:\n\nAccessibilityNotification.Announcement(\"Game piece hit\").post()\n\nProvide alternatives to input that involves physical movement\n\nReduced mobility can affect a person’s ability to interact with your app’s content. When designing your app’s input model, avoid experiences that require specific body movements or positions. For example, if your app supports custom hand gestures, add menu commands for each gesture so someone can enter them using a keyboard or assistive device.\n\nSome assistive technologies let people interact with your app using only their eyes. Using these technologies they can select, scroll, long press, or drag items in your interface. Even if you support other types of interactions, give people a way to access all of your app’s behavior using only these interactions.\n\nAvoid head-anchored content\n\nSome assistive technologies allow people to navigate or view your app’s interface using head movements. As the person’s head moves, the assistive technology focuses on the item directly in front of them. Content that follows the movements of the person’s head interferes with the behavior of these assistive technologies.\n\nWhen designing your interface, place content in windows or anchor it to locations other than the virtual camera. If you do need head-anchored content, provide an alternative solution when relevant assistive technologies are in use. For example, you might move head-anchored content to an anchor point that doesn’t follow the person’s head movements.\n\nTo determine when to change the anchoring approach for your content, check the accessibilityPrefersHeadAnchorAlternative environment variable in SwiftUI, or call the doc://com.apple.documentation/documentation/accessibility/4278279-axprefersheadanchoralternative function. This environment variable is true when an assistive technology is in use that conflicts with head-anchored content. Adapt your content to use alternate anchoring mechanisms at that time.\n\nLimit motion effects in your content\n\nMotion effects on any immersive device can be jarring, even for people who aren’t sensitive to motion. Limit the use of motion effects that incorporate rapid movement, bouncing or wave-like movement, zooming animations, multi-axis movement, spinning, or rotations. When the person wearing the device is sensitive to motion effects, eliminate the use of these effects altogether.\n\nThe Reduce Motion system setting lets you know when to provide alternatives for all of your app’s motion effects. Access this setting using the accessibilityReduceMotion environment variable in SwiftUI or with the isReduceMotionEnabled property in UIKit. When the setting is true, provide suitable alternatives for motion effects or eliminate them altogether. For example, show a static snapshot of the ocean instead of a video.\n\nFor more information, see Human Interface Guidelines > Motion.\n\nInclude captions for audio content\n\nFor people who are deaf or hard of hearing, provide high-quality captions for your app’s content. Captions are a necessity to some, but are practical for everyone in certain situations. For example, captions are useful to someone watching a video in a noisy environment. Remember to include captions not just for text and dialogue, but also for music and sound effects in your content. For Spatial Audio content, include information in your captions that indicates the direction of various sounds.\n\nAVKit and AVFoundation provide built-in support for displaying captioned content. These frameworks configure the font, size, color, and style of the captions automatically according to the person’s accessibility settings. For example, the frameworks adopt the current Dynamic Type setting when displaying text.\n\nIf you have a custom video engine, check the isClosedCaptioningEnabled accessibility setting to determine when to display captions. To get the correct appearance information for your captioned content, adopt Media Accessibility in your project. This framework provides you with the optimal font, color, and opacity information to apply to captioned text and images.\n\nSee Also\nDesign\nDesigning for visionOS\nWhen people wear Apple Vision Pro, they enter an infinite 3D space where they can engage with your app or game while staying connected to their surroundings.\nAdopting best practices for privacy and user preferences\nMinimize your use of sensitive information and provide a clear statement of what information you do use and how you use it."
  },
  {
    "title": "Drawing sharp layer-based content in visionOS | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/drawing-sharp-layer-based-content",
    "html": "Overview\n\nIf your app uses Core Animation layers directly, update your layer code to draw a high-resolution version of your content when appropriate. SwiftUI and UIKit views use Core Animation layers to manage interface content efficiently. When a view draws its content, the underlying layer captures that content and caches it to improve subsequent render operations.\n\nCore Animation on most Apple platforms rasterizes your layer at the same resolution as the screen, but Core Animation on visionOS can rasterize at different resolutions to maximize both content clarity and performance. The system follows the person’s eyes and renders content immediately in front of them at the highest possible resolution. Outside of this focal area, the system renders content at progressively lower resolutions to reduce GPU workloads. Because the content is in the person’s peripheral vision, these lower resolutions don’t impact the content’s clarity. As the person’s eyes move around, the system redraws content at different resolutions to match the change in focus.\n\nA figure at 2x resolution\n\nA figure at 8x resolution\n\nIf you deliver content using custom CALayer objects, you can configure your custom layers to support drawing at different resolutions. If you don’t perform this extra configuration step, each layer rasterizes its content at a @2x scale factor, which is good enough for most content and matches what the layer provides on a Retina display. However, if you opt in to drawing at different resolutions, the layer rasterizes its content at up to @8x scale factor in visionOS, which adds significant detail to text and vector-based content.\n\nRequest dynamic scaling for custom layers\n\nDynamic content scaling is off by default for all Core Animation layers, and frameworks or apps must turn on this support explicitly. If your interface uses only SwiftUI or UIKit views, you don’t need to do anything to support this feature. SwiftUI and UIKit enable it automatically for views that benefit from the added detail, such as text views and image views with SF Symbols or other vector-based artwork. However, the frameworks don’t enable the feature for all views, including UIView and View.\n\nIf your visionOS interface includes custom Core Animation layers, you can enable the wantsDynamicContentScaling property of any CALayer objects that contain vector-based content. Setting this property to true tells the system that you support rendering your layer’s content at different resolutions. However, the setting is not a guarantee that the system applies dynamic content scaling to your content. The system can disable the feature if your layer draws using incompatible functions or techniques.\n\nThe following example shows how to enable this feature for a CATextLayer object. After configuring the layer, set the wantsDynamicContentScaling property to true and add the layer to your layer hierarchy.\n\nlet layer = CATextLayer()\n\n\nlayer.string = \"Hello, World!\"\nlayer.foregroundColor = UIColor.black.cgColor\nlayer.frame = parentLayer.bounds\n\n\n// Setting this property to true enables content scaling \n// and calls setNeedsDisplay to redraw the layer's content.\nlayer.wantsDynamicContentScaling = true\n\n\nparentLayer.addSublayer(layer)\n\n\nDynamic content scaling works best when the layer contains text or vector-based content. Don’t enable the feature if you do any of the following in your layer:\n\nYou set the layer’s content using the contents property.\n\nYou draw primarily bitmap-based content.\n\nYou redraw your layer’s contents repeatedly over a short time period.\n\nThe CAShapeLayer class ignores the value of the wantsDynamicContentScaling property and always enables dynamic content scaling. For other Core Animation layers, you must enable the feature explicitly to take advantage of it.\n\nDraw the layer’s content dynamically\n\nDynamic content scaling requires you to draw your layer’s contents using one of the prescribed methods. If you define a custom subclass of CALayer, draw your layer’s content in the draw(in:) method. If you use a CALayerDelegate object to draw the layer’s content, use the delgate’s draw(_:in:) method instead.\n\nWhen you enable dynamic content scaling for a layer, the system captures your app’s drawing commands for playback later. As the person’s eyes move, the system draws the layer at higher resolutions when someone looks directly at it, or at lower resolutions otherwise. Because the redraw operations implicitly communicate what the person is looking at, the system performs them outside of your app’s process. Letting the system handle these operations maintains the person’s privacy while still giving your app the benefits of high-resolution drawing.\n\nSome Core Graphics routines are incompatible with dynamic content scaling. Even if you enable dynamic content scaling for your layer, the system automatically disables the feature if your layer uses any of the following:\n\nCore Graphics shaders.\n\nAPIs that set intent, quality, or other bitmap-related properties. For example, don’t call CGContextSetInterpolationQuality.\n\nA CGBitmapContext to draw content.\n\nIf your app creates timer-based animations, don’t animate layer changes using your drawing method. Calling setNeedsDisplay() on your layer repeatedly in a short time causes the system to draw the layer multiple times in quick succession. Because visionOS needs a little extra time to draw a layer at high resolution, each redraw request forces it to throw away work. A better option is to animate layer-based properties to achieve the same effect, or use a CAShapeLayer to animate paths when needed.\n\nModify layer hierarchies to improve performance\n\nThe backing store for a layer consumes more memory at higher resolutions than at lower resolutions. Measure your app’s memory usage before and after you enable dynamic content scaling to make sure the increased memory cost is worth the benefit. If your app’s memory usage increases too much, limit which layers adopt dynamic content scaling. You can also reduce the amount of memory each layer uses in the following ways:\n\nMake your layer the smallest size possible. Larger layers require significantly more memory, especially at higher resolutions. Make the size of the layer match the size of your content by eliminating padding or extra space.\n\nSeparate complex content into different layers. Instead of drawing everything in a single layer, build your content from multiple layers and arrange them hierarchically to achieve the same result. Enable dynamic content scaling only in the layers that actually need it.\n\nApply special effects using layer properties whenever possible. Applying effects during drawing might require you to increase the layer’s size. For example, apply scale and rotation effects to the layer’s transform property, instead of during drawing.\n\nDon’t draw your layer’s content at different resolutions in advance and cache the images. Maintaining multiple images requires more memory. If you do cache images, draw them only at @2x scale factor.\n\nDon’t use your drawing code to draw a single image. If your layer’s content consists of an image, assign that image to the layer’s contents property directly.\n\nComplex drawing code can also lead to performance issues. A layer with many strokes can render quickly at lower scale factors, but might be computationally too complex to render at larger scales. If a complex layer doesn’t render correctly at higher resolutions, turn off dynamic content scaling and measure the render times again.\n\nSee Also\nApp construction\nCreating your first visionOS app\nBuild a new visionOS app using SwiftUI and add platform-specific features.\nAdding 3D content to your app\nAdd depth and dimension to your visionOS app and discover how to incorporate your app’s content into a person’s surroundings.\nCreating fully immersive experiences in your app\nBuild fully immersive experiences by combining spaces with content you create using RealityKit or Metal."
  },
  {
    "title": "Adding 3D content to your app | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/adding-3d-content-to-your-app",
    "html": "Overview\n\nA device with a stereoscopic display lets people experience 3D content in a way that feels more real. Content appears to have real depth, and people can view it from different angles, making it seem like it’s there in front of them.\n\nWhen building an app for visionOS, think about ways you might add depth to your app’s interface. The system provides several ways to display 3D content, including in your existing windows, in a volume, and in an immersive space. Choose the options that work best for your app and the content you offer.\n\nWindow\n\nVolume\n\nImmersive space\n\nAdd depth to traditional 2D windows\n\nWindows are an important part of your app’s interface. With visionOS, apps automatically get materials with the visionOS look and feel, fully resizable windows with spacing tuned for eyes and hands input, and access to highlighting adjustments for your custom controls.\n\nPlay\n\nIncorporate depth effects into your custom views as needed, and use 3D layout options to arrange views in your windows.\n\nApply a shadow(color:radius:x:y:) or visualEffect(_:) modifier to the view.\n\nLift or highlight the view when someone looks at it using a hoverEffect(_:isEnabled:) modifier.\n\nLay out views using a ZStack.\n\nAnimate view-related changes with transform3DEffect(_:).\n\nRotate the view using a rotation3DEffect(_:axis:anchor:anchorZ:perspective:) modifier.\n\nIn addition to giving 2D views more depth, you can also add static 3D models to your 2D windows. The Model3D view loads a USDZ file or other asset type and displays it at its intrinsic size in your window. Use this in places where you already have the model data in your app, or can download it from the network. For example, a shopping app might use this type of view to display a 3D version of a product.\n\nDisplay dynamic 3D scenes using RealityKit\n\nRealityKit is Apple’s technology for building 3D models and scenes that you update dynamically onscreen. In visionOS, use RealityKit and SwiftUI together to seamlessly couple your app’s 2D and 3D content. Load existing USDZ assets or create scenes in Reality Composer Pro that incorporate animation, physics, lighting, sounds, and custom behaviors for your content. To use a Reality Composer Pro project in your app, add the Swift package to your Xcode project and import its module in your Swift file. For more information, see Managing files and folders in your Xcode project.\n\nWhen you’re ready to display 3D content in your interface, use a RealityView. This SwiftUI view serves as a container for your RealityKit content, and lets you update that content using familiar SwiftUI techniques.\n\nThe following example shows a view that uses a RealityView to display a 3D sphere. The code in the view’s closure creates a RealityKit entity for the sphere, applies a texture to the surface of the sphere, and adds the sphere to the view’s content.\n\n struct SphereView: View {\n    var body: some View {\n        RealityView { content in\n            let model = ModelEntity(\n                         mesh: .generateSphere(radius: 0.1),\n                         materials: [SimpleMaterial(color: .white, isMetallic: true)])\n            content.add(model)\n        }\n    }\n}\n\n\nWhen SwiftUI displays your RealityView, it executes your code once to create the entities and other content. Because creating entities is relatively expensive, the view runs your creation code only once. When you want to update the state of your entities, change the state of your view and use an update closure to apply those changes to your content. The following example uses an update closure to change the size of the sphere when the value in the scale property changes:\n\nstruct SphereView: View {\n    var scale = false\n\n\n    var body: some View {\n        RealityView { content in\n            let model = ModelEntity(\n                         mesh: .generateSphere(radius: 0.1),\n                         materials: [SimpleMaterial(color: .white, isMetallic: true)])\n            content.add(model)\n        } update: { content in\n            if let model = content.entities.first {\n                model.transform.scale = scale ? [1.2, 1.2, 1.2] : [1.0, 1.0, 1.0]\n            }\n        }\n    }\n}\n\n\nFor information about how to create content using RealityKit, see RealityKit.\n\nRespond to interactions with RealityKit content\n\nTo handle interactions with the entities of your RealityKit scenes:\n\nAttach a gesture recognizer to your RealityView and add the targetedToAnyEntity() modifier to it.\n\nAttach an InputTargetComponent to the entity or one of its parent entities.\n\nAdd collision shapes to the RealityKit entities that support interactions.\n\nThe targetedToAnyEntity() modifier provides a bridge between the gesture recognizer and your RealityKit content. For example, to recognize when someone drags an entity, specify a DragGesture and add the modifier to it. When the specified gesture occurs on an entity, SwiftUI executes the provided closure.\n\nThe following example adds a tap gesture recognizer to the sphere view from the previous example. The code also adds InputTargetComponent and CollisionComponent components to the shape to allow the interactions to occur. If you omit these components, the view doesn’t detect the interactions with your entity.\n\nstruct SphereView: View {\n    @State private var scale = false\n\n\n    var body: some View {\n        RealityView { content in\n            let model = ModelEntity(\n                mesh: .generateSphere(radius: 0.1),\n                materials: [SimpleMaterial(color: .white, isMetallic: true)])\n\n\n            // Enable interactions on the entity.\n            model.components.set(InputTargetComponent())\n            model.components.set(CollisionComponent(shapes: [.generateSphere(radius: 0.1)]))\n            content.add(model)\n        } update: { content in\n            if let model = content.entities.first {\n                model.transform.scale = scale ? [1.2, 1.2, 1.2] : [1.0, 1.0, 1.0]\n            }\n        }\n        .gesture(TapGesture().targetedToAnyEntity().onEnded { _ in\n            scale.toggle()\n        })\n    }\n}\n\nDisplay 3D content in a volume\n\nA volume is a type of window that grows in three dimensions to match the size of the content it contains. Windows and volumes both accommodate 2D and 3D content, and are alike in many ways. However, windows clip 3D content that extends too far from the window’s surface, so volumes are the better choice for content that is primarily 3D.\n\nTo create a volume, add a WindowGroup scene to your app and set its style to volumetric. This style tells SwiftUI to create a window for 3D content. Include any 2D or 3D views you want in your volume. You can also add a RealityView to build your content using RealityKit. The following example creates a volume with a static 3D model of some balloons stored in the app’s bundle:\n\nstruct MyApp: App {\n    var body: some Scene {\n        WindowGroup {\n            Model3D(\"balloons\")\n        }.windowStyle(style: .volumetric)\n    }\n}\n\n\nWindows and volumes are a convenient way to display bounded 2D and 3D content, but your app doesn’t control the placement of that content in the person’s surroundings. The system sets the initial position of each window and volume at display time. The system also adds a window bar to allow someone to reposition the window or resize it.\n\nFor more information about when to use volumes, see Human Interface Guidelines > Windows.\n\nDisplay 3D content in a person’s surroundings\n\nWhen you need more control over the placement of your app’s content, add that content to an ImmersiveSpace. An immersive space offers an unbounded area for your content, and you control the size and placement of content within the space. After receiving permission from the user, you can also use ARKit with an immersive space to integrate content into their surroundings. For example, you can use ARKit scene reconstruction to obtain a mesh of furniture and nearby objects and have your content interact with that mesh.\n\nAn ImmersiveSpace is a scene type that you create alongside your app’s other scenes. The following example shows an app that contains an immersive space and a window:\n\n@main\nstruct MyImmersiveApp: App {\n    var body: some Scene {\n        WindowGroup() {\n            ContentView()\n        }\n\n\n        ImmersiveSpace(id: \"solarSystem\") {\n            SolarSystemView()\n        }\n    }\n}\n\n\nIf you don’t add a style modifier to your ImmersiveSpace declaration, the system creates that space using the mixed style. This style displays your content together with the passthrough content that shows the person’s surroundings. Other styles let you hide passthrough to varying degrees. Use the immersionStyle(selection:in:) modifier to specify which styles your space supports. If you specify more than one style, you can toggle between the styles using the selection parameter of the modifier.\n\nWarning\n\nBe mindful of how much content you include in immersive scenes that use the mixed style. Content that fills a significant portion of the screen, even if that content is partially transparent, can prevent the person from seeing potential hazards in their surroundings. If you want to immerse the person in your content, configure your space with the full style. For more information, see, Creating fully immersive experiences in your app.\n\nRemember to set the position of items you place in an ImmersiveSpace. Position SwiftUI views using modifiers, and position a RealityKit entity using its transform component. SwiftUI places the origin of a space at a person’s feet initially, but can change this origin in response to other events. For example, the system might shift the origin to accommodate a SharePlay activity that displays your content with Spatial Personas. If you need to position SwiftUI views and RealityKit entities relative to one another, perform any needed coordinate conversions using the methods in the content parameter of RealityView.\n\nTo display your ImmersiveSpace scene, open it using the openImmersiveSpace action, which you obtain from the SwiftUI environment. This action runs asynchronously and uses the provided information to find and initialize your scene. The following example shows a button that opens the space with the solarSystem identifier:\n\nButton(\"Show Solar System\") {\n    Task {\n        let result = await openImmersiveSpace(id: \"solarSystem\")\n        if case .error = result {\n            print(\"An error occurred\")\n        }\n    }\n}\n\n\nWhen an app presents an ImmersiveSpace, the system hides the content of other apps to prevent visual conflicts. The other apps remain hidden while your space is visible but return when you dismiss it. If your app defines multiple spaces, you must dismiss the currently visible space before displaying a different space. If you don’t dismiss the visible space, the system issues a runtime warning when you try to open the other space.\n\nSee Also\nApp construction\nCreating your first visionOS app\nBuild a new visionOS app using SwiftUI and add platform-specific features.\nCreating fully immersive experiences in your app\nBuild fully immersive experiences by combining spaces with content you create using RealityKit or Metal.\nDrawing sharp layer-based content in visionOS\nDeliver text and vector images at multiple resolutions from custom Core Animation layers in visionOS."
  },
  {
    "title": "Creating fully immersive experiences in your app | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/creating-fully-immersive-experiences",
    "html": "Overview\n\nA fully immersive experience replaces everything the person sees with custom content you create. You might use this type of experience to:\n\nOffer a temporary transitional experience\n\nCreate a distraction-free space for your content\n\nImplement a virtual reality (VR) game\n\nPresent a virtual world to explore\n\nWith a fully immersive experience, you’re responsible for everything that appears onscreen. The system hides passthrough video and displays the content you provide, showing the person’s hands only when they come into view. To achieve the best performance, use RealityKit or Metal to create and animate your content.\n\nPlay\n\nTypically, you combine a fully immersive experience with other types of experiences and provide transitions between them. When you display a window first and then offer controls to enter your immersive experience, you give people time to prepare for the transition. It also gives them the option to skip the experience if they prefer to use your app’s windows instead.\n\nPlay\nPrepare someone for your app’s transitions\n\nGive people control over when they enter or exit fully immersive experiences, and provide clear transitions to and from those experiences. Clear visual transitions make it easier to adjust to such a large change. Sudden transitions might be disorienting, unpleasant, or make the person think something went wrong.\n\nAt launch time, display windows or other content that allows the person to see their surroundings. Add controls to that content to initiate the transition to the fully immersive experience, and provide a clear indication of what the controls do. Inside your experience, provide clear controls and instructions on how to exit the experience.\n\nWarning\n\nWhen you start a fully immersive experience, visionOS defines a system boundary that extends approximately 1.5 meters from the initial position of the person’s head. If their head moves outside of that zone, the system automatically stops the immersive experience and turns on the external video again. This feature is an assistant to help prevent someone from colliding with objects.\n\nFor guidelines on how to design fully immersive experiences, see Human Interface Guidelines.\n\nOpen an immersive space\n\nTo create a fully immersive experience, open an ImmersiveSpace and set its style to full. An immersive space is a type of SwiftUI scene that lets you place content anywhere in the person’s surroundings. Applying the full style to the scene tells the system to hide passthrough video and display only your app’s content.\n\nDeclare spaces in the body property of your app object, or anywhere you manage SwiftUI scenes. The following example shows an app with a main window and a fully immersive space. At launch time, the app displays the window.\n\n@main\nstruct MyImmersiveApp: App {\n    @State private var currentStyle: ImmersionStyle = .full\n\n\n    var body: some Scene {\n        WindowGroup() {\n            ContentView()\n        }\n\n\n        // Display a fully immersive space.\n        ImmersiveSpace(id: \"solarSystem\") {\n            SolarSystemView()\n        }.immersionStyle(selection: $currentStyle, in: .full)\n    }\n}\n\n\nTo display an ImmersiveSpace, open it using the openImmersiveSpace action, which you obtain from the SwiftUI environment. This action runs asynchronously and uses the provided information to find and initialize your scene. The following example shows a button that opens the space with the solarSystem identifier:\n\nButton(\"Show Solar System\") {\n    Task {\n        let result = await openImmersiveSpace(id: \"solarSystem\")\n        if case .error = result {\n            print(\"An error occurred\")\n        }\n    }\n}\n\n\nAn app can display only one space at a time, and it’s an error for you to try to open a space while another space is visible. To dismiss an open space, use the dismissImmersiveSpace action.\n\nFor more information about displaying spaces, see the ImmersiveSpace type.\n\nDraw your content using RealityKit\n\nRealityKit works well when your content consists of primitive shapes or existing content in USD files. Organize the contents of your scene using RealityKit entities, and animate that content using components and systems. Use Reality Composer Pro to assemble your content visually, and to attach dynamic shaders, animations, audio, and other behaviors to your content. Display the contents of your RealityKit scene in a RealityView in your scene.\n\nTo load a Reality Composer Pro scene at runtime, fetch the URL of your Reality Composer Pro package file, and load the root entity of your scene. The following example shows how to create the entity for a package located in the app’s bundle:\n\nimport MyRealityBundle\n\n\nlet url = MyRealityBundle.bundle.url(forResource:\n         \"MyRealityBundle\", withExtension: \"reality\")\nlet scene = try await Entity(contentsOf: url)\n\n\nFor more information about how to display content in a RealityView and manage interactions with your content, see Adding 3D content to your app.\n\nDraw your content using Metal\n\nAnother option for creating fully immersive scenes is to draw everything yourself using Metal. When using Metal to draw your content, use the Compositor Services framework to place that content onscreen. Compositor Services provides the code you need to set up your Metal rendering engine and start drawing.\n\nFor details on how to render content using Metal and Compositor Services, and manage interactions with your content, see Drawing fully immersive content using Metal.\n\nSee Also\nApp construction\nCreating your first visionOS app\nBuild a new visionOS app using SwiftUI and add platform-specific features.\nAdding 3D content to your app\nAdd depth and dimension to your visionOS app and discover how to incorporate your app’s content into a person’s surroundings.\nDrawing sharp layer-based content in visionOS\nDeliver text and vector images at multiple resolutions from custom Core Animation layers in visionOS."
  },
  {
    "title": "Creating your first visionOS app | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionos/creating-your-first-visionos-app",
    "html": "Overview\n\nIf you’re new to visionOS, start with a new Xcode project to learn about the platform features, and to familiarize yourself with visionOS content and techniques. When you build an app for visionOS, SwiftUI is an excellent choice because it gives you full access to visionOS features. Although you can also use UIKit to build portions of your app, you need to use SwiftUI for many features that are unique to the platform.\n\nNote\n\nDeveloping for visionOS requires a Mac with Apple silicon.\n\nPlay\n\nIn any SwiftUI app, you place content onscreen using scenes. A scene contains the views and controls to display onscreen. Scenes also define the appearance of those views and controls when they appear onscreen. In visionOS, you can include both 2D and 3D views in the same scene, and you can present those views in a window or as part of the person’s surroundings.\n\nScene with a window\n\nScene with a window and 3D objects\n\nStart with a new Xcode project and add features to familiarize yourself with visionOS content and techniques. Run your app in Simulator to verify your content looks like you expect, and run it on device to see your 3D content come to life.\n\nOrganize your content around one or more scenes, which manage your app’s interface. Each scene contains the views and controls you want to display, and the scene type determines whether your content adopts a 2D or 3D appearance. SwiftUI adds 3D scene types specifically for visionOS, and also adds 3D elements and layout options for all scene types.\n\nCreate your Xcode project\n\nCreate a new project in Xcode by choosing File > New > Project. Navigate to the visionOS section of the template chooser, and choose the App template. When prompted, specify a name for your project along with other options.\n\nWhen creating a new visionOS app, you can configure your app’s initial scene types from the configuration dialog. To display primarily 2D content in your initial scene, choose a Window as your initial scene type. For primarily 3D content, choose a Volume. You can also add an immersive scene to place your content in the person’s surroundings.\n\nInclude a Reality Composer Pro project file when you want to create 3D assets or scenes to display from your app. Use this project file to build content from primitive shapes and existing USDZ assets. You can also use it to build and test custom RealityKit animations and behaviors for your content.\n\nModify the existing window\n\nBuild your initial interface using standard SwiftUI views. Views provide the basic content for your interface, and you customize the appearance and behavior of them using SwiftUI modifiers. For example, the .background modifier adds a partially transparent tint color behind your content:\n\n@main\nstruct MyApp: App {\n    var body: some Scene {\n        WindowGroup {\n            ContentView()\n               .background(.black.opacity(0.8))\n        }\n\n\n        ImmersiveSpace(id: \"Immersive\") {\n            ImmersiveView()\n        }\n    }\n}\n\n\nTo learn more about how to create and configure interfaces using SwiftUI, see SwiftUI Essentials.\n\nHandle events in your views\n\nMany SwiftUI views handle interactions automatically — all you do is provide code to run when the interactions occur. You can also add SwiftUI gesture recognizers to a view to handle tap, long-press, drag, rotate, and zoom gestures. The system automatically maps the following types of input to your SwiftUI event-handling code:\n\nIndirect input. The person’s eyes indicate the target of an interaction. To start the interaction, the person touches their thumb and forefinger together on one or both hands. Additional finger and hand movements define the gesture type.\n\nDirect input. When a person’s finger occupies the same space as an onscreen item, the system reports an interaction. Additional finger and hand movements define the gesture type.\n\nKeyboard input. People can use a connected mouse, trackpad, or keyboard to interact with items, trigger menu commands, and perform gestures.\n\nFor more information about handling interactions in SwiftUI views, see Handling User Input in the SwiftUI Essentials tutorial.\n\nBuild and run your app\n\nBuild and run your app in Simulator to see how it looks. Simulator for visionOS has a virtual background as the backdrop for your app’s content. Use your keyboard and your mouse or trackpad to navigate around the environment and interact with your app.\n\nTap and drag the window bar below your app’s content to reposition the window in the environment. Move the pointer over the circle next to the window bar to reveal the window’s close button. Move the cursor to one of the window’s corners to turn the window bar into a resizing control.\n\nNote\n\nApps don’t control the placement of windows in the space. The system places each window in its initial position, and updates that position based on further interactions with the app.\n\nFor additional information about how to interact with your app in Simulator, see Interacting with your app in the visionOS simulator.\n\nSee Also\nApp construction\nAdding 3D content to your app\nAdd depth and dimension to your visionOS app and discover how to incorporate your app’s content into a person’s surroundings.\nCreating fully immersive experiences in your app\nBuild fully immersive experiences by combining spaces with content you create using RealityKit or Metal.\nDrawing sharp layer-based content in visionOS\nDeliver text and vector images at multiple resolutions from custom Core Animation layers in visionOS."
  },
  {
    "title": "visionOS | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/visionOS",
    "html": "Overview\n\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\n\nExpand your app into immersive spaces\n\nStart with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\n\nBuild your app’s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\n\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures.\n\nTap to select\n\nPinch to rotate\n\nManipulate objects\n\nCreate custom gestures\n\nDive into featured sample apps\n\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\n\nHello World\nUse windows, volumes, and immersive spaces to teach people about the Earth.\nView sample code\nDestination Video\nLeverage 3D video and Spatial Audio to deliver an immersive experience.\nView sample code\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nView sample code\nDiorama\nDesign scenes for your visionOS app using Reality Composer Pro.\nView sample code\nSwift Splash\nUse RealityKit to create an interactive ride in visionOS.\nView sample code\nTopics\nApp construction\nCreating your first visionOS app\nBuild a new visionOS app using SwiftUI and add platform-specific features.\nAdding 3D content to your app\nAdd depth and dimension to your visionOS app and discover how to incorporate your app’s content into a person’s surroundings.\nCreating fully immersive experiences in your app\nBuild fully immersive experiences by combining spaces with content you create using RealityKit or Metal.\nDrawing sharp layer-based content in visionOS\nDeliver text and vector images at multiple resolutions from custom Core Animation layers in visionOS.\nDesign\nDesigning for visionOS\nWhen people wear Apple Vision Pro, they enter an infinite 3D space where they can engage with your app or game while staying connected to their surroundings.\nAdopting best practices for privacy and user preferences\nMinimize your use of sensitive information and provide a clear statement of what information you do use and how you use it.\nImproving accessibility support in your visionOS app\nUpdate your code to ensure everyone can access your app’s content in visionOS.\nSwiftUI\nHello World\nUse windows, volumes, and immersive spaces to teach people about the Earth.\nPresenting windows and spaces\nOpen and close the scenes that make up your app’s interface.\nPositioning and sizing windows\nInfluence the initial geometry of windows that your app presents.\nRealityKit and Reality Composer Pro\nSwift Splash\nUse RealityKit to create an interactive ride in visionOS.\nDiorama\nDesign scenes for your visionOS app using Reality Composer Pro.\nUnderstanding RealityKit’s modular architecture\nLearn how everything fits together in RealityKit.\nDesigning RealityKit content with Reality Composer Pro\nDesign RealityKit scenes for your visionOS app.\nCapturing screenshots and video from Apple Vision Pro for 2D viewing\nCreate screenshots and record high-quality video of your visionOS app and its surroundings for app previews.\nARKit\nHappy Beam\nLeverage a Full Space to create a fun game using ARKit.\nSetting up access to ARKit data\nCheck whether your app can use ARKit and respect people’s privacy.\nIncorporating real-world surroundings in an immersive experience\nCreate an immersive experience by making your app’s content respond to the local shape of the world.\nPlacing content on detected planes\nDetect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.\nTracking specific points in world space\nRetrieve the position and orientation of anchors your app stores in ARKit.\nTracking preregistered images in 3D space\nPlace content based on the current position of a known image in a person’s surroundings.\nVideo playback\nDestination Video\nLeverage 3D video and Spatial Audio to deliver an immersive experience.\nConfiguring your app for media playback\nConfigure apps to enable standard media playback behavior.\nAdopting the system player interface in visionOS\nProvide an optimized viewing experience for watching 3D video content.\nControlling the transport behavior of a player\nPlay, pause, and seek through a media presentation.\nMonitoring playback progress in your app\nObserve the playback of a media asset to update your app’s user-interface state.\nTrimming and exporting media in visionOS\nDisplay standard controls in your app to edit the timeline of the currently playing media.\nXcode and Instruments\nDiagnosing and resolving bugs in your running app\nInspect your app to isolate bugs, locate crashes, identify excess system-resource usage, visualize memory bugs, and investigate problems in its appearance.\nDiagnosing issues in the appearance of a running app\nInspect your running app to investigate issues in the appearance and placement of the content it displays.\nCreating a performance plan for your visionOS app\nIdentify your app’s performance and power goals and create a plan to measure and assess them.\nAnalyzing the performance of your visionOS app\nUse the RealityKit Trace template in Instruments to evaluate and improve the performance of your visionOS app.\nConfiguring your app icon\nAdd app icon variations to represent your app in places such as Settings, search results, and the App Store.\nSimulator\nRunning your app in Simulator or on a device\nLaunch your app in a simulated iOS, tvOS, watchOS, or visionOS device, or on a device connected to a Mac.\nInteracting with your app in the visionOS simulator\nUse your Mac to navigate spaces and control interactions with your visionOS apps in Simulator.\niOS migration and compatibility\nBringing your existing apps to visionOS\nBuild a version of your iPadOS or iOS app using the visionOS SDK, and update your code for platform differences.\nBringing your ARKit app to visionOS\nUpdate an iPadOS or iOS app that uses ARKit, and provide an equivalent experience in visionOS.\nChecking whether your existing app is compatible with visionOS\nDetermine whether your existing iOS or iPadOS app runs as is in visionOS or requires modifications to handle platform differences.\nMaking your existing app compatible with visionOS\nModify your iPadOS or iOS app to run successfully in visionOS."
  }
]
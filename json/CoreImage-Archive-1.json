[
  {
    "title": "Glossary",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_glossary/opengl_glossary.html#//apple_ref/doc/uid/TP40001987-CH502-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nGlossary\n\nThis glossary contains terms that are used specifically for the Apple implementation of OpenGL and a few terms that are common in graphics programming. For definitions of additional OpenGL terms, see OpenGL Programming Guide, by the Khronos OpenGL Working Group\n\naliased  \n\nSaid of graphics whose edges appear jagged; can be remedied by performing antialiasing operations.\n\n\n\nantialiasing  \n\nIn graphics, a technique used to smooth and soften the jagged (or aliased) edges that are sometimes apparent when graphical objects such as text, line art, and images are drawn.\n\n\n\nARB  \n\nThe Khronos OpenGL Working Group, which is the group that oversees the OpenGL specification and extensions to it.\n\n\n\nattach  \n\nTo establish a connection between two existing objects. Compare bind.\n\n\n\nbind  \n\nTo create a new object and then establish a connection between that object and a rendering context. Compare attach.\n\n\n\nbitmap  \n\nA rectangular array of bits.\n\n\n\nbitplane  \n\nA rectangular array of pixels.\n\n\n\nbuffer  \n\nA block of memory dedicated to storing a specific kind of data, such as depth values, green color values, stencil index values, and color index values.\n\n\n\nCGL (Core OpenGL) framework  \n\nThe Apple framework for using OpenGL graphics in OS X applications that need low-level access to OpenGL.\n\n\n\nclipping  \n\nAn operation that identifies the area of drawing. Anything not in the clipping region is not drawn.\n\n\n\nclip coordinates  \n\nThe coordinate system used for view-volume clipping. Clip coordinates are applied after applying the projection matrix and prior to perspective division.\n\n\n\ncolor lookup table  \n\nA table of values used to map color indexes into actual color values.\n\n\n\ncompleteness  \n\nA state that indicates whether a framebuffer object meets all the requirements for drawing.\n\n\n\ncontext  \n\nA set of OpenGL state variables that affect how drawing is performed for a drawable object attached to that context. Also called a rendering context.\n\n\n\nculling  \n\nEliminating parts of a scene that can't be seen by the observer.\n\n\n\ncurrent context  \n\nThe rendering context to which OpenGL routes commands issued by your application.\n\n\n\ncurrent matrix  \n\nA matrix used by OpenGL to transform coordinates in one system to those of another system, such as the modelview matrix, the perspective matrix, and the texture matrix. GL shading language allows user-defined matrices.\n\n\n\ndepth  \n\nIn OpenGL, refers to the z coordinate and specifies how far a pixel lies from the observer.\n\n\n\ndepth buffer  \n\nA block of memory used to store a depth value for each pixel. The depth buffer is used to determine whether or not a pixel can be seen by the observer. Those that are hidden are typically removed.\n\n\n\ndisplay list  \n\nA list of OpenGL commands that have an associated name and that are uploaded to the GPU, preprocessed, and then executed at a later time. Display lists are often used for computing-intensive commands.\n\n\n\ndouble buffering  \n\nThe practice of using a front and back color buffer to achieve smooth animation. The back buffer is not displayed, but swapped with the front buffer.\n\n\n\ndrawable object  \n\nIn OS X, an object allocated outside of OpenGL that can serve as an OpenGL framebuffer. A drawable object can be any of the following: a window, a view, a pixel buffer, offscreen memory, or a full-screen graphics device. See also framebuffer object\n\n\n\nextension  \n\nA feature of OpenGL that's not part of the OpenGL core API and therefore not guaranteed to be supported by every implementation of OpenGL. The naming conventions used for extensions indicate how widely accepted the extension is. The name of an extension supported only by a specific company includes an abbreviation of the company name. If more then one company adopts the extension, the extension name is changed to include EXT instead of a company abbreviation. If the Khronos OpenGL Working Group approves an extension, the extension name changes to include ARB instead of EXT or a company abbreviation.\n\n\n\neye coordinates  \n\nThe coordinate system with the observer at the origin. Eye coordinates are produced by the modelview matrix and passed to the projection matrix.\n\n\n\nfence  \n\nA token used by the GL_APPLE_fence extension to determine whether a given command has completed or not.\n\n\n\nfiltering  \n\nA process that modifies an image by combining pixels or texels.\n\n\n\nfog  \n\nAn effect achieved by fading colors to a background color based on the distance from the observer. Fog provides depth cues to the observer.\n\n\n\nfragment  \n\nThe color and depth values for a single pixel; can also include texture coordinate values. A fragment is the result of rasterizing primitives.\n\n\n\nframebuffer  \n\nThe collection of buffers associated with a window or a rendering context.\n\n\n\nframebuffer attachable image  \n\nThe rendering destination for a framebuffer object.\n\n\n\nframebuffer object  \n\nAn OpenGL extension that allows rendering to a destination other than the usual OpenGL buffers or destinations provided by the windowing system. A framebuffer object (FBO) contains state information for the OpenGL framebuffer and its set of images. A framebuffer object is similar to a drawable object, except that a drawable object is a window-system specific object whereas a framebuffer object is a window-agnostic object. The context that's bound to a framebuffer object can be bound to a window-system-provided drawable object for the purpose of displaying the content associated with the framebuffer object.\n\n\n\nfrustum  \n\nThe region of space that is seen by the observer and that is warped by perspective division.\n\n\n\nFSAA (full scene antialiasing)  \n\nA technique that takes multiple samples at a pixel and combines them with coverage values to arrive at a final fragment.\n\n\n\ngamma correction  \n\nA function that changes color intensity values to correct for the nonlinear response of the eye or of a display.\n\n\n\nGLU  \n\nGraphics library utilities.\n\n\n\nGL  \n\nGraphics library.\n\n\n\nGLUT  \n\nGraphics Library Utilities Toolkit, which is independent of the window system. In OS X, GLUT is implemented on top of Cocoa.\n\n\n\nGLX  \n\nAn OpenGL extension that supports using OpenGL within a window provided by the X Window system.\n\n\n\nimage  \n\nA rectangular array of pixels.\n\n\n\nimmediate mode  \n\nThe practice of OpenGL executing commands at the time an application issues them. To prevent commands from being issued immediately, an application can use a display list.\n\n\n\ninterleaved data  \n\nArrays of dissimilar data that are grouped together, such as vertex data and texture coordinates. Interleaving can speed data retrieval.\n\n\n\nmipmaps  \n\nA set of texture maps, provided at various resolutions, whose purpose is to minimize artifacts that can occur when a texture is applied to a geometric primitive whose onscreen resolution doesn't match the source texture map. Mipmapping derives from the latin phrase multum in parvo, which means \"many things in a small place.\"\n\n\n\nmodelview matrix  \n\nA 4 X 4 matrix used by OpenGL to transforms points, lines, polygons, and positions from object coordinates to eye coordinates.\n\n\n\nmutex  \n\nA mutual exclusion object in a multithreaded application.\n\n\n\nNURBS (nonuniform rational basis spline)  \n\nA methodology use to specify parametric curves and surfaces.\n\n\n\npacking  \n\nConverting pixel color components from a buffer into the format needed by an application.\n\n\n\npbuffer  \n\nSee pixel buffer.\n\n\n\npixel  \n\nA picture element; the smallest element that the graphics hardware can display on the screen. A pixel is made up of all the bits at the location x, y, in all the bitplanes in the framebuffer.\n\n\n\npixel buffer  \n\nA type of drawable object that allows the use of offscreen buffers as sources for OpenGL texturing. Pixel buffers allow hardware-accelerated rendering to a texture.\n\n\n\npixel depth  \n\nThe number of bits per pixel in a pixel image.\n\n\n\npixel format  \n\nA format used to store pixel data in memory. The format describes the pixel components (that is, red, blue, green, alpha), the number and order of components, and other relevant information, such as whether a pixel contains stencil and depth values.\n\n\n\nprimitives  \n\nThe simplest elements in OpenGL—points, lines, polygons, bitmaps, and images.\n\n\n\nprojection matrix  \n\nA matrix that OpenGL uses to transform points, lines, polygons, and positions from eye coordinates to clip coordinates.\n\n\n\nrasterization  \n\nThe process of converting vertex and pixel data to fragments, each of which corresponds to a pixel in the framebuffer.\n\n\n\nrenderbuffer  \n\nA rendering destination for a 2D pixel image, used for generalized offscreen rendering, as defined in the OpenGL specification for the GL_EXT_framebuffer_object extension.\n\n\n\nrenderer  \n\nA combination of hardware and software that OpenGL uses to create an image from a view and a model. The hardware portion of a renderer is associated with a particular display device and supports specific capabilities, such as the ability to support a certain color depth or buffering mode. A renderer that uses only software is called a software renderer and is typically used as a fallback.\n\n\n\nrendering context  \n\nA container for state information.\n\n\n\nrendering pipeline  \n\nThe order of operations used by OpenGL to transform pixel and vertex data to an image in the framebuffer.\n\n\n\nrender-to-texture  \n\nAn operation that draws content directly to a texture target.\n\n\n\nRGBA  \n\nRed, green, blue, and alpha color components.\n\n\n\nshader  \n\nA program that computes surface properties.\n\n\n\nshading language  \n\nA high-level language, accessible in C, used to produce advanced imaging effects.\n\n\n\nstencil buffer  \n\nMemory used specifically for stencil testing. A stencil test is typically used to identify masking regions, to identify solid geometry that needs to be capped, and to overlap translucent polygons.\n\n\n\nsurface  \n\nThe internal representation of a single buffer that OpenGL actually draws to and reads from. For windowed drawable objects, this surface is what the OS X window server uses to composite OpenGL content on the desktop.\n\n\n\ntearing  \n\nA visual anomaly caused when part of the current frame overwrites previous frame data in the framebuffer before the current frame is fully rendered on the screen.\n\n\n\ntessellation  \n\nAn operation that reduces a surface to a mesh of polygons, or a curve to a sequence of lines.\n\n\n\ntexel  \n\nA texture element used to specify the color to apply to a fragment.\n\n\n\ntexture  \n\nImage data used to modify the color of rasterized fragments; can be one-, two-, or three- dimensional or be a cube map.\n\n\n\ntexture mapping  \n\nThe process of applying a texture to a primitive.\n\n\n\ntexture matrix  \n\nA 4 x 4 matrix that OpenGL uses to transform texture coordinates to the coordinates that are used for interpolation and texture lookup.\n\n\n\ntexture object  \n\nAn opaque data structure used to store all data related to a texture. A texture object can include such things as an image, a mipmap, and texture parameters (width, height, internal format, resolution, wrapping modes, and so forth).\n\n\n\nvertex  \n\nA three-dimensional point. A set of vertices specify the geometry of a shape. Vertices can have a number of additional attributes such as color and texture coordinates. See vertex array.\n\n\n\nvertex array  \n\nA data structure that stores a block of data that specifies such things as vertex coordinates, texture coordinates, surface normals, RGBA colors, color indices, and edge flags.\n\n\n\nvirtual screen  \n\nA combination of hardware, renderer, and pixel format that OpenGL selects as suitable for an imaging task. When the current virtual screen changes, the current renderer typically changes.\n\n\n\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Document Revision History",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLProfilerUserGuide/RevisionHistory.html#//apple_ref/doc/uid/TP40006475-CH9999-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Profiler User Guide\nTable of Contents\nIntroduction\nGetting Started\nUsing Breakpoints\nIdentifying and Solving Performance Issues\nControlling Profiling Programmatically\nRevision History\nPrevious\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nDocument Revision History\n\nThis table describes the changes to OpenGL Profiler User Guide.\n\nDate\tNotes\n2015-03-09\t\n\nMoved to Retired Documents Library.\n\n\n2012-02-16\t\n\nUpdated links.\n\n\n2008-02-08\t\n\nFixed a link.\n\n\n2007-12-11\t\n\nNew document that explains how to assess the efficiency of an OpenGL application.\n\n\n\nPrevious\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "Optimizing OpenGL for High Resolution",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/EnablingOpenGLforHighResolution/EnablingOpenGLforHighResolution.html#//apple_ref/doc/uid/TP40001987-CH1001-SW4",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nOptimizing OpenGL for High Resolution\n\nOpenGL is a pixel-based API so the NSOpenGLView class does not provide high-resolution surfaces by default. Because adding more pixels to renderbuffers has performance implications, you must explicitly opt in to support high-resolution screens. It’s easy to enable high-resolution backing for an OpenGL view. When you do, you’ll want to perform a few additional tasks to ensure the best possible high-resolution experience for your users.\n\nEnable High-Resolution Backing for an OpenGL View\n\nYou can opt in to high resolution by calling the method setWantsBestResolutionOpenGLSurface: when you initialize the view, and supplying YES as an argument:\n\n[self  setWantsBestResolutionOpenGLSurface:YES];\n\nIf you don’t opt in, the system magnifies the rendered results.\n\nThe wantsBestResolutionOpenGLSurface property is relevant only for views to which an NSOpenGLContext object is bound. Its value does not affect the behavior of other views. For compatibility, wantsBestResolutionOpenGLSurface defaults to NO, providing a 1-pixel-per-point framebuffer regardless of the backing scale factor for the display the view occupies. Setting this property to YES for a given view causes AppKit to allocate a higher-resolution framebuffer when appropriate for the backing scale factor and target display.\n\nTo function correctly with wantsBestResolutionOpenGLSurface set to YES, a view must perform correct conversions between view units (points) and pixel units as needed. For example, the common practice of passing the width and height of [self bounds] to glViewport() will yield incorrect results at high resolution, because the parameters passed to the glViewport() function must be in pixels. As a result, you’ll get only partial instead of complete coverage of the render surface. Instead, use the backing store bounds:\n\n [self convertRectToBacking:[self bounds]];\n\nYou can also opt in to high resolution by enabling the Supports Hi-Res Backing setting for the OpenGL view in Xcode, as shown in Figure 3-1.\n\nFigure 3-1  Enabling high-resolution backing for an OpenGL view\nSet Up the Viewport to Support High Resolution\n\nThe viewport dimensions are in pixels relative to the OpenGL surface. Pass the width and height to glViewPort and use 0,0 for the x and y offsets. Listing 3-1 shows how to get the view dimensions in pixels and take the backing store size into account.\n\nListing 3-1  Setting up the viewport for drawing\n\n- (void)drawRect:(NSRect)rect   // NSOpenGLView subclass\n\n\n{\n\n\n    // Get view dimensions in pixels\n\n\n    NSRect backingBounds = [self convertRectToBacking:[self bounds]];\n\n\n \n\n\n    GLsizei backingPixelWidth  = (GLsizei)(backingBounds.size.width),\n\n\n            backingPixelHeight = (GLsizei)(backingBounds.size.height);\n\n\n \n\n\n    // Set viewport\n\n\n    glViewport(0, 0, backingPixelWidth, backingPixelHeight);\n\n\n \n\n\n    // draw…\n\n\n}\n\nYou don’t need to perform rendering in pixels, but you do need to be aware of the coordinate system you want to render in. For example, if you want to render in points, this code will work:\n\nglOrtho(NSWidth(bounds), NSHeight(bounds),...)\nAdjust Model and Texture Assets\n\nIf you opt in to high-resolution drawing, you also need to adjust the model and texture assets of your app. For example, when running on a high-resolution display, you might want to choose larger models and more detailed textures to take advantage of the increased number of pixels. Conversely, on a standard-resolution display, you can continue to use smaller models and textures.\n\nIf you create and cache textures when you initialize your app, you might want to consider a strategy that accommodates changing the texture based on the resolution of the display.\n\nCheck for Calls Defined in Pixel Dimensions\n\nThese functions use pixel dimensions:\n\nglViewport (GLint x, GLint y, GLsizei width, GLsizei height)\n\nglScissor (GLint x, GLint y, GLsizei width, GLsizei height)\n\nglReadPixels (GLint x, GLint y, GLsizei width, GLsizei height, ...)\n\nglLineWidth (GLfloat width)\n\nglRenderbufferStorage (..., GLsizei width, GLsizei height)\n\nglTexImage2D (..., GLsizei width, GLsizei height, ...)\n\nTune OpenGL Performance for High Resolution\n\nPerformance is an important factor when determining whether to support high-resolution content. The quadrupling of pixels that occurs when you opt in to high resolution requires more work by the fragment processor. If your app performs many per-fragment calculations, the increase in pixels might reduce its frame rate. If your app runs significantly slower at high resolution, consider the following options:\n\nOptimize fragment shader performance. (See Tuning Your OpenGL Application.)\n\nChoose a simpler algorithm to implement in your fragment shader. This reduces the quality of each individual pixel to allow for rendering the overall image at a higher resolution.\n\nUse a fractional scale factor between 1.0 and 2.0. A scale factor of 1.5 provides better quality than a scale factor of 1.0, but it needs to fill fewer pixels than an image scaled to 2.0.\n\nMultisampling antialiasing can be costly with marginal benefit at high resolution. If you are using it, you might want to reconsider.\n\nThe best solution depends on the needs of your OpenGL app; you should test more than one of these options and choose the approach that provides the best balance between performance and image quality.\n\nUse a Layer-Backed View to Overlay Text on OpenGL Content\n\nWhen you draw standard controls and Cocoa text to a layer-backed view, the system handles scaling the contents of that layer for you. You need to perform only a few steps to set and use the layer. Compare the controls and text in standard and high resolutions, as shown in Figure 3-2. The text looks the same on both without any additional work on your part.\n\nFigure 3-2  A text overlay scales automatically for standard resolution (left) and high resolution (right)\nTo set up a layer-backed view for OpenGL content\nUse an Application Window for Fullscreen Operation\n\nFor the best user experience, if you want your app to run full screen, create a window that covers the entire screen. This approach offers two advantages:\n\nThe system provides optimized context performance.\n\nUsers will be able to see critical system dialogs above your content.\n\nYou should avoid changing the display mode of the system.\n\nConvert the Coordinate Space When Hit Testing\n\nAlways convert window event coordinates when performing hit testing in OpenGL. The locationInWindow method of the NSEvent class returns the receiver’s location in the base coordinate system of the window. You then need to call the convertPoint:fromView: method to get the local coordinates for the OpenGL view.\n\nNSPoint aPoint = [theEvent locationInWindow];\n\n\nNSPoint localPoint = [myOpenGLView convertPoint:aPoint fromView:nil];\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "OpenGL on the Mac Platform",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_pg_concepts/opengl_pg_concepts.html#//apple_ref/doc/uid/TP40001987-CH208-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nOpenGL on the Mac Platform\n\nYou can tell that Apple has an implementation of OpenGL on its platform by looking at the user interface for many of the applications that are installed with OS X. The reflections built into iChat (Figure 1-1) provide one of the more notable examples. The responsiveness of the windows, the instant results of applying an effect in iPhoto, and many other operations in OS X are due to the use of OpenGL. OpenGL is available to all Macintosh applications.\n\nOpenGL for OS X is implemented as a set of frameworks that contain the OpenGL runtime engine and its drawing software. These frameworks use platform-neutral virtual resources to free your programming as much as possible from the underlying graphics hardware. OS X provides a set of application programming interfaces (APIs) that Cocoa applications can use to support OpenGL drawing.\n\nFigure 1-1  OpenGL provides the reflections in iChat\n\nThis chapter provides an overview of OpenGL and the interfaces your application uses on the Mac platform to tap into it.\n\nOpenGL Concepts\n\nTo understand how OpenGL fits into OS X and your application, you should first understand how OpenGL is designed.\n\nOpenGL Implements a Client-Server Model\n\nOpenGL uses a client-server model, as shown in Figure 1-2. When your application calls an OpenGL function, it talks to an OpenGL client. The client delivers drawing commands to an OpenGL server. The nature of the client, the server, and the communication path between them is specific to each implementation of OpenGL. For example, the server and clients could be on different computers, or they could be different processes on the same computer.\n\nFigure 1-2  OpenGL client-server model\n\nA client-server model allows the graphics workload to be divided between the client and the server. For example, all Macintosh computers ship with dedicated graphics hardware that is optimized to perform graphics calculations in parallel. Figure 1-3 shows a common arrangement of CPUs and GPUs. With this hardware configuration, the OpenGL client executes on the CPU and the server executes on the GPU.\n\nFigure 1-3  Graphics platform model\nOpenGL Commands Can Be Executed Asynchronously\n\nA benefit of the OpenGL client-server model is that the client can return control to the application before the command has finished executing. An OpenGL client may also buffer or delay execution of OpenGL commands. If OpenGL required all commands to complete before returning control to the application, then either the CPU or the GPU would be idle waiting for the other to provide it data, resulting in reduced performance.\n\nSome OpenGL commands implicitly or explicitly require the client to wait until some or all previously submitted commands have completed. OpenGL applications should be designed to reduce the frequency of client-server synchronizations. See OpenGL Application Design Strategies for more information on how to design your OpenGL application.\n\nOpenGL Commands Are Executed In Order\n\nOpenGL guarantees that commands are executed in the order they are received by OpenGL.\n\nOpenGL Copies Client Data at Call-Time\n\nWhen an application calls an OpenGL function, the OpenGL client copies any data provided in the parameters before returning control to the application. For example, if a parameter points at an array of vertex data stored in application memory, OpenGL must copy that data before returning. Therefore, an application is free to change memory it owns regardless of calls it makes to OpenGL.\n\nThe data that the client copies is often reformatted before it is transmitted to the server. Copying, modifying, and transmitting parameters to the server adds overhead to calling OpenGL. Applications should be designed to minimize copy overhead.\n\nOpenGL Relies on Platform-Specific Libraries For Critical Functionality\n\nOpenGL provides a rich set of cross-platform drawing commands, but does not define functions to interact with an operating system’s graphics subsystem. Instead, OpenGL expects each implementation to define an interface to create rendering contexts and associate them with the graphics subsystem. A rendering context holds all of the data stored in the OpenGL state machine. Allowing multiple contexts allows the state in one machine to be changed by an application without affecting other contexts.\n\nAssociating OpenGL with the graphic subsystem usually means allowing OpenGL content to be rendered to a specific window. When content is associated with a window, the implementation creates whatever resources are required to allow OpenGL to render and display images.\n\nOpenGL in OS X\n\nOpenGL in OS X implements the OpenGL client-server model using a common OpenGL framework and plug-in drivers. The framework and driver combine to implement the client portion of OpenGL, as shown in Figure 1-4. Dedicated graphics hardware provides the server. Although this is the common scenario, Apple also provides a software renderer implemented entirely on the CPU.\n\nFigure 1-4  MacOS X OpenGL driver model\n\nOS X supports a display space that can include multiple dissimilar displays, each driven by different graphics cards with different capabilities. In addition, multiple OpenGL renderers can drive each graphics card. To accommodate this versatility, OpenGL for OS X is segmented into well-defined layers: a window system layer, a framework layer, and a driver layer, as shown in Figure 1-5. This segmentation allows for plug-in interfaces to both the window system layer and the framework layer. Plug-in interfaces offer flexibility in software and hardware configuration without violating the OpenGL standard.\n\nFigure 1-5  Layers of OpenGL for OS X\n\nThe window system layer is an OS X–specific layer that your application uses to create OpenGL rendering contexts and associate them with the OS X windowing system. The NSOpenGL classes and Core OpenGL (CGL) API also provide some additional controls for how OpenGL operates on that context. See OpenGL APIs Specific to OS X for more information. Finally, this layer also includes the OpenGL libraries—GL, GLU, and GLUT. (See Apple-Implemented OpenGL Libraries for details.)\n\nThe common OpenGL framework layer is the software interface to the graphics hardware. This layer contains Apple's implementation of the OpenGL specification.\n\nThe driver layer contains the optional GLD plug-in interface and one or more GLD plug-in drivers, which may have different software and hardware support capabilities. The GLD plug-in interface supports third-party plug-in drivers, allowing third-party hardware vendors to provide drivers optimized to take best advantage of their graphics hardware.\n\nAccessing OpenGL Within Your Application\n\nThe programming interfaces that your application calls fall into two categories—those specific to the Macintosh platform and those defined by the OpenGL Working Group. The Apple-specific programming interfaces are what Cocoa applications use to communicate with the OS X windowing system. These APIs don't create OpenGL content, they manage content, direct it to a drawing destination, and control various aspects of the rendering operation. Your application calls the OpenGL APIs to create content. OpenGL routines accept vertex, pixel, and texture data and assemble the data to create an image. The final image resides in a framebuffer, which is presented to the user through the windowing-system specific API.\n\nFigure 1-6  The programing interfaces used for OpenGL content\nOpenGL APIs Specific to OS X\n\nOS X offers two easy-to-use APIs that are specific to the Macintosh platform: the NSOpenGL classes and the CGL API. Throughout this document, these APIs are referred to as the Apple-specific OpenGL APIs.\n\nCocoa provides many classes specifically for OpenGL:\n\nThe NSOpenGLContext class implements a standard OpenGL rendering context.\n\nThe NSOpenGLPixelFormat class is used by an application to specify the parameters used to create the OpenGL context.\n\nThe NSOpenGLView class is a subclass of NSView that uses NSOpenGLContext and NSOpenGLPixelFormat to display OpenGL content in a view. Applications that subclass NSOpenGLView do not need to directly subclass NSOpenGLPixelFormat or NSOpenGLContext. Applications that need customization or flexibility, can subclass NSView and create NSOpenGLPixelFormat and NSOpenGLContext objects manually.\n\nThe NSOpenGLLayer class allows your application to integrate OpenGL drawing with Core Animation.\n\nThe NSOpenGLPixelBuffer class provides hardware-accelerated offscreen drawing.\n\nThe Core OpenGL API (CGL) resides in the OpenGL framework and is used to implement the NSOpenGL classes. CGL offers the most direct access to system functionality and provides the highest level of graphics performance and control for drawing to the full screen. CGL Reference provides a complete description of this API.\n\nApple-Implemented OpenGL Libraries\n\nOS X also provides the full suite of graphics libraries that are part of every implementation of OpenGL: GL, GLU, GLUT, and GLX. Two of these—GL and GLU—provide low-level drawing support. The other two—GLUT and GLX—support drawing to the screen.\n\nYour application typically interfaces directly with the core OpenGL library (GL), the OpenGL Utility library (GLU), and the OpenGL Utility Toolkit (GLUT). The GL library provides a low-level modular API that allows you to define graphical objects. It supports the core functions defined by the OpenGL specification. It provides support for two fundamental types of graphics primitives: objects defined by sets of vertices, such as line segments and simple polygons, and objects that are pixel-based images, such as filled rectangles and bitmaps. The GL API does not handle complex custom graphical objects; your application must decompose them into simpler geometries.\n\nThe GLU library combines functions from the GL library to support more advanced graphics features. It runs on all conforming implementations of OpenGL. GLU is capable of creating and handling complex polygons (including quartic equations), processing nonuniform rational b-spline curves (NURBs), scaling images, and decomposing a surface to a series of polygons (tessellation).\n\nThe GLUT library provides a cross-platform API for performing operations associated with the user windowing environment—displaying and redrawing content, handling events, and so on. It is implemented on most UNIX, Linux, and Windows platforms. Code that you write with GLUT can be reused across multiple platforms. However, such code is constrained by a generic set of user interface elements and event-handling options. This document does not show how to use GLUT. The GLUTBasics sample project shows you how to get started with GLUT.\n\nGLX is an OpenGL extension that supports using OpenGL within a window provided by the X Window system. X11 for OS X is available as an optional installation. (It's not shown in Figure 1-6.) See OpenGL Programming for the X Window System, published by Addison Wesley for more information.\n\nThis document does not show how to use these libraries. For detailed information, either go to the OpenGL Foundation website http://www.opengl.org or see the most recent version of \"The Red book\"—OpenGL Programming Guide, published by Addison Wesley.\n\nTerminology\n\nThere are a number of terms that you’ll want to understand so that you can write code effectively using OpenGL: renderer, renderer attributes, buffer attributes, pixel format objects, rendering contexts, drawable objects, and virtual screens. As an OpenGL programmer, some of these may seem familiar to you. However, understanding the Apple-specific nuances of these terms will help you get the most out of OpenGL on the Macintosh platform.\n\nRenderer\n\nA renderer is the combination of the hardware and software that OpenGL uses to execute OpenGL commands. The characteristics of the final image depend on the capabilities of the graphics hardware associated with the renderer and the device used to display the image. OS X supports graphics accelerator cards with varying capabilities, as well as a software renderer. It is possible for multiple renderers, each with different capabilities or features, to drive a single set of graphics hardware. To learn how to determine the exact features of a renderer, see Determining the OpenGL Capabilities Supported by the Renderer.\n\nRenderer and Buffer Attributes\n\nYour application uses renderer and buffer attributes to communicate renderer and buffer requirements to OpenGL. The Apple implementation of OpenGL dynamically selects the best renderer for the current rendering task and does so transparently to your application. If your application has very specific rendering requirements and wants to control renderer selection, it can do so by supplying the appropriate renderer attributes. Buffer attributes describe such things as color and depth buffer sizes, and whether the data is stereoscopic or monoscopic.\n\nRenderer and buffer attributes are represented by constants defined in the Apple-specific OpenGL APIs. OpenGL uses the attributes you supply to perform the setup work needed prior to drawing content. Drawing to a Window or View provides a simple example that shows how to use renderer and buffer attributes. Choosing Renderer and Buffer Attributes explains how to choose renderer and buffer attributes to achieve specific rendering goals.\n\nPixel Format Objects\n\nA pixel format describes the format for pixel data storage in memory. The description includes the number and order of components as well as their names (typically red, blue, green and alpha). It also includes other information, such as whether a pixel contains stencil and depth values. A pixel format object is an opaque data structure that holds a pixel format along with a list of renderers and display devices that satisfy the requirements specified by an application.\n\nEach of the Apple-specific OpenGL APIs defines a pixel format data type and accessor routines that you can use to obtain the information referenced by this object. See Virtual Screens for more information on renderer and display devices.\n\nOpenGL Profiles\n\nOpenGL profiles are new in OS X 10.7. An OpenGL profile is a renderer attribute used to request a specific version of the OpenGL specification. When your application provides an OpenGL profile as part of its renderer attributes, it only receives renderers that provide the complete feature set promised by that profile. The render can implement a different version of the OpenGL so long as the version it supplies to your application provides the same functionality that your application requested.\n\nRendering Contexts\n\nA rendering context, or simply context, contains OpenGL state information and objects for your application. State variables include such things as drawing color, the viewing and projection transformations, lighting characteristics, and material properties. State variables are set per context. When your application creates OpenGL objects (for example, textures), these are also associated with the rendering context.\n\nAlthough your application can maintain more than one context, only one context can be the current context in a thread. The current context is the rendering context that receives OpenGL commands issued by your application.\n\nDrawable Objects\n\nA drawable object refers to an object allocated by the windowing system that can serve as an OpenGL framebuffer. A drawable object is the destination for OpenGL drawing operations. The behavior of drawable objects is not part of the OpenGL specification, but is defined by the OS X windowing system.\n\nA drawable object can be any of the following: a Cocoa view, offscreen memory, a full-screen graphics device, or a pixel buffer.\n\nNote: A pixel buffer (pbuffer) is an OpenGL buffer designed for hardware-accelerated offscreen drawing and as a source for texturing. An application can render an image into a pixel buffer and then use the pixel buffer as a texture for other OpenGL commands. Although pixel buffers are supported on Apple’s implementation of OpenGL, Apple recommends you use framebuffer objects instead. See Drawing Offscreen for more information on offscreen rendering.\n\nBefore OpenGL can draw to a drawable object, the object must be attached to a rendering context. The characteristics of the drawable object narrow the selection of hardware and software specified by the rendering context. Apple’s OpenGL automatically allocates buffers, creates surfaces, and specifies which renderer is the current renderer.\n\nThe logical flow of data from an application through OpenGL to a drawable object is shown in Figure 1-7. The application issues OpenGL commands that are sent to the current rendering context. The current context, which contains state information, constrains how the commands are interpreted by the appropriate renderer. The renderer converts the OpenGL primitives to an image in the framebuffer. (See also Running an OpenGL Program in OS X .)\n\nFigure 1-7  Data flow through OpenGL\nVirtual Screens\n\nThe characteristics and quality of the OpenGL content that the user sees depend on both the renderer and the physical display used to view the content. The combination of renderer and physical display is called a virtual screen. This important concept has implications for any OpenGL application running on OS X.\n\nA simple system, with one graphics card and one physical display, typically has two virtual screens. One virtual screen consists of a hardware-based renderer and the physical display and the other virtual screen consists of a software-based renderer and the physical display. OS X provides a software-based renderer as a fallback. It's possible for your application to decline the use of this fallback. You'll see how in Choosing Renderer and Buffer Attributes.\n\nThe green rectangle around the OpenGL image in Figure 1-8 surrounds a virtual screen for a system with one graphics card and one display. Note that a virtual screen is not the physical display, which is why the green rectangle is drawn around the application window that shows the OpenGL content. In this case, it is the renderer provided by the graphics card combined with the characteristics of the display.\n\nFigure 1-8  A virtual screen displays what the user sees\n\nBecause a virtual screen is not simply the physical display, a system with one display can use more than one virtual screen at a time, as shown in Figure 1-9. The green rectangles are drawn to point out each virtual screen. Imagine that the virtual screen on the right side uses a software-only renderer and that the one on the left uses a hardware-dependent renderer. Although this is a contrived example, it illustrates the point.\n\nFigure 1-9  Two virtual screens\n\nIt's also possible to have a virtual screen that can represent more than one physical display. The green rectangle in Figure 1-10 is drawn around a virtual screen that spans two physical displays. In this case, the same graphics hardware drives a pair of identical displays. A mirrored display also has a single virtual screen associated with multiple physical displays.\n\nFigure 1-10  A virtual screen can represent more than one physical screen\n\nThe concept of a virtual screen is particularly important when the user drags an image from one physical screen to another. When this happens, the virtual screen may change, and with it, a number of attributes of the imaging process, such as the current renderer, may change. With the dual-headed graphics card shown in Figure 1-10, dragging between displays preserves the same virtual screen. However, Figure 1-11 shows the case for which two displays represent two unique virtual screens. Not only are the two graphics cards different, but it's possible that the renderer, buffer attributes, and pixel characteristics are different. A change in any of these three items can result in a change in the virtual screen.\n\nWhen the user drags an image from one display to another, and the virtual screen is the same for both displays, the image quality should appear similar. However, for the case shown in Figure 1-11, the image quality can be quite different.\n\nFigure 1-11  Two virtual screens and two graphics cards\n\nOpenGL for OS X transparently manages rendering across multiple monitors. A user can drag a window from one monitor to another, even though their display capabilities may be different or they may be driven by dissimilar graphics cards with dissimilar resolutions and color depths.\n\nOpenGL dynamically switches renderers when the virtual screen that contains the majority of the pixels in an OpenGL window changes. When a window is split between multiple virtual screens, the framebuffer is rasterized entirely by the renderer driving the screen that contains the largest segment of the window. The regions of the window on the other virtual screens are drawn by copying the rasterized image. When the entire OpenGL drawable object is displayed on one virtual screen, there is no performance impact from multiple monitor support.\n\nApplications need to track virtual screen changes and, if appropriate, update the current application state to reflect changes in renderer capabilities. See Working with Rendering Contexts.\n\nOffline Renderer\n\nAn offline renderer is one that is not currently associated with a display. For example, a graphics processor might be powered down to conserve power, or there might not be a display hooked up to the graphics card. Offline renderers are not normally visible to your application, but your application can enable them by adding the appropriate renderer attribute. Taking advantage of offline renderers is useful because it gives the user a seamless experience when they plug in or remove displays.\n\nFor more information about configuring a context to see offline renderers, see Choosing Renderer and Buffer Attributes. To enable your application to switch to a renderer when a display is attached, see Update the Rendering Context When the Renderer or Geometry Changes.\n\nRunning an OpenGL Program in OS X\n\nFigure 1-12 shows the flow of data in an OpenGL program, regardless of the platform that the program runs on.\n\nFigure 1-12  The flow of data through OpenGL\n\nPer-vertex operations include such things as applying transformation matrices to add perspective or to clip, and applying lighting effects. Per-pixel operations include such things as color conversion and applying blur and distortion effects. Pixels destined for textures are sent to texture assembly, where OpenGL stores textures until it needs to apply them onto an object.\n\nOpenGL rasterizes the processed vertex and pixel data, meaning that the data are converged to create fragments. A fragment encapsulates all the values for a pixel, including color, depth, and sometimes texture values. These values are used during antialiasing and any other calculations needed to fill shapes and to connect vertices.\n\nPer-fragment operations include applying environment effects, depth and stencil testing, and performing other operations such as blending and dithering. Some operations—such as hidden-surface removal—end the processing of a fragment. OpenGL draws fully processed fragments into the appropriate location in the framebuffer.\n\nThe dashed arrows in Figure 1-12 indicate reading pixel data back from the framebuffer. They represent operations performed by OpenGL functions such as glReadPixels, glCopyPixels, and glCopyTexImage2D.\n\nSo far you've seen how OpenGL operates on any platform. But how do Cocoa applications provide data to the OpenGL for processing? A Mac application must perform these tasks:\n\nSet up a list of buffer and renderer attributes that define the sort of drawing you want to perform. (See Renderer and Buffer Attributes.)\n\nRequest the system to create a pixel format object that contains a pixel format that meets the constraints of the buffer and render attributes and a list of all suitable combinations of displays and renderers. (See Pixel Format Objects and Virtual Screens.)\n\nCreate a rendering context to hold state information that controls such things as drawing color, view and projection matrices, characteristics of light, and conventions used to pack pixels. When you set up this context, you must provide a pixel format object because the rendering context needs to know the set of virtual screens that can be used for drawing. (See Rendering Contexts.)\n\nBind a drawable object to the rendering context. The drawable object is what captures the OpenGL drawing sent to that rendering context. (See Drawable Objects.)\n\nMake the rendering context the current context. OpenGL automatically targets the current context. Although your application might have several rendering contexts set up, only the current one is the active one for drawing purposes.\n\nIssue OpenGL drawing commands.\n\nFlush the contents of the rendering context. This causes previously submitted commands to be rendered to the drawable object and displays them to the user.\n\nThe tasks described in the first five bullet items are platform-specific. Drawing to a Window or View provides simple examples of how to perform them. As you read other parts of this document, you'll see there are a number of other tasks that, although not mandatory for drawing, are really quite necessary for any application that wants to use OpenGL to perform complex 3D drawing efficiently on a wide variety of Macintosh systems.\n\nMaking Great OpenGL Applications on the Macintosh\n\nOpenGL lets you create applications with outstanding graphics performance as well as a great user experience—but neither of these things come for free. Your application performs best when it works with OpenGL rather than against it. With that in mind, here are guidelines you should follow to create high-performance, future-looking OpenGL applications:\n\nEnsure your application runs successfully with offline renderers and multiple graphics cards.\n\nApple ships many sophisticated hardware configurations. Your application should handle renderer changes seamlessly. You should test your application on a Mac with multiple graphics processors and include tests for attaching and removing displays. For more information on how to implement hot plugging correctly, see Working with Rendering Contexts\n\nAvoid finishing and flushing operations.\n\nPay particular attention to OpenGL functions that force previously submitted commands to complete. Synchronizing the graphics hardware to the CPU may result in dramatically lower performance. Performance is covered in detail in OpenGL Application Design Strategies.\n\nUse multithreading to improve the performance of your OpenGL application.\n\nMany Macs support multiple simultaneous threads of execution. Your application should take advantage of concurrency. Well-behaved applications can take advantage of concurrency in just a few line of code. See Concurrency and OpenGL.\n\nUse buffer objects to manage your data.\n\nVertex buffer objects (VBOs) allow OpenGL to manage your application’s vertex data. Using vertex buffer objects gives OpenGL more opportunities to cache vertex data in a format that is friendly to the graphics hardware, improving application performance. For more information see Best Practices for Working with Vertex Data.\n\nSimilarly, pixel buffer objects (PBOs) should be used to manage your image data. See Best Practices for Working with Texture Data\n\nUse framebuffer objects (FBOs) when you need to render to offscreen memory.\n\nFramebuffer objects allow your application to create offscreen rendering targets without many of the limitations of platform-dependent interfaces. See Rendering to a Framebuffer Object.\n\nGenerate objects before binding them.\n\nEarlier version of OpenGL allowed your applications to create its own object names before binding them. However, you should avoid this. Always use the OpenGL API to generate object names.\n\nMigrate your OpenGL Applications to OpenGL 3.2\n\nThe OpenGL 3.2 Core profile provides a clean break from earlier versions of OpenGL in favor of a simpler shader-based pipeline. For better compatibility with future hardware and OS X releases, migrate your applications away from legacy versions of OpenGL. Many of the recommendations listed above are required when your application uses OpenGL 3.2.\n\nHarness the power of Apple’s development tools.\n\nApple provides many tools that help create OpenGL applications and analyze and tune their performance. Learning how to use these tools helps you create fast, reliable applications. Tuning Your OpenGL Application describes many of these tools.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Using Breakpoints",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLProfilerUserGuide/Breakpoints/Breakpoints.html#//apple_ref/doc/uid/TP40006475-CH30-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Profiler User Guide\nTable of Contents\nIntroduction\nGetting Started\nUsing Breakpoints\nIdentifying and Solving Performance Issues\nControlling Profiling Programmatically\nRevision History\nNext\nPrevious\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nUsing Breakpoints\n\nYou can use OpenGL Profiler to set breakpoints in your application. A breakpoint can occur before or after a specific OpenGL function, when your application uses a software renderer, or when an error or thread conflict occurs. When your application is paused, you can perform a number of tasks, including:\n\nExamining the call state and OpenGL state\n\nViewing and modifying resources\n\nViewing buffer contents\n\nExecuting a script that you attach to the breakpoint\n\nYou can also choose to disable the execution of one or more OpenGL functions.\n\nThis chapter describes how to set breakpoints and describes all the tasks that you can perform only after your application pauses at a breakpoint.\n\nSetting Breakpoints on OpenGL Functions\n\nYou set breakpoints in the Breakpoints window. To open this window, choose Views > Breakpoints. As shown in Figure 2-1, the left side of the window lists the OpenGL functions that you can set breakpoints on. This list includes functions defined by the OpenGL specification and the CGL calls available in OS X. Keep in mind that AGL functions and Cocoa OpenGL class methods call into the CGL framework, so AGL and Cocoa methods aren’t explicitly on the function list.\n\nThe right side of the window lets you view the call stack, as shown in Figure 2-1, or OpenGL state variables, as shown in Figure 2-2.\n\nFigure 2-1  The Breakpoints window displaying the call stack\n\nTo set a breakpoint, choose the OpenGL function in the function list that you want to set a breakpoint on. Then click one of the following columns:\n\nBefore pauses your application just before it executes the OpenGL function.\n\nAfter pauses your application just after it executes the OpenGL function.\n\nA blue indicator appears wherever you set a breakpoint.\n\nFigure 2-2  The Breakpoints window displaying OpenGL state variables\n\nThe checkboxes below the function list let you set breakpoint conditions. You can also turn the function list breakpoints on and off quickly by selecting “Ignore all breakpoints.”\n\nViewing and Modifying Resources\n\nOpenGL resources include textures, programs, shaders, framebuffer objects, renderbuffer objects, vertex buffer objects, and vertex array objects. You can view these resources by choosing Views > Resources. The right side of the Resources window lets you choose a resource type. You can see the individual resources, for the resource types that your application uses, listed in the left side. Click a resource name to view its settings and other relevant information on the right side. For example, see Figure 2-3 which shows two vertex buffer objects. The parameters and values for the one that’s selected are shown on the right side.\n\nFigure 2-3  The Resources window open to vertex buffer objects\n\nWhen your application pauses at a breakpoint, you can modify programs and shaders. The modifications take effect when your application resumes, allowing you to get immediate feedback on how changes effect the behavior of your application. For details, see:\n\nModifying Programs\n\nModifying Shaders\n\nYou can make modifications to textures and renderbuffers, but these affect only the view in the resource window, allowing for better or correct viewing within OpenGL Profiler. For details, see:\n\nModifying Texture Display Parameters\n\nModifying a Renderbuffer\n\nModifying Texture Display Parameters\n\nYou can modify how a texture displays by changing its mipmap level, zoom level, source and destination blend modes, and the background color and opacity. You cannot change a texture’s target, internal format, source format, source type, or dimensions. Keep in mind that modifications affect only what you see in the Resources window.\n\nTo modify the display parameters for a texture:\n\nClick Textures in the Resources window.\n\nThe texture names are listed on the left, as shown in Figure 2-4.\n\nChoose a texture name.\n\nModify the display parameters that you’d like to change.\n\nYou should see the changes take effect immediately.\n\nClick Continue in the Breakpoints window.\n\nYou need to refresh the texture only if the application modifies it after you open the Resources window.\n\nFigure 2-4  The Textures pane in the Resources window\nModifying Programs\n\nA program is similar to a shader in that it describes the source code that modifies data in the GPU vertex or fragment pipeline. Programs were available for coding vertex and fragment operations prior to shaders. They use an arcane coding language compared to the C-like syntax used for OpenGL Shading Language. Listing 2-1 shows a program for a basic shader.\n\nListing 2-1  A program for a basic shader\n\n!!ARBvp1.0\n\n\n \n\n\nATTRIB vertexPosition  = vertex.position;\n\n\nOUTPUT outputPosition  = result.position;\n\n\n \n\n\n# Transform the vertex by the modelview/projection matrix\n\n\nDP4    outputPosition.x, state.matrix.mvp.row[0], vertexPosition;\n\n\nDP4    outputPosition.y, state.matrix.mvp.row[1], vertexPosition;\n\n\nDP4    outputPosition.z, state.matrix.mvp.row[2], vertexPosition;\n\n\nDP4    outputPosition.w, state.matrix.mvp.row[3], vertexPosition;\n\n\n \n\n\n# Pass the color and texture coordinate through\n\n\nMOV    result.color, vertex.color;\n\n\nMOV    result.texcoord, vertex.texcoord;\n\n\n \n\n\nEND\n\nTo modify program code:\n\nClick Programs in the Resources window.\n\nChoose a program from the list on the left.\n\nModify the code.\n\nClick Compile.\n\nClick Continue in the Breakpoints window.\n\nWhen the application resumes, you should see the result of your modified program.\n\nModifying Shaders\n\nShaders are small programs, written using OpenGL Shading Language, that compute surface properties. OpenGL lets you modify shaders that are part of your application, giving you the ability to get immediate feedback on the changes.\n\nTo modify a shader:\n\nClick Shaders in the Resources window.\n\nThe shader objects are listed on the left, as shown in Figure 2-5.\n\nChoose a shader object from the list on the left.\n\nClick Source and modify the shader code.\n\nClick Compile.\n\nIf compilation fails, click Log to examine the errors. You can return to the original code by clicking Revert.\n\nIf the compilation succeeds, release the breakpoint and click Continue in the breakpoints view.\n\nFigure 2-5  The Shaders pane in the Resources window\nModifying a Renderbuffer\n\nA renderbuffer is an offscreen rendering destination for a 2D pixel image. Renderbuffers are defined in the OpenGL specification for the GL_EXT_framebuffer_object extension. OpenGL Profiler lets you modify how the contents of the renderbuffer display by changing its zoom level, source and destination blend modes, and the background color and opacity. You cannot change a renderbuffer’s target, source format, source type, or image dimensions. Keep in mind that modifications affect only what you see in the Resources window.\n\nTo modify the display parameters of a renderbuffer object:\n\nClick Renderbuffers in the Resources window.\n\nThe renderbuffer objects are listed on the left, as shown in Figure 2-6.\n\nChoose a renderbuffer object.\n\nModify the display parameters that you’d like to change.\n\nYou should see the changes take effect immediately.\n\nClick Continue in the Breakpoints window.\n\nYou need to refresh the image only if the application modifies it after you open the Resources window.\n\nFigure 2-6  The Renderbuffers pane in the Resources window\nViewing Buffers\n\nYou can view the contents of an OpenGL buffer by choosing Views > Buffers and then choosing a buffer—Front Buffer, Back Buffer, Alpha Buffer, Stencil Buffer, or Depth Buffer. Buffers not used by your application appear dimmed. Buffer views are available only when your application pauses at a breakpoint.\n\nAfter selecting a buffer, its contents appear in a window. Each time your application pauses at a breakpoint, each buffer view updates automatically.\n\nFigure 2-7 shows the depth buffer contents for the GLSLShowpiece application available from http://developer.apple.com.\n\nFigure 2-7  The Depth Buffer window\nCreating and Attaching Scripts to a Breakpoint\n\nOpenGL Profiler allows you to execute (at a performance cost) small amounts of OpenGL code or other scripting calls when your application pauses at a breakpoint. A script is a text file that contains OpenGL function calls. For example, a script can be as simple as the following line of code:\n\nglClear(1.0, 0.0, 0.0, 1.0);\n\nOr it can be more complex, containing many lines of code, such as:\n\nglBegin(GL_POINTS);\n\n\nglVertex3f(0,0,1);\n\n\nglVertex3f(1,0,1);\n\n\nglVertex3f(1,1,1);\n\n\nglVertex3f(0,1,1);\n\n\nglEnd();\n\nYou can create a script using any text editor, or you can enter a script directly into the Scripts window.\n\nTo add a script:\n\nChoose Views > Scripts to open the Scripts window.\n\nIf you created the script in a text editor, click Open, navigate to the script, and choose it. If you want to create a script in the Scripts window, click the plus (+) button, enter a script name, then type commands in the text window on the right.\n\nAny script that you add in the Scripts window will be available from the Breakpoints window.\n\nAfter you add a script to OpenGL Profiler, you can either execute it manually or attach it to a breakpoint.\n\nTo execute a script manually when your application pauses at a breakpoint:\n\nChoose a script in the Scripts window.\n\nClick Execute.\n\nOpenGL Profiler sends the commands to your application for execution. Errors in your script appear in the pane located in the lower left of the Scripts window.\n\nTo attach a script to a breakpoint:\n\nOpen the Breakpoints window.\n\nChoose Actions > Attach Script.\n\nIn the sheet that appears, choose a script.\n\nSelect one or more execution options.\n\nYou can choose to have OpenGL Profiler execute the script before, after, or both before and after the OpenGL function. You can also choose to have your application continue after executing the script.\n\nClick Attach.\n\nA small document icon appears in the functions list next to the blue breakpoint indicator for the function.\n\nNote: Attaching a script to a function that your application uses repeatedly is an expensive operation. Data that you look at when the script executes is not an accurate measure of performance.\n\nTo remove a script from a breakpoint, choose Actions > Remove Script.\n\nDisabling OpenGL Execution\n\nUnder some circumstances you might want to disable the execution of one or more OpenGL functions in your application.\n\nTo disable the execution of all OpenGL functions in your application, in the Breakpoints window choose Actions > Execute none. To enable execution, choose Actions > Execute all.\n\nTo disable execution of individual OpenGL functions, in the Breakpoints window, select the function you want to disable, and click the Execute column. OpenGL Profiler ignores breakpoints set on disabled functions. To enable a function, click the Execute column so that the blue indicator appears.\n\nNote: You cannot disable functions that have a dimmed indicator in the Execute column. These functions manage the context.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "Getting Started",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLProfilerUserGuide/GettedStarted/GettingStarted.html#//apple_ref/doc/uid/TP40006475-CH20-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Profiler User Guide\nTable of Contents\nIntroduction\nGetting Started\nUsing Breakpoints\nIdentifying and Solving Performance Issues\nControlling Profiling Programmatically\nRevision History\nNext\nPrevious\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nGetting Started\n\nUsers expect OpenGL applications to have fluid graphics that display without glitches. The more complex the graphics in an OpenGL application are, the more important it is for you to optimize performance and use resources wisely. When an OpenGL application performs less optimally than desired, the cause is often not obvious. That’s where the OpenGL Profiler can be of value to you. Download the GraphicsTools app from http://developer.apple.com/downloads.\n\nThis chapter:\n\nGives an overview of how OpenGL Profiler works\n\nShows how to set up your computer to use the profiler\n\nDescribes how to start a profiling session for an OpenGL application\n\nExplains how to view and interpret data collected by the profiler\n\nTells how to look at and modify application resources and parameters\n\nBefore reading this chapter, you may want to read the “Improving Performance” chapter in OpenGL Programming Guide for Mac. That chapter provides a list of best programming practices for OpenGL and shows how to gather baseline data using a few Apple tools in addition to OpenGL Profiler. In fact, prior to using OpenGL Profiler, it’s best to start your analysis with Instruments. The results from Instruments will help you determine how to focus your efforts with OpenGL Profiler, or whether you even need to use the profiler application. It may be that your application’s problems are not due to your OpenGL code!\n\nOverview\n\nOpenGL Profiler collects trace and statistics for applications that use OpenGL. A trace is an ordered list of the OpenGL calls made by an application. Each entry in a trace shows a function name and the values of the parameters passed to the function. Statistics show cumulative totals, by function, for the number of times an application calls a function and the execution time of the function. You can also see the average execution time for a function, the percentage of time a function is used by OpenGL, and the percentage of time a function is used by the application.\n\nThe OpenGL functions that you’ll see in the trace and statistics include those defined by the OpenGL specification (see http://www.opengl.org) as well as the functions that are part of the low-level OS X OpenGL programming interfaces—CGL. (See CGL Reference.)\n\nOpenGL Profiler is also useful for inspecting and controlling various aspects of your application. For example, you can:\n\nView the OpenGL resources your application uses, such as textures, vertex programs, and shaders.\n\nView buffer contents, such as the depth and back buffers\n\nSet breakpoints on specific OpenGL functions and view the call stack\n\nAttach a script of OpenGL commands that executes at a breakpoint\n\nEnable or disable individual OpenGL commands\n\nBefore You Use OpenGL Profiler on a System\n\nIf you’ve never used OpenGL Profiler on your system, if you want to allow the profiler to attach to a running application you must set the environment variable GL_ENABLE_DEBUG_ATTACH. You also might want to set preferences.\n\nSetting Up the Environment Variables\n\nThere are two locations that require the environment variable:\n\nYour shell startup file. The name of this file depends on the shell that you use: ~/.login, ~/.cshrc, ~/.bashrc, ~/.profile, and so on. Add the following to the appropriate shell file:\n\nsetenv GL_ENABLE_DEBUG_ATTACH YES\n\nThe OS X environment property list file (~/.MacOSX/environment.plist). The Finder uses this file to set the environment variable when it launches applications. Typically OpenGL Profiler creates this file for you the first time that you launch OpenGL Profiler on a system. The first time you launch the application you should see a dialog that asks whether you want to enable the Attach feature. Click Enable to have the property list file created for you.\n\nYou can skip the rest of this section unless, for some reason, the property list file is missing. If that’s the case, to use the Attach feature you’ll need to create a plain text file, name it environment.plist, and enter the following into the file:\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n\n<!DOCTYPE plist PUBLIC \"-//Apple Computer//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n\n\n<plist version=\"1.0\">\n\n\n<dict>\n\n\n        <key>GL_ENABLE_DEBUG_ATTACH</key>\n\n\n        <string>YES</string>\n\n\n</dict>\n\n\n</plist>\n\nCopy the file to your ~/.MacOSX directory. If the directory does not exist, open Terminal and create the directory from within your shell.\n\nSetting Preferences (Optional)\n\nYou can view preferences by choosing OpenGL Profiler > Preferences. You can set the following:\n\nFrame rate meter interval, which allows you to change the frames-per-second readout from you application. If you need a finer grain of measurement and a more immediate result, you might want to increase this value.\n\nThe function sample interval in the statistics view\n\nThe number of slices to keep in the statistics view. You can limit the number of timing samples that OpenGL Profiler records.\n\nTrace view font. You can set the font face and the font size.\n\nTrace data file. You can change the file location and name.\n\nStarting a Profiling Session\n\nWhen you double-click the OpenGL Profiler icon, the window shown in Figure 1-1 opens. You use this window to set up a profiling session. You can start profiling an application either by launching it through OpenGL Profiler or by attaching the profiler to an application that’s already running.\n\nFigure 1-1  The main window in OpenGL Profiler\n\nBefore you attach or launch your application, select Collect Trace. You also have the option to collect backtraces. After your application launches or attaches, you may see small pauses or stutters in the application. This is normal behavior that is due to the large amount of data that OpenGL Profiler writes out when collecting a trace. It does not significantly affect the performance statistics.\n\nAttaching to a Running Application\n\nTo attach OpenGL Profiler to a running application:\n\nSelect “Attach to application.”\n\nIn the list that appears, select the application you want to profile.\n\nClick Attach.\n\nThe status changes from idle to running, and the application list dims. Now that the profiler is attached to the application, you can perform any of the tasks described in the rest of this document.\n\nOpenGL Profiler begins collecting data as soon as it attaches to your application. Data collection ends when you quit the application or click the Detach button in the OpenGL Profiler window. You can temporarily stop data collection by clicking Suspend.\n\nDetaching lets the application continue to run, which is useful for applications (like shell tools) that do not have a user interface.\n\nLaunching an Application from OpenGL Profiler\n\nTo set up OpenGL Profiler to launch an application:\n\nSelect “Launch application.”\n\nClick the plus (+) button and navigate to the application you want to profile, then choose it.\n\nYou can add as many applications as you’d like, but you can profile only one at a time.\n\nIf your application needs launch arguments, click the Launch Arguments text field and add them.\n\nClick the disclosure triangle to view the optional launch settings.\n\nYou can use these settings when you want to profile an application under specific conditions. For example, you can use a custom pixel format, simulate how the application would work using a specific graphics driver, and set additional environment variables. See Customizing Launch Settings for details.\n\nClick Launch.\n\nThe status changes from idle to running, and the application and environment variables lists dim.\n\nOpenGL Profiler begins collecting data as soon as it launches your application. Data collection ends when you quit the application or click the Kill button in the OpenGL Profiler window. You can temporarily stop data collection by clicking Suspend.\n\nCustomizing Launch Settings\n\nClick the disclosure triangle next to Launch Settings in the OpenGL Profiler window to customize how your application launches. These options allow you to observe how your application operates under specific conditions.\n\nSetting a Custom Pixel Format\n\nSelect “Use custom pixel format” and click Edit to try out different settings for pixel format attributes. You can quickly see the results of modifying attribute values without changing the source code of the application.\n\nUsing a Driver Emulation\n\nOpenGL Profiler contains parameters and values for a variety of OpenGL drivers, not only those available in the system that you are currently running. This feature comes in handy if you want to examine the values that OpenGL returns for a particular driver. Emulation mode does not affect the performance of your application in any way. The purpose of this feature is to help you determine whether code that depends on certain hardware features works properly. Keep in mind that OpenGL Profiler does not replace the drivers your system.\n\nWhen using a driver emulation, OpenGL Profiler does changes the return values for some of the glGet functions. For example, if you enable the Rage 128 emulator for OS X v10.2 and earlier, calling glGetString(GL_RENDERER) returns \"ATi Rage 128 Pro OpenGL Engine\" instead of the renderer that’s actually in your system. In addition, any glGet functions that return driver-specific parameters will return the values you’d get if your application ran on the emulated card. For example, calling glGetInteger(GL_MAX_LIGHTS, &maxLights) assigns 8 to maxLights.\n\nTo set a specific driver:\n\nChoose a driver from the pop-up menu.\n\nClick View to inspect the parameters and values for the driver.\n\nApple provides a set of driver emulation files. These are property lists that you can edit yourself. You can create an emulation file for a driver not in the list by using a text editor and then saving the file in this directory:\n\n~/Library/OpenGL Profiler/Driver Emulators\n\nYou’ll need to restart OpenGL Profiler to see the new file in the driver list.\n\nSetting Environment Variables\n\nYou can add environment variables by clicking the plus (+) button. Then enter the variable name and its value. You can add any variable your application reads using getenv(3).\n\nSetting a Working Directory\n\nThe working directory refers to your application’s current working directory. It’s the path returned by the function getcwd(3) and the same path that’s automatically prepended to any relative path in your application. If your application does not conform to Apple’s application wrapper scheme, you need to set the working directory so that your application can find resources such as texture maps.\n\nFor example, if you launch a nonconforming application from Finder, for this line of code to execute properly:\n\nfopen(\"mytexture\", \"r+\")\n\nyou need to set the working directory to the folder that contains \"mytexture\".\n\nViewing a Trace\n\nYou can view a trace by choosing Views > Trace. The Trace window displays a running list of the OpenGL function calls your application makes. The time value next to the function name gives you an idea of the performance cost of the call. Keep in mind that a lot of OpenGL calls set bits and are acted upon only at draw or flush time. This means that in addition to looking at the time for a particular function, you also need to consider the execution time of the next draw or flush function.\n\nFor strategies on interpreting trace data, see:\n\nMaking Sure You Use Functions Correctly\n\nIdentifying Problem Areas in Your Application\n\nManaging Trace Data\n\nViewing Statistics\n\nChoose Views > Statistics to open the Statistics window. This view aggregates the trace data so that you can see function call frequency, execution time sums and averages, and these percentages:\n\n% GL Time indicates the amount of time a function takes to execute compared only to the OpenGL code in an application.\n\n% Application Time indicates the amount of time a function takes to execute compared to all code in an application.\n\nEstimated % Time in OpenGL indicates the amount of time an application spends executing OpenGL code compared to all application code.\n\nThese statistics can help you identify the portions of your OpenGL code that consume the most time. Finding that your application stalls during certain calls or that some functions seem to be called at an unusually high frequency can help you pinpoint the portions of your code that might need fine-tuning.\n\nOne way to find out where to focus your optimization efforts is to compare the time spent executing OpenGL calls to the time spent executing non-OpenGL calls. For example, if your application runs slowly but spends most of its time executing non-OpenGL calls, you'll get the most performance gains by analyzing and optimizing the non-OpenGL portion.\n\nFor other strategies on interpreting statistics data, see:\n\nMaking Sure You Use Functions Correctly\n\nIdentifying Problem Areas in Your Application\n\nViewing Pixel Format Context Parameters\n\nTo view the context parameters for the application’s pixel format, choose Views > Pixel Format. For each OpenGL context, you’ll see a list of its parameters and the value for each parameter.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "OpenGL Application Design Strategies",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_designstrategies/opengl_designstrategies.html#//apple_ref/doc/uid/TP40001987-CH2-SW6",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nOpenGL Application Design Strategies\n\nOpenGL performs many complex operations—transformations, lighting, clipping, texturing, environmental effects, and so on—on large data sets. The size of your data and the complexity of the calculations performed on it can impact performance, making your stellar 3D graphics shine less brightly than you'd like. Whether your application is a game using OpenGL to provide immersive real-time images to the user or an image processing application more concerned with image quality, use the information in this chapter to help you design your application.\n\nVisualizing OpenGL\n\nThe most common way to visualize OpenGL is as a graphics pipeline, as shown in Figure 9-1. Your application sends vertex and image data, configuration and state changes, and rendering commands to OpenGL. Vertices are processed, assembled into primitives, and rasterized into fragments. Each fragment is calculated and merged into the framebuffer. The pipeline model is useful for identifying exactly what work your application must perform to generate the results you want. OpenGL allows you to customize each stage of the graphics pipeline, either through customized shader programs or by configuring a fixed-function pipeline through OpenGL function calls.\n\nIn most implementations, each pipeline stage can act in parallel with the others. This is a key point. If any one pipeline stage performs too much work, then the other stages sit idle waiting for it to complete. Your design should balance the work performed in each pipeline stage to the capabilities of the renderer. When you tune your application’s performance, the first step is usually to determine which stage the application is bottlenecked in, and why.\n\nFigure 9-1  OpenGL graphics pipeline\n\nAnother way to visualize OpenGL is as a client-server architecture, as shown in Figure 9-2. OpenGL state changes, texture and vertex data, and rendering commands must all travel from the application to the OpenGL client. The client transforms these items so that the graphics hardware can understand them, and then forwards them to the GPU. Not only do these transformations add overhead, but the bandwidth between the CPU and the graphics hardware is often lower than other parts of the system.\n\nTo achieve great performance, an application must reduce the frequency of calls they make to OpenGL, minimize the transformation overhead, and carefully manage the flow of data between the application and the graphics hardware. For example, OpenGL provides mechanisms that allow some kinds of data to be cached in dedicated graphics memory. Caching reusable data in graphics memory reduces the overhead of transmitting data to the graphics hardware.\n\nFigure 9-2  OpenGL client-server architecture\nDesigning a High-Performance OpenGL Application\n\nTo summarize, a well-designed OpenGL application needs to:\n\nExploit parallelism in the OpenGL pipeline.\n\nManage data flow between the application and the graphics hardware.\n\nFigure 9-3 shows a suggested process flow for an application that uses OpenGL to perform animation to the display.\n\nFigure 9-3  Application model for managing resources\n\nWhen the application launches, it creates and initializes any static resources it intends to use in the renderer, encapsulating those resources into OpenGL objects where possible. The goal is to create any object that can remain unchanged for the runtime of the application. This trades increased initialization time for better rendering performance. Ideally, complex commands or batches of state changes should be replaced with OpenGL objects that can be switched in with a single function call. For example, configuring the fixed-function pipeline can take dozens of function calls. Replace it with a graphics shader that is compiled at initialization time, and you can switch to a different program with a single function call. In particular, OpenGL objects that are expensive to create or modify should be created as static objects.\n\nThe rendering loop processes all of the items you intend to render to the OpenGL context, then swaps the buffers to display the results to the user. In an animated scene, some data needs to be updated for every frame. In the inner rendering loop shown in Figure 9-3, the application alternates between updating rendering resources (possibly creating or modifying OpenGL objects in the process) and submitting rendering commands that use those resources. The goal of this inner loop is to balance the workload so that the CPU and GPU are working in parallel, without blocking each other by using the same resources simultaneously.\n\nA goal for the inner loop is to avoid copying data back from the graphics processor to the CPU. Operations that require the CPU to read results back from the graphics hardware are sometimes necessary, but in general reading back results should be used sparingly. If those results are also used to render the current frame, as shown in the middle rendering loop, this can be very slow. Copying data from the GPU to the CPU often requires that some or all previously submitted drawing commands have completed.\n\nAfter the application submits all drawing commands needed in the frame, it presents the results to the screen. Alternatively, a non-interactive application might read the final image back to the CPU, but this is also slower than presenting results to the screen. This step should be performed only for results that must be read back to the application. For example, you might copy the image in the back buffer to save it to disk.\n\nFinally, when your application is ready to shut down, it deletes static and dynamic resources to make more hardware resources available to other applications. If your application is moved to the background, releasing resources to other applications is also good practice.\n\nTo summarize the important characteristics of this design:\n\nCreate static resources, whenever practical.\n\nThe inner rendering loop alternates between modifying dynamic resources and submitting rendering commands. Enough work should be included in this loop so that when the application needs to read or write to any OpenGL object, the graphics processor has finished processing any commands that used it.\n\nAvoid reading intermediate rendering results into the application.\n\nThe rest of this chapter provides useful OpenGL programming techniques to implement the features of this rendering loop. Later chapters demonstrate how to apply these general techniques to specific areas of OpenGL programming.\n\nUpdate OpenGL Content Only When Your Data Changes\n\nAvoid Synchronizing and Flushing Operations\n\nAllow OpenGL to Manage Your Resources\n\nUse Optimal Data Types and Formats\n\nUse Double Buffering to Avoid Resource Conflicts\n\nBe Mindful of OpenGL State Variables\n\nUse OpenGL Macros\n\nReplace State Changes with OpenGL Objects\n\nUpdate OpenGL Content Only When Your Data Changes\n\nOpenGL applications should avoid recomputing a scene when the data has not changed. This is critical on portable devices, where power conservation is critical to maximizing battery life. You can ensure that your application draws only when necessary by following a few simple guidelines:\n\nIf your application is rendering animation, use a Core Video display link to drive the animation loop. Listing 9-1 provides code that allows your application to be notified when a new frame needs to be displayed. This code also synchronizes image updates to the refresh rate of the display. See Synchronize with the Screen Refresh Rate for more information.\n\nIf your application does not animate its OpenGL content, you should allow the system to regulate drawing. For example, in Cocoa call the setNeedsDisplay: method when your data changes.\n\nIf your application does not use a Core Video display link, you should still advance an animation only when necessary. To determine when to draw the next frame of an animation, calculate the difference between the current time and the start of the last frame. Use the difference to determine how much to advance the animation. You can use the Core Foundation function CFAbsoluteTimeGetCurrent to obtain the current time.\n\nListing 9-1  Setting up a Core Video display link\n\n@interface MyView : NSOpenGLView\n\n\n{\n\n\n    CVDisplayLinkRef displayLink; //display link for managing rendering thread\n\n\n}\n\n\n@end\n\n\n \n\n\n- (void)prepareOpenGL\n\n\n{\n\n\n   // Synchronize buffer swaps with vertical refresh rate\n\n\n    GLint swapInt = 1;\n\n\n    [[self openGLContext] setValues:&swapInt forParameter:NSOpenGLCPSwapInterval];\n\n\n \n\n\n    // Create a display link capable of being used with all active displays\n\n\n    CVDisplayLinkCreateWithActiveCGDisplays(&displayLink);\n\n\n \n\n\n    // Set the renderer output callback function\n\n\n    CVDisplayLinkSetOutputCallback(displayLink, &MyDisplayLinkCallback, self);\n\n\n \n\n\n    // Set the display link for the current renderer\n\n\n    CGLContextObj cglContext = [[self openGLContext] CGLContextObj];\n\n\n    CGLPixelFormatObj cglPixelFormat = [[self pixelFormat] CGLPixelFormatObj];\n\n\n    CVDisplayLinkSetCurrentCGDisplayFromOpenGLContext(displayLink, cglContext, cglPixelFormat);\n\n\n \n\n\n    // Activate the display link\n\n\n    CVDisplayLinkStart(displayLink);\n\n\n}\n\n\n \n\n\n// This is the renderer output callback function\n\n\nstatic CVReturn MyDisplayLinkCallback(CVDisplayLinkRef displayLink, const CVTimeStamp* now, const CVTimeStamp* outputTime,\n\n\nCVOptionFlags flagsIn, CVOptionFlags* flagsOut, void* displayLinkContext)\n\n\n{\n\n\n    CVReturn result = [(MyView*)displayLinkContext getFrameForTime:outputTime];\n\n\n    return result;\n\n\n}\n\n\n \n\n\n- (CVReturn)getFrameForTime:(const CVTimeStamp*)outputTime\n\n\n{\n\n\n    // Add your drawing codes here\n\n\n \n\n\n    return kCVReturnSuccess;\n\n\n}\n\n\n \n\n\n- (void)dealloc\n\n\n{\n\n\n    // Release the display link\n\n\n    CVDisplayLinkRelease(displayLink);\n\n\n \n\n\n    [super dealloc];\n\n\n}\nSynchronize with the Screen Refresh Rate\n\nTearing is a visual anomaly caused when part of the current frame overwrites previous frame data in the framebuffer before the current frame is fully rendered on the screen. To avoid tearing, applications use a double-buffered context and synchronize buffer swaps with the screen refresh rate (sometimes called VBL, vertical blank, or vsynch) to eliminate frame tearing.\n\nNote: During development, it's best to disable synchronization so that you can more accurately benchmark your application. Enable synchronization when you are ready to deploy your application.\n\nThe refresh rate of the display limits how often the screen can be refreshed. The screen can be refreshed at rates that are divisible by integer values. For example, a CRT display that has a refresh rate of 60 Hz can support screen refresh rates of 60 Hz, 30 Hz, 20 Hz, and 15 Hz. LCD displays do not have a vertical retrace in the CRT sense and are typically considered to have a fixed refresh rate of 60 Hz.\n\nAfter you tell the context to swap the buffers, OpenGL must defer any rendering commands that follow that swap until after the buffers have successfully been exchanged. Applications that attempt to draw to the screen during this waiting period waste time that could be spent performing other drawing operations or saving battery life and minimizing fan operation.\n\nListing 9-2 shows how an NSOpenGLView object can synchronize with the screen refresh rate; you can use a similar approach if your application uses CGL contexts. It assumes that you set up the context for double buffering. The swap interval can be set only to 0 or 1. If the swap interval is set to 1, the buffers are swapped only during the vertical retrace.\n\nListing 9-2  Setting up synchronization\n\nGLint swapInterval = 1;\n\n\n[[self openGLContext] setValues:&swapInt forParameter:NSOpenGLCPSwapInterval];\nAvoid Synchronizing and Flushing Operations\n\nOpenGL is not required to execute most commands immediately. Often, they are queued to a command buffer and read and executed by the hardware at a later time. Usually, OpenGL waits until the application has queued up a significant number of commands before sending the buffer to the hardware—allowing the graphics hardware to execute commands in batches is often more efficient. However, some OpenGL functions must flush the buffer immediately. Other functions not only flush the buffer, but also block until previously submitted commands have completed before returning control to the application. Your application should restrict the use of flushing and synchronizing commands only to those cases where that behavior is necessary. Excessive use of flushing or synchronizing commands add additional stalls waiting for the hardware to finish rendering. On a single-buffered context, flushing may also cause visual anomalies, such as flickering or tearing.\n\nThese situations require OpenGL to submit the command buffer to the hardware for execution.\n\nThe function glFlush waits until commands are submitted but does not wait for the commands to finish executing.\n\nThe function glFinish waits for all previously submitted commands to complete executing.\n\nFunctions that retrieve OpenGL state (for example, glGetError), also wait for submitted commands to complete.\n\nBuffer swapping routines (the flushBuffer method of the NSOpenGLContext class or the CGLFlushDrawable function) implicitly call glFlush. Note that when using the NSOpenGLContext class or the CGL API, the term flush actually refers to a buffer-swapping operation. For single-buffered contexts, glFlush and glFinish are equivalent to a swap operation, since all rendering is taking place directly in the front buffer.\n\nThe command buffer is full.\n\nUsing glFlush Effectively\n\nMost of the time you don't need to call glFlush to move image data to the screen. There are only a few cases that require you to call the glFlush function:\n\nIf your application submits rendering commands that use a particular OpenGL object, and it intends to modify that object in the near future. If you attempt to modify an OpenGL object that has pending drawing commands, your application may be forced to wait until those commands have been completed. In this situation, calling glFlush ensures that the hardware begins processing commands immediately. After flushing the command buffer, your application should perform work that does not need that resource. It can perform other work (even modifying other OpenGL objects).\n\nYour application needs to change the drawable object associated with the rendering context. Before you can switch to another drawable object, you must call glFlush to ensure that all commands written in the command queue for the previous drawable object have been submitted.\n\nWhen two contexts share an OpenGL object. After submitting any OpenGL commands, call glFlush before switching to the other context.\n\nTo keep drawing synchronized across multiple threads and prevent command buffer corruption, each thread should submit its rendering commands and then call glFlush.\n\nAvoid Querying OpenGL State\n\nCalls to glGet*(), including glGetError(), may require OpenGL to execute previous commands before retrieving any state variables. This synchronization forces the graphics hardware to run lockstep with the CPU, reducing opportunities for parallelism.\n\nYour application should keep shadow copies of any OpenGL state that you need to query, and maintain these shadow copies as you change the state.\n\nWhen errors occur, OpenGL sets an error flag that you can retrieve with the function glGetError. During development, it's crucial that your code contains error checking routines, not only for the standard OpenGL calls, but for the Apple-specific functions provided by the CGL API. If you are developing a performance-critical application, retrieve error information only in the debugging phase. Calling glGetError excessively in a release build degrades performance.\n\nUse Fences for Finer-Grained Synchronization\n\nAvoid using glFinish in your application, because it waits until all previously submitted commands are completed before returning control to your application. Instead, you should use the fence extension (APPLE_fence). This extension was created to provide the level of granularity that is not provided by glFinish. A fence is a token used to mark the current point in the command stream. When used correctly, it allows you to ensure that a specific series of commands has been completed. A fence helps coordinate activity between the CPU and the GPU when they are using the same resources.\n\nFollow these steps to set up and use a fence:\n\nAt initialization time, create the fence object by calling the function glGenFencesAPPLE.\n\nGLint myFence;\n\n\nglGenFencesAPPLE(1,&myFence);\n\nCall the OpenGL functions that must complete prior to the fence.\n\nSet up the fence by calling the function glSetFenceAPPLE. This function inserts a token into the command stream and sets the fence state to false.\n\nvoid glSetFenceAPPLE(GLuint fence);\n\nfence specifies the token to insert. For example:\n\nglSetFenceAPPLE(myFence);\n\nCall glFlush to force the commands to be sent to the hardware. This step is optional, but recommended to ensure that the hardware begins processing OpenGL commands.\n\nPerform other work in your application.\n\nWait for all OpenGL commands issued prior to the fence to complete by calling the function glFinishFenceAPPLE.\n\nglFinishFenceAPPLE(myFence);\n\nAs an alternative to calling glFinishFenceAPPLE, you can call glTestFenceAPPLE to determine whether the fence has been reached. The advantage of testing the fence is that your application does not block waiting for the fence to complete. This is useful if your application can continue processing other work while waiting for the fence to trigger.\n\nglTestFenceAPPLE(myFence);\n\nWhen your application no longer needs the fence, delete it by calling the function glDeleteFencesAPPLE.\n\nglDeleteFencesAPPLE(1,&myFence);\n\nThere is an art to determining where to insert a fence in the command stream. If you insert a fence for too few drawing commands, you risk having your application stall while it waits for drawing to complete. You'll want to set a fence so your application operates as asynchronously as possible without stalling.\n\nThe fence extension also lets you synchronize buffer updates for objects such as vertex arrays and textures. For that you call the function glFinishObjectAPPLE, supplying an object name along with the token.\n\nFor detailed information on this extension, see the OpenGL specification for the Apple fence extension.\n\nAllow OpenGL to Manage Your Resources\n\nOpenGL allows many data types to be stored persistently inside OpenGL. Creating OpenGL objects to store vertex, texture, or other forms of data allows OpenGL to reduce the overhead of transforming the data and sending them to the graphics processor. If data is used more frequently than it is modified, OpenGL can substantially improve the performance of your application.\n\nOpenGL allows your application to hint how it intends to use the data. These hints allow OpenGL to make an informed choice of how to process your data. For example, static data might be placed in high-speed graphics memory directly connected to the graphics processor. Data that changes frequently might be kept in main memory and accessed by the graphics hardware through DMA.\n\nUse Double Buffering to Avoid Resource Conflicts\n\nResource conflicts occur when your application and OpenGL want to access a resource at the same time. When one participant attempts to modify an OpenGL object being used by the other, one of two problems results:\n\nThe participant that wants to modify the object blocks until it is no longer in use. Then the other participant is not allowed to read from or write to the object until the modifications are complete. This is safe, but these can be hidden bottlenecks in your application.\n\nSome extensions allow OpenGL to access application memory that can be simultaneously accessed by the application. In this situation, synchronizing between the two participants is left to the application to manage. Your application calls glFlush to force OpenGL to execute commands and uses a fence or glFinish to ensure that no commands that access that memory are pending.\n\nWhether your application relies on OpenGL to synchronize access to a resource, or it manually synchronizes access, resource contention forces one of the participants to wait, rather than allowing them both to execute in parallel. Figure 9-4 demonstrates this problem. There is only a single buffer for vertex data, which both the application and OpenGL want to use and therefore the application must wait until the GPU finishes processing commands before it modifies the data.\n\nFigure 9-4  Single-buffered vertex array data\n\nTo solve this problem, your application could fill this idle time with other processing, even other OpenGL processing that does not need the objects in question. If you need to process more OpenGL commands, the solution is to create two of the same resource type and let each participant access a resource. Figure 9-5 illustrates the double-buffered approach. While the GPU operates on one set of vertex array data, the CPU is modifying the other. After the initial startup, neither processing unit is idle. This example uses a fence to ensure that access to each buffer is synchronized.\n\nFigure 9-5  Double-buffered vertex array data\n\nDouble buffering is sufficient for most applications, but it requires that both participants finish processing their commands before a swap can occur. For a traditional producer-consumer problem, more than two buffers may prevent a participant from blocking. With triple buffering, the producer and consumer each have a buffer, with a third idle buffer. If the producer finishes before the consumer finishes processing commands, it takes the idle buffer and continues to process commands. In this situation, the producer idles only if the consumer falls badly behind.\n\nBe Mindful of OpenGL State Variables\n\nThe hardware has one current state, which is compiled and cached. Switching state is expensive, so it's best to design your application to minimize state switches.\n\nDon't set a state that's already set. Once a feature is enabled, it does not need to be enabled again. Calling an enable function more than once does nothing except waste time because OpenGL does not check the state of a feature when you call glEnable or glDisable. For instance, if you call glEnable(GL_LIGHTING) more than once, OpenGL does not check to see if the lighting state is already enabled. It simply updates the state value even if that value is identical to the current value.\n\nYou can avoid setting a state more than necessary by using dedicated setup or shutdown routines rather than putting such calls in a drawing loop. Setup and shutdown routines are also useful for turning on and off features that achieve a specific visual effect—for example, when drawing a wire-frame outline around a textured polygon.\n\nIf you are drawing 2D images, disable all irrelevant state variables, similar to what's shown in Listing 9-3.\n\nListing 9-3  Disabling state variables\n\nglDisable(GL_DITHER);\n\n\nglDisable(GL_ALPHA_TEST);\n\n\nglDisable(GL_BLEND);\n\n\nglDisable(GL_STENCIL_TEST);\n\n\nglDisable(GL_FOG);\n\n\nglDisable(GL_TEXTURE_2D);\n\n\nglDisable(GL_DEPTH_TEST);\n\n\nglPixelZoom(1.0,1.0);\n\n\n// Disable other state variables as appropriate.\nReplace State Changes with OpenGL Objects\n\nThe Be Mindful of OpenGL State Variables section suggests that reducing the number of state changes can improve performance. Some OpenGL extensions also allow you to create objects that collect multiple OpenGL state changes into an object that can be bound with a single function call. Where such techniques are available, they are recommended. For example, configuring the fixed-function pipeline requires many function calls to change the state of the various operators. Not only does this incur overhead for each function called, but the code is more complex and difficult to manage. Instead, use a shader. A shader, once compiled, can have the same effect but requires only a single call to glUseProgram.\n\nOther examples of objects that take the place of multiple state changes include the Vertex Array Range Extension and Uniform Buffers.\n\nUse Optimal Data Types and Formats\n\nIf you don't use data types and formats that are native to the graphics hardware, OpenGL must convert those data types into a format that the graphics hardware understands.\n\nFor vertex data, use GLfloat, GLshort, or GLubyte data types. Most graphics hardware handle these types natively.\n\nFor texture data, you’ll get the best performance if you use the following format and data type combination:\n\nGL_BGRA, GL_UNSIGNED_INT_8_8_8_8_REV\n\nThese format and data type combinations also provide acceptable performance:\n\nGL_BGRA, GL_UNSIGNED_SHORT_1_5_5_5_REV\n\nGL_YCBCR_422_APPLE, GL_UNSIGNED_SHORT_8_8_REV_APPLE\n\nThe combination GL_RGBA and GL_UNSIGNED_BYTE needs to be swizzled by many cards when the data is loaded, so it's not recommended.\n\nUse OpenGL Macros\n\nOpenGL performs a global context and renderer lookup for each command it executes to ensure that all OpenGL commands are issued to the correct rendering context and renderer. There is significant overhead associated with these lookups; applications that have extremely high call frequencies may find that the overhead measurably affects performance. OS X allows your application to use macros to provide a local context variable and cache the current renderer in that variable. You get more benefit from using macros when your code makes millions of function calls per second.\n\nBefore implementing this technique, consider carefully whether you can redesign your application to perform less function calls. Frequently changing OpenGL state, pushing or popping matrices, or even submitting one vertex at a time are all examples of techniques that should be replaced with more efficient operations.\n\nYou can use the CGL macro header (CGL/CGLMacro.h) if your application uses CGL from a Cocoa application. You must define the local variable cgl_ctx to be equal to the current context. Listing 9-4 shows what's needed to set up macro use for the CGL API. First, you need to include the correct macro header. Then, you must set the current context.\n\nListing 9-4  Using CGL macros\n\n#include <CGL/CGLMacro.h> // include the header\n\n\nCGL_MACRO_DECLARE_VARIABLES // set the current context\n\n\nglBegin (GL_QUADS);     // This code now uses the macro\n\n\n    // draw here\n\n\nglEnd ();\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Identifying and Solving Performance Issues",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLProfilerUserGuide/Strategies/Strategies.html#//apple_ref/doc/uid/TP40006475-CH40-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Profiler User Guide\nTable of Contents\nIntroduction\nGetting Started\nUsing Breakpoints\nIdentifying and Solving Performance Issues\nControlling Profiling Programmatically\nRevision History\nNext\nPrevious\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nIdentifying and Solving Performance Issues\n\nThis chapter describes a number of strategies that can help you track down performance issues and understand how OpenGL works with your application. Before you read this chapter, you should already be familiar with how to start and run a profiling session, and how to set breakpoints.\n\nMaking Sure You Use Functions Correctly\n\nOpenGL is an evolving specification. As time goes on, programming practices that were acceptable in the past are replaced by techniques that work much better. There are several functions in the OpenGL specification that you should watch for when you profile your application. If you are using any of these OpenGL functions in your application, make sure that you really need to use them, and that you are using them correctly.\n\nAvoid glBegin because it signals that you are using immediate mode. Unless you are drawing a simple shape or creating a small prototype, immediate mode is typically not an optimal technique for using OpenGL.\n\nglFinish forces your application to wait until all OpenGL commands in the pipeline have executed on the graphics card.\n\nglFlush is typically not needed because flushing is usually handled by other calls, such as CGLFlushDrawable. The function glFlush flushes at the macro level, an expensive operation. Flushing calls provided by the OS X interfaces (AGL, CGL, Cocoa OpenGL classes) act at a microlevel to give finer control over flushing and, as a result, much better performance.\n\nglTexImage and related calls. Check to see if you are using this function to redefine the texture each frame. If so, change your code to define the texture outside frame rendering. If your data changes, you can instead use a data replacement technique, such as pixel buffer objects, or call glTexSubImage to specify the image again. Keep in mind that you can call glTexSubImage to specify the entire image again (maintaining the current texture parameters), and it will be a less expensive call than glTexImage. You should call glTexImage only if you must specify a larger image.\n\nAny form of glGet or glIs. Getting state values slows your application. Unless your application is a “middle ware” application, you shouldn’t need to retrieve state values. During development, however, it’s quite common to call glGetError. When your application is ready to go into production, make sure that your remove glGetError calls and any other state getting and checking functions. As an alternative during development, you can look for errors by setting OpenGL Profiler to break on errors.\n\nglPushAttrib or glPopAttrib. You should keep track of OpenGL state instead of using the server attribute stack.\n\nThe function glDrawPixels is a convenience function that, under the hood, uses screen-aligned textured quads. If you use screen-aligned textured quads directly, you’ll save the overhead of calling glDrawPixels. Make sure that when you use textured quads that you also use appropriate texture filtering.\n\nIf you are calling glReadPixels, you should also be using pixel buffer objects.\n\nglProgramLocalParameter and glProgramEnvParameter calls. These calls, defined by the GL_ARB_vertex_program extension, load only one parameter at a time. It’s more efficient to use glProgramLocalParameters and glProgramEnvParameters, defined by the GL_EXT_gpu_program_parameters extension, which loads multiple parameters.\n\nIdentifying Problem Areas in Your Application\n\nYou can get a quick view of what your application is doing by collecting a trace for a single frame. Although viewing the trace and statistics for one frame provides a narrow view of application behavior, you can use this strategy to narrow down problem areas in your application.\n\nTo collect a trace for a single frame:\n\nLaunch or attach to the application of interest.\n\nOpen the Statistics window.\n\nNavigate in the application to the area where you suspect a problem.\n\nSet a breakpoint on the function CGLFlushDrawable. If you are using a single-buffered rendering context, you might also need to set a breakpoint on the function glFlush.\n\nWhen your application pauses, click Clear in the Statistics window.\n\nClick Continue in the Breakpoints window. When your application pauses, make sure it rendered the frame completely. If not, your code likely calls more than one flush operation per frame.\n\nThen, check for the following in the Statistics window:\n\nState management. Check to see if you are calling glPopAttrib calls. If possible you should instead track your own state. It’s not a good idea to set state on a per frame basis. It’s best to consolidate state changes and set them outside the frame.\n\nCalls that take significantly more time than the others.\n\nAny OpenGL calls that are listed in Making Sure You Use Functions Correctly.\n\nClick the Continue button in the Breakpoints window to resume execution of your application.\n\nChecking for Optimal Data Types and Formats\n\nTo get the best performance, make sure your data is using an optimal data type and format combination. You won’t get the best performance otherwise.\n\nIdeally, you’ll want to use:\n\nGL_BGRA, GL_UNSIGNED_INT_8_8_8_8_REV\n\nThis is the fastest data type and format combination. If it isn’t fast, you may have a problem with your driver.\n\nIf that’s not possible, you can often get acceptable performance from the following. Just make sure to test these combinations on a device-by-device basis:\n\nGL_BGRA, GL_UNSIGNED_SHORT_1_5_5_5_REV\n\nGL_YCBR_422_APPLE, GL_UNSIGNED_SHORT_8_8_REV_APPLE\n\nOpenGL Profiler does not know what the inbound data format is. So you need to check the point at which your data gets uploaded by OpenGL by performing this steps:\n\nSelect Collect Trace and Include Backtraces in the OpenGL Profiler window.\n\nSet a breakpoint where your data is uploaded.\n\nWhen your application breaks, click Call Stack in the Trace window.\n\nManaging Trace Data\n\nThe amount of data generated when collecting a trace can be overwhelming. Most of the time you’ll collect a trace for only one frame, which is typically enough to track down the most common issues. (See Identifying Problem Areas in Your Application.) If you do need to collect more than a frame of data, you can create a custom shell script that operates on trace data so that you can get it into a more manageable state. When you want to apply a script, click the Filter button in the Trace window. OpenGL Profiler provides the trace data as input (stdin) to your script and writes the results from your scripting calls to stdout.\n\nIf a drawing call is slow and you suspect a shader is the cause, you may need to collect more than frame of data. You can then use a script to pare down the data to suspicious calls. Listing 3-1 shows a very simple script that sorts calls by function call time, with the slowest calls last. Your script would need to be customized so that it performs operations appropriate for the problem you are trying to isolate. After you use a script to identify suspicious calls, you can use the line number of the output to trace back to where the call actually took place.\n\nListing 3-1  A shell script for finding the slowest calls in a trace\n\n#!/bin/tcsh -f\n\n\nawk '{ print $3\" \"$0 }' | sort -n\n\nTo create a script:\n\nOpen an application, such as Xcode, that lets you create a plain text file.\n\nEnter the appropriate script commands using any scripting language that accepts input from stdin.\n\nSave the script with a .filter extension.\n\nYou can save it to any location that you’d like.\n\nFor the first script that you create, select it in the Finder and choose File > Get Info.\n\nMake sure that the “Open with” pop-up menu is set to the application that you used for creating the script. This ensures that OpenGL Profiler will use this application to open any files with the .filter extension.\n\nTo filter trace data:\n\nOpen the Trace window.\n\nClick Browse, navigate to the script you want to use, and select it.\n\nYou can modify an existing script by clicking Open.\n\nClick Filter, then provide a name for the output file.\n\nChecking for Application Errors, Thread Conflicts, and Software Fallbacks\n\nYou can quickly check for errors in your application by setting one or more error breakpoints. To use OpenGL Profiler to check for application errors:\n\nChoose View > Breakpoints.\n\nSelect one or more of these options:\n\nBreak on error. Your application pauses when it encounters any type of error.\n\nBreak on VAR error. Your application pauses when there is a problem using the vertex array range extension.\n\nBreak on thread conflict. You can select this if your application is multithreaded.\n\nBreak on SW fallback. You application pauses when it uses the software renderer as a fallback. Although this condition is not strictly an error, it alerts you to situations for which the system does not have the appropriate hardware renderer to carry out a particular OpenGL call.\n\nIf you have not already done so, launch or attach your application.\n\nMonitor the Breakpoints window for errors.\n\nEvaluating The Effect Of The Multithreaded OpenGL Engine\n\nIn OS X v10.5 and later, the OpenGL framework can offload processing onto a separate thread that runs on a different CPU core. You use the CGLEnable function to enable multithreaded execution programmatically using this code:\n\n#include <OpenGL/OpenGL.h>\n\n\n \n\n\nCGLError err = 0;\n\n\nCGLContextObj ctx = CGLGetCurrentContext();\n\n\n \n\n\n// Enable the multi-threading\n\n\nerr =  CGLEnable( ctx, kCGLCEMPEngine);\n\n\n \n\n\nif (err != kCGLNoError )\n\n\n{\n\n\n     // Multi-threaded execution is possibly not available\n\n\n     // Insert your code to take appropriate action\n\n\n}\n\nFor more details see Technical Note TN2085: Enabling multi-threaded execution of the OpenGL framework.\n\nAfter enabling multithreading, some applications see a dramatic increase in OpenGL performance; others might not. In general, the multithreaded OpenGL engine is a good option for applications that are CPU bound.\n\nIf your application enables the multithreaded OpenGL engine, it’s a good idea to check whether it actually improves performance. After you programmatically enable multithreading, evaluate its effect by following these steps:\n\nLaunch or attach to the application of interest.\n\nIn the main OpenGL Profiler window, check the frame rate at a point in your application that is repeatable. You’ll check this same point later.\n\nChoose Views > Breakpoints.\n\nMake sure the multithreaded control option is set to “App control.”\n\nIn the main OpenGL Profiler window, check the frame rate.\n\nSet a breakpoint on a function.\n\nAlthough you can choose any function, typically you’d set a breakpoint on CGLFlushDrawable for a double-buffered rendering context or glFlush for a single-buffered rendering context.\n\nWhen your application pauses, select the “Force off” option for multithreaded control.\n\nPress Continue to resume execution of your application.\n\nIn the main OpenGL Profiler window, check the frame rate.\n\nCompare this frame rate to the rate you observed when using the multithreaded OpenGL engine.\n\nNote: If you are using the multithreaded OpenGL engine, debugging is often easier after you use the option in OpenGL Profiler to turn off multithreading.\n\nMonitoring GPU Use\n\nApplications that can’t use the GPU for some reason (such as the graphics card does not support some of the OpenGL extensions that your code uses) use the software renderer as a fallback. If you notice a drop in the performance of your application, you may want to check whether the application is using the GPU as you expect.\n\nStarting in OS X v10.5, you can set your application to break whenever it uses the software renderer as a fallback. (See Checking for Application Errors, Thread Conflicts, and Software Fallbacks.) Prior to OS X v10.5, you can monitor the GPU use of your application whenever your application pauses at a breakpoint. The best breakpoints to check are:\n\nCGLFlushDrawable\n\nAfter glUseProgramObjectARB\n\nAfter glBindProgramARB\n\nAny glDraw or related command, including the following:\n\nglAccum\n\nglBegin\n\nglBlitFramebufferEXT\n\nglBitmap\n\nglClear\n\nglCopyPixels\n\nglDrawPixels\n\nglReadPixels\n\nglCopyTexSubImage*D\n\nglCopyTexImage*D\n\nglDrawArrays\n\nglDrawElements\n\nglDrawRangeElements\n\nWhen your application pauses at these breakpoints, check the values of kCGLCPGPUFragmentProcessing and kCGLCPGPUVertexProcessing shown in the Call Stack pane of the Breakpoints window. A value of GL_TRUE indicates that your application is using the GPU for the process associated with the constant.\n\nUsing Window Resizing to Diagnose Performance\n\nIf your application renders to a window, you can often identify the cause of performance issues by resizing the window. While your application renders to a window, resize it. After you shrink the window, if the execution time is significantly faster, then the issue might be related to low VRAM. If the execution time is faster proportional to the window size, then your application is fragment bound. If the execution time is the same, then your application either is vertex bound on the GPU or is CPU bound.\n\nIdentifying Unmatched Calls\n\nMany calls in OpenGL are used as sets, such as:\n\nglBegin and glEnd\n\nglEnable and glDisable\n\nYou can check the # of Calls column in the Statistics window to make sure that these (and similar sets) are always matched in your application. For example, if you find 5 glBegin calls but only 3 glEnd call, you should modify your code so that you have the same number of each. Unmatched calls typically are a symptom of unneeded code and always indicate imprecise code. Call sets should always match within a frame.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "Drawing to the Full Screen",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_fullscreen/opengl_cgl.html#//apple_ref/doc/uid/TP40001987-CH210-SW6",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nDrawing to the Full Screen\n\nIn OS X, you have the option to draw to the entire screen. This is a common scenario for games and other immersive applications, and OS X applies additional optimizations to improve the performance of full-screen contexts.\n\nFigure 4-1  Drawing OpenGL content to the full screen\n\nOS X v10.6 and later automatically optimize the performance of screen-sized windows, allowing your application to take complete advantage of the window server environment on OS X. For example, critical operating system dialogs may be displayed over your content when necessary.\n\nFor information about high-resolution and full-screen drawing, see Use an Application Window for Fullscreen Operation.\n\nCreating a Full-Screen Application\n\nCreating a full-screen context is very simple. Your application should follow these steps:\n\nCreate a screen-sized window on the display you want to take over:\n\nNSRect mainDisplayRect = [[NSScreen mainScreen] frame];\n\n\nNSWindow *fullScreenWindow = [[NSWindow alloc] initWithContentRect: mainDisplayRect styleMask:NSBorderlessWindowMask backing:NSBackingStoreBuffered defer:YES];\n\nSet the window level to be above the menu bar.:\n\n[fullScreenWindow setLevel:NSMainMenuWindowLevel+1];\n\nPerform any other window configuration you desire:\n\n[fullScreenWindow setOpaque:YES];\n\n\n[fullScreenWindow setHidesOnDeactivate:YES];\n\nCreate a view with a double-buffered OpenGL context and attach it to the window:\n\nNSOpenGLPixelFormatAttribute attrs[] =\n\n\n{\n\n\n    NSOpenGLPFADoubleBuffer,\n\n\n    0\n\n\n};\n\n\nNSOpenGLPixelFormat* pixelFormat = [[NSOpenGLPixelFormat alloc] initWithAttributes:attrs];\n\n\n \n\n\nNSRect viewRect = NSMakeRect(0.0, 0.0, mainDisplayRect.size.width, mainDisplayRect.size.height);\n\n\nMyOpenGLView *fullScreenView = [[MyOpenGLView alloc] initWithFrame:viewRect pixelFormat: pixelFormat];\n\n\n[fullScreenWindow setContentView: fullScreenView];\n\nShow the window:\n\n[fullScreenWindow makeKeyAndOrderFront:self];\n\nThat’s all you need to do. Your content is in a window that is above most other content, but because it is in a window, OS X can still show critical UI elements above your content when necessary (such as error dialogs). When there is no content above your full-screen window, OS X automatically attempts to optimize this context’s performance. For example, when your application calls flushBuffer on the NSOpenGLContext object, the system may swap the buffers rather than copying the contents of the back buffer to the front buffer. These performance optimizations are not applied when your application adds the NSOpenGLPFABackingStore attribute to the context. Because the system may choose to swap the buffers rather than copy them, your application must completely redraw the scene after every call to flushBuffer. For more information on NSOpenGLPFABackingStore, see Ensuring That Back Buffer Contents Remain the Same.\n\nAvoid changing the display resolution from that chosen by the user. If your application needs to render data at a lower resolution for performance reasons, you can explicitly create a back buffer at the desired resolution and allow OpenGL to scale those results to the display. See Controlling the Back Buffer Size.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Controlling Profiling Programmatically",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLProfilerUserGuide/ProgrammaticControl/ProgrammaticControl.html#//apple_ref/doc/uid/TP40006475-CH50-SW9",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Profiler User Guide\nTable of Contents\nIntroduction\nGetting Started\nUsing Breakpoints\nIdentifying and Solving Performance Issues\nControlling Profiling Programmatically\nRevision History\nNext\nPrevious\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nControlling Profiling Programmatically\n\nYou can add code to your application that allows it to interact with OpenGL Profiler during a profiling session. This chapter shows you how to perform these tasks programmatically:\n\nSetting a Breakpoint\n\nWriting Comments to the Trace Window\n\nControlling Trace Collection\n\nControlling Statistics Collection\n\nSetting a Breakpoint\n\nYour application can programmatically set breakpoints when it is attached to OpenGL Profiler.\n\nTo set a breakpoint:\n\nInclude the CGLProfiler.h and CGLProfilerFunctionEnum.h header files in your application.\n\nDeclare an array of three GLint values, set to the following:\n\nThe function ID, as defined in the header file CGLProfilerFunctionEnum.h.\n\nThe logical OR of kCGLProfBreakBefore or kCGLProfBreakAfter, indicating how you want the breakpoint to stop—before entering the OpenGL function, on return from it, or both.\n\nA Boolean that turns the breakpoint on or off.\n\nCall the function CGLSetOption, passing the array as a parameter.\n\nListing 4-1 shows code that sets a breakpoint before the CGLFlushDrawable function.\n\nListing 4-1  Code that sets a breakpoint\n\n#include \"OpenGL/CGLProfiler.h\"\n\n\n#include \"OpenGL/CGLProfilerFunctionEnum.h\"\n\n\n...\n\n\n   GLint myBreakpoint[] = { kCGLFECGLFlushDrawable, kCGLProfBreakBefore, 1;}\n\n\n   CGLSetOption( kCGLGOEnableBreakpoint, myBreakpoint );\n\n\n...\nWriting Comments to the Trace Window\n\nYour application can programmatically write comments to the Trace window during a profiling session. To write comments:\n\nInclude the CGLProfiler.h header file in your application.\n\nCall the function CGLSetOption with the constant kCGLGOComment and your comment cast as a long.\n\nListing 4-2 shows code that writes a comment that looks like this in the Trace window:\n\n21561: 0.00 µs /* ***** My Comment is here ***** */\n\nListing 4-2  Code that writes a comment to the Trace window\n\n#include <OpenGL/CGLProfiler.h>\n\n\n...\n\n\n  CGLSetOption(kCGLGOComment, (long) \"***** My Comment is here *****\");\n\n\n...\nControlling Trace Collection\n\nYour application can programmatically control when to start and stop collecting a trace, which lets you control which traces to collect in a specific part of your application or during a particular period of time. You can also clear the Trace window.\n\nTo control trace collection:\n\nInclude the CGLProfiler.h header file in your application.\n\nCall the function CGLSetOption with the constant kCGLGOEnableFunctionTrace and either GL_TRUE (to turn on trace collection) or GL_FALSE (to turn off trace collection).\n\nListing 4-3 shows code that enables trace collection.\n\nListing 4-3  Code that enables trace collection\n\n#include <OpenGL/CGLProfiler.h>\n\n\n...\n\n\n  CGLSetOption(kCGLGOEnableFunctionTrace, GL_TRUE);\n\n\n...\n\nTo clear the Trace window:\n\nInclude the CGLProfiler.h header file in your application.\n\nCall the function CGLSetOption with the constant kCGLGOResetFunctionTrace and the value NULL.\n\nListing 4-4 shows code that enables trace collection.\n\nListing 4-4  Code that clears the Trace window\n\n#include <OpenGL/CGLProfiler.h>\n\n\n...\n\n\n  CGLSetOption(kCGLGOResetFunctionTrace, NULL);\n\n\n...\nControlling Statistics Collection\n\nYou application can programmatically control when to start and stop collecting statistics. You must make sure that the Statistics window in OpenGL Profiler is open when you profile your application.\n\nTo control statistics collection:\n\nInclude the CGLProfiler.h header file in your application.\n\nCall the function CGLSetOption with the constant kCGLGOResetFunctionStatistics and the value NULL to first reset counters to 0. This step is optional.\n\nCall the function CGLSetOption with the constant kCGLGOResetFunctionStatistics and the value GL_TRUE to start statistics collection.\n\nWhen you are done collecting statistics, call the function CGLSetOption with the constant kCGLGOResetFunctionStatistics and the value GL_FALSE.\n\nListing 4-5 shows code that resets counters, starts statistics collection, and then stops it.\n\nListing 4-5  Code that starts and stops statistics collection\n\n#include <OpenGL/CGLProfiler.h>\n\n\n...\n\n\n  // Reset counters to 0\n\n\n  CGLSetOption(kCGLGOResetFunctionStatistics, NULL);\n\n\n  // Start statistics collection\n\n\n  CGLSetOption(kCGLGOEnableFunctionStatistics, GL_TRUE);\n\n\n...\n\n\n  // Stop statistics collection\n\n\n  CGLSetOption(kCGLGOEnableFunctionStatistics, GL_FALSE);\n\n\n...\nNext\nPrevious\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "Document Revision History",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_rev_hx/opengl_rev_hx.html#//apple_ref/doc/uid/TP40001987-CH1000-TP9",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nDocument Revision History\n\nThis table describes the changes to OpenGL Programming Guide for Mac.\n\nDate\tNotes\n2018-06-04\t\n\nUpdated with information on supporting high-resolution displays.\n\n\n2012-07-23\t\n\nUpdated with information on supporting high-resolution displays.\n\n\n2011-06-06\t\n\nAdded new context options.\n\n\n2010-11-15\t\n\nFixed a few small errors in the texture chapter.\n\n\n \t\n\nUpdated the recommendations on when to use each texture uploading and downloading technique.\n\n\n \t\n\nUpdated the code for creating a texture from a view’s contents to use newer, better supported techniques.\n\n\n2010-06-14\t\n\nCorrected texture creation code snippets.\n\n\n2010-03-24\t\n\nMinor updates and clarifications.\n\n\n2010-02-24\t\n\nSubstantial revisions to describe behaviors for OpenGL on OS X v10.5 and OS X v10.6. Removed information on obsolete and deprecated behaviors.\n\n\n2009-08-28\t\n\nCorrected errors in code listings. Pixel format attribute lists should be terminated with 0, not NULL. One call to glTexImage2D had an incorrect number of parameters.\n\n\n2008-06-09\t\n\nUpdated the Cocoa OpenGL tutorial and made numerous other minor changes.\n\n\n \t\n\nFixed compilation errors in Listing 8-1.\n\n\n \t\n\nAdded Getting Decompressed Raw Pixel Data from a Source Image.\n\n\n \t\n\nUpdated links to OpenGL extensions.\n\n\n \t\n\nMade several minor edits.\n\n\n2007-12-04\t\n\nCorrected minor typographical and technical errors.\n\n\n \t\n\nAdded Ensuring That Back Buffer Contents Remain the Same.\n\n\n \t\n\nRevised Deprecated Attributes.\n\n\n2007-08-07\t\n\nFixed several technical issues.\n\n\n2007-05-29\t\n\nFixed a broken link.\n\n\n2007-05-17\t\n\nFixed a few technical inaccuracies in the code listings.\n\n\n \t\n\nChanged attribs to attributes in Listing 6-2.\n\n\n \t\n\nFixed drawRect method implementation in Drawing to a Window or View.\n\n\n2006-12-20\t\n\nFixed minor errors.\n\n\n \t\n\nAdded information concerning the Apple client storage extension. Fixed a typographical error.\n\n\n2006-11-07\t\n\nAdded information about performance issues and processor queries.\n\n\n \t\n\nSee Determining Whether Vertex and Fragment Processing Happens on the GPU.\n\n\n2006-10-03\t\n\nAdded a section on checking for GPU processing.\n\n\n \t\n\nAdded Determining Whether Vertex and Fragment Processing Happens on the GPU.\n\n\n \t\n\nFixed a number of minor typos in the code and in the text.\n\n\n2006-09-05\t\n\nFixed minor technical problems.\n\n\n2006-07-24\t\n\nMade minor technical and typograhical changes throughout.\n\n\n \t\n\nAdded information to Surface Drawing Order Specifies the Position of the OpenGL Surface Relative to the Window.\n\n\n \t\n\nChanged glCopyTexSubImage to glCopyTexSubImage2D in Downloading Texture Data.\n\n\n \t\n\nMade minor improvements to Listing 11-6.\n\n\n \t\n\nRemoved information about 1-D textures.\n\n\n2006-06-28\t\n\nMade several minor technical corrections.\n\n\n \t\n\nRedirected links to the OpenGL specification for the framebuffer object extension so that they point to the SGI Open Source website, which hosts the most up-to-date version of this specification.\n\n\n \t\n\nRemoved the logic operation blending entry from Table A-6 because this functionality is not available in OpenGL 2.0.\n\n\n2006-05-23\t\n\nFirst version.\n\n\n \t\n\nThis document replaces Macintosh OpenGL Programming Guide and AGL Programming Guide.\n\n\n \t\n\nThis document incorporates information from the following Technical Notes:\n\n\n \t\n\nTN2007 “The CGDirectDisplay API”\n\n\n \t\n\nTN2014 “Insights on OpenGL”\n\n\n \t\n\nTN2080 “Understanding and Detecting OpenGL Functionality”\n\n\n \t\n\nTN2093 “OpenGL Performance Optimization: The Basics”\n\n\n \t\n\nThis document incorporates information from the following Technical Q&As:\n\n\n \t\n\nTechnical Q&A OGL01 “aglChoosePixelFormat, The Inside Scoop”\n\n\n \t\n\nTechnical Q&A OGL02 “Correct Setup of an AGLDrawable”\n\n\n \t\n\nTechnical Q&A QA1158 “glFlush() vs. glFinish()”\n\n\n \t\n\nTechnical Q&A QA1167 “Using Interface Builder's NSOpenGLView or Custom View objects for an OpenGL application”\n\n\n \t\n\nTechnical Q&A QA1188 “GetProcAdress and OpenGL Entry Points”\n\n\n \t\n\nTechnical Q&A QA1209 “Updating OpenGL Contexts”\n\n\n \t\n\nTechnical Q&A QA1248 “Context Sharing Tips”\n\n\n \t\n\nTechnical Q&A QA1268 “Sharpening Full Scene Anti-Aliasing Details”\n\n\n \t\n\nTechnical Q&A QA1269 “OS X OpenGL Interfaces”\n\n\n \t\n\nTechnical Q&A QA1325 “Creating an OpenGL texture from an NSView”\n\n\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Getting Started",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLShaderBuilderUserGuide/UserInterface/UserInterface.html#//apple_ref/doc/uid/TP40006476-CH2-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Shader Builder User Guide\nTable of Contents\nIntroduction\nGetting Started\nBuilding Shaders\nRevision History\nNext\nPrevious\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nGetting Started\n\nOpenGL Shader Builder is a development environment for writing, testing, and experimenting with OpenGL shaders. OpenGL Shader Builder not only speeds development for seasoned shader developers, but it can help those new to writing shaders to explore how shaders work. Using it, you can focus on the shader code; OpenGL Shader Builder takes care of the rest. You can use it to:\n\nParse source code and check the syntax\n\nCompile and link source code files to create shader objects\n\nChange and animate the values of uniform variables\n\nPreview textures before applying them to an object\n\nBenchmark performance\n\nEnable and disable a shader so you can see its effect more clearly\n\nDownload the GraphicsTools app from http://developer.apple.com/downloads.\n\nIf you’ve used OpenGL Shader Builder before, you’ll notice that the user interface for version 2.0 is a bit different from the previous version. Before you start using it, you’ll want to get acquainted with the four views it provides—Program, Render, Textures, and Symbols.\n\nProgram View\n\nWhen you launch OpenGL Shader Builder, it opens to the Program view shown in Figure 1-1. You use this view to manage source code files and to check linking and validation of the code.\n\nFigure 1-1  The Program view\n\nThe top part of the view is used for listing source code filenames. You can add files in any of these ways:\n\nDrag previously created files to the window.\n\nUse the Add Shaders button to navigate and choose previously created files.\n\nChoose File > New > to create a new file for a GLSL program (vertex, fragment, geometry) or an ARB program (vertex, fragment).\n\nFigure 1-2 shows the Program view after you’ve adding fragment and vertex source code files. The link log, link results, and validation log appear below the file list. The link status, which in this case is “Link succeeded,” appears in the lower-right corner of the window.\n\nFigure 1-2  The Program view after adding two files\nSource Code Files\n\nYou can view and modify the contents of each source code file by double-clicking its name in the file list. The file opens in a new window, as shown in Figure 1-3. When you create a new source code file, it opens automatically in a new document window. In contrast, new source code files open in a new document window automatically. These new source code files contain template code that you can modify or replace to suit your needs.\n\nFigure 1-3  A source code file opens in a separate document window\nControls for Geometry Shaders\n\nA geometry shader object is made up of a geometry program and a vertex program. When you add the geometry source code file to the program list, the user interface changes (see Figure 1-4) to show controls for the following OpenGL parameters, which the GL_NV_geometry_shader4 extension defines:\n\nGEOMETRY_VERTICES_OUT_EXT is the maximum number of vertices produced by the geometry shader.\n\nGEOMETRY_INPUT_TYPE_EXT is the geometry that the shader takes as input: POINTS, LINES, LINES_ADJACENCY_EXT, TRIANGLES, or TRIANGLES_ADJACENCY_EXT.\n\nGEOMETRY_OUTPUT_TYPE_EXT is the geometry that the shader produces: POINTS, LINE_STRIP, or TRIANGLE_STRIP.\n\nFigure 1-4  The Program view after adding a geometry shader\nRender View\n\nThe Render view, shown in Figure 1-5, visualizes what your code does. Although you can click the Render tab to switch between the Program and Render views, it’s more efficient to double-click the Render tab to open the view in a separate Render window. That way, you can look at the rendering results side-by-side with the contents of the Program, Textures, and Symbols views.\n\nFor details on customizing the layout of windows, see Creating and Saving a Layout.\n\nFigure 1-5  The Render view\n\nThe pop-up menu lets you choose from among several geometries—Teapot Wire, Teapot Point, Plane, Teapot, Squiggle, Sphere, or Torus—to apply your code to. You can interact with any of the 3D geometries by clicking and dragging the pointer.\n\nTextures View\n\nThe Textures view, shown in Figure 1-6, lets you add and set up textures to use as input to fragment programs. To add a texture, you simply drag it to one of the image wells on the right side of the view. When you select an image well, the texture appears on the left side of the view.\n\nAfter selecting a texture, you can adjust any of the following by choosing the appropriate OpenGL constant from the provided pop-up menus:\n\nTexture types: 1D, 2D, Rectangle, 3D, Cube_MAP, SHADOW_1D, SHADOW_2D, or SHADOW_RECTANGLE\n\nMethods of filtering: NEAREST, LINEAR, NEAREST_MIPMAP_NEAREST, LINEAR_MIPMAP_NEAREST, NEAREST_MIPMAP_LINEAR, or LINEAR_MIPMAP_LINEAR\n\nWrapping modes: REPEAT, CLAMP, CLAMP_TO_EDGE, CLAMP_TO_BORDER, or MIRRORED_REPEAT\n\nWhen you change the texture type, filter, or wrapping mode, you get immediate feedback on the effect. As a result, you’ll be able to compare filtering methods and wrapping modes easily.\n\nFigure 1-6  The Textures view\n\nYou can get an idea of how OpenGL maps a texture to an object by looking an an alternate view of the texture. To see an alternate view, double-click the texture that appears in the view on the left. The default view is a flat representation of the texture without the wrapping mode. The alternate view maps the texture in the dimension of its type (1D, 2D, 3D, Cube) and applies the filtering and wrapping modes.\n\nFigure 1-7 is the alternate view for the texture shown in Figure 1-6. This view shows the repeating pattern caused by choosing the REPEAT wrapping mode.\n\nFigure 1-7  The Textures alternate view\n\nFor more details on working with textures, see Adding Textures and Using Alternate Texture Views.\n\nSymbols View\n\nAfter you add shaders to the Program view, you can view its uniform variables in the Symbols view, as shown in Figure 1-8. You can modify and animate GLSL uniform variables and ARB environment and local parameters.\n\nFigure 1-8  The Symbols view\n\nFor more information, see Modifying Uniform Variables.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "Building Shaders",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLShaderBuilderUserGuide/Tasks/Tasks.html#//apple_ref/doc/uid/TP40006476-CH3-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Shader Builder User Guide\nTable of Contents\nIntroduction\nGetting Started\nBuilding Shaders\nRevision History\nNext\nPrevious\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nBuilding Shaders\n\nThis chapter explains how to use OpenGL Shader Builder to set up projects, add shader code, add resources, and modify variables. Before reading this chapter, you should already be familiar with OpenGL and know how to write at least one of the following:\n\nARB fragment and vertex programs. See the OpenGL extensions GL_ARB_fragment_program and GL_ARB_vertex_program.\n\nGLSL fragment and vertex shaders. See OpenGL Shading Language (PDF).\n\nOpenGL Shader Builder also supports geometry shaders, a recent addition to the OpenGL specification (see the OpenGL Extension GL_NV_geometry_shader4). Geometry shaders are not supported on all graphics cards. But because the Apple software renderer steps in as a fallback when necessary, you can use OpenGL Shader Builder to develop them.\n\nCreating and Saving Projects\n\nA project is the set of resources that make up one program—vertex, fragment, and geometry source files, and textures. As with any development environment, you can name and save projects. You can also have more than one project open at a time.\n\nWhen you launch OpenGL Shader Builder, it opens to an empty, untitled project. To save the project, choose File > Save Project and enter a project name. A project can contain as many source files and textures as you’d like. Using the checkboxes in the file list, you can select which source files to make active.\n\nCreating and Saving a Layout\n\nA layout specifies the location and number of windows that you want OpenGL Shader Builder to provide when you open new and existing projects.\n\nTo create and save a layout:\n\nLaunch OpenGL Shader Builder.\n\nDouble-click each tab whose view you want to open in a separate window.\n\nArrange the windows to suit your preference.\n\nChoose Window > Save Layout.\n\nWhenever you launch OpenGL Shader Builder, it automatically sets up the environment for you using your preferred layout.\n\nAdding Textures\n\nTo add a texture to the Textures view, drag the texture file to an image well. You can add any of these texture targets: 1D, 2D, RECTANGLE, SHADOW_1D, SHADOW_2D, and SHADOW_RECTANGLE.\n\nTo add a 3D texture, you drag all the necessary files to an image well. The number of images in this texture target must be a power of 2 (2, 4, 8, 16, 32, and so on). Otherwise, OpenGL Shader Builder inserts a default image in the z direction.\n\nYou can also drag CUBE_MAP textures to an image well, but because this texture targets require more than one file, you’ll first need to name the files so that OpenGL Shader Builder can place them properly. Figure 2-1 shows the layout that OpenGL Shader Builder uses for cube maps.\n\nFigure 2-1  Cube face layout for a cube map\n\nTip: When adding cube maps or textures, after you add one image file to the image well, you can then drag other images to the individual faces that OpenGL Shader Builder automatically generates.\n\nTo add a cube map :\n\nName each texture file using a convention that specifies the location within the cube map or 3D texture.\n\nFor cube maps, you can use any of the conventions listed in Table 2-1. For example, if the base filename for a cube map is mycube, you could name the texture files: mycube_back, mycube_down, mycube_forward, mycube_left, mycube_right, and mycube_up.\n\nIn the Finder, select the texture files and drag them to an image well.\n\nTip: If you name the cube map files correctly and place them in the same directory, you need to drag only one file in that directory to the view on the left side. OpenGL Shader Builder then reads all files in entire directory and places all the images for you.\n\nTable 2-1  Naming conventions that map texture files to cube faces in a cube map\n\nX+ face\n\n\t\n\nX– face\n\n\t\n\nY+ face\n\n\t\n\nY– face\n\n\t\n\nZ+ face\n\n\t\n\nZ– face\n\n\n\n\nposx\n\n\t\n\nnegx\n\n\t\n\nposy\n\n\t\n\nnegy\n\n\t\n\nposz\n\n\t\n\nnegz\n\n\n\n\nxpos\n\n\t\n\nxneg\n\n\t\n\nypos\n\n\t\n\nyneg\n\n\t\n\nzpos\n\n\t\n\nzneg\n\n\n\n\nright\n\n\t\n\nleft\n\n\t\n\ntop\n\n\t\n\ndown\n\n\t\n\nfront\n\n\t\n\nback\n\n\n\n\nrt\n\n\t\n\nlf\n\n\t\n\nup\n\n\t\n\ndn\n\n\t\n\nft\n\n\t\n\nbk\n\n\n\n\n+x\n\n\t\n\n–x\n\n\t\n\n+y\n\n\t\n\n–y\n\n\t\n\n+z\n\n\t\n\n–z\n\nUsing Alternate Texture Views\n\nOpenGL Shader Builder provides two ways for you to view textures. The default view shows the texture data simply as a flat representation. The alternate view shows how the texture appears when applied to a target, using the filter and wrap modes that you choose. The alternate view is especially useful if you are unsure of how a particular filter or wrap mode will affect the outcome. You might also find the alternate view helpful to visualize 3D and cube maps.\n\nThe alternate view is particularly useful for cube maps. The default view in Figure 2-2 shows the “unfolded” cube, while the alternate view in Figure 2-3 projects the cube faces in three dimensions.\n\nTo see the alternate view, double-click the texture.\n\nFigure 2-2  The default view for a cube map\n\nIf you have not supplied multiple files for a cube map or 3D texture or if you’ve not used a location-based naming convention (see Adding Textures), you’ll notice rectangles with a number or letters in them when you switch to alternate view.\n\nFigure 2-3  The alternate view for a cube map\nModifying Uniform Variables\n\nOpenGL Shader Builder automatically lists uniform variables from your source code in the Symbols view. How these uniform variable are displayed depend on the type of program. GLSL shaders use a shared symbol table for a single program object, so you’ll see the uniform variables appear in a single list . In contrast, ARB fragment and vertex programs have their own symbol table per pipeline state which are separated by local and environment variables per stage. Therefore ARB local and environment variables appear in separate lists.\n\nThe controls for a variable reflect that variable’s data type, no matter how complex the type (see Figure 2-4). You can use the controls to manually change a value, or you can select Animate to automatically vary a uniform from one value to another. No matter which you choose, you will get immediate feedback by looking at the Render view as long as the auto compile and auto link options are enabled.\n\nFigure 2-4  Controls for a matrix structure\nBuilding Shaders\n\nTo build a shader and make sure it runs correctly, follow these steps:\n\nOpen OpenGL Shader Builder.\n\nAdd shader source code.\n\nIf you’ve already written the shader source files, click Add Shaders. Then, navigate to the files you want to add to the file list and choose them.\n\nIf you want to enter the source code, choose File > New and then choose the type of shader you want to write. A source code file opens in its own window. You can modify the default code provided in the template.\n\nTip: You can drag existing source files to the program list.\n\nCheck the Link Log to make sure the programs linked successfully. You might also want to check the link results.\n\nBy default, OpenGL Shader Builder has automatic linking enabled. If you disabled this feature, you’ll need to enable it or click the Link button.\n\nIf the link fails, check to make sure that you added all necessary source files. For example, if you add a fragment or geometry program without adding the associated vertex program, linking fails.\n\nIn the Textures view, add any textures that are appropriate for your shader.\n\nYou’ll most likely want to replace the default texture.\n\nIn the Symbol view, animate one or more of the uniform variables.\n\nIn the Render view, choose a geometry from the pop-up menu.\n\nYou can drag the pointer to move the rendered image.\n\nAfter your shader is running, you may want to benchmark its performance.\n\nChecking Shader Performance\n\nIt’s a good idea to check the frame rate of your shader before and after you make adjustments to the code. You can measure the frame rate by following these steps:\n\nIn the Render view, click Benchmark.\n\nA benchmark window opens.\n\nEnter the number of seconds for the benchmark test. Then, click Run.\n\nNote the elapsed time and the frames per second.\n\nIf you find the frame rate is much lower than you’d like, check to see if you are performing:\n\nTasks inside a loop, such as setting state, that should really be performed outside the loop\n\nComplex calculations, such as arcsin. You can improve performance by pre-calculating results and storing them in a texture. Then, when you need a result, perform a texture look-up operation instead of the complex calculation.\n\nAfter you are certain that your shader performs well in isolation, you should add it to your OpenGL application. Then, use OpenGL Profiler to make sure that your shader and the surrounding OpenGL application run as optimally as possible.\n\nFor information identifying and solving performance issues with OpenGL applications, see OpenGL Profiler User Guide.\n\nTroubleshooting Errors\n\nShaders can have validation errors for a number of reasons. You’ll want to be familiar with the OpenGL extensions that apply to the type of GPU program you are writing, because each extension outlines the conditions that can cause such errors. This section provides guidelines for a few of the common errors.\n\nMake sure that your code sets uniform variables for samplers at validation time.\n\nIf your code depends on support for certain features, make sure you add a directive to require the appropriate extension, otherwise your code won’t parse.\n\nIf a geometry shader fails, check to see whether any of the following apply:\n\nThe geometry program has no associated vertex program for supplying varying variables.\n\nYou used the wrong input or output types. Geometry shaders use fixed input and output primitive types.\n\nThe value of GEOMETRY_VERTICES_OUT_EXT is 0.\n\nYou can sometimes troubleshoot errors by examining well-written code and comparing it to our own. You may want to look at the following:\n\nThe GLSLShowpiece and other sample code that’s available through the ADC Reference Library.\n\nSample code that’s available on http://www.opengl.org/.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "Legacy OpenGL Functionality by Version",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_api_versions/opengl_api_versions.html#//apple_ref/doc/uid/TP40001987-CH301-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nLegacy OpenGL Functionality by Version\n\nOpenGL functionality changes with each version of the OpenGL API. This appendix describes the functionality that was added with each version. See the official OpenGL specification for detailed information.\n\nThe functionality for each version is guaranteed to be available through the OpenGL API even if a particular renderer does not support all of the extensions in a version. For example, a renderer that claims to support OpenGL 1.3 might not export the GL_ARB_texture_env_combine or GL_EXT_texture_env_combine extensions. It's important that you query both the renderer version and extension string to make sure that the renderer supports any functionality that you want to use.\n\nNote: It's possible for vendor and ARB extensions to provide similar functionality. As particular functionality becomes widely adopted, it can be moved into the core OpenGL API. As a result, functionality that you want to use could be included as an extension, as part of the core API, or both. You should read the extensions and the core OpenGL specifications carefully to see the differences. Furthermore, as an extension is promoted, the API associated with that functionality can change. For more information, see Determining the OpenGL Capabilities Supported by the Renderer.\n\nIn the following tables, the extensions describe the feature that the core functionality is based on. The core functionality might not be the same as the extension. For example, compare the core texture crossbar functionality with the extension that it's based on.\n\nVersion 1.1\nTable A-1  Functionality added in OpenGL 1.1\n\nFunctionality\n\n\t\n\nExtension\n\n\n\n\nCopy texture and subtexture\n\n\t\n\nGL_EXT_copy_texture and GL_EXT_subtexture\n\n\n\n\nLogical operation\n\n\t\n\nGL_EXT_blend_logic_op\n\n\n\n\nPolygon offset\n\n\t\n\nGL_EXT_polygon_offset\n\n\n\n\nTexture image formats\n\n\t\n\nGL_EXT_texture\n\n\n\n\nTexture objects\n\n\t\n\nGL_EXT_texture_object\n\n\n\n\nTexture proxies\n\n\t\n\nGL_EXT_texture\n\n\n\n\nTexture replace environment\n\n\t\n\nGL_EXT_texture\n\n\n\n\nVertex array\n\n\t\n\nGL_EXT_vertex_array\n\nThere were a number of other minor changes outlined in Appendix C section 9 of the OpenGL specification. See http://www.opengl.org.\n\nVersion 1.2\nTable A-2  Functionality added in OpenGL 1.2\n\nFunctionality\n\n\t\n\nExtension\n\n\n\n\nBGRA pixel formats\n\n\t\n\nGL_EXT_bgra\n\n\n\n\nImaging subset (optional)\n\n\t\n\nGL_SGI_color_table , GL_EXT_color_subtable, GL_EXT_convolution, GL_HP_convolution_border_modes, GL_SGI_color_matrix, GL_EXT_histogram, GL_EXT_blend_minmax, and GL_EXT_blend_subtract\n\n\n\n\nNormal rescaling\n\n\t\n\nGL_EXT_rescale_normal\n\n\n\n\nPacked pixel formats\n\n\t\n\nGL_EXT_packed_pixels\n\n\n\n\nSeparate specular color\n\n\t\n\nGL_EXT_separate_specular_color\n\n\n\n\nTexture coordinate edge clamping\n\n\t\n\nGL_SGIS_texture_edge_clamp\n\n\n\n\nTexture level of detail control\n\n\t\n\nGL_SGIS_texture_lod\n\n\n\n\nThree-dimensional texturing\n\n\t\n\nGL_EXT_texture3D\n\n\n\n\nVertex array draw element range\n\n\t\n\nGL_EXT_draw_range_elements\n\nNote: The imaging subset might not be present on all implementations; you must verify by checking for the ARB_imaging extension.\n\nOpenGL 1.2.1 introduced ARB extensions with no specific core API changes.\n\nVersion 1.3\nTable A-3  Functionality added in OpenGL 1.3\n\nFunctionality\n\n\t\n\nExtension\n\n\n\n\nCompressed textures\n\n\t\n\nGL_ARB_texture_compression\n\n\n\n\nCube map textures\n\n\t\n\nGL_ARB_texture_cube_map\n\n\n\n\nMultisample\n\n\t\n\nGL_ARB_multisample\n\n\n\n\nMultitexture\n\n\t\n\nGL_ARB_multitexture\n\n\n\n\nTexture add environment mode\n\n\t\n\nGL_ARB_texture_env_add\n\n\n\n\nTexture border clamp\n\n\t\n\nGL_ARB_texture_border_clamp\n\n\n\n\nTexture combine environment mode\n\n\t\n\nGL_ARB_texture_env_combine\n\n\n\n\nTexture dot3 environment mode\n\n\t\n\nGL_ARB_texture_env_dot3\n\n\n\n\nTranspose matrix\n\n\t\n\nGL_ARB_transpose_matrix\n\nVersion 1.4\nTable A-4  Functionality added in OpenGL 1.4\n\nFunctionality\n\n\t\n\nExtension\n\n\n\n\nAutomatic mipmap generation\n\n\t\n\nGL_SGIS_generate_mipmap\n\n\n\n\nBlend function separate\n\n\t\n\nGL_ARB_blend_func_separate\n\n\n\n\nBlend squaring\n\n\t\n\nGL_NV_blend_square\n\n\n\n\nDepth textures\n\n\t\n\nGL_ARB_depth_texture\n\n\n\n\nFog coordinate\n\n\t\n\nGL_EXT_fog_coord\n\n\n\n\nMultiple draw arrays\n\n\t\n\nGL_EXT_multi_draw_arrays\n\n\n\n\nPoint parameters\n\n\t\n\nGL_ARB_point_parameters\n\n\n\n\nSecondary color\n\n\t\n\nGL_EXT_secondary_color\n\n\n\n\nSeparate blend functions\n\n\t\n\nGL_EXT_blend_func_separate, GL_EXT_blend_color\n\n\n\n\nShadows\n\n\t\n\nGL_ARB_shadow\n\n\n\n\nStencil wrap\n\n\t\n\nGL_EXT_stencil_wrap\n\n\n\n\nTexture crossbar environment mode\n\n\t\n\nGL_ARB_texture_env_crossbar\n\n\n\n\nTexture level of detail bias\n\n\t\n\nGL_EXT_texture_lod_bias\n\n\n\n\nTexture mirrored repeat\n\n\t\n\nGL_ARB_texture_mirrored_repeat\n\n\n\n\nWindow raster position\n\n\t\n\nGL_ARB_window_pos\n\nVersion 1.5\nTable A-5  Functionality added in OpenGL 1.5\n\nFunctionality\n\n\t\n\nExtension\n\n\n\n\nBuffer objects\n\n\t\n\nGL_ARB_vertex_buffer_object\n\n\n\n\nOcclusion queries\n\n\t\n\nGL_ARB_occlusion_query\n\n\n\n\nShadow functions\n\n\t\n\nGL_EXT_shadow_funcs\n\nVersion 2.0\nTable A-6  Functionality added in OpenGL 2.0\n\nFunctionality\n\n\t\n\nExtension\n\n\n\n\nMultiple render targets\n\n\t\n\nGL_ARB_draw_buffers\n\n\n\n\nNon–power-of-two textures\n\n\t\n\nGL_ARB_texture_non_power_of_two\n\n\n\n\nPoint sprites\n\n\t\n\nGL_ARB_point_sprite\n\n\n\n\nSeparate blend equation\n\n\t\n\nGL_EXT_blend_equation_separate\n\n\n\n\nSeparate stencil\n\n\t\n\nGL_ATI_separate_stencil\n\nGL_EXT_stencil_two_side\n\n\n\n\nShading language\n\n\t\n\nGL_ARB_shading_language_100\n\n\n\n\nShader objects\n\n\t\n\nGL_ARB_shader_objects\n\n\n\n\nShader programs\n\n\t\n\nGL_ARB_fragment_shader\n\nGL_ARB_vertex_shader\n\nVersion 2.1\nTable A-7  Functionality added in OpenGL 2.1\n\nFunctionality\n\n\t\n\nExtension\n\n\n\n\nPixel buffer objects\n\n\t\n\nGL_ARB_pixel_buffer_object\n\n\n\n\nsRGB textures\n\n\t\n\nGL_EXT_texture_sRGB\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Document Revision History",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLDriverMonitorUserGuide/RevisionHistory.html#//apple_ref/doc/uid/TP40006474-CH5-DontLinkElementID_18",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Driver Monitor User Guide\nTable of Contents\nIntroduction\nUsing OpenGL Driver Monitor\nIdentifying and Solving Performance Issues\nAppendix A: OpenGL Driver Monitor Parameters\nRevision History\nPrevious\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nDocument Revision History\n\nThis table describes the changes to OpenGL Driver Monitor User Guide.\n\nDate\tNotes\n2015-03-09\t\n\nMoved to Retired Documents Library.\n\n\n2008-02-08\t\n\nFixed a link.\n\n\n2007-12-11\t\n\nNew document that explains how to view the properties supported by the OpenGL drivers available on the system.\n\n\n\nPrevious\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "Concurrency and OpenGL",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_threading/opengl_threading.html#//apple_ref/doc/uid/TP40001987-CH409-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nConcurrency and OpenGL\n\nConcurrency is the notion of multiple things happening at the same time. In the context of computers, concurrency usually refers to executing tasks on more than one processor at the same time. By performing work in parallel, tasks complete sooner, and applications become more responsive to the user. The good news is that well-designed OpenGL applications already exhibit a specific form of concurrency—concurrency between application processing on the CPU and OpenGL processing on the GPU. Many of the techniques introduced in OpenGL Application Design Strategies are aimed specifically at creating OpenGL applications that exhibit great CPU-GPU parallelism. However, modern computers not only contain a powerful GPU, but also contain multiple CPUs. Sometimes those CPUs have multiple cores, each capable of performing calculations independently of the others. It is critical that applications be designed to take advantage of concurrency where possible. Designing a concurrent application means decomposing the work your application performs into subtasks and identifying which tasks can safely operate in parallel and which tasks must be executed sequentially—that is, which tasks are dependent on either resources used by other tasks or results returned from those tasks.\n\nEach process in OS X is made up of one or more threads. A thread is a stream of execution that runs code for the process. Multicore systems offer true concurrency by allowing multiple threads to execute simultaneously. Apple offers both traditional threads and a feature called Grand Central Dispatch (GCD). Grand Central Dispatch allows you to decompose your application into smaller tasks without requiring the application to manage threads. GCD allocates threads based on the number of cores available on the system and automatically schedules tasks to those threads.\n\nAt a higher level, Cocoa offers NSOperation and NSOperationQueue to provide an Objective-C abstraction for creating and scheduling units of work. On OS X v10.6, operation queues use GCD to dispatch work; on OS X v10.5, operation queues create threads to execute your application’s tasks.\n\nThis chapter does not attempt describe these technologies in detail. Before you consider how to add concurrency to your OpenGL application, you should first readConcurrency Programming Guide. If you plan on managing threads manually, you should also read Threading Programming Guide. Regardless of which technique you use, there are additional restrictions when calling OpenGL on multithreaded systems. This chapter helps you understand when multithreading improves your OpenGL application’s performance, the restrictions OpenGL places on multithreaded applications, and common design strategies you might use to implement concurrency in an OpenGL application. Some of these design techniques can get you an improvement in just a few lines of code.\n\nIdentifying Whether an OpenGL Application Can Benefit from Concurrency\n\nCreating a multithreaded application requires significant effort in the design, implementation, and testing of your application. Threads also add complexity and overhead to an application. For example, your application may need to copy data so that it can be handed to a worker thread, or multiple threads may need to synchronize access to the same resources. Before you attempt to implement concurrency in an OpenGL application, you should optimize your OpenGL code in a single-threaded environment using the techniques described in OpenGL Application Design Strategies. Focus on achieving great CPU-GPU parallelism first and then assess whether concurrent programming can provide an additional performance benefit.\n\nA good candidate has either or both of the following characteristics:\n\nThe application performs many tasks on the CPU that are independent of OpenGL rendering. Games, for example, simulate the game world, calculate artificial intelligence from computer-controlled opponents, and play sound. You can exploit parallelism in this scenario because many of these tasks are not dependent on your OpenGL drawing code.\n\nProfiling your application has shown that your OpenGL rendering code spends a lot of time in the CPU. In this scenario, the GPU is idle because your application is incapable of feeding it commands fast enough. If your CPU-bound code has already been optimized, you may be able to improve its performance further by splitting the work into tasks that execute concurrently.\n\nIf your application is blocked waiting for the GPU, and has no work it can perform in parallel with its OpenGL drawing commands, then it is not a good candidate for concurrency. If the CPU and GPU are both idle, then your OpenGL needs are probably simple enough that no further tuning is useful.\n\nFor more information on how to determine where your application spends its time, see Tuning Your OpenGL Application.\n\nOpenGL Restricts Each Context to a Single Thread\n\nEach thread in an OS X process has a single current OpenGL rendering context. Every time your application calls an OpenGL function, OpenGL implicitly looks up the context associated with the current thread and modifies the state or objects associated with that context.\n\nOpenGL is not reentrant. If you modify the same context from multiple threads simultaneously, the results are unpredictable. Your application might crash or it might render improperly. If for some reason you decide to set more than one thread to target the same context, then you must synchronize threads by placing a mutex around all OpenGL calls to the context, such as gl* and CGL*. OpenGL commands that block—such as fence commands—do not synchronize threads.\n\nGCD and NSOperationQueue objects can both execute your tasks on a thread of their choosing. They may create a thread specifically for that task, or they may reuse an existing thread. But in either case, you cannot guarantee which thread executes the task. For an OpenGL application, that means:\n\nEach task must set the context before executing any OpenGL commands.\n\nYour application must ensure that two tasks that access the same context are not allowed to execute concurrently.\n\nStrategies for Implementing Concurrency in OpenGL Applications\n\nA concurrent OpenGL application wants to focus on CPU parallelism so that OpenGL can provide more work to the GPU. Here are a few recommended strategies for implementing concurrency in an OpenGL application:\n\nDecompose your application into OpenGL and non-OpenGL tasks that can execute concurrently. Your OpenGL rendering code executes as a single task, so it still executes in a single thread. This strategy works best when your application has other tasks that require significant CPU processing.\n\nIf performance profiling reveals that your application spends a lot of CPU time inside OpenGL, you can move some of that processing to another thread by enabling the multithreading in the OpenGL engine. The advantage of this method is its simplicity; enabling the multithreaded OpenGL engine takes just a few lines of code. See Multithreaded OpenGL.\n\nIf your application spends a lot of CPU time preparing data to send to openGL, you can divide the work between tasks that prepare rendering data and tasks that submit rendering commands to OpenGL. See Perform OpenGL Computations in a Worker Task\n\nIf your application has multiple scenes it can render simultaneously or work it can perform in multiple contexts, it can create multiple tasks, with an OpenGL context per task. If the contexts can share the same resources, you can use context sharing when the contexts are created to share surfaces or OpenGL objects: display lists, textures, vertex and fragment programs, vertex array objects, and so on. See Use Multiple OpenGL Contexts\n\nMultithreaded OpenGL\n\nWhenever your application calls OpenGL, the renderer processes the parameters to put them in a format that the hardware understands. The time required to process these commands varies depending on whether the inputs are already in a hardware-friendly format, but there is always some overhead in preparing commands for the hardware.\n\nIf your application spends a lot of time performing calculations inside OpenGL, and you’ve already taken steps to pick ideal data formats, your application might gain an additional benefit by enabling multithreading inside the OpenGL engine. The multithreaded OpenGL engine automatically creates a worker thread and transfers some of its calculations to that thread. On a multicore system, this allows internal OpenGL calculations performed on the CPU to act in parallel with your application, improving performance. Synchronizing functions continue to block the calling thread.\n\nListing 14-1 shows the code required to enable the multithreaded OpenGL engine.\n\nListing 14-1  Enabling the multithreaded OpenGL engine\n\nCGLError err = 0;\n\n\nCGLContextObj ctx = CGLGetCurrentContext();\n\n\n \n\n\n// Enable the multithreading\n\n\nerr =  CGLEnable( ctx, kCGLCEMPEngine);\n\n\n \n\n\nif (err != kCGLNoError )\n\n\n{\n\n\n     // Multithreaded execution may not be available\n\n\n     // Insert your code to take appropriate action\n\n\n}\n\nNote: Enabling or disabling multithreaded execution causes OpenGL to flush previous commands as well as incurring the overhead of setting up the additional thread. You should enable or disable multithreaded execution in an initialization function rather than in the rendering loop.\n\nEnabling multithreading comes at a cost—OpenGL must copy parameters to transmit them to the worker thread. Because of this overhead, you should always test your application with and without multithreading enabled to determine whether it provides a substantial performance improvement.\n\nPerform OpenGL Computations in a Worker Task\n\nSome applications perform lots of calculations on their data before passing that data down to the OpenGL renderer. For example, the application might create new geometry or animate existing geometry. Where possible, such calculations should be performed inside OpenGL. For example, vertex shaders and the transform feedback extension might allow you to perform these calculations entirely within OpenGL. This takes advantage of the greater parallelism available inside the GPU, and reduces the overhead of copying results between your application and OpenGL.\n\nThe approach described in Figure 9-3 alternates between updating OpenGL objects and executing rendering commands that use those objects. OpenGL renders on the GPU in parallel with your application’s updates running on the CPU. If the calculations performed on the CPU take more processing time than those on the GPU, then the GPU spends more time idle. In this situation, you may be able to take advantage of parallelism on systems with multiple CPUs. Split your OpenGL rendering code into separate calculation and processing tasks, and run them in parallel. Figure 14-1 shows a clear division of labor. One task produces data that is consumed by the second and submitted to OpenGL.\n\nFigure 14-1  CPU processing and OpenGL on separate threads\n\nFor best performance, your application should avoid copying data between the tasks. For example, rather than calculating the data in one task and copying it into a vertex buffer object in the other, map the vertex buffer object in the setup code and hand the pointer directly to the worker task.\n\nIf your application can further decompose the modifications task into subtasks, you may see better benefits. For example, assume two or more vertex buffers, each of which needs to be updated before submitting drawing commands. Each can be recalculated independently of the others. In this scenario, the modifications to each buffer becomes an operation, using an NSOperationQueue object to manage the work:\n\nSet the current context.\n\nMap the first buffer.\n\nCreate an NSOperation object whose task is to fill that buffer.\n\nQueue that operation on the operation queue.\n\nPerform steps 2 through 4 for the other buffers.\n\nCall waitUntilAllOperationsAreFinished on the operation queue.\n\nUnmap the buffers.\n\nExecute rendering commands.\n\nOn a multicore system, multiple threads of execution may allow the buffers to be filled simultaneously. Steps 7 and 8 could even be performed by a separate operation queued onto the same operation queue, provided that operation set the proper dependencies.\n\nUse Multiple OpenGL Contexts\n\nIf your application has multiple scenes that can be rendered in parallel, you can use a context for each scene you need to render. Create one context for each scene and assign each context to an operation or task. Because each task has its own context, all can submit rendering commands in parallel.\n\nThe Apple-specific OpenGL APIs also provide the option for sharing data between contexts, as shown in Figure 14-2. Shared resources are automatically set up as mutual exclusion (mutex) objects. Notice that thread 2 draws to a pixel buffer that is linked to the shared state as a texture. Thread 1 can then draw using that texture.\n\nFigure 14-2  Two contexts on separate threads\n\nThis is the most complex model for designing an application. Changes to objects in one context must be flushed so that other contexts see the changes. Similarly, when your application finishes operating on an object, it must flush those commands before exiting, to ensure that all rendering commands have been submitted to the hardware.\n\nGuidelines for Threading OpenGL Applications\n\nFollow these guidelines to ensure successful threading in an application that uses OpenGL:\n\nUse only one thread per context. OpenGL commands for a specific context are not thread safe. You should never have more than one thread accessing a single context simultaneously.\n\nContexts that are on different threads can share object resources. For example, it is acceptable for one context in one thread to modify a texture, and a second context in a second thread to modify the same texture. The shared object handling provided by the Apple APIs automatically protects against thread errors. And, your application is following the \"one thread per context\" guideline.\n\nWhen you use an NSOpenGLView object with OpenGL calls that are issued from a thread other than the main one, you must set up mutex locking. Mutex locking is necessary because unless you override the default behavior, the main thread may need to communicate with the view for such things as resizing.\n\nApplications that use Objective-C with multithreading can lock contexts using the functions CGLLockContext and CGLUnlockContext. If you want to perform rendering in a thread other than the main one, you can lock the context that you want to access and safely execute OpenGL commands. The locking calls must be placed around all of your OpenGL calls in all threads.\n\nCGLLockContext blocks the thread it is on until all other threads have unlocked the same context using the function CGLUnlockContext. You can use CGLLockContext recursively. Context-specific CGL calls by themselves do not require locking, but you can guarantee serial processing for a group of calls by surrounding them with CGLLockContext and CGLUnlockContext. Keep in mind that calls from the OpenGL API (the API provided by the Khronos OpenGL Working Group) require locking.\n\nKeep track of the current context. When switching threads it is easy to switch contexts inadvertently, which causes unforeseen effects on the execution of graphic commands. You must set a current context when switching to a newly created thread.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Introduction",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLProfilerUserGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40006475",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Profiler User Guide\nTable of Contents\nIntroduction\nGetting Started\nUsing Breakpoints\nIdentifying and Solving Performance Issues\nControlling Profiling Programmatically\nRevision History\nNext\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nIntroduction\n\nImportant The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nOpenGL Profiler is an application that’s useful for debugging and assessing performance. It lets you look inside a running application and observe how the application uses OpenGL. It can track the OpenGL functions used by an application, how often each is used, and the execution time of each function. Using this data, you can determine how efficiently an OpenGL application uses the GPU. You can then use the data to guide application development, modifying those parts of the code that slow performance or appear to use resources inefficiently.\n\nOpenGL Profiler has a variety of interactive features. After setting breakpoints, developers can investigate application resources (textures, programs, shaders, and so on), examine the values of OpenGL context parameters, look at buffer contents, and check other aspects of the OpenGL state.\n\nYou’ll want to read this document if you develop applications that use OpenGL on OS X. By reading it, you’ll learn how to set up OpenGL Profiler, collect data, set breakpoints, and use the results to track down problems.\n\nOrganization of This Document\n\nThis document is organized into the following chapters:\n\nGetting Started shows how to get OpenGL Profiler running and how to start a profiling session.\n\nUsing Breakpoints provides details on setting breakpoints and describes the tasks you can accomplish when your application pauses.\n\nIdentifying and Solving Performance Issues gives advice on how to use OpenGL Profiler to track down and analyze performance issues in your application.\n\nControlling Profiling Programmatically describes how to add code to your application that will control various aspects of OpenGL Profiler during a profiling session.\n\nSee Also\n\nThese documents contain information that can help you analyze and optimize your OpenGL code:\n\nOpenGL Programming Guide for Mac shows how to program using OpenGL on OS X. You’ll want to read the chapter “Improving Performance” to get an overview of the best programming practices to use as well as how to use Apple’s tools for identifying bottlenecks, and gathering and analyzing performance data.\n\nOpenGL Driver Monitor User Guide which is a developer tool that lets you investigate how the graphics processing unit (GPU) works on a system-wide basis.\n\nInstruments User Guide describes how to use Instruments to profile your application.\n\nNext\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "Setting Up Function Pointers to OpenGL Routines",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_entrypts/opengl_entrypts.html#//apple_ref/doc/uid/TP40001987-CH402-SW6",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nSetting Up Function Pointers to OpenGL Routines\n\nFunction pointers to OpenGL routines allow you to deploy your application across multiple versions of OS X regardless of whether the entry point is supported at link time or runtime. This practice also provides support for code that needs to run cross-platform—in both OS X and Windows.\n\nNote: If you are deploying your application only in OS X v10.4 or later, you do not need to read this chapter. Instead, consider the alternative, which is to set the gcc attribute that allows weak linking of symbols. Keep in mind, however, that weak linking may impact your application's performance. For more information, see Frameworks and Weak Linking.\n\nThis appendix discusses the tasks needed to set up and use function pointers as entry points to OpenGL routines:\n\nObtaining a Function Pointer to an Arbitrary OpenGL Entry Point shows how to write a generic routine that you can reuse for any OpenGL application on the Macintosh platform.\n\nInitializing Entry Points describes how to declare function pointer type definitions and initialize them with the appropriate OpenGL command entry points for your application.\n\nObtaining a Function Pointer to an Arbitrary OpenGL Entry Point\n\nGetting a pointer to an OpenGL entry point function is fairly straightforward from Cocoa. You can use the Dynamic Loader function NSLookupAndBindSymbol to get the address of an OpenGL entry point.\n\nKeep in mind that getting a valid function pointer means that the entry point is exported by the OpenGL framework; it does not guarantee that a particular routine is supported and valid to call from within your application. You still need to check for OpenGL functionality on a per-renderer basis as described in Detecting Functionality.\n\nListing C-1 shows how to use NSLookupAndBindSymbol from within the function MyNSGLGetProcAddress. When provided a symbol name, this application-defined function returns the appropriate function pointer from the global symbol table. A detailed explanation for each numbered line of code appears following the listing.\n\nListing C-1  Using NSLookupAndBindSymbol to obtain a symbol for a symbol name\n\n#import <mach-o/dyld.h>\n\n\n#import <stdlib.h>\n\n\n#import <string.h>\n\n\nvoid * MyNSGLGetProcAddress (const char *name)\n\n\n{\n\n\n    NSSymbol symbol;\n\n\n    char *symbolName;\n\n\n    symbolName = malloc (strlen (name) + 2); \n// 1\n\n\n    strcpy(symbolName + 1, name); \n// 2\n\n\n    symbolName[0] = '_'; \n// 3\n\n\n    symbol = NULL;\n\n\n    if (NSIsSymbolNameDefined (symbolName)) \n// 4\n\n\n        symbol = NSLookupAndBindSymbol (symbolName);\n\n\n    free (symbolName); \n// 5\n\n\n    return symbol ? NSAddressOfSymbol (symbol) : NULL; \n// 6\n\n\n}\n\nHere's what the code does:\n\nAllocates storage for the symbol name plus an underscore character ('_'). The underscore character is part of the UNIX C symbol-mangling convention, so make sure that you provide storage for it.\n\nCopies the symbol name into the string variable, starting at the second character, to leave room for prefixing the underscore character.\n\nCopies the underscore character into the first character of the symbol name string.\n\nChecks to make sure that the symbol name is defined, and if it is, looks up the symbol.\n\nFrees the symbol name string because it is no longer needed.\n\nReturns the appropriate pointer if successful, or NULL if not successful. Before using this pointer, you should make sure that is it valid.\n\nInitializing Entry Points\n\nListing C-2 shows how to use the MyNSGLGetProcAddress function from Listing C-1 to obtain a few OpenGL entry points. A detailed explanation for each numbered line of code appears following the listing.\n\nListing C-2  Using NSGLGetProcAddress to obtain an OpenGL entry point\n\n#import \"MyNSGLGetProcAddress.h\" \n// 1\n\n\nstatic void InitEntryPoints (void);\n\n\nstatic void DeallocEntryPoints (void);\n\n\n \n\n\n// Function pointer type definitions\n\n\ntypedef void (*glBlendColorProcPtr)(GLclampf red,GLclampf green,\n\n\n                        GLclampf blue,GLclampf alpha);\n\n\ntypedef void (*glBlendEquationProcPtr)(GLenum mode);\n\n\n typedef void (*glDrawRangeElementsProcPtr)(GLenum mode, GLuint start,\n\n\n                GLuint end,GLsizei count,GLenum type,const GLvoid *indices);\n\n\n \n\n\nglBlendColorProcPtr pfglBlendColor = NULL; \n// 2\n\n\nglBlendEquationProcPtr pfglBlendEquation = NULL;\n\n\nglDrawRangeElementsProcPtr pfglDrawRangeElements = NULL;\n\n\n \n\n\nstatic void InitEntryPoints (void) \n// 3\n\n\n{\n\n\n    pfglBlendColor = (glBlendColorProcPtr) MyNSGLGetProcAddress\n\n\n                                    (\"glBlendColor\");\n\n\n    pfglBlendEquation = (glBlendEquationProcPtr)MyNSGLGetProcAddress\n\n\n                                (\"glBlendEquation\");\n\n\n    pfglDrawRangeElements = (glDrawRangeElementsProcPtr)MyNSGLGetProcAddress\n\n\n                                (\"glDrawRangeElements\");\n\n\n}\n\n\n// -------------------------\n\n\nstatic void DeallocEntryPoints (void) \n// 4\n\n\n{\n\n\n    pfglBlendColor = NULL;\n\n\n    pfglBlendEquation = NULL;\n\n\n    pfglDrawRangeElements = NULL;;\n\n\n}\n\nHere's what the code does:\n\nImports the header file that contains the MyNSGLProcAddress function from Listing C-1.\n\nDeclares function pointers for the functions of interest. Note that each function pointer uses the prefix pf to distinguish it from the function it points to. Although using this prefix is not a requirement, it's best to avoid using the exact function names.\n\nInitializes the entry points. This function repeatedly calls the MyNSGLProcAddress function to obtain function pointers for each of the functions of interest—glBlendColor, glBlendEquation, and glDrawRangeElements.\n\nSets each of the function pointers to NULL when they are no longer needed.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Using OpenGL Driver Monitor",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLDriverMonitorUserGuide/Using/UsingOGLDM.html#//apple_ref/doc/uid/TP40006474-CH2-SW5",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Driver Monitor User Guide\nTable of Contents\nIntroduction\nUsing OpenGL Driver Monitor\nIdentifying and Solving Performance Issues\nAppendix A: OpenGL Driver Monitor Parameters\nRevision History\nNext\nPrevious\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nUsing OpenGL Driver Monitor\n\nOpenGL Driver Monitor is a developer tool that lets you investigate how the graphics processing unit (GPU) works on a system-wide basis. Using it, you can:\n\nInspect the renderers available on an OS X system\n\nSee what OpenGL extensions each renderer supports\n\nMonitor rendering parameters and resources in real-time\n\nTrack the interaction between the GPU and the CPU\n\nThis chapter provides an overview of OpenGL Driver Monitor and shows how you can use it to look at and track OpenGL parameters.\n\nSetting Preferences\n\nFor most cases, the default values that OpenGL Driver Monitor uses are optimal. Before you start using the driver monitor for the first time, you might want to familiarize yourself with its preferences to see whether the default values are acceptable for your needs.\n\nTo set preferences:\n\nChoose OpenGL Driver Monitor > Preferences.\n\nAdjust the sampling interval.\n\nOpenGL Driver Monitor measures data throughput in bytes per sampling interval. For example, the default sampling interval is 1 second, so the data throughput is in bytes per second.\n\nIf you change the sampling interval, the OpenGL Driver Monitor measures values for that interval; the values are not normalized to a 1 second interval. For example, if you set the sampling interval to 2 seconds, and the throughput value is 500 bytes, the reported sampling rate is 500 bytes per interval which, in this case, is 250 bytes per second.\n\nAdjust the maximum data samples.\n\nThe value is the maximum number bytes per sampling interval. The default is 2048.\n\nAdjust graph colors.\n\nIf you don’t like that default colors for the graph background or graph labels, click the color wells and choose other colors.\n\nSelect whether or not to use descriptive parameter names.\n\nOpenGL Driver Monitor provides a list of parameters from which you choose the ones you want to track. You can either view the list as the symbolic names used by driver monitor (such as command2DBytes, gartMapOutBytes, finishAll2DWaitTime) or as descriptive names (such as 2D Command Data, AGP Data Unmapped, CPU Wait for Operations to Finish).\n\nIf you want to monitor this computer remotely, select “Enable remote monitor.“\n\nSee Monitoring an OpenGL Driver Remotely for additional details.\n\nClick Apply to commit to the changes you made. Then click OK to close the Preferences window.\n\nCollecting and Viewing Data\n\nOpenGL Driver Monitor lets you collect system-wide data for a specific OpenGL driver. You can collect data for the system that you are running the driver monitor on or a system that you already set up for remote monitoring. (See Monitoring an OpenGL Driver Remotely.) The next few sections show how to collect the data.\n\nAfter OpenGL Driver Monitor launches, choose a driver to investigate from the Monitors > Driver Monitors menu. Most systems have only one driver, but if your system has more than one graphics card installed, you’ll see more than one entry. After choosing a driver, the driver window opens with a view of an empty graph. You’ll notice that the Driver Monitor Parameters list below the graph is empty.\n\nClick Parameters to open a drawer that lists all the driver parameters that you can monitor. Hover the pointer over a parameter name to see its description. See also OpenGL Driver Monitor Parameters for definitions of the parameter names and for cross-references between the symbolic name and the descriptive name for a parameter.\n\nEither double-click each of the parameters (shown in Figure 1-1) you want to monitor, or drag them to the Driver Monitor Parameters list below the graph. As you might expect, not all parameters are equally useful for every scenario. You’ll need to choose accordingly.\n\nKeep in mind that the parameters shown in Figure 1-1 are for a particular driver. Not all drivers support the same parameters, so it’s possible that the list you see doesn’t match either what’s shown in the figure or what’s listed in the glossary.\n\nYou’ll notice that when you see similarly named parameters, one of them is typically a “super parameter.” The value of a super parameter includes the values of all its “child parameters.” For example, the super parameter commandBytes (Total Command Data) includes all quantities represented by the similarly named parameters command2DBytes (2D Command Data), commandGLBytes (OpenGL Command Data), and commandDVDBytes (DVD Command Data).\n\nFigure 1-1  The parameters drawer\nViewing Graphed Data\n\nAfter you choose a parameter, OpenGL Driver Monitor adds it to the Driver Monitor Parameter list and starts to display data on the graph and in the columns next to the parameter name. Figure 1-2 shows the driver monitor graph window after displaying two parameters—CPU Wait for CPU and Current Free Video Memory. If you prefer to use colors other than the default ones for graphing the values, click the color well for a parameter and choose another from the color panel that appears.\n\nFigure 1-2  The driver monitor graph window\n\nThe y-axis values on the left side of the graph are values that represent different units depending on the parameter. The y-axis values on the right side represent percentages. Keep the following in mind when reading the graph:\n\nTime-based parameters values represent nanoseconds. For example, 1 giga-nanosecond (or 1G, as shown on the graph) represents about 1 second spent on an operation. If the sampling interval is 1 second, then the percentage is 100%.\n\nParameters that represent counts are absolute numbers, with quantities measured once per sampling interval. Counts are not affected by the length of the sampling interval, but may vary during the interval.\n\nYou can adjust the scale of the x-axis and the base (log or linear) of the y-axis to help you see changes in the values.\n\nViewing Tabular Data\n\nYou can view the data in tabular format if you prefer. This lets you compare running values among the parameters you are monitoring, as shown in Figure 1-3.\n\nFigure 1-3  The driver monitor table window\nEnabling and Disabling Parameters\n\nYou enable and disable the driver monitor parameters that you are monitoring by using the Action pop-up menu shown in Figure 1-4. You can also click the Show column to show or hide data for a particular parameter.\n\nFigure 1-4  The Action pop-up menu\nLooking at Renderer Information\n\nAn OpenGL Driver has at least two renderers associated with it—one hardware renderer and one software renderer. You can take a look at what each renderer supports by choosing Monitors > Renderer Information. Then, when the OpenGL Renderer Info window opens, you can click each of the disclosure triangles to view the settings for a particular renderer.\n\nAs Figure 1-5 shows, you can view the vender name and version, the OpenGL extensions that the driver supports, the buffer modes, various processing capabilities, and so on.\n\nFigure 1-5  The renderer information window\n\nIf you want to view renderer information for renderers other than those on your system, you can set up remote monitoring on the computers whose renderers you want to inspect. See Monitoring an OpenGL Driver Remotely.\n\nMonitoring an OpenGL Driver Remotely\n\nBefore you can monitor an OpenGL driver on a remote computer, you need to enable remote monitoring on that computer by following these steps:\n\nLaunch OpenGL Driver Monitor on the computer that you want to monitor remotely.\n\nChoose OpenGL Driver Monitor > Preferences.\n\nClick “Enable remote monitor.“\n\nMake a note of the remote monitor name. You’ll need this later.\n\nClick Apply.\n\nTo monitor a computer after you enable remote monitoring:\n\nLaunch OpenGL Driver Monitor on the computer that you plan to monitor from.\n\nChoose Monitors > Connect To.\n\nWhen the remote host window opens (see Figure 1-6), select the name of the computer you want to monitor and click Choose.\n\nFigure 1-6  The remote host window\n\nCollect, view, and interpret data the same way you would for a local OpenGL driver.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "OpenGL Driver Monitor Parameters",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLDriverMonitorUserGuide/Glossary/Glossary.html#//apple_ref/doc/uid/TP40006474-CH4-SW57",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Driver Monitor User Guide\nTable of Contents\nIntroduction\nUsing OpenGL Driver Monitor\nIdentifying and Solving Performance Issues\nAppendix A: OpenGL Driver Monitor Parameters\nRevision History\nNext\nPrevious\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nOpenGL Driver Monitor Parameters\n\nOpenGL Driver Monitor parameters are represented as descriptive names and as symbolic names. The parameters you can set for a specific driver are often a subset of what’s listed here.\n\nDescriptive Names\n\nEach descriptive name describes what the parameter represents and lists its symbolic name.\n\n2D Command Data\n\nThe number of bytes sent using 2D graphics contexts. command2DBytes\n\n2D Context Switches\n\nThe total number of context switches to a 2D context on the GPU. context2DSwitchCount\n\n2D Contexts\n\nThe total number of 2D contexts in use on the GPU. context2DCount\n\nAGP Data Mapped\n\nThe number of bytes that are mapped into the AGP Graphics Address Remapping Table (GART) or equivalent hardware. gartMapInBytes\n\nAGP Data Unmapped\n\nThe number of bytes that are unmapped from the AGP Graphics Address Remapping Table (GART) (or equivalent hardware). gartMapOutBytes\n\nBuffer Swaps\n\nThe total number of buffer swaps (or blits) that the GPU perform. bufferSwapCount\n\nCPU Texture Page-off Wait (non-DMA)\n\nThe amount of time, in nanoseconds, that the CPU waits for the GPU to finish activity. This is the that the CPU could use to modify a texture prior to paging the texture. This metric applies only if the texture must be paged using the CPU and not using direct memory access (DMA). texturePageOffWaitTime\n\nCPU Texture Page-on Wait\n\nThe amount of time, in nanoseconds, that the CPU waits for a texture upload command to be completed by the GPU. This is the that the CPU could use for updating and reloading a texture. Typically, there is very little, if any time, spent waiting here. texturePageInWaitTime\n\nCPU Texture Upload Wait (2D context only)\n\nThe amount of time, in nanoseconds, that the CPU waits for a texture upload to complete before the buffer can be modified. This particular metric tracks usage only by 2D contexts, and is somewhat obsolete. textureWaitTime\n\nCPU Wait for 2D Operations to Finish\n\nThe amount of time, in nanoseconds, that the CPU waits for all 2D commands issued on a single context to complete. finish2DWaitTime\n\nCPU Wait for 2D Swap to Complete\n\nThe amount of time, in nanoseconds, that the CPU waits for a previously issued 2D buffer swap to complete. swapComplete2DWaitTime\n\nCPU Wait for DVD Operations to Finish\n\nThe amount of time, in nanoseconds, that the CPU waits for all DVD commands issued on a single context to complete. finishDVDWaitTime\n\nCPU Wait for DVD Swap to Complete\n\nThe amount of time, in nanoseconds, that the CPU waits for a previously issued DVD buffer swap to complete. swapCompleteDVDWaitTime\n\nCPU Wait for Free 2D Command Buffer\n\nThe amount of time, in nanoseconds, that the CPU waits for a 2D command buffer to become available. freeCommandBuffer2DWaitTime\n\nCPU Wait for Free OpenGL Command Buffer\n\nThe amount of time, in nanoseconds, that the CPU waits for an OpenGL command buffer to become available. freeCommandBufferGLWaitTime\n\nCPU Wait for Free OpenGL Data Buffer\n\nThe amount of time, in nanoseconds, that the CPU waits for an OpenGL data buffer to become available. freeDataBufferGLWaitTime\n\nCPU Wait for Free 2D Context Switch Buffer\n\nThe amount of time, in nanoseconds, that the CPU waits for a 2D context-switching buffer to become available. freeContextBuffer2DWaitTime\n\nCPU Wait for Free OpenGL Context Switch Buffer\n\nThe amount of time, in nanoseconds, that the CPU waits for an OpenGL context-switching buffer to become available. freeContextBufferGLWaitTime\n\nCPU Wait for Free DVD Context Switch Buffer\n\nThe amount of time, in nanoseconds, that the CPU waits for a DVD context-switching buffer to become available. freeContextBufferDVDWaitTime\n\nCPU Wait for GPU\n\nThe amount of time, in nanoseconds, that the CPU stalled while waiting on the GPU for any reason. hardwareWaitTime\n\nCPU Wait for Mapped AGP Buffer Removal\n\nThe amount of time, in nanoseconds, the CPU waits for the GPU to finish an operation on a buffer that needs to be removed from the Graphics Address Remapping Table (GART). removeFromGARTWaitTime\n\nCPU Wait for OpenGL Swap to Complete\n\nThe amount of time, in nanoseconds, that the CPU waits for a previously issued OpenGL buffer swap to complete. swapCompleteGLWaitTime\n\nCPU Wait for Operations to Finish\n\nThe amount of time, in nanoseconds, that the CPU waits for all GPU operations to complete and then to be idle. Generally, only the window server waits for this state. finishAll2DWaitTime\n\nCPU Wait for OpenGL Operations to Finish\n\nThe amount of time, in nanoseconds, that the CPU waits for all OpenGL commands issued on a single context to complete. This is essentially the time spent in glFinish. finishGLWaitTime\n\nCPU Wait in User Code\n\nThe amount of time, in nanoseconds, that the CPU waits while the client (user-level) OpenGL driver waits for a hardware time stamp to arrive (usually for making texture modifications or waiting for a fence to complete). clientGLWaitTime\n\nCPU Wait to perform Surface Read\n\nThe amount of time, in nanoseconds, that the CPU waits for the GPU to become idle so that the CPU may read from a surface. surfaceReadLockIdleWaitTime\n\nCPU Wait to perform Surface Resize\n\nThe amount of time, in nanoseconds, that the CPU waits for the GPU to become idle so that the CPU may change the dimensions of a surface. surfaceSetShapeIdleWaitTime\n\nCPU Wait to perform Surface Write\n\nThe amount of time, in nanoseconds, that the CPU waits for the GPU to become idle so that the CPU may write to a surface. surfaceWriteLockIdleWaitTime\n\nCPU Wait to perform VRAM Surface Page-off\n\nThe amount of time, in nanoseconds, that the CPU waits for the GPU to become idle so that the CPU can page a surface out of VRAM. surfaceCopyOutWaitTime\n\nCPU Wait to perform VRAM Surface Page-on\n\nThe amount of time, in nanoseconds, that the CPU waits for the GPU to become idle so that the CPU can page a surface in to VRAM. surfaceCopyInWaitTime\n\nCPU Wait to Submit Commands\n\nThe amount of time, in nanoseconds, that the CPU waits before being able to submit a new batch of commands to the GPU. hardwareSubmitWaitTime\n\nCurrent AGP Memory\n\nThe total size, in bytes, of the AGP Graphics Address Remapping Table (GART). gartSizeBytes\n\nCurrent Free AGP Memory\n\nThe total number of free bytes in the AGP Graphics Address Remapping Table (GART). gartFreeBytes\n\nCurrent Mapped AGP Memory\n\nThe total number of bytes mapped into AGP Graphics Address Remapping Table (GART). gartUsedBytes\n\nCurrent Free Video Memory\n\nThe total number of bytes of free VRAM. This parameter is vendor specific and not available for all drivers. vramFreeBytes\n\nCurrent Largest Free Video Memory Block\n\nThe largest free contiguous chunk of VRAM, in bytes. This parameter is vendor specific and not available for all drivers. vramLargestFree\n\nCurrent Video Memory in Use\n\nThe total number of bytes of VRAM in use. This parameter is vendor specific and not available for all drivers. vramUsedBytes\n\nDVD Command Data\n\nThe number of bytes sent using DVD contexts. commandDVDBytes\n\nDVD Context Switches\n\nThe total number of context switches to a DVD context on the GPU. contextDVDSwitchCount\n\nDVD Contexts\n\nThe total number of DVD contexts in use on the GPU. contextDVDCount\n\nExtra OpenGL Data\n\nThe number of bytes used for extra OpenGL command traffic (usually vertex data). Not used by all drivers in all modes. dataGLBytes\n\nLast GPU Submission Time\n\nThe last submitted time stamp to the GPU, as an absolute time value. submitStamp\n\nLast Read GPU time\n\nThe last time stamp read back from the GPU, as an absolute time value. lastReadStamp\n\nOpenGL Command Data\n\nThe number of bytes sent using OpenGL contexts. commandGLBytes\n\nOpenGL Contexts\n\nThe total number of OpenGL contexts in use on the GPU. contextGLCountcontextGLCount\n\nOpenGL Data Buffers\n\nThe total number of extra OpenGL data buffers allocated. dataBufferCount\n\nOpenGL Context Switches\n\nThe total number of context switches to an OpenGL context on the GPU. contextGLSwitchCount\n\nSurface Page Off Data (Non-AGP)\n\nThe number of bytes transferred due to surface page-off operations. surfacePageOffBytes\n\nSurface Page On Data (Non-AGP)\n\nThe number of bytes transferred due to surface page-on operations. surfacePageInBytes\n\nSurfaces\n\nThe total number of surfaces allocated by the GPU. surfaceCount\n\nSwap Data\n\nThe number of bytes sent by swap commands. swapBytes\n\nTarget Minimum Mapped AGP Memory\n\nThe minimum amount of data, in bytes, that a driver tries to keep mapped into AGP Graphics Address Remapping Table (GART). gartCacheBytes\n\nTexture Page Off Data (Non-AGP)\n\nThe number of bytes transferred for texture page-off operations. Under most conditions, textures are not paged off but are simply thrown away since a backup exists in system memory. Texture page-off traffic usually happens when VRAM pressure forces a page-off of a texture that only has valid data in VRAM, such as a texture created using the function glCopyTexImage, or modified using the functiona glCopyTexSubImage or glTexSubImage. texturePageOutBytes\n\nTexture Page On Data (Non-AGP)\n\nThe number of bytes transferred for texture page-ins. Textures mapped using AGP will not show up here. texturePageInBytes\n\nTextures\n\nThe total number of kernel textures allocated by the GPU. textureCount\n\nTotal Command Data\n\nThe number of bytes sent using all graphics contexts (2D, OpenGL, DVD). commandBytes\n\nSymbolic Names\n\nEach symbolic name describes what the parameter represents and lists its descriptive name.\n\nbufferSwapCount\n\nThe total number of buffer swaps (or blits) that the GPU perform. (Buffer Swaps)\n\nclientGLWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits while the client (user-level) OpenGL driver waits for a hardware time stamp to arrive (usually for making texture modifications or waiting for a fence to complete). (CPU Wait in User Code)\n\ncommand2DBytes\n\nThe number of bytes sent using 2D graphics contexts. (2D Command Data)\n\ncommandBytes\n\nThe number of bytes sent using all graphics contexts (2D, OpenGL, DVD). (Total Command Data)\n\ncommandDVDBytes\n\nThe number of bytes sent using DVD contexts. (DVD Command Data)\n\ncommandGLBytes\n\nThe number of bytes sent using OpenGL contexts. (OpenGL Command Data)\n\ncontext2DCount\n\nThe total number of 2D contexts in use on the GPU. (2D Contexts)\n\ncontext2DSwitchCount\n\nThe total number of context switches to a 2D context on the GPU. (2D Context Switches)\n\ncontextDVDCount\n\nThe total number of DVD contexts in use on the GPU. (DVD Contexts)\n\ncontextDVDSwitchCount\n\nThe total number of context switches to a DVD context on the GPU. (DVD Context Switches)\n\ncontextGLCount\n\nThe total number of OpenGL contexts in use on the GPU. (OpenGL Contexts)\n\ncontextGLSwitchCount\n\nThe total number of context switches to an OpenGL context on the GPU. (OpenGL Context Switches)\n\ndataBufferCount\n\nThe total number of extra OpenGL data buffers allocated. (OpenGL Data Buffers)\n\ndataGLBytes\n\nThe number of bytes used for extra OpenGL command traffic (usually vertex data). Not used by all drivers in all modes. (Extra OpenGL Data)\n\nfinish2DWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for all 2D commands issued on a single context to complete. (CPU Wait for 2D Operations to Finish)\n\nfinishAll2DWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for all GPU operations to complete and then to be idle. Generally, only the window server waits for this state. (CPU Wait for Operations to Finish)\n\nfinishDVDWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for all DVD commands issued on a single context to complete. (CPU Wait for DVD Operations to Finish)\n\nfinishGLWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for all OpenGL commands issued on a single context to complete. This is essentially the time spent in glFinish. (CPU Wait for OpenGL Operations to Finish)\n\nfreeCommandBuffer2DWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for a 2D command buffer to become available. (CPU Wait for Free 2D Command Buffer)\n\nfreeCommandBufferGLWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for an OpenGL command buffer to become available. (CPU Wait for Free OpenGL Command Buffer)\n\nfreeContextBuffer2DWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for a 2D context-switching buffer to become available. (CPU Wait for Free 2D Context Switch Buffer)\n\nfreeContextBufferDVDWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for a DVD context-switching buffer to become available. (CPU Wait for Free DVD Context Switch Buffer)\n\nfreeContextBufferGLWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for an OpenGL context-switching buffer to become available. (CPU Wait for Free OpenGL Context Switch Buffer)\n\nfreeDataBufferGLWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for an OpenGL data buffer to become available. (CPU Wait for Free OpenGL Data Buffer)\n\ngartCacheBytes\n\nThe minimum amount of data, in bytes, that a driver tries to keep mapped into AGP Graphics Address Remapping Table (GART). (Target Minimum Mapped AGP Memory)\n\ngartFreeBytes\n\nThe total number of free bytes in the AGP Graphics Address Remapping Table (GART). (Current Free AGP Memory)\n\ngartMapInBytes\n\nThe number of bytes that are mapped into the AGP Graphics Address Remapping Table (GART) or equivalent hardware. (AGP Data Mapped)\n\ngartMapOutBytes\n\nThe number of bytes that are unmapped from the AGP Graphics Address Remapping Table (GART) (or equivalent hardware). (AGP Data Unmapped)\n\ngartSizeBytes\n\nThe total size, in bytes, of the AGP Graphics Address Remapping Table (GART). (Current AGP Memory)\n\ngartUsedBytes\n\nThe total number of bytes mapped into AGP Graphics Address Remapping Table (GART). (Current Mapped AGP Memory)\n\nhardwareSubmitWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits before being able to submit a new batch of commands to the GPU. (CPU Wait to Submit Commands)\n\nhardwareWaitTime\n\nThe amount of time, in nanoseconds, that the CPU stalled while waiting on the GPU for any reason. (CPU Wait for GPU)\n\nlastReadStamp\n\nThe last time stamp read back from the GPU, as an absolute time value. (Last Read GPU time)\n\nremoveFromGARTWaitTime\n\nThe amount of time, in nanoseconds, the CPU waits for the GPU to finish an operation on a buffer that needs to be removed from the Graphics Address Remapping Table (GART). (CPU Wait for Mapped AGP Buffer Removal)\n\nsubmitStamp\n\nThe last submitted time stamp to the GPU, as an absolute time value. (Last GPU Submission Time)\n\nsurfaceCount\n\nThe total number of surfaces allocated by the GPU.(Surfaces)\n\nsurfaceCopyInWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for the GPU to become idle so that the CPU can page a surface in to VRAM. (CPU Wait to perform VRAM Surface Page-on)\n\nsurfaceCopyOutWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for the GPU to become idle so that the CPU can page a surface out of VRAM. (CPU Wait to perform VRAM Surface Page-off)\n\nsurfacePageInBytes\n\nThe number of bytes transferred due to surface page-on operations. (Surface Page On Data (Non-AGP))\n\nsurfacePageOffBytes\n\nThe number of bytes transferred due to surface page-off operations. (Surface Page Off Data (Non-AGP))\n\nsurfaceReadLockIdleWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for the GPU to become idle so that the CPU may read from a surface. (CPU Wait to perform Surface Read)\n\nsurfaceSetShapeIdleWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for the GPU to become idle so that the CPU may change the dimensions of a surface. (CPU Wait to perform Surface Resize)\n\nsurfaceWriteLockIdleWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for the GPU to become idle so that the CPU may write to a surface. (CPU Wait to perform Surface Write)\n\nswapBytes\n\nThe number of bytes sent by swap commands. (Swap Data)\n\nswapComplete2DWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for a previously issued 2D buffer swap to complete. (CPU Wait for 2D Swap to Complete)\n\nswapCompleteDVDWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for a previously issued DVD buffer swap to complete. (CPU Wait for DVD Swap to Complete)\n\nswapCompleteGLWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for a previously issued OpenGL buffer swap to complete. (CPU Wait for OpenGL Swap to Complete)\n\ntextureCount\n\nThe total number of kernel textures allocated by the GPU. (Textures)\n\ntexturePageInBytes\n\nThe number of bytes transferred for texture page-ins. Textures mapped using AGP will not show up here. (Texture Page On Data (Non-AGP))\n\ntexturePageInWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for a texture upload command to be completed by the GPU. This is the that the CPU could use for updating and reloading a texture. Typically, there is very little, if any time, spent waiting here. (CPU Texture Page-on Wait)\n\ntexturePageOffWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for the GPU to finish activity. This is the that the CPU could use to modify a texture prior to paging the texture. This metric applies only if the texture must be paged using the CPU and not using direct memory access (DMA). (CPU Texture Page-off Wait (non-DMA))\n\ntexturePageOutBytes\n\nThe number of bytes transferred for texture page-off operations. Under most conditions, textures are not paged off but are simply thrown away since a backup exists in system memory. Texture page-off traffic usually happens when VRAM pressure forces a page-off of a texture that only has valid data in VRAM, such as a texture created using the function glCopyTexImage, or modified using the functions glCopyTexSubImage or glTexSubImage. (Texture Page Off Data (Non-AGP))\n\ntextureWaitTime\n\nThe amount of time, in nanoseconds, that the CPU waits for a texture upload to complete before the buffer can be modified. This particular metric tracks usage only by 2D contexts, and is somewhat obsolete. (CPU Texture Upload Wait (2D context only))\n\nvramFreeBytes\n\nThe total number of bytes of free VRAM. This parameter is vendor specific and not available for all drivers. (Current Free Video Memory)\n\nvramLargestFree\n\nThe largest free contiguous chunk of VRAM, in bytes. This parameter is vendor specific and not available for all drivers. (Current Largest Free Video Memory Block)\n\nvramUsedBytes\n\nThe total number of bytes of VRAM in use. This parameter is vendor specific and not available for all drivers. (Current Video Memory in Use)\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "Document Revision History",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLShaderBuilderUserGuide/RevisionHistory.html#//apple_ref/doc/uid/TP40006476-CH99-DontLinkElementID_7",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Shader Builder User Guide\nTable of Contents\nIntroduction\nGetting Started\nBuilding Shaders\nRevision History\nPrevious\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nDocument Revision History\n\nThis table describes the changes to OpenGL Shader Builder User Guide.\n\nDate\tNotes\n2015-03-09\t\n\nMoved to Retired Documents Library.\n\n\n2012-02-16\t\n\nUpdated installation information.\n\n\n2008-06-23\t\n\nNew document that explains how to use OpenGL Shader Builder to develop and test GPU programs.\n\n\n\nPrevious\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "Identifying and Solving Performance Issues",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLDriverMonitorUserGuide/Performance/performance.html#//apple_ref/doc/uid/TP40006474-CH3-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Driver Monitor User Guide\nTable of Contents\nIntroduction\nUsing OpenGL Driver Monitor\nIdentifying and Solving Performance Issues\nAppendix A: OpenGL Driver Monitor Parameters\nRevision History\nNext\nPrevious\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nIdentifying and Solving Performance Issues\n\nOpenGL Driver Monitor is not the primary tool for analyzing performance issues in an OpenGL application. It’s the backup tool that experts use when Instruments and OpenGL Profiler don’t reveal the cause of a performance problem. This chapter assumes that you have already used Apple’s other tools to analyze your OpenGL application.\n\nThe strategies described here can help you identify the most common problems that occur in OpenGL applications. Keep in mind that analyzing difficult performance problems is more of an art than a science. Although you’ll want to start with these basic strategies, you’ll need to devise additional ones tailored to the type of problem you see, and to whether you are trying to solve a driver issue or an application one.\n\nChecking for Best Practices\n\nBefore you begin to use OpenGL Driver Monitor as an analysis tool, it’s a good idea to check your code to see if you are following the most recent best practices for using OpenGL. See:\n\nThe “Improving Performance” chapter in OpenGL Programming Guide for Mac for a discussion of best practices\n\nOpenGL Profiler User Guide. You’ll find advice on functions that you need to make sure you use correctly, if you use them at all.\n\nChecking Data Transfer Rates\n\nTo check data transfer rates, monitor the following:\n\nVRAM usage. See whether VRAM usage is at capacity by looking at Current Video Memory in Use (vramUsedBytes) or Current Free Video Memory (vramFreeBytes). If it is at capacity, investigate whether the system is low on VRAM or whether VRAM usage is unusually high for an application running on the system.\n\nSwap rate. A variety of parameters, such as Buffer Swaps (bufferSwapCount), let you investigate the cause of unusually high swap rates. Check to see whether the swapped data is dynamic or static. If the data os static, make sure you are using caches, vertex buffer objects, or some other technique that’s optimized for static data. Use swaps only for dynamic data, and only when the data changes.\n\nThe time spent by the CPU waiting for the GPU. Look at CPU Wait for GPU (hardwareWaitTime). If the CPU spends a lot of time simply waiting, check to see whether you are calling glFlush or glFinish inappropriately. There are only a few cases where you actually need to use these calls, and these cases are rare. For more information, see the “Improving Performance” chapter in OpenGL Programming Guide for Mac.\n\nChecking for Suboptimal Surface and Texture Paging\n\nYou need to make sure that your application is not paging texture and surface data unnecessarily. When it does page, you should use the accelerated graphics port (AGP pathway, which is also known as DMA transfer). Non-AGP transfers slow performance. You can check for less optimal paging by looking at these parameters:\n\nSurface Page Off Data (Non-AGP) (surfacePageOffBytes)\n\nSurface Page On Data (Non-AGP) (surfacePageInBytes)\n\nTexture Page Off Data (Non-AGP) (texturePageOutBytes)\n\nTexture Page Off Data (Non-AGP) (texturePageInBytes)\n\nNon-AGP transfer is acceptable only if you must reorder data or align it. If possible, use this type of data transfer at initialization time and not during a rendering loop.\n\nIf your application has a lot of paging activity, whether it’s AGP or non-AGP, consider using framebuffer objects.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "Choosing Renderer and Buffer Attributes",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_pixelformats/opengl_pixelformats.html#//apple_ref/doc/uid/TP40001987-CH214-SW9",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nChoosing Renderer and Buffer Attributes\n\nRenderer and buffer attributes determine the renderers that the system chooses for your application. Each of the Apple-specific OpenGL APIs provides constants that specify a variety of renderer and buffer attributes. You supply a list of attribute constants to one of the Apple OpenGL functions for choosing a pixel format object. The pixel format object maintains a list of renderers that meet the requirements defined by those attributes.\n\nIn a real-world application, selecting attributes is an art because you don't know the exact combination of hardware and software that your application will run on. An attribute list that is too restrictive may miss out on future capabilities or it may fail to return renderers on some systems. For example, if you specify a buffer of a specific depth, your application won't be able to take advantage of a larger buffer when more memory is available in the future. In this case, you might specify a required minimum and direct OpenGL to use the maximum available.\n\nAlthough you might specify attributes that make your OpenGL content look and run its best, you also need to consider whether your application should run on a less-capable system with less speed or detail. If tradeoffs are acceptable, you need to set the attributes accordingly.\n\nOpenGL Profiles (OS X v10.7)\n\nWhen your application is running on OS X v10.7, it should always include the kCGLPFAOpenGLProfile attribute, followed by a constant for the profile whose functionality your application requires. A profile affects different parts of OpenGL in OS X:\n\nA profile requires that a specific version of the OpenGL API must provided by the renderer. The renderer may implement a different version of the OpenGL specification only if that version implements the same functions and constants required by the profile; typically, this means a renderer that supports a later version of the OpenGL specification that did not remove or alter behavior specified in the version of the OpenGL specification your application requested.\n\nThe profile alters the list of OpenGL extensions returned by the renderer. For example, extensions whose functionality is provided by the version of the OpenGL specification you requested are not also returned in the list of extensions.\n\nOn OS X, the profile affects what other renderer and buffer attributes may be included in the attributes list.\n\nFollow these guidelines to choose an OpenGL profile:\n\nIf you are developing a new OS X v10.7 application, implement your OpenGL functionality using the OpenGL 3.2 Core profile; include the kCGLOGLPVersion_3_2_Core constant.\n\nThe OpenGL 3.2 core profile is defined by Khronos and explicitly removes deprecated features described in earlier versions of the OpenGL specification; further the core profile prohibits these functions from being added back into OpenGL using extensions. OpenGL 3.2 core represents a complete break from the fixed function pipeline of OpenGL 1.x in favor of a clean, lean shader-based pipeline.\n\nWhen you use the OpenGL 3.2 Core profile on OS X, legacy extensions are removed wherever their functionality is already provided by OpenGL 3.2. Further, pixel and buffer format attributes that are marked as deprecated may not be used in conjunction with the OpenGL 3.2 core profile.\n\nIf you are updating an existing OS X application, include the kCGLOGLPVersion_Legacy constant.\n\nThe legacy profile provides the same functionality found in earlier versions of OS X, with no changes. It continues to support older extensions as well as deprecated pixel and buffer format attributes. No new functionality will be added to the legacy profile in future versions of OS X.\n\nIf you want to use OpenGL 3.2 in your application, but also want to support earlier versions of OS X or Macs that lack hardware support for OpenGL 3.2, you must implement multiple OpenGL rendering options in your application. On OS X v10.7, your application should first test to see if OpenGL 3.2 is supported. If OpenGL 3.2 is supported, create a context and provide it to your OpenGL 3.2 rendering path. Otherwise, search for a pixel format using the legacy profile instead.\n\nFor more information on migrating an application to OpenGL 3.2, see Updating an Application to Support the OpenGL 3.2 Core Specification.\n\nBuffer Size Attribute Selection Tips\n\nFollow these guidelines to choose buffer attributes that specify buffer size:\n\nTo choose color, depth, and accumulation buffers that are greater than or equal to a size you specify, use the minimum policy attribute (NSOpenGLPFAMinimumPolicy or kCGLPFAMinimumPolicy).\n\nTo choose color, depth, and accumulation buffers that are closest to a size you specify, use the closest policy attribute (NSOpenGLPFAClosestPolicy or kCGLPFAClosestPolicy).\n\nTo choose the largest color, depth, and accumulation buffers available, use the maximum policy attribute (NSOpenGLPFAMaximumPolicy or kCGLPFAMaximumPolicy). As long as you pass a value that is greater than 0, this attribute specifies the use of color, depth, and accumulation buffers that are the largest size possible.\n\nEnsuring That Back Buffer Contents Remain the Same\n\nWhen your application uses a double-buffered context, it displays the rendered image by calling a function to flush the image to the screen— theNSOpenGLContext class’s flushBuffer method or the CGL function CGLFlushDrawable. When the image is displayed, the contents of the back buffer are not preserved. The next time your application wants to update the back buffer, it must completely redraw the scene.\n\nYour application can add a backing store attribute (NSOpenGLPFABackingStore or kCGLPFABackingStore) to preserve the contents of the buffer after the back buffer is flushed. Adding this attribute disables some optimizations that the system can perform, which may impact the performance of your application.\n\nEnsuring a Valid Pixel Format Object\n\nThe pixel format routines (the initWithAttributes: method of the NSOpenGLPixelFormat class and the CGLChoosePixelFormat function) return a pixel format object to your application that you use to create a rendering context. The buffer and renderer attributes that you supply to the pixel format routine determine the characteristics of the OpenGL drawing sent to the rendering context. If the system can't find at least one pixel format that satisfies the constraints specified by the attribute array, it returns NULL for the pixel format object. In this case, your application should have an alternative that ensures it can obtain a valid object.\n\nOne alternative is to set up your attribute array with the least restrictive attribute first and the most restrictive attribute last. Then, it is fairly easy to adjust the attribute list and make another request for a pixel format object. The code in Listing 6-1 illustrates this technique using the CGL API. Notice that the initial attributes list is set up with the supersample attribute last in the list. If the function CGLChoosePixelFormat returns NULL, it clears the supersample attribute to NULL and tries again.\n\nListing 6-1  Using the CGL API to create a pixel format object\n\nint last_attribute = 6;\n\n\nCGLPixelFormatAttribute attribs[] =\n\n\n{\n\n\n    kCGLPFAAccelerated,\n\n\n    kCGLPFAColorSize, 24\n\n\n    kCGLPFADepthSize, 16,\n\n\n    kCGLPFADoubleBuffer,\n\n\n    kCGLPFASupersample,\n\n\n    0\n\n\n};\n\n\n \n\n\nCGLPixelFormatObj pixelFormatObj;\n\n\nGLint numPixelFormats;\n\n\nlong value;\n\n\n \n\n\nCGLChoosePixelFormat (attribs, &pixelFormatObj, &numPixelFormats);\n\n\n \n\n\nif( pixelFormatObj == NULL ) {\n\n\n    attribs[last_attribute] = NULL;\n\n\n    CGLChoosePixelFormat (attribs, &pixelFormatObj, &numPixelFormats);\n\n\n}\n\n\n \n\n\nif( pixelFormatObj == NULL ) {\n\n\n    // Your code to notify the user and take action.\n\n\n}\nEnsuring a Specific Type of Renderer\n\nThere are times when you want to ensure that you obtain a pixel format that supports a specific renderer type, such as a hardware-accelerated renderer. Table 6-1 lists attributes that support specific types of renderers. The table reflects the following tips for setting up pixel formats:\n\nTo select only hardware-accelerated renderers, use both the accelerated and no-recovery attributes.\n\nTo use only the floating-point software renderer, use the appropriate generic floating-point constant.\n\nTo render to system memory, use the offscreen pixel attribute. Note that this rendering option does not use hardware acceleration.\n\nTo render offscreen with hardware acceleration, specify a pixel buffer attribute. (See Rendering to a Pixel Buffer.)\n\nTable 6-1  Renderer types and pixel format attributes\n\nRenderer type\n\n\t\n\nCGL\n\n\t\n\nCocoa\n\n\n\n\nHardware-accelerated onscreen\n\n\t\n\nkCGLPFAAccelerated\n\nkCGLPFANoRecovery\n\n\t\n\nNSOpenGLPFAAccelerated\n\nNSOpenGLPFANoRecovery\n\n\n\n\nSoftware (floating-point)\n\n\t\n\nkCGLPFARendererID\n\nkCGLRendererGenericFloatID\n\n\t\n\nNSOpenGLPFARendererID\n\nkCGLRendererGenericFloatID\n\n\n\n\nSystem memory (not accelerated)\n\n\t\n\nkCGLPFAOffScreen\n\n\t\n\nNSOpenGLPFAOffScreen\n\n\n\n\nHardware-accelerated offscreen\n\n\t\n\nkCGLPFAPBuffer\n\n\t\n\nNSOpenGLPFAPixelBuffer\n\nEnsuring a Single Renderer for a Display\n\nIn some cases you may want to use a specific hardware renderer and nothing else. Since the OpenGL framework normally provides a software renderer as a fallback in addition to whatever hardware renderer it chooses, you need to prevent OpenGL from choosing the software renderer as an option. To do this, specify the no-recovery attribute for a windowed drawable object.\n\nLimiting a context to use a specific display, and thus a single renderer, has its risks. If your application runs on a system that uses more than one display, dragging a windowed drawable object from one display to the other is likely to yield a less than satisfactory result. Either rendering fails, or OpenGL uses the specified renderer and then copies the result to the second display. The same unsatisfactory result happens when attaching a full-screen context to another display. If you choose to use the hardware renderer associated with a specific display, you need to add code that detects and handles display changes.\n\nThe code examples that follow show how to use each of the Apple-specific OpenGL APIs to set up a context that uses a single renderer. Listing 6-2 shows how to set up an NSOpenGLPixelFormat object that supports a single renderer. The attribute NSOpenGLPFANoRecovery specifies to OpenGL not to provide the fallback option of the software renderer.\n\nListing 6-2  Setting an NSOpenGLContext object to use a specific display\n\n#import <Cocoa/Cocoa.h>\n\n\n+ (NSOpenGLPixelFormat*)defaultPixelFormat\n\n\n{\n\n\n    NSOpenGLPixelFormatAttribute attributes [] = {\n\n\n                        NSOpenGLPFAScreenMask, 0,\n\n\n                        NSOpenGLPFANoRecovery,\n\n\n                        NSOpenGLPFADoubleBuffer,\n\n\n                        (NSOpenGLPixelFormatAttribute)nil };\n\n\nCGDirectDisplayID display = CGMainDisplayID ();\n\n\n// Adds the display mask attribute for selected display\n\n\nattributes[1] = (NSOpenGLPixelFormatAttribute)\n\n\n                    CGDisplayIDToOpenGLDisplayMask (display);\n\n\nreturn [[(NSOpenGLPixelFormat *)[NSOpenGLPixelFormat alloc] initWithAttributes:attributes]\n\n\n                                       autorelease];\n\n\n}\n\nListing 6-3 shows how to use CGL to set up a context that uses a single renderer. The attribute kCGLPFANoRecovery ensures that OpenGL does not provide the fallback option of the software renderer.\n\nListing 6-3  Setting a CGL context to use a specific display\n\n#include <OpenGL/OpenGL.h>\n\n\nCGLPixelFormatAttribute attribs[] = { kCGLPFADisplayMask, 0,\n\n\n                                 kCGLPFANoRecovery,\n\n\n                                 kCGLPFADoubleBuffer,\n\n\n                                  0 };\n\n\nCGLPixelFormatObj pixelFormat = NULL;\n\n\nGLint numPixelFormats = 0;\n\n\nCGLContextObj cglContext = NULL;\n\n\nCGDirectDisplayID display = CGMainDisplayID ();\n\n\n// Adds the display mask attribute for selected display\n\n\nattribs[1] = CGDisplayIDToOpenGLDisplayMask (display);\n\n\nCGLChoosePixelFormat (attribs, &pixelFormat, &numPixelFormats);\nAllowing Offline Renderers\n\nAdding the attribute NSOpenGLPFAAllowOfflineRenderers allows OpenGL to include offline renderers in the list of virtual screens returned in the pixel format object. Apple recommends you include this attribute, because it allows your application to work better in environments where renderers come and go, such as when a new display is plugged into a Mac.\n\nIf your application includes NSOpenGLPFAAllowOfflineRenderers in the list of attributes, your application must also watch for display changes and update its rendering context. See Update the Rendering Context When the Renderer or Geometry Changes.\n\nOpenCL\n\nIf your applications uses OpenCL to perform other computations, you may want to find an OpenGL renderer that also supports OpenCL. To do this, add the attribute NSOpenGLPFAAcceleratedCompute to the pixel format attribute list. Adding this attribute restricts the list of renderers to those that also support OpenCL.\n\nMore information on OpenCL can be found in the OpenCL Programming Guide for Mac.\n\nDeprecated Attributes\n\nThere are several renderer and buffer attributes that are no longer recommended either because they are too narrowly focused or no longer useful. Your application should move away from using any of these attributes:\n\nThe robust attribute (NSOpenGLPFARobust or kCGLPFARobust) specifies only those renderers that do not have any failure modes associated with a lack of video card resources.\n\nThe multiple-screen attribute (NSOpenGLPFAMultiScreen or kCGLPFAMultiScreen) specifies only those renderers that can drive more than one screen at a time.\n\nThe multiprocessing-safe attribute (kCGLPFAMPSafe) specifies only those renderers that are thread safe. This attribute is deprecated in OS X because all renderers can accept commands for threads running on a second processor. However, this does not mean that all renderers are thread safe or reentrant. See Concurrency and OpenGL.\n\nThe compliant attribute (NSOpenGLPFACompliant or kCGLPFACompliant) specifies only OpenGL-compliant renderers. All OS X renderers are OpenGL-compliant, so this attribute is no longer useful.\n\nThe full screen attribute (kCGLPFAFullScreen) requested special full screen contexts. The window screen attribute (kCGLPFAWindow) required the context to support windowed contexts. OS X no longer requires a special full screen context to be created, as it automatically provides the same performance benefits with a properly formatted window.\n\nThe offscreen buffer attribute (kCGLPFAOffScreen) selects renderers capable of rendering to offscreen memory. Instead, use a frame buffer object as the rendering target and read the final results back to application memory.\n\nThe pixel buffer attributes (kCGLPFAPBuffer and kCGLPFARemotePBuffer are no longer recommended; use frame buffer objects instead.\n\nThe auxiliary buffers attribute (kCGLPFAAuxBuffers) specifies the number of required auxiliary buffers your application requires. Auxiliary buffers are not supported by the OpenGL 3.2 Core profile. Because auxiliary buffers are not supported, the kCGLPFAAuxDepthStencil attribute that modifies it is also deprecated.\n\nThe accumulation buffer size attribute (kCGLPFAAccumSize) specifies the desired size for the accumulation buffer. Accumulation buffers are not supported by the OpenGL 3.2 Core Profile.\n\nImportant: Your application may not use any of the deprecated attributes in conjunction with a profile other than the legacy profile; if you do, pixel format creation fails.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Introduction",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLDriverMonitorUserGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40006474",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Driver Monitor User Guide\nTable of Contents\nIntroduction\nUsing OpenGL Driver Monitor\nIdentifying and Solving Performance Issues\nAppendix A: OpenGL Driver Monitor Parameters\nRevision History\nNext\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nIntroduction\n\nImportant The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nOpenGL Driver Monitor is a developer tool that has two purposes. It is:\n\nAn application that lets developers see how OpenGL works on a specific system and look at the capabilities of a driver\n\nAn advanced diagnostic tool that OpenGL driver developers and experts can use to track down thorny performance issues\n\nMost OpenGL developers should not use the driver monitor application to analyze performance issues; they should instead use Instruments and OpenGL Profiler.\n\nYou’ll want to read this document if you:\n\nDevelop applications that use OpenGL on OS X and you are curious as to how the GPU and CPU interact\n\nWant to look at the capabilities of a particular OpenGL driver\n\nAre an OpenGL driver developer who needs to investigate a driver bug\n\nAre an advanced OpenGL developer or consultant trying to track down a performance issue that you’ve been unable to analyze using Instruments and OpenGL Profiler\n\nOrganization of This Document\n\nThis document is organized into the following chapters:\n\nUsing OpenGL Driver Monitor describes how to set preferences, collect real-time parameter values locally or remotely, and view renderer information.\n\nIdentifying and Solving Performance Issues provides strategies for using OpenGL Driver Monitor to analyze performance issues.\n\nOpenGL Driver Monitor Parameters describes, by symbolic and descriptive names, the parameters that you can monitor.\n\nSee Also\n\nThese documents contain information that can help you analyze and optimize your OpenGL code:\n\nOpenGL Programming Guide for Mac discusses best practices for getting optimal performance.\n\nOpenGL Profiler User Guide explains how to collect and analyze data that can help you tune your OpenGL application.\n\nInstruments User Guide describes how to optimize application performance using this tool.\n\nNext\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "Working with Rendering Contexts",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_contexts/opengl_contexts.html#//apple_ref/doc/uid/TP40001987-CH216-SW12",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nWorking with Rendering Contexts\n\nA rendering context is a container for state information. When you designate a rendering context as the current rendering context, subsequent OpenGL commands modify that context’s state, objects attached to that context, or the drawable object associated with that context. The actual drawing surfaces are never owned by the rendering context but are created, as needed, when the rendering context is actually attached to a drawable object. You can attach multiple rendering contexts to the same drawing surfaces. Each context maintains its own drawing state.\n\nDrawing to a Window or View, Drawing to the Full Screen, and Drawing Offscreen show how to create a rendering context and attach it to a drawable object. This chapter describes advanced ways to interact with rendering contexts.\n\nUpdate the Rendering Context When the Renderer or Geometry Changes\n\nA renderer change can occur when the user drags a window from one display to another or when a display is attached or removed. Geometry changes occur when the display mode changes or when a window is resized or moved. If your application uses an NSOpenGLView object to maintain the context, it is automatically updated. An application that creates a custom view to hold the rendering context must track the appropriate system events and update the context when the geometry or display changes.\n\nUpdating a rendering context notifies it of geometry changes; it doesn't flush content. Calling an update function updates the attached drawable objects and ensures that the renderer is properly updated for any virtual screen changes. If you don't update the rendering context, you may see rendering artifacts.\n\nThe routine that you call for updating determines how events related to renderer and geometry changes are handled. For applications that use or subclass NSOpenGLView, Cocoa calls the update method automatically. Applications that create an NSOpenGLContext object manually must call the update method of NSOpenGLContext directly. For a full-screen Cocoa application, calling the setFullScreen method of NSOpenGLContext ensures that depth, size, or display changes take affect.\n\nYour application must update the rendering context after the system event but before drawing to the context. If the drawable object is resized, you may want to issue a glViewport command to ensure that the content scales properly.\n\nNote: Some system-level events (such as display mode changes) that require a context update could reallocate the buffers of the context; thus you need to redraw the entire scene after all context updates.\n\nIt's important that you don't update rendering contexts more than necessary. Your application should respond to system-level events and notifications rather than updating every frame. For example, you'll want to respond to window move and resize operations and to display configuration changes such as a color depth change.\n\nTracking Renderer Changes\n\nIt's fairly straightforward to track geometry changes, but how are renderer changes tracked? This is where the concept of a virtual screen becomes important (see Virtual Screens). A change in the virtual screen indicates a renderer change, a change in renderer capability, or both. When your application detects a window resize event, window move event, or display change, it should check for a virtual screen change and respond to the change to ensure that the current application state reflects any changes in renderer capabilities.\n\nEach of the Apple-specific OpenGL APIs has a function that returns the current virtual screen number:\n\nThe currentVirtualScreen method of the NSOpenGLContext class\n\nThe CGLGetVirtualScreen function\n\nThe virtual screen number represents an index in the list of virtual screens that were set up specifically for the pixel format object used for the rendering context. The number is unique to the list but is meaningless otherwise.\n\nWhen the renderer changes, the limits and extensions available to OpenGL may also change. Your application should retest the capabilities of the renderer and use these to choose its rendering algorithms appropriately. See Determining the OpenGL Capabilities Supported by the Renderer.\n\nUpdating a Rendering Context for a Custom Cocoa View\n\nIf you subclass NSView instead of using the NSOpenGLView class, your application must update the rendering context. That's due to a slight difference between the events normally handled by the NSView class and those handled by the NSOpenGLView class. Cocoa does not call a reshape method for the NSView class when the size changes because that class does not export a reshape method to override. Instead, you need to perform reshape operations directly in your drawRect: method, looking for changes in view bounds prior to drawing content. Using this approach provides results that are equivalent to using the reshape method of the NSOpenGLView class.\n\nListing 7-1 is a partial implementation of a custom view that shows how to handle context updates. The update method is called after move, resize, and display change events and when the surface needs updating. The class adds an observer to the notification NSViewGlobalFrameDidChangeNotification, which is posted whenever an NSView object that has attached surfaces (that is, NSOpenGLContext objects) resizes, moves, or changes coordinate offsets.\n\nIt's slightly more complicated to handle changes in the display configuration. For that, you need to register for the notification NSApplicationDidChangeScreenParametersNotification through the NSApplication class. This notification is posted whenever the configuration of any of the displays attached to the computer is changed (either programmatically or when the user changes the settings in the interface).\n\nListing 7-1  Handling context updates for a custom view\n\n#import <Cocoa/Cocoa.h>\n\n\n#import <OpenGL/OpenGL.h>\n\n\n#import <OpenGL/gl.h>\n\n\n \n\n\n@class NSOpenGLContext, NSOpenGLPixelFormat;\n\n\n \n\n\n@interface CustomOpenGLView : NSView\n\n\n{\n\n\n  @private\n\n\n  NSOpenGLContext*   _openGLContext;\n\n\n  NSOpenGLPixelFormat* _pixelFormat;\n\n\n}\n\n\n \n\n\n- (id)initWithFrame:(NSRect)frameRect\n\n\n        pixelFormat:(NSOpenGLPixelFormat*)format;\n\n\n \n\n\n- (void)update;\n\n\n@end\n\n\n \n\n\n@implementation CustomOpenGLView\n\n\n \n\n\n- (id)initWithFrame:(NSRect)frameRect\n\n\n        pixelFormat:(NSOpenGLPixelFormat*)format\n\n\n{\n\n\n  self = [super initWithFrame:frameRect];\n\n\n  if (self != nil) {\n\n\n    _pixelFormat   = [format retain];\n\n\n  [[NSNotificationCenter defaultCenter] addObserver:self\n\n\n                   selector:@selector(_surfaceNeedsUpdate:)\n\n\n                   name:NSViewGlobalFrameDidChangeNotification\n\n\n                   object:self];\n\n\n  }\n\n\n  return self;\n\n\n}\n\n\n \n\n\n- (void)dealloc\n\n\n  [[NSNotificationCenter defaultCenter] removeObserver:self\n\n\n                     name:NSViewGlobalFrameDidChangeNotification\n\n\n                     object:self];\n\n\n  [self clearGLContext];\n\n\n}\n\n\n \n\n\n- (void)update\n\n\n{\n\n\n  if ([_openGLContext view] == self) {\n\n\n    [_openGLContext update];\n\n\n  }\n\n\n}\n\n\n \n\n\n- (void) _surfaceNeedsUpdate:(NSNotification*)notification\n\n\n{\n\n\n  [self update];\n\n\n}\n\n\n \n\n\n@end\nContext Parameters Alter the Context’s Behavior\n\nA rendering context has a variety of parameters that you can set to suit the needs of your OpenGL drawing. Some of the most useful, and often overlooked, context parameters are discussed in this section: swap interval, surface opacity, surface drawing order, and back-buffer size control.\n\nEach of the Apple-specific OpenGL APIs provides a routine for setting and getting rendering context parameters:\n\nThe setValues:forParameter: method of the NSOpenGLContext class takes as arguments a list of values and a list of parameters.\n\nThe CGLSetParameter function takes as parameters a rendering context, a constant that specifies an option, and a value for that option.\n\nSome parameters need to be enabled for their values to take effect. The reference documentation for a parameter indicates whether a parameter needs to be enabled. See NSOpenGLContext Class Reference, and CGL Reference.\n\nSwap Interval Allows an Application to Synchronize Updates to the Screen Refresh\n\nIf the swap interval is set to 0 (the default), buffers are swapped as soon as possible, without regard to the vertical refresh rate of the monitor. If the swap interval is set to any other value, the buffers are swapped only during the vertical retrace of the monitor. For more information, see Synchronize with the Screen Refresh Rate.\n\nYou can use the following constants to specify that you are setting the swap interval value:\n\nFor Cocoa, use NSOpenGLCPSwapInterval.\n\nIf you are using the CGL API, use kCGLCPSwapInterval as shown in Listing 7-2.\n\nListing 7-2  Using CGL to set up synchronization\n\nGLint sync = 1;\n\n\n// ctx must be a valid context\n\n\nCGLSetParameter (ctx, kCGLCPSwapInterval, &sync);\nSurface Opacity Specifies How the OpenGL Surface Blends with Surfaces Behind It\n\nOpenGL surfaces are typically rendered as opaque. Thus the background color for pixels with alpha values of 0.0 is the surface background color. If you set the value of the surface opacity parameter to 0, then the contents of the surface are blended with the contents of surfaces behind the OpenGL surface. This operation is equivalent to OpenGL blending with a source contribution proportional to the source alpha and a background contribution proportional to 1 minus the source alpha. A value of 1 means the surface is opaque (the default); 0 means completely transparent.\n\nYou can use the following constants to specify that you are setting the surface opacity value:\n\nFor Cocoa, use NSOpenGLCPSurfaceOpacity.\n\nIf you are using the CGL API, use kCGLCPSurfaceOpacity as shown in Listing 7-3.\n\nListing 7-3  Using CGL to set surface opacity\n\nGLint opaque = 0;\n\n\n// ctx must be a valid context\n\n\nCGLSetParameter (ctx, kCGLCPSurfaceOpacity, &opaque);\nSurface Drawing Order Specifies the Position of the OpenGL Surface Relative to the Window\n\nA value of 1 means that the position is above the window; a value of –1 specifies a position that is below the window. When you have overlapping views, setting the order to -1 causes OpenGL to draw underneath, 1 causes OpenGL to draw on top. This parameter is useful for drawing user interface controls on top of an OpenGL view.\n\nYou can use the following constants to specify that you are setting the surface drawing order value:\n\nFor Cocoa, use NSOpenGLCPSurfaceOrder.\n\nIf you are using the CGL API, use kCGLCPSurfaceOrder as shown in Listing 7-4.\n\nListing 7-4  Using CGL to set surface drawing order\n\nGLint order = –1; // below window\n\n\n// ctx must be a valid context\n\n\nCGLSetParameter (ctx, kCGLCPSurfaceOrder, &order);\nDetermining Whether Vertex and Fragment Processing Happens on the GPU\n\nCGL provides two parameters for checking whether the system is using the GPU for processing: kCGLCPGPUVertexProcessing and kCGLCPGPUFragmentProcessing. To check vertex processing, pass the vertex constant to the CGLGetParameter function. To check fragment processing, pass the fragment constant to CGLGetParameter. Listing 7-5 demonstrates how to use these parameters.\n\nImportant: Although you can perform these queries at any time, keep in mind that such queries force an internal state validation, which can impact performance. For best performance, do not use these queries inside your drawing loop. Instead, perform the queries once at initialization or context setup time to determine whether OpenGL is using the CPU or the GPU for processing, and then act appropriately in your drawing loop.\n\nListing 7-5  Using CGL to check whether the GPU is processing vertices and fragments\n\nBOOL gpuProcessing;\n\n\nGLint fragmentGPUProcessing, vertexGPUProcessing;\n\n\nCGLGetParameter (CGLGetCurrentContext(), kCGLCPGPUFragmentProcessing,\n\n\n                                         &fragmentGPUProcessing);\n\n\nCGLGetParameter(CGLGetCurrentContext(), kCGLCPGPUVertexProcessing,\n\n\n                                         &vertexGPUProcessing);\n\n\ngpuProcessing = (fragmentGPUProcessing && vertexGPUProcessing) ? YES : NO;\nControlling the Back Buffer Size\n\nNormally, the back buffer is the same size as the window or view that it's drawn into, and it changes size when the window or view changes size. For a window whose size is 720×pixels, the OpenGL back buffer is sized to match. If the window grows to 1024×768 pixels, for example, then the back buffer is resized as well. If you do not want this behavior, use the back buffer size control parameter.\n\nUsing this parameter fixes the size of the back buffer and lets the system scale the image automatically when it moves the data to a variable size buffer (see Figure 7-1). The size of the back buffer remains fixed at the size that you set up regardless of whether the image is resized to display larger onscreen.\n\nYou can use the following constants to specify that you are setting the surface backing size:\n\nIf you are using the CGL API, use kCGLCPSurfaceBackingSize, as shown in Listing 7-6.\n\nListing 7-6  Using CGL to set up back buffer size control\n\nGLint dim[2] = {720, 480};\n\n\n// ctx must be a valid context\n\n\nCGLSetParameter(ctx, kCGLCPSurfaceBackingSize, dim);\n\n\nCGLEnable (ctx, kCGLCESurfaceBackingSize);\nFigure 7-1  A fixed size back buffer and variable size front buffer\nSharing Rendering Context Resources\n\nA rendering context does not own the drawing objects attached to it, which leaves open the option for sharing. Rendering contexts can share resources and can be attached to the same drawable object (see Figure 7-2) or to different drawable objects (see Figure 7-3). You set up context sharing—either with more than one drawable object or with another context—at the time you create a rendering context.\n\nContexts can share object resources and their associated object state by indicating a shared context at context creation time. Shared contexts share all texture objects, display lists, vertex programs, fragment programs, and buffer objects created before and after sharing is initiated. The state of the objects is also shared but not other context state, such as current color, texture coordinate settings, matrix and lighting settings, rasterization state, and texture environment settings. You need to duplicate context state changes as required, but you need to set up individual objects only once.\n\nFigure 7-2  Shared contexts attached to the same drawable object\n\nWhen you create an OpenGL context, you can designate another context whose object resources you want to share. All sharing is peer to peer. Shared resources are reference-counted and thus are maintained until explicitly released or when the last context-sharing resource is released.\n\nNot every context can be shared with every other context. Both contexts must share the same OpenGL profile. You must also ensure that both contexts share the same set of renderers. You meet these requirements by ensuring each context uses the same virtual screen list, using either of the following techniques:\n\nUse the same pixel format object to create all the rendering contexts that you want to share.\n\nCreate pixel format objects using attributes that narrow down the choice to a single display. This practice ensures that the virtual screen is identical for each pixel format object.\n\nFigure 7-3  Shared contexts and more than one drawable object\n\nSetting up shared rendering contexts is very straightforward. Each Apple-specific OpenGL API provides functions with an option to specify a context to share in its context creation routine:\n\nUse the share argument for the initWithFormat:shareContext: method of the NSOpenGLContext class. See Listing 7-7.\n\nUse the share parameter for the function CGLCreateContext. See Listing 7-8.\n\nListing 7-7 ensures the same virtual screen list by using the same pixel format object for each of the shared contexts.\n\nListing 7-7  Setting up an NSOpenGLContext object for sharing\n\n#import <Cocoa/Cocoa.h>\n\n\n+ (NSOpenGLPixelFormat*)defaultPixelFormat\n\n\n{\n\n\n NSOpenGLPixelFormatAttribute attributes [] = {\n\n\n                    NSOpenGLPFADoubleBuffer,\n\n\n                    (NSOpenGLPixelFormatAttribute)nil };\n\n\nreturn [(NSOpenGLPixelFormat *)[NSOpenGLPixelFormat alloc]\n\n\n                        initWithAttributes:attribs];\n\n\n}\n\n\n \n\n\n- (NSOpenGLContext*)openGLContextWithShareContext:(NSOpenGLContext*)context\n\n\n{\n\n\n     if (_openGLContext == NULL) {\n\n\n            _openGLContext = [[NSOpenGLContext alloc]\n\n\n                    initWithFormat:[[self class] defaultPixelFormat]\n\n\n                    shareContext:context];\n\n\n        [_openGLContext makeCurrentContext];\n\n\n        [self prepareOpenGL];\n\n\n            }\n\n\n    return _openGLContext;\n\n\n}\n\n\n \n\n\n- (void)prepareOpenGL\n\n\n{\n\n\n     // Your code here to initialize the OpenGL state\n\n\n}\n\nListing 7-8 ensures the same virtual screen list by using the same pixel format object for each of the shared contexts.\n\nListing 7-8  Setting up a CGL context for sharing\n\n#include <OpenGL/OpenGL.h>\n\n\n \n\n\nCGLPixelFormatAttribute attrib[] = {kCGLPFADoubleBuffer, 0};\n\n\nCGLPixelFormatObj pixelFormat = NULL;\n\n\nGlint numPixelFormats = 0;\n\n\nCGLContextObj cglContext1 = NULL;\n\n\nCGLContextObj cglContext2 = NULL;\n\n\nCGLChoosePixelFormat (attribs, &pixelFormat, &numPixelFormats);\n\n\nCGLCreateContext(pixelFormat, NULL, &cglContext1);\n\n\nCGLCreateContext(pixelFormat, cglContext1, &cglContext2);\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Introduction",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGLShaderBuilderUserGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40006476",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Shader Builder User Guide\nTable of Contents\nIntroduction\nGetting Started\nBuilding Shaders\nRevision History\nNext\nRetired Document\n\nImportant: The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nIntroduction\n\nImportant The information in this document is deprecated in Xcode 6. For Xcode 6 and later, read Instruments User Guide.\n\nOpenGL Shader Builder is a tool for developing and debugging programs for the graphics processing unit (GPU). It can help you visualize and preview shader objects without the complexity of surrounding code. Using it, you can:\n\nGet immediate feedback as you enter and modify GPU programs\n\nExplore the effect of changing texture parameters\n\nTrack down link and compile errors\n\nObserve the effect of making changes to symbol values\n\nDevelopers who are writing GPU programs will want to read this document to find out how to use OpenGL Shader Builder. You can use the shader builder with programs written with OpenGL Shading Language or with older-style ARB vertex and fragment programs. OpenGL Shader Builder also supports geometry shaders, a recent addition to the OpenGL specification.\n\nOrganization of This Document\n\nThis document is organized into the following chapters:\n\nGetting Started gives an overview of the user interface and the main features of OpenGL Shader Builder.\n\nBuilding Shaders provides step-by-step instructions for the most common tasks you can accomplish.\n\nSee Also\n\nYou may want to consult these documents as you develop shaders for the GPU:\n\nOpenGL Shading Language (PDF) provides an overview of shaders and a complete reference to the language.\n\nOpenGL Shading Language (GLSl) Quick Reference Guide (PDF) is a two-page list of symbols that includes cross-references to the full specification.\n\nThe following OpenGL specifications define the extensions that support GPU programs:\n\nGL_EXT_geometry_shader4 is for generating primitives.\n\nGL_ARB_fragment_program is for processing fragments.\n\nGL_ARB_vertex_program is for processing vertices.\n\nNext\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-03-09"
  },
  {
    "title": "Tuning Your OpenGL Application",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_performance/opengl_performance.html#//apple_ref/doc/uid/TP40001987-CH213-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nTuning Your OpenGL Application\n\nAfter you design and implement your application, it is important that you spend some time analyzing its performance. The key to performance tuning your OpenGL application is to successively refine the design and implementation of your application. You do this by alternating between measuring your application, identifying where the bottleneck is, and removing the bottleneck.\n\nIf you are unfamiliar with general performance issues on the Macintosh platform, you will want to read Performance Overview. Performance Overview contains general performance tips that are useful to all applications. It also describes most of the performance tools provided with OS X.\n\nNext, take a close look at Instruments. Instruments consolidates many measurement tools into a single comprehensive performance-tuning application.\n\nThere are two tools other than OpenGL Profiler that are specific for OpenGL development—OpenGL Driver Monitor and OpenGL Shader Builder. OpenGL Driver Monitor collects real-time data from the hardware. OpenGL Shader Builder provides immediate feedback on vertex and fragment programs that you write.\n\nFor more information on these tools, see:\n\nOpenGL for OS X\n\nInstruments User Guide\n\nReal world profiling with the OpenGL Profiler\n\nOpenGL Driver Monitor User Guide\n\nOpenGL Shader Builder User Guide\n\nThe following books contain many techniques for getting the most performance from the GPU:\n\nGPU Gems: Programming Techniques, Tips and Tricks for Real Time Graphics, Randima Fernando. In particular, Graphics Pipeline Performance is a critical article for understanding how to find the bottlenecks in your OpenGL application.\n\nGPU Gems 2: Programming Techniques for High-Performance Graphics and General-Purpose Computation, Matt Pharr and Randima Fernando.\n\nGathering and Analyzing Baseline Performance Data\n\nAnalyzing performance is a systematic process that starts with gathering baseline data. OS X provides several applications that you can use to assess baseline performance for an OpenGL application:\n\ntop is a command-line utility that you run in the Terminal window. You can use top to assess how much CPU time your application consumes.\n\nOpenGL Profiler is an application that determines how much time an application spends in OpenGL. It also provides function traces that you can use to look for redundant calls.\n\nOpenGL Driver Monitor lets you gather real-time data on the operation of the GPU and lets you look at information (OpenGL extensions supported, buffer modes, sample modes, and so forth) for the available renderers.\n\nThis section shows how to use top along with OpenGL Profiler to analyze where to spend your optimization efforts—in your OpenGL code, your other application code, or in both. You'll see how to gather baseline data and how to determine the relationship of OpenGL performance to overall application performance.\n\nLaunch your OpenGL application.\n\nOpen a Terminal window and place it side-by-side with your application window.\n\nIn the Terminal window, type top and press Return. You'll see output similar to that shown in Figure 15-1.\n\nThe top program indicates the amount of CPU time that an application uses. The CPU time serves as a good baseline value for gauging how much tuning your code needs. Figure 15-1 shows the percentage of CPU time for the OpenGL application GLCarbon1C (highlighted). Note this application utilizes 31.5% of CPU resources.\n\nFigure 15-1  Output produced by the top application\n\nOpen the OpenGL Profiler application, located in /Developer/Applications/Graphics Tools/. In the window that appears, select the options to collect a trace and include backtraces, as shown in Figure 15-2.\n\nFigure 15-2  The OpenGL Profiler window\n\nSelect the option “Attach to application”, then select your application from the Application list.\n\nYou may see small pauses or stutters in the application, particularly when OpenGL Profiler is collecting a function trace. This is normal and does not significantly affect the performance statistics. The glitches are due to the large amount of data that OpenGL Profiler is writing out.\n\nClick Suspend to stop data collection.\n\nOpen the Statistics and Trace windows by choosing them from the Views menu.\n\nFigure 15-3 provides an example of what the Statistics window looks like. Figure 15-4 shows a Trace window.\n\nThe estimated percentage of time spent in OpenGL is shown at the bottom of Figure 15-3. Note that for this example, it is 28.91%. The higher this number, the more time the application is spending in OpenGL and the more opportunity there may be to improve application performance by optimizing OpenGL code.\n\nYou can use the amount of time spent in OpenGL along with the CPU time to calculate a ratio of the application time versus OpenGL time. This ratio indicates where to spend most of your optimization efforts.\n\nFigure 15-3  A statistics window\n\nIn the Trace window, look for duplicate function calls and redundant or unnecessary state changes.\n\nLook for back-to-back function calls with the same or similar data. These are areas that can typically be optimized. Functions that are called more than necessary include glTexParameter, glPixelStore, glEnable, and glDisable. For most applications, these functions can be called once from a setup or state modification routine and called only when necessary.\n\nIt's generally good practice to keep state changes out of rendering loops (which can be seen in the function trace as the same sequence of state changes and drawing over and over again) as much as possible and use separate routines to adjust state as necessary.\n\nLook at the time value to the left of each function call to determine the cost of the call.\n\nFigure 15-4  A Trace window\n\nDetermine what the performance gain would be if it were possible to reduce the time to execute all OpenGL calls to zero.\n\nFor example, take the performance data from the GLCarbon1C application used in this section to determine the performance attributable to the OpenGL calls.\n\nTotal Application Time (from top) = 31.5%\n\nTotal Time in OpenGL (from OpenGL Profiler) = 28.91%\n\nAt first glance, you might think that optimizing the OpenGL code could improve application performance by almost 29%, thus reducing the total application time by 29%. This isn't the case. Calculate the theoretical performance increase by multiplying the total CPU time by the percentage of time spent in OpenGL. The theoretical performance improvement for this example is:\n\n31.5 X .2891 = 9.11%\n\nIf OpenGL took no time at all to execute, the application would see a 9.11% increase in performance. So, if the application runs at 60 frames per second (FPS), it would perform as follows:\n\nNew FPS = previous FPS * (1 +(% performance increase)) = 60 fps *(1.0911) = 65.47 fps\n\nThe application gains almost 5.5 frames per second by reducing OpenGL from 28.91% to 0%. This shows that the relationship of OpenGL performance to application performance is not linear. Simply reducing the amount of time spent in OpenGL may or may not offer any noticeable benefit in application performance.\n\nUsing OpenGL Driver Monitor to Measure Stalls\n\nYou can use OpenGL Driver Monitor to measure how long the CPU waits for the GPU, as shown in Figure 15-5. OpenGL Driver Monitor is useful for analyzing other parameters as well. You can choose which parameters to monitor simply by clicking a parameter name from the drawer shown in the figure.\n\nFigure 15-5  The graph view in OpenGL Driver Monitor\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Updating an Application to Support the OpenGL 3.2 Core Specification",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/UpdatinganApplicationtoSupportOpenGL3/UpdatinganApplicationtoSupportOpenGL3.html#//apple_ref/doc/uid/TP40001987-CH3-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nUpdating an Application to Support the OpenGL 3.2 Core Specification\n\nThe OpenGL 3.0 specification deprecated many areas of functionality defined in earlier versions of the OpenGL specification. The OpenGL 3.2 Core profile explicitly removes these deprecated features and adjusts other parts of the specification to provide a streamlined, clean programming interface to OpenGL. Use this chapter to assist you in migrating your application away from this deprecated functionality.\n\nRemoved Functionality\n\nThe features that were removed from OpenGL are described in in Appendix E of the OpenGL 3.2 Core specification, and you should use that as the definitive guide for the changes you need to make in your application. Here is a summary of most significant areas that changed:\n\nIf your application uses the fixed-function pipeline, it must be rewritten to use shaders instead.\n\nIf your application uses shaders, you must rewrite your shaders to use OpenGL Shading Language 1.5; many built-in shader variables provided in earlier versions of the OpenGL Shading Language were explicitly removed from the OpenGL Shading Language 1.5 specification. Similarly, your application may no longer provide vertex data using the fixed-function routines; all vertex attributes are now specified as generic vertex attributes.\n\nYour application must explicitly generate object names using the OpenGL API.\n\nVertex data must be provided to OpenGL using buffer objects.\n\nThe built-in matrix stack functionality from earlier versions of OpenGL has been removed; you must recreate this functionality using shader inputs.\n\nSupport for auxiliary and accumulation buffers has been removed; use framebuffer objects instead.\n\nYour application no longer fetches the list of extensions as a single string. Instead, you first fetch the number of extensions and then separately fetch each extension string.\n\nExtension Changes on OS X\n\nOpenGL 3.2 provides functionality that earlier versions of OpenGL provided through extensions. Other extensions that were previously supported on OS X are no longer supported when your application uses the OpenGL 3.2 Core profile. Table B-1 lists extensions described elsewhere in this guide; use this table to determine whether the extension is supported, and if not, what equivalent functionality is supported.\n\nTable B-1  Extensions described in this guide\n\nExtension\n\n\t\n\nStatus\n\n\n\n\nAPPLE_fence\n\n\t\n\nObsolete. Use the ARB_Sync functionality provided by OpenGL 3.2 (Core).\n\n\n\n\nARB_vertex_buffer_object\n\n\t\n\nFunctionality provided by OpenGL 3.2 (Core).\n\n\n\n\nAPPLE_vertex_array_object\n\n\t\n\nObsolete. Use the ARB_vertex_array_object functionality provided by OpenGL 3.2 (Core).\n\n\n\n\nAPPLE_vertex_array_range\n\n\t\n\nObsolete. Use the ARB_map_buffer_range functionality provided by OpenGL 3.2 (Core).\n\n\n\n\nAPPLE_flush_buffer_range\n\n\t\n\nObsolete. Use the ARB_map_buffer_range functionality provided by OpenGL 3.2 (Core).\n\n\n\n\nAPPLE_client_storage\n\n\t\n\nSupported.\n\n\n\n\nAPPLE_texture_range\n\n\t\n\nSupported.\n\n\n\n\nARB_texture_rectangle\n\n\t\n\nFunctionality provided by OpenGL 3.2 (Core).\n\n\n\n\nARB_shader_objects\n\n\t\n\nFunctionality provided by OpenGL 3.2 (Core).\n\n\n\n\nARB_vertex_shader\n\n\t\n\nFunctionality provided by OpenGL 3.2 (Core).\n\n\n\n\nARB_fragment_shader\n\n\t\n\nFunctionality provided by OpenGL 3.2 (Core).\n\n\n\n\nEXT_transform_feedback\n\n\t\n\nFunctionality provided by OpenGL 3.2 (Core).\n\n\n\n\nEXT_gpu_shader4\n\n\t\n\nObsolete. Functionality included in GLSL 1.5\n\n\n\n\nEXT_geometry_shader4\n\n\t\n\nFunctionality provided by OpenGL 3.2 (Core).\n\n\n\n\nEXT_bindable_uniform\n\n\t\n\nObsolete. Use the ARB_uniform_buffer_object functionality provided by OpenGL 3.2 (Core).\n\n\n\n\nARB_pixel_buffer_object\n\n\t\n\nFunctionality provided by OpenGL 3.2 (Core).\n\n\n\n\nEXT_framebuffer_object\n\n\t\n\nObsolete. Use the ARB_framebuffer_object functionality provided by OpenGL 3.2 (Core).\n\n\n\n\nAPPLE_pixel_buffer\n\n\t\n\nObsolete. Use framebuffer objects instead.\n\n\n\n\nNV_multisample_filter_hint\n\n\t\n\nObsolete. Use multisampled renderbuffers to precisely control multisampling.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Best Practices for Working with Texture Data",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_texturedata/opengl_texturedata.html#//apple_ref/doc/uid/TP40001987-CH407-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nBest Practices for Working with Texture Data\n\nTextures add realism to OpenGL objects. They help objects defined by vertex data take on the material properties of real-world objects, such as wood, brick, metal, and fur. Texture data can originate from many sources, including images.\n\nMany of the same techniques your application uses on vertex data can also be used to improve texture performance.\n\nFigure 11-1  Textures add realism to a scene\n\nTextures start as pixel data that flows through an OpenGL program, as shown in Figure 11-2.\n\nFigure 11-2  Texture data path\n\nThe precise route that texture data takes from your application to its final destination can impact the performance of your application. The purpose of this chapter is to provide techniques you can use to ensure optimal processing of texture data in your application. This chapter\n\nshows how to use OpenGL extensions to optimize performance\n\nlists optimal data formats and types\n\nprovides information on working with textures whose dimensions are not a power of two\n\ndescribes creating textures from image data\n\nshows how to download textures\n\ndiscusses using double buffers for texture data\n\nUsing Extensions to Improve Texture Performance\n\nWithout any optimizations, texture data flows through an OpenGL program as shown in Figure 11-3. Data from your application first goes to the OpenGL framework, which may make a copy of the data before handing it to the driver. If your data is not in a native format for the hardware (see Optimal Data Formats and Types), the driver may also make a copy of the data to convert it to a hardware-specific format for uploading to video memory. Video memory, in turn, can keep a copy of the data. Theoretically, there could be four copies of your texture data throughout the system.\n\nFigure 11-3  Data copies in an OpenGL program\n\nData flows at different rates through the system, as shown by the size of the arrows in Figure 11-3. The fastest data transfer happens between VRAM and the GPU. The slowest transfer occurs between the OpenGL driver and VRAM. Data moves between the application and the OpenGL framework, and between the framework and the driver at the same \"medium\" rate. Eliminating any of the data transfers, but the slowest one in particular, will improve application performance.\n\nThere are several extensions you can use to eliminate one or more data copies and control how texture data travels from your application to the GPU:\n\nGL_ARB_pixel_buffer_object allows your application to use OpenGL buffer objects to manage texture and image data. As with vertex buffer objects, they allow your application to hint how a buffer is used and to decide when data is copied to OpenGL.\n\nGL_APPLE_client_storage allows you to prevent OpenGL from copying your texture data into the client. Instead, OpenGL keeps the memory pointer you provided when creating the texture. Your application must keep the texture data at that location until the referencing OpenGL texture is deleted.\n\nGL_APPLE_texture_range, along with a storage hint, either GL_STORAGE_CACHED_APPLE or GL_STORAGE_SHARED_APPLE, allows you to specify a single block of texture memory and manage it as you see fit.\n\nGL_ARB_texture_rectangle provides support for non-power of-two textures.\n\nHere are some recommendations:\n\nIf your application requires optimal texture upload performance, use GL_APPLE_client_storage and GL_APPLE_texture_range together to manage your textures.\n\nIf your application requires optimal texture download performance, use pixel buffer objects.\n\nIf your application requires cross-platform techniques, use pixel buffer objects for both texture uploads and texture downloads.\n\nUse GL_ARB_texture_rectangle when your source images are not aligned to a power-of-2 size.\n\nThe sections that follow describe the extensions and show how to use them.\n\nPixel Buffer Objects\n\nPixel buffer objects are a core feature of OpenGL 2.1 and also available through the GL_ARB_pixel_buffer_object extension. The procedure for setting up a pixel buffer object is almost identical to that of vertex buffer objects.\n\nUsing Pixel Buffer Objects to Efficiently Load Textures\n\nCall the function glGenBuffers to create a new name for a buffer object.\n\nvoid glGenBuffers(sizei n, uint *buffers );\n\nn is the number of buffers you wish to create identifiers for.\n\nbuffers specifies a pointer to memory to store the buffer names.\n\nCall the function glBindBuffer to bind an unused name to a buffer object. After this call, the newly created buffer object is initialized with a memory buffer of size zero and a default state. (For the default setting, see the OpenGL specification for ARB_vertex_buffer_object.)\n\nvoid glBindBuffer(GLenum target, GLuint buffer);\n\ntarget should be be set to GL_PIXEL_UNPACK_BUFFER to use the buffer as the source of pixel data.\n\nbuffer specifies the unique name for the buffer object.\n\nCreate and initialize the data store of the buffer object by calling the function glBufferData. Essentially, this call uploads your data to the GPU.\n\nvoid glBufferData(GLenum target, sizeiptr size,\n\n\n            const GLvoid *data, GLenum usage);\n\ntarget must be set to GL_PIXEL_UNPACK_BUFFER.\n\nsize specifies the size of the data store.\n\n*data points to the source data. If this is not NULL, the source data is copied to the data store of the buffer object. If NULL, the contents of the data store are undefined.\n\nusage is a constant that provides a hint as to how your application plans to use the data store. For more details on buffer hints, see Buffer Usage Hints\n\nWhenever you call glDrawPixels, glTexSubImage or similar functions that read pixel data from the application, those functions use the data in the bound pixel buffer object instead.\n\nTo update the data in the buffer object, your application calls glMapBuffer. Mapping the buffer prevents the GPU from operating on the data, and gives your application a pointer to memory it can use to update the buffer.\n\nvoid *glMapBuffer(GLenum target, GLenum access);\n\ntarget must be set to PIXEL_UNPACK_BUFFER.\n\naccess indicates the operations you plan to perform on the data. You can supply READ_ONLY, WRITE_ONLY, or READ_WRITE.\n\nModify the texture data using the pointer provided by map buffer.\n\nWhen you have finished modifying the texture, call the function glUnmapBuffer. You should supplyPIXEL_UNPACK_BUFFER. Once the buffer is unmapped, your application can no longer access the buffer’s data through the pointer, and the buffer’s contents are uploaded again to the GPU.\n\nUsing Pixel Buffer Objects for Asynchronous Pixel Transfers\n\nglReadPixels normally blocks until previous commands have completed, which includes the slow process of copying the pixel data to the application. However, if you call glReadPixels while a pixel buffer object is bound, the function returns immediately. It does not block until you actually map the pixel buffer object to read its content.\n\nCall the function glGenBuffers to create a new name for a buffer object.\n\nvoid glGenBuffers(sizei n, uint *buffers );\n\nn is the number of buffers you wish to create identifiers for.\n\nbuffers specifies a pointer to memory to store the buffer names.\n\nCall the function glBindBuffer to bind an unused name to a buffer object. After this call, the newly created buffer object is initialized with a memory buffer of size zero and a default state. (For the default setting, see the OpenGL specification for ARB_vertex_buffer_object.)\n\nvoid glBindBuffer(GLenum target, GLuint buffer);\n\ntarget should be be set to GL_PIXEL_PACK_BUFFER to use the buffer as the destination for pixel data.\n\nbuffer specifies the unique name for the buffer object.\n\nCreate and initialize the data store of the buffer object by calling the function glBufferData.\n\nvoid glBufferData(GLenum target, sizeiptr size,\n\n\n            const GLvoid *data, GLenum usage);\n\ntarget must be set to GL_PIXEL_PACK_BUFFER.\n\nsize specifies the size of the data store.\n\n*data points to the source data. If this is not NULL, the source data is copied to the data store of the buffer object. If NULL, the contents of the data store are undefined.\n\nusage is a constant that provides a hint as to how your application plans to use the data store. For more details on buffer hints, see Buffer Usage Hints\n\nCall glReadPixels or a similar function. The function inserts a command to read the pixel data into the bound pixel buffer object and then returns.\n\nTo take advantage of asynchronous pixel reads, your application should perform other work.\n\nTo retrieve the data in the pixel buffer object, your application calls glMapBuffer. This blocks OpenGL until the previously queued glReadPixels command completes, maps the data, and provides a pointer to your application.\n\nvoid *glMapBuffer(GLenum target, GLenum access);\n\ntarget must be set to GL_PIXEL_PACK_BUFFER.\n\naccess indicates the operations you plan to perform on the data. You can supply READ_ONLY, WRITE_ONLY, or READ_WRITE.\n\nWrite vertex data to the pointer provided by map buffer.\n\nWhen you no longer need the vertex data, call the function glUnmapBuffer. You should supply GL_PIXEL_PACK_BUFFER. Once the buffer is unmapped, the data is no longer accessible to your application.\n\nUsing Pixel Buffer Objects to Keep Data on the GPU\n\nThere is no difference between a vertex buffer object and a pixel buffer object except for the target to which they are bound. An application can take the results in one buffer and use them as another buffer type. For example, you could use the pixel results from a fragment shader and reinterpret them as vertex data in a future pass, without ever leaving the GPU:\n\nSet up your first pass and submit your drawing commands.\n\nBind a pixel buffer object and call glReadPixels to fetch the intermediate results into a buffer.\n\nBind the same buffer as a vertex buffer.\n\nSet up the second pass of your algorithm and submit your drawing commands.\n\nKeeping your intermediate data inside the GPU when performing multiple passes can result in great performance increases.\n\nApple Client Storage\n\nThe Apple client storage extension (APPLE_client_storage) lets you provide OpenGL with a pointer to memory that your application allocates and maintains. OpenGL retains a pointer to your data but does not copy the data. Because OpenGL references your data, your application must retain its copy of the data until all referencing textures are deleted. By using this extension you can eliminate the OpenGL framework copy as shown in Figure 11-4. Note that a texture width must be a multiple of 32 bytes for OpenGL to bypass the copy operation from the application to the OpenGL framework.\n\nFigure 11-4  The client storage extension eliminates a data copy\n\nThe Apple client storage extension defines a pixel storage parameter, GL_UNPACK_CLIENT_STORAGE_APPLE, that you pass to the OpenGL function glPixelStorei to specify that your application retains storage for textures. The following code sets up client storage:\n\nglPixelStorei(GL_UNPACK_CLIENT_STORAGE_APPLE, GL_TRUE);\n\nFor detailed information, see the OpenGL specification for the Apple client storage extension.\n\nApple Texture Range and Rectangle Texture\n\nThe Apple texture range extension (APPLE_texture_range) lets you define a region of memory used for texture data. Typically you specify an address range that encompasses the storage for a set of textures. This allows the OpenGL driver to optimize memory usage by creating a single memory mapping for all of the textures. You can also provide a hint as to how the data should be stored: cached or shared. The cached hint specifies to cache texture data in video memory. This hint is recommended when you have textures that you plan to use multiple times or that use linear filtering. The shared hint indicates that data should be mapped into a region of memory that enables the GPU to access the texture data directly (via DMA) without the need to copy it. This hint is best when you are using large images only once, perform nearest-neighbor filtering, or need to scale down the size of an image.\n\nThe texture range extension defines the following routine for making a single memory mapping for all of the textures used by your application:\n\nvoid glTextureRangeAPPLE(GLenum target, GLsizei length, GLvoid *pointer);\n\ntarget is a valid texture target, such as GL_TEXTURE_2D.\n\nlength specifies the number of bytes in the address space referred to by the pointer parameter.\n\n*pointer points to the address space that your application provides for texture storage.\n\nYou provide the hint parameter and a parameter value to to the OpenGL function glTexParameteri. The possible values for the storage hint parameter (GL_TEXTURE_STORAGE_HINT_APPLE) are GL_STORAGE_CACHED_APPLE or GL_STORAGE_SHARED_APPLE.\n\nSome hardware requires texture dimensions to be a power-of-two before the hardware can upload the data using DMA. The rectangle texture extension (ARB_texture_rectangle) was introduced to allow texture targets for textures of any dimensions—that is, rectangle textures (GL_TEXTURE_RECTANGLE_ARB). You need to use the rectangle texture extension together with the Apple texture range extension to ensure OpenGL uses DMA to access your texture data. These extensions allow you to bypass the OpenGL driver, as shown in Figure 11-5.\n\nNote that OpenGL does not use DMA for a power-of-two texture target (GL_TEXTURE_2D). So, unlike the rectangular texture, the power-of-two texture will incur one additional copy and performance won't be quite as fast. The performance typically isn't an issue because games, which are the applications most likely to use power-of-two textures, load textures at the start of a game or level and don't upload textures in real time as often as applications that use rectangular textures, which usually play video or display images.\n\nThe next section has code examples that use the texture range and rectangle textures together with the Apple client storage extension.\n\nFigure 11-5  The texture range extension eliminates a data copy\n\nFor detailed information on these extensions, see the OpenGL specification for the Apple texture range extension and the OpenGL specification for the ARB texture rectangle extension.\n\nCombining Client Storage with Texture Ranges\n\nYou can use the Apple client storage extension along with the Apple texture range extension to streamline the texture data path in your application. When used together, OpenGL moves texture data directly into video memory, as shown in Figure 11-6. The GPU directly accesses your data (via DMA). The set up is slightly different for rectangular and power-of-two textures. The code examples in this section upload textures to the GPU. You can also use these extensions to download textures, see Downloading Texture Data.\n\nFigure 11-6  Combining extensions to eliminate data copies\n\nListing 11-1 shows how to use the extensions for a rectangular texture. After enabling the texture rectangle extension you need to bind the rectangular texture to a target. Next, set up the storage hint. Call glPixelStorei to set up the Apple client storage extension. Finally, call the function glTexImage2D with a with a rectangular texture target and a pointer to your texture data.\n\nNote: The texture rectangle extension limits what can be done with rectangular textures. To understand the limitations in detail, read the OpenGL extension for texture rectangles. See Working with Non–Power-of-Two Textures for an overview of the limitations and an alternative to using this extension.\n\nListing 11-1  Using texture extensions for a rectangular texture\n\nglEnable (GL_TEXTURE_RECTANGLE_ARB);\n\n\nglBindTexture(GL_TEXTURE_RECTANGLE_ARB, id);\n\n\nglTexParameteri(GL_TEXTURE_RECTANGLE_ARB,\n\n\n        GL_TEXTURE_STORAGE_HINT_APPLE,\n\n\n        GL_STORAGE_CACHED_APPLE);\n\n\nglPixelStorei(GL_UNPACK_CLIENT_STORAGE_APPLE, GL_TRUE);\n\n\nglTexImage2D(GL_TEXTURE_RECTANGLE_ARB,\n\n\n            0, GL_RGBA, sizex, sizey, 0, GL_BGRA,\n\n\n            GL_UNSIGNED_INT_8_8_8_8_REV,\n\n\n            myImagePtr);\n\nSetting up a power-of-two texture to use these extensions is similar to what's needed to set up a rectangular texture, as you can see by looking at Listing 11-2. The difference is that the GL_TEXTURE_2D texture target replaces the GL_TEXTURE_RECTANGLE_ARB texture target.\n\nListing 11-2  Using texture extensions for a power-of-two texture\n\nglBindTexture(GL_TEXTURE_2D, myTextureName);\n\n\n \n\n\nglTexParameteri(GL_TEXTURE_2D,\n\n\n        GL_TEXTURE_STORAGE_HINT_APPLE,\n\n\n        GL_STORAGE_CACHED_APPLE);\n\n\n \n\n\nglPixelStorei(GL_UNPACK_CLIENT_STORAGE_APPLE, GL_TRUE);\n\n\n \n\n\nglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA,\n\n\n            sizex, sizey, 0, GL_BGRA,\n\n\n            GL_UNSIGNED_INT_8_8_8_8_REV, myImagePtr);\nOptimal Data Formats and Types\n\nThe best format and data type combinations to use for texture data are:\n\nGL_BGRA, GL_UNSIGNED_INT_8_8_8_8_REV\n\nGL_BGRA, GL_UNSIGNED_SHORT_1_5_5_5_REV)\n\nGL_YCBCR_422_APPLE, GL_UNSIGNED_SHORT_8_8_REV_APPLE\n\nThe combination GL_RGBA and GL_UNSIGNED_BYTE needs to be swizzled by many cards when the data is loaded, so it's not recommended.\n\nWorking with Non–Power-of-Two Textures\n\nOpenGL is often used to process video and images, which typically have dimensions that are not a power-of-two. Until OpenGL 2.0, the texture rectangle extension (ARB_texture_rectangle) provided the only option for a rectangular texture target. This extension, however, imposes the following restrictions on rectangular textures:\n\nYou can't use mipmap filtering with them.\n\nYou can use only these wrap modes: GL_CLAMP, GL_CLAMP_TO_EDGE, and GL_CLAMP_TO_BORDER.\n\nThe texture cannot have a border.\n\nThe texture uses non-normalized texture coordinates. (See Figure 11-7.)\n\nOpenGL 2.0 adds another option for a rectangular texture target through the ARB_texture_non_power_of_two extension, which supports these textures without the limitations of the ARB_texture_rectangle extension. Before using it, you must check to make sure the functionality is available. You'll also want to consult the OpenGL specification for the non—power-of-two extension.\n\nFigure 11-7  Normalized and non-normalized coordinates\n\nIf your code runs on a system that does not support either the ARB_texture_rectangle or ARB_texture_non_power_of_two extensions you have these options for working with with rectangular images:\n\nUse the OpenGL function gluScaleImage to scale the image so that it fits in a rectangle whose dimensions are a power of two. The image undoes the scaling effect when you draw the image from the properly sized rectangle back into a polygon that has the correct aspect ratio for the image.\n\nNote: This option can result in the loss of some data. But if your application runs on hardware that doesn't support the ARB_texture_rectangle extension, you may need to use this option.\n\nSegment the image into power-of-two rectangles, as shown in Figure 11-8 by using one image buffer and different texture pointers. Notice how the sides and corners of the image shown in Figure 11-8 are segmented into increasingly smaller rectangles to ensure that every rectangle has dimensions that are a power of two. Special care may be needed at the borders between each segment to avoid filtering artifacts if the texture is scaled or rotated.\n\nFigure 11-8  An image segmented into power-of-two tiles\nCreating Textures from Image Data\n\nOpenGL on the Macintosh provides several options for creating high-quality textures from image data. OS X supports floating-point pixel values, multiple image file formats, and a variety of color spaces. You can import a floating-point image into a floating-point texture. Figure 11-9 shows an image used to texture a cube.\n\nFigure 11-9  Using an image as a texture for a cube\n\nFor Cocoa, you need to provide a bitmap representation. You can create an NSBitmapImageRep object from the contents of an NSView object. You can use the Image I/O framework (see CGImageSource Reference). This framework has support for many different file formats, floating-point data, and a variety of color spaces. Furthermore, it is easy to use. You can import image data as a texture simply by supplying a CFURL object that specifies the location of the texture. There is no need for you to convert the image to an intermediate integer RGB format.\n\nCreating a Texture from a Cocoa View\n\nYou can use the NSView class or a subclass of it for texturing in OpenGL. The process is to first store the image data from an NSView object in an NSBitmapImageRep object so that the image data is in a format that can be readily used as texture data by OpenGL. Then, after setting up the texture target, you supply the bitmap data to the OpenGL function glTexImage2D. Note that you must have a valid, current OpenGL context set up.\n\nNote: You can't create an OpenGL texture from image data that's provided by a view created from the following classes: NSProgressIndicator, NSMovieView, and NSOpenGLView. This is because these views do not use the window backing store, which is what the method initWithFocusedViewRect: reads from.\n\nListing 11-3 shows a routine that uses this process to create a texture from the contents of an NSView object. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 11-3  Building an OpenGL texture from an NSView object\n\n-(void)myTextureFromView:(NSView*)theView\n\n\n                textureName:(GLuint*)texName\n\n\n{\n\n\n    NSBitmapImageRep * bitmap =  [theView bitmapImageRepForCachingDisplayInRect:\n\n\n            [theView visibleRect]]; // 1\n\n\n    int samplesPerPixel = 0;\n\n\n \n\n\n    [theView cacheDisplayInRect:[theView visibleRect] toBitmapImageRep:bitmap]; // 2\n\n\n    samplesPerPixel = [bitmap samplesPerPixel]; \n// 3\n\n\n    glPixelStorei(GL_UNPACK_ROW_LENGTH, [bitmap bytesPerRow]/samplesPerPixel); \n// 4\n\n\n    glPixelStorei (GL_UNPACK_ALIGNMENT, 1); \n// 5\n\n\n    if (*texName == 0) \n// 6\n\n\n            glGenTextures (1, texName);\n\n\n    glBindTexture (GL_TEXTURE_RECTANGLE_ARB, *texName); \n// 7\n\n\n    glTexParameteri(GL_TEXTURE_RECTANGLE_ARB,\n\n\n                    GL_TEXTURE_MIN_FILTER, GL_LINEAR); \n// 8\n\n\n \n\n\n   if(![bitmap isPlanar] &&\n\n\n       (samplesPerPixel == 3 || samplesPerPixel == 4)) { \n// 9\n\n\n        glTexImage2D(GL_TEXTURE_RECTANGLE_ARB,\n\n\n                     0,\n\n\n                     samplesPerPixel == 4 ? GL_RGBA8 : GL_RGB8,\n\n\n                     [bitmap pixelsWide],\n\n\n                     [bitmap pixelsHigh],\n\n\n                     0,\n\n\n                     samplesPerPixel == 4 ? GL_RGBA : GL_RGB,\n\n\n                     GL_UNSIGNED_BYTE,\n\n\n                    [bitmap bitmapData]);\n\n\n    } else {\n\n\n       // Your code to report unsupported bitmap data\n\n\n    }\n\n\n}\n\nHere's what the code does:\n\nAllocates an NSBitmapImageRep object.\n\nInitializes the NSBitmapImageRep object with bitmap data from the current view.\n\nGets the number of samples per pixel.\n\nSets the appropriate unpacking row length for the bitmap.\n\nSets the byte-aligned unpacking that's needed for bitmaps that are 3 bytes per pixel.\n\nIf a texture object is not passed in, generates a new texture object.\n\nBinds the texture name to the texture target.\n\nSets filtering so that it does not use a mipmap, which would be redundant for the texture rectangle extension.\n\nChecks to see if the bitmap is nonplanar and is either a 24-bit RGB bitmap or a 32-bit RGBA bitmap. If so, retrieves the pixel data using the bitmapData method, passing it along with other appropriate parameters to the OpenGL function for specifying a 2D texture image.\n\nCreating a Texture from a Quartz Image Source\n\nQuartz images (CGImageRef data type) are defined in the Core Graphics framework (ApplicationServices/CoreGraphics.framework/CGImage.h) while the image source data type for reading image data and creating Quartz images from an image source is declared in the Image I/O framework (ApplicationServices/ImageIO.framework/CGImageSource.h). Quartz provides routines that read a wide variety of image data.\n\nTo use a Quartz image as a texture source, follow these steps:\n\nCreate a Quartz image source by supplying a CFURL object to the function CGImageSourceCreateWithURL.\n\nCreate a Quartz image by extracting an image from the image source, using the function CGImageSourceCreateImageAtIndex.\n\nExtract the image dimensions using the function CGImageGetWidth and CGImageGetHeight. You'll need these to calculate the storage required for the texture.\n\nAllocate storage for the texture.\n\nCreate a color space for the image data.\n\nCreate a Quartz bitmap graphics context for drawing. Make sure to set up the context for pre-multiplied alpha.\n\nDraw the image to the bitmap context.\n\nRelease the bitmap context.\n\nSet the pixel storage mode by calling the function glPixelStorei.\n\nCreate and bind the texture.\n\nSet up the appropriate texture parameters.\n\nCall glTexImage2D, supplying the image data.\n\nFree the image data.\n\nListing 11-4 shows a code fragment that performs these steps. Note that you must have a valid, current OpenGL context.\n\nListing 11-4  Using a Quartz image as a texture source\n\nCGImageSourceRef myImageSourceRef = CGImageSourceCreateWithURL(url, NULL);\n\n\nCGImageRef myImageRef = CGImageSourceCreateImageAtIndex (myImageSourceRef, 0, NULL);\n\n\nGLint myTextureName;\n\n\nsize_t width = CGImageGetWidth(myImageRef);\n\n\nsize_t height = CGImageGetHeight(myImageRef);\n\n\nCGRect rect = {{0, 0}, {width, height}};\n\n\nvoid * myData = calloc(width * 4, height);\n\n\nCGColorSpaceRef space = CGColorSpaceCreateDeviceRGB();\n\n\nCGContextRef myBitmapContext = CGBitmapContextCreate (myData,\n\n\n                        width, height, 8,\n\n\n                        width*4, space,\n\n\n                        kCGBitmapByteOrder32Host |\n\n\n                          kCGImageAlphaPremultipliedFirst);\n\n\nCGContextSetBlendMode(myBitmapContext, kCGBlendModeCopy);\n\n\nCGContextDrawImage(myBitmapContext, rect, myImageRef);\n\n\nCGContextRelease(myBitmapContext);\n\n\nglPixelStorei(GL_UNPACK_ROW_LENGTH, width);\n\n\nglPixelStorei(GL_UNPACK_ALIGNMENT, 1);\n\n\nglGenTextures(1, &myTextureName);\n\n\nglBindTexture(GL_TEXTURE_RECTANGLE_ARB, myTextureName);\n\n\nglTexParameteri(GL_TEXTURE_RECTANGLE_ARB,\n\n\n                    GL_TEXTURE_MIN_FILTER, GL_LINEAR);\n\n\nglTexImage2D(GL_TEXTURE_RECTANGLE_ARB, 0, GL_RGBA8, width, height,\n\n\n                    0, GL_BGRA_EXT, GL_UNSIGNED_INT_8_8_8_8_REV, myData);\n\n\nfree(myData);\n\nFor more information on using Quartz, see Quartz 2D Programming Guide, CGImage Reference, and CGImageSource Reference.\n\nGetting Decompressed Raw Pixel Data from a Source Image\n\nYou can use the Image I/O framework together with a Quartz data provider to obtain decompressed raw pixel data from a source image, as shown in Listing 11-5. You can then use the pixel data for your OpenGL texture. The data has the same format as the source image, so you need to make sure that you use a source image that has the layout you need.\n\nAlpha is not premultiplied for the pixel data obtained in Listing 11-5, but alpha is premultiplied for the pixel data you get when using the code described in Creating a Texture from a Cocoa View and Creating a Texture from a Quartz Image Source.\n\nListing 11-5  Getting pixel data from a source image\n\nCGImageSourceRef myImageSourceRef = CGImageSourceCreateWithURL(url, NULL);\n\n\nCGImageRef myImageRef = CGImageSourceCreateImageAtIndex (myImageSourceRef, 0, NULL);\n\n\nCFDataRef data = CGDataProviderCopyData(CGImageGetDataProvider(myImageRef));\n\n\nvoid *pixelData = CFDataGetBytePtr(data);\nDownloading Texture Data\n\nA texture download operation uses the same data path as an upload operation except that the data path is reversed. Downloading transfers texture data, using direct memory access (DMA), from VRAM into a texture that can then be accessed directly by your application. You can use the Apple client range, texture range, and texture rectangle extensions for downloading, just as you would for uploading.\n\nTo download texture data using the Apple client storage, texture range, and texture rectangle extensions:\n\nBind a texture name to a texture target.\n\nSet up the extensions\n\nCall the function glCopyTexSubImage2D to copy a texture subimage from the specified window coordinates. This call initiates an asynchronous DMA transfer to system memory the next time you call a flush routine. The CPU doesn't wait for this call to complete.\n\nCall the function glGetTexImage to transfer the texture into system memory. Note that the parameters must match the ones that you used to set up the texture when you called the function glTexImage2D. This call is the synchronization point; it waits until the transfer is finished.\n\nListing 11-6 shows a code fragment that downloads a rectangular texture that uses cached memory. Your application processes data between the glCopyTexSubImage2D and glGetTexImage calls. How much processing? Enough so that your application does not need to wait for the GPU.\n\nListing 11-6  Code that downloads texture data\n\nglBindTexture(GL_TEXTURE_RECTANGLE_ARB, myTextureName);\n\n\nglTexParameteri(GL_TEXTURE_RECTANGLE_ARB, GL_TEXTURE_STORAGE_HINT_APPLE,\n\n\n                GL_STORAGE_SHARED_APPLE);\n\n\nglPixelStorei(GL_UNPACK_CLIENT_STORAGE_APPLE, GL_TRUE);\n\n\nglTexImage2D(GL_TEXTURE_RECTANGLE_ARB, 0, GL_RGBA,\n\n\n                sizex, sizey, 0, GL_BGRA,\n\n\n                GL_UNSIGNED_INT_8_8_8_8_REV, myImagePtr);\n\n\n \n\n\nglCopyTexSubImage2D(GL_TEXTURE_RECTANGLE_ARB,\n\n\n                0, 0, 0, 0, 0, image_width, image_height);\n\n\nglFlush();\n\n\n// Do other work processing here, using a double or triple buffer\n\n\n \n\n\nglGetTexImage(GL_TEXTURE_RECTANGLE_ARB, 0, GL_BGRA,\n\n\n                GL_UNSIGNED_INT_8_8_8_8_REV, pixels);\n\nDouble Buffering Texture Data\n\nWhen you use any technique that allows the GPU to access your texture data directly, such as the texture range extension, it's possible for the GPU and CPU to access the data at the same time. To avoid such a collision, you must synchronize the GPU and the CPU. The simplest way is shown in Figure 11-10. Your application works on the data, flushes it to the GPU and waits until the GPU is finished before working on the data again.\n\nOne technique for ensuring that the GPU is finished executing commands before your application sends more data is to insert a token into the command stream and use that to determine when the CPU can touch the data again, as described in Use Fences for Finer-Grained Synchronization. Figure 11-10 uses the fence extension command glFinishObject to synchronize buffer updates for a stream of single-buffered texture data. Notice that when the CPU is processing texture data, the GPU is idle. Similarly, when the GPU is processing texture data, the CPU is idle. It's much more efficient for the GPU and CPU to work asynchronously than to work synchronously. Double buffering data is a technique that allows you to process data asynchronously, as shown in Figure 11-11.\n\nFigure 11-10  Single-buffered data\n\nTo double buffer data, you must supply two sets of data to work on. Note in Figure 11-11 that while the GPU is rendering one frame of data, the CPU processes the next. After the initial startup, neither processing unit is idle. Using the glFinishObject function provided by the fence extension ensures that buffer updating is synchronized.\n\nFigure 11-11  Double-buffered data\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Customizing the OpenGL Pipeline with Shaders",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_shaders/opengl_shaders.html#//apple_ref/doc/uid/TP40001987-CH1-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nCustomizing the OpenGL Pipeline with Shaders\n\nOpenGL 1.x used fixed functions to deliver a useful graphics pipeline to application developers. To configure the various stages of the pipeline shown in Figure 12-1, applications called OpenGL functions to tweak the calculations that were performed for each vertex and fragment. Complex algorithms required multiple rendering passes and dozens of function calls to configure the calculations that the programmer desired. Extensions offered new configuration options, but did not change the complex nature of OpenGL programming.\n\nFigure 12-1  OpenGL fixed-function pipeline\n\nStarting with OpenGL 2.0, some stages of the OpenGL pipeline can be replaced with shaders. A shader is a program written in a special shading language. This program is compiled by OpenGL and uploaded directly into the graphics hardware. Figure 12-2 shows where your applications can hook into the pipeline with shaders.\n\nFigure 12-2  OpenGL shader pipeline\n\nShaders offer a considerable number of advantages to your application:\n\nShaders give you precise control over the operations that are performed to render your images.\n\nShaders allow for algorithms to be written in a terse, expressive format. Rather than writing complex blocks of configuration calls to implement a mathematical operation, you write code that expresses the algorithm directly.\n\nOlder graphics processors implemented the fixed-function pipeline in hardware or microcode, but now graphics processors are general-purpose computing devices. The fixed function pipeline is itself implemented as a shader.\n\nShaders allow for longer and more complex algorithms to be implemented using a single rendering pass. Because you have extensive control over the pipeline, it is also easier to implement multipass algorithms without requiring the data to be read back from the GPU.\n\nYour application can switch between different shaders with a single function call. In contrast, configuring the fixed-function pipeline incurs significant function-call overhead.\n\nIf your application uses the fixed-function pipeline, a critical task is to replace those tasks with shaders.\n\nIf you are new to shaders, OpenGL Shading Language, by Randi J. Rost, is an excellent guide for those looking to learn more about writing shaders and integrating them into your application. The rest of this chapter provides some boilerplate code, briefly describe the extensions that implement shaders, and discusses tools that Apple provides to assist you in writing shaders.\n\nShader Basics\n\nOpenGL 2.0 offers vertex and fragment shaders, to take over the processing of those two stages of the graphics pipeline. These same capabilities are also offered by the ARB_shader_objects, ARB_vertex_shader and ARB_fragment_shaderextensions. Vertex shading is available on all hardware running OS X v10.5 or later. Fragment shading is available on all hardware running OS X v10.6 and the majority of hardware running OS X v10.5.\n\nCreating a shader program is an expensive operation compared to other OpenGL state changes. Listing 12-1 presents a typical strategy to load, compile, and verify a shader program.\n\nListing 12-1  Loading a Shader\n\n/** Initialization-time for shader **/\n            GLuint shader, prog;\n            GLchar *shaderText = “... shader text ...”;\n\n            // Create ID for shader\n           shader = glCreateShader(GL_VERTEX_SHADER);\n\n           // Define shader text\n           glShaderSource(shaderText);\n\n           // Compile shader\n           glCompileShader(shader);\n\n           // Associate shader with program\n           glAttachShader(prog, shader);\n\n          // Link program\n           glLinkProgram(prog);\n    \n           // Validate program\n           glValidateProgram(prog);\n\n           // Check the status of the compile/link\n           glGetProgramiv(prog, GL_INFO_LOG_LENGTH, &logLen);\n           if(logLen > 0)\n           {\n               // Show any errors as appropriate\n               glGetProgramInfoLog(prog, logLen, &logLen, log);\n               fprintf(stderr, “Prog Info Log: %s\\n”, log);\n       }\n\n     // Retrieve all uniform locations that are determined during link phase\n           for(i = 0; i < uniformCt; i++)\n           {\n               uniformLoc[i] = glGetUniformLocation(prog, uniformName);\n           }\n\n           // Retrieve all attrib locations that are determined during link phase\n           for(i = 0; i < attribCt; i++)\n           {\n               attribLoc[i] = glGetAttribLocation(prog, attribName);\n           }\n\n    /** Render stage for shaders **/\n    glUseProgram(prog);\n\nThis code loads the text source for a vertex shader, compiles it, and adds it to the program. A more complex example might also attach fragment and geometry shaders. The program is linked and validated for correctness. Finally, the program retrieves information about the inputs to the shader and stores then in its own arrays. When the application is ready to use the shader, it calls glUseProgram to make it the current shader.\n\nFor best performance, your application should create shaders when your application is initialized, and not inside the rendering loop. Inside your rendering loop, you can quickly switch in the appropriate shaders by calling glUseProgram. For best performance, use the vertex array object extension to also switch in the vertex pointers. See Vertex Array Object for more information.\n\nAdvanced Shading Extensions\n\nIn addition to the standard shader, some Macs offer additional shading extensions to reveal advanced hardware capabilities. Not all of these extensions are available on all hardware, so you need to assess whether the features of each extension are worth implementing in your application.\n\nTransform Feedback\n\nThe EXT_transform_feedback extension is available on all hardware running OS X v10.5 or later. With the feedback extension, you can capture the results of the vertex shader into a buffer object, which can be used as an input to future commands. This is similar to the pixel buffer object technique described in Using Pixel Buffer Objects to Keep Data on the GPU, but more directly captures the results you desire.\n\nGPU Shader 4\n\nThe EXT_gpu_shader4 extension extends the OpenGL shading language to offer new operations, including:\n\nFull integer support.\n\nBuilt-in shader variable to reference the current vertex.\n\nBuilt-in shader variable to reference the current primitive. This makes it easier to use a shader to use the same static vertex data to render multiple primitives, using a shader and uniform variables to customize each instance of that primitive.\n\nUnfiltered texture fetches using integer coordinates.\n\nQuerying the size of a texture within a shader.\n\nOffset texture lookups.\n\nExplicit gradient and LOD texture lookups.\n\nDepth Cubemaps.\n\nGeometry Shaders\n\nThe EXT_geometry_shader4 extension allows your create geometry shaders. A geometry shader accepts transformed vertices and can add or remove vertices before passing them down to the rasterizer. This allows the application to add or remove geometry based on the calculated values in the vertex. For example, given a triangle and its neighboring vertices, your application could emit additional vertices to better create a more accurate appearance of a curved surface.\n\nUniform Buffers\n\nThe EXT_bindable_uniform extension allows your application to allocate buffer objects and use them as the source for uniform data in your shaders. Instead of relying on a single block of uniform memory supplied by OpenGL, your application allocates buffer objects using the same API that it uses to implement vertex buffer objects (Vertex Buffers). Instead of making a function call for each uniform variable you want to change, you can swap all of the uniform data by binding to a different uniform buffer.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Best Practices for Working with Vertex Data",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_vertexdata/opengl_vertexdata.html#//apple_ref/doc/uid/TP40001987-CH406-SW3",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nBest Practices for Working with Vertex Data\n\nComplex shapes and detailed 3D models require large amounts of vertex data to describe them in OpenGL. Moving vertex data from your application to the graphics hardware incurs a performance cost that can be quite large depending on the size of the data set.\n\nFigure 10-1  Vertex data sets can be quite large\n\nApplications that use large vertex data sets can adopt one or more of the strategies described in OpenGL Application Design Strategies to optimize how vertex data is delivered to OpenGL.This chapter expands on those best practices with specific techniques for working with vertex data.\n\nUnderstand How Vertex Data Flows Through OpenGL\n\nUnderstanding how vertex data flows through OpenGL is important to choosing strategies for handling the data. Vertex data enters into the vertex stage, where it is processed by either the built-in fixed function vertex stage or a custom vertex.\n\nFigure 10-2  Vertex data path\n\nFigure 10-3 takes a closer look at the vertex data path when using immediate mode. Without any optimizations, your vertex data may be copied at various points in the data path. If your application uses immediate mode to each vertex separately, calls to OpenGL first modify the current vertex, which is copied into the command buffer whenever your application makes a glVertex* call. This is not only expensive in terms of copy operations, but also in function overhead to specify each vertex.\n\nFigure 10-3  Immediate mode requires a copy of the current vertex data\n\nThe OpenGL commands glDrawRangeElements, glDrawElements, and glDrawArrays render multiple geometric primitives from array data, using very few subroutine calls. Listing 10-1 shows a typical implementation. Your application creates a vertex structure that holds all the elements for each vertex. For each element , you enable a client array and provide a pointer and offset to OpenGL so that it knows how to find those elements.\n\nListing 10-1  Submitting vertex data using glDrawElements.\n\ntypedef struct _vertexStruct\n\n\n{\n\n\n    GLfloat position[2];\n\n\n    GLubyte color[4];\n\n\n} vertexStruct;\n\n\n \n\n\nvoid DrawGeometry()\n\n\n{\n\n\n    const vertexStruct vertices[] = {...};\n\n\n    const GLubyte indices[] = {...};\n\n\n \n\n\n    glEnableClientState(GL_VERTEX_ARRAY);\n\n\n    glVertexPointer(2, GL_FLOAT, sizeof(vertexStruct), &vertices[0].position);\n\n\n    glEnableClientState(GL_COLOR_ARRAY);\n\n\n    glColorPointer(4, GL_UNSIGNED_BYTE, sizeof(vertexStruct), &vertices[0].color);\n\n\n \n\n\n    glDrawElements(GL_TRIANGLE_STRIP, sizeof(indices)/sizeof(GLubyte), GL_UNSIGNED_BYTE, indices);\n\n\n}\n\nEach time you call glDrawElements, OpenGL must copy all of the vertex data into the command buffer, which is later copied to the hardware. The copy overhead is still expensive.\n\nTechniques for Handling Vertex Data\n\nAvoiding unnecessary copies of your vertex data is critical to application performance. This section summarizes common techniques for managing your vertex data using either built-in functionality or OpenGL extensions. Before using these techniques, you must ensure that the necessary functions are available to your application. See Detecting Functionality.\n\nAvoid the use of glBegin and glEnd to specify your vertex data. The function and copying overhead makes this path useful only for very small data sets. Also, applications written with glBegin and glEnd are not portable to OpenGL ES on iOS.\n\nMinimize data type conversions by supplying OpenGL data types for vertex data. Use GLfloat, GLshort, or GLubyte data types because most graphics processors handle these types natively. If you use some other type, then OpenGL may need to perform a costly data conversion.\n\nThe preferred way to manage your vertex data is with vertex buffer objects. Vertex buffer objects are buffers owned by OpenGL that hold your vertex information. These buffers allow OpenGL to place your vertex data into memory that is accessible to the graphics hardware. See Vertex Buffers for more information.\n\nIf vertex buffer objects are not available, your application can search for the GL_APPLE_vertex_array_range and APPLE_fence extensions. Vertex array ranges allow you to prevent OpenGL from copying your vertex data into the command buffer. Instead, your application must avoid modifying or deleting the vertex data until OpenGL finishes executing drawing commands. This solution requires more effort from the application, and is not compatible with other platforms, including iOS. See Vertex Array Range Extension for more information.\n\nComplex vertex operations require many array pointers to be enabled and set before you call glDrawElements. The GL_APPLE_vertex_array_object extension allows your application to consolidate a group of array pointers into a single object. Your application switches multiple pointers by binding a single vertex array object, reducing the overhead of changing state. See Vertex Array Object.\n\nUse double buffering to reduce resource contention between your application and OpenGL. See Use Double Buffering to Avoid Resource Conflicts.\n\nIf you need to compute new vertex information between frames, consider using vertex shaders and buffer objects to perform and store the calculations.\n\nVertex Buffers\n\nVertex buffers are available as a core feature starting in OpenGL 1.5, and on earlier versions of OpenGL through the vertex buffer object extension (GL_ARB_vertex_buffer_object). Vertex buffers are used to improve the throughput of static or dynamic vertex data in your application.\n\nA buffer object is a chunk of memory owned by OpenGL. Your application reads from or writes to the buffer using OpenGL calls such as glBufferData, glBufferSubData, and glGetBufferSubData. Your application can also gain a pointer to this memory, an operation referred to as mapping a buffer. OpenGL prevents your application and itself from simultaneously using the data stored in the buffer. When your application maps a buffer or attempts to modify it, OpenGL may block until previous drawing commands have completed.\n\nUsing Vertex Buffers\n\nYou can set up and use vertex buffers by following these steps:\n\nCall the function glGenBuffers to create a new name for a buffer object.\n\nvoid glGenBuffers(sizei n, uint *buffers );\n\nn is the number of buffers you wish to create identifiers for.\n\nbuffers specifies a pointer to memory to store the buffer names.\n\nCall the function glBindBuffer to bind an unused name to a buffer object. After this call, the newly created buffer object is initialized with a memory buffer of size zero and a default state. (For the default setting, see the OpenGL specification for ARB_vertex_buffer_object.)\n\nvoid glBindBuffer(GLenum target, GLuint buffer);\n\ntarget must be set to GL_ARRAY_BUFFER.\n\nbuffer specifies the unique name for the buffer object.\n\nFill the buffer object by calling the function glBufferData. Essentially, this call uploads your data to the GPU.\n\nvoid glBufferData(GLenum target, sizeiptr size,\n\n\n            const GLvoid *data, GLenum usage);\n\ntarget must be set to GL_ARRAY_BUFFER.\n\nsize specifies the size of the data store.\n\n*data points to the source data. If this is not NULL, the source data is copied to the data stored of the buffer object. If NULL, the contents of the data store are undefined.\n\nusage is a constant that provides a hint as to how your application plans to use the data stored in the buffer object. These examples use GL_STREAM_DRAW, which indicates that the application plans to both modify and draw using the buffer, and GL_STATIC_DRAW, which indicates that the application will define the data once but use it to draw many times. For more details on buffer hints, see Buffer Usage Hints\n\nEnable the vertex array by calling glEnableClientState and supplying the GL_VERTEX_ARRAY constant.\n\nPoint to the contents of the vertex buffer object by calling a function such as glVertexPointer. Instead of providing a pointer, you provide an offset into the vertex buffer object.\n\nTo update the data in the buffer object, your application calls glMapBuffer. Mapping the buffer prevents the GPU from operating on the data, and gives your application a pointer to memory it can use to update the buffer.\n\nvoid *glMapBuffer(GLenum target, GLenum access);\n\ntarget must be set to GL_ARRAY_BUFFER.\n\naccess indicates the operations you plan to perform on the data. You can supply READ_ONLY, WRITE_ONLY, or READ_WRITE.\n\nWrite pixel data to the pointer received from the call to glMapBuffer.\n\nWhen your application has finished modifying the buffer contents, call the function glUnmapBuffer. You must supply GL_ARRAY_BUFFER as the parameter to this function. Once the buffer is unmapped, the pointer is no longer valid, and the buffer’s contents are uploaded again to the GPU.\n\nListing 10-2 shows code that uses the vertex buffer object extension for dynamic data. This example overwrites all of the vertex data during every draw operation.\n\nListing 10-2  Using the vertex buffer object extension with dynamic data\n\n//  To set up the vertex buffer object extension\n\n\n#define BUFFER_OFFSET(i) ((char*)NULL + (i))\n\n\nglBindBuffer(GL_ARRAY_BUFFER, myBufferName);\n\n\n \n\n\nglEnableClientState(GL_VERTEX_ARRAY);\n\n\nglVertexPointer(3, GL_FLOAT, stride, BUFFER_OFFSET(0));\n\n\n \n\n\n//  When you want to draw using the vertex data\n\n\ndraw_loop {\n\n\n    glBufferData(GL_ARRAY_BUFFER, bufferSize, NULL, GL_STREAM_DRAW);\n\n\n    my_vertex_pointer = glMapBuffer(GL_ARRAY_BUFFER, GL_WRITE_ONLY);\n\n\n    GenerateMyDynamicVertexData(my_vertex_pointer);\n\n\n    glUnmapBuffer(GL_ARRAY_BUFFER);\n\n\n    PerformDrawing();\n\n\n}\n\nListing 10-3 shows code that uses the vertex buffer object extension with static data.\n\nListing 10-3   Using the vertex buffer object extension with static data\n\n//  To set up the vertex buffer object extension\n\n\n#define BUFFER_OFFSET(i) ((char*)NULL + (i))\n\n\nglBindBuffer(GL_ARRAY_BUFFER, myBufferName);\n\n\nglBufferData(GL_ARRAY_BUFFER, bufferSize, NULL, GL_STATIC_DRAW);\n\n\nGLvoid* my_vertex_pointer = glMapBuffer(GL_ARRAY_BUFFER, GL_WRITE_ONLY);\n\n\nGenerateMyStaticVertexData(my_vertex_pointer);\n\n\nglUnmapBuffer(GL_ARRAY_BUFFER);\n\n\n \n\n\nglEnableClientState(GL_VERTEX_ARRAY);\n\n\nglVertexPointer(3, GL_FLOAT, stride, BUFFER_OFFSET(0));\n\n\n \n\n\n//  When you want to draw using the vertex data\n\n\ndraw_loop {\n\n\n    PerformDrawing();\n\n\n}\nBuffer Usage Hints\n\nA key advantage of buffer objects is that the application can provide information on how it uses the data stored in each buffer. For example, Listing 10-2 and Listing 10-3 differentiated between cases where the data were expected to never change (GL_STATIC_DRAW) and cases where the buffer data might change (GL_DYNAMIC_DRAW). The usage parameter allows an OpenGL renderer to alter its strategy for allocating the vertex buffer to improve performance. For example, static buffers may be allocated directly in GPU memory, while dynamic buffers may be stored in main memory and retrieved by the GPU via DMA.\n\nIf OpenGL ES compatibility is useful to you, you should limit your usage hints to one of three usage cases:\n\nGL_STATIC_DRAW should be used for vertex data that is specified once and never changed. Your application should create these vertex buffers during initialization and use them repeatedly until your application shuts down.\n\nGL_DYNAMIC_DRAW should be used when the buffer is expected to change after it is created. Your application should still allocate these buffers during initialization and periodically update them by mapping the buffer.\n\nGL_STREAM_DRAW is used when your application needs to create transient geometry that is rendered and then discarded. This is most useful when your application must dynamically change vertex data every frame in a way that cannot be performed in a vertex shader. To use a stream vertex buffer, your application initially fills the buffer using glBufferData, then alternates between drawing using the buffer and modifying the buffer.\n\nOther usage constants are detailed in the vertex buffer specification.\n\nIf different elements in your vertex format have different usage characteristics, you may want to split the elements into one structure for each usage pattern and allocate a vertex buffer for each. Listing 10-4 shows how to implement this. In this example, position data is expected to be the same in each frame, while color data may be animated in every frame.\n\nListing 10-4  Geometry with different usage patterns\n\ntypedef struct _vertexStatic\n\n\n{\n\n\n    GLfloat position[2];\n\n\n} vertexStatic;\n\n\n \n\n\ntypedef struct _vertexDynamic\n\n\n{\n\n\n    GLubyte color[4];\n\n\n} vertexDynamic;\n\n\n \n\n\n// Separate buffers for static and dynamic data.\n\n\nGLuint    staticBuffer;\n\n\nGLuint    dynamicBuffer;\n\n\nGLuint    indexBuffer;\n\n\n \n\n\nconst vertexStatic staticVertexData[] = {...};\n\n\nvertexDynamic dynamicVertexData[] = {...};\n\n\nconst GLubyte indices[] = {...};\n\n\n \n\n\nvoid CreateBuffers()\n\n\n{\n\n\n    glGenBuffers(1, &staticBuffer);\n\n\n    glGenBuffers(1, &dynamicBuffer);\n\n\n    glGenBuffers(1, &indexBuffer);\n\n\n \n\n\n// Static position data\n\n\n    glBindBuffer(GL_ARRAY_BUFFER, staticBuffer);\n\n\n    glBufferData(GL_ARRAY_BUFFER, sizeof(staticVertexData), staticVertexData, GL_STATIC_DRAW);\n\n\n \n\n\n    glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, indexBuffer);\n\n\n    glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW);\n\n\n \n\n\n// Dynamic color data\n\n\n// While not shown here, the expectation is that the data in this buffer changes between frames.\n\n\n    glBindBuffer(GL_ARRAY_BUFFER, dynamicBuffer);\n\n\n    glBufferData(GL_ARRAY_BUFFER, sizeof(dynamicVertexData), dynamicVertexData, GL_DYNAMIC_DRAW);\n\n\n}\n\n\n \n\n\nvoid DrawUsingVertexBuffers()\n\n\n{\n\n\n    glBindBuffer(GL_ARRAY_BUFFER, staticBuffer);\n\n\n    glEnableClientState(GL_VERTEX_ARRAY);\n\n\n    glVertexPointer(2, GL_FLOAT, sizeof(vertexStatic), (void*)offsetof(vertexStatic,position));\n\n\n    glBindBuffer(GL_ARRAY_BUFFER, dynamicBuffer);\n\n\n    glEnableClientState(GL_COLOR_ARRAY);\n\n\n    glColorPointer(4, GL_UNSIGNED_BYTE, sizeof(vertexDynamic), (void*)offsetof(vertexDynamic,color));\n\n\n    glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, indexBuffer);\n\n\n    glDrawElements(GL_TRIANGLE_STRIP, sizeof(indices)/sizeof(GLubyte), GL_UNSIGNED_BYTE, (void*)0);\n\n\n}\nFlush Buffer Range Extension\n\nWhen your application unmaps a vertex buffer, the OpenGL implementation may copy the full contents of the buffer to the graphics hardware. If your application changes only a subset of a large buffer, this is inefficient. The APPLE_flush_buffer_range extension allows your application to tell OpenGL exactly which portions of the buffer were modified, allowing it to send only the changed data to the graphics hardware.\n\nTo use the flush buffer range extension, follow these steps:\n\nTurn on the flush buffer extension by calling glBufferParameteriAPPLE.\n\nglBufferParameteriAPPLE(GL_ARRAY_BUFFER,GL_BUFFER_FLUSHING_UNMAP_APPLE, GL_FALSE);\n\nThis disables the normal flushing behavior of OpenGL.\n\nBefore you unmap a buffer, you must call glFlushMappedBufferRangeAPPLE for each range of the buffer that was modified by the application.\n\nvoid glFlushMappedBufferRangeAPPLE(enum target, intptr offset, sizeiptr size);\n\ntarget is the type of buffer being modified; for vertex data it’s ARRAY_BUFFER.\n\noffset is the offset into the buffer for the modified data.\n\nsize is the length of the modified data in bytes.\n\nCall glUnmapBuffer. OpenGL unmaps the buffer, but it is required to update only the portions of the buffer your application explicitly marked as changed.\n\nFor more information see the APPLE_flush_buffer_range specification.\n\nVertex Array Range Extension\n\nThe vertex array range extension (APPLE_vertex_array_range) lets you define a region of memory for your vertex data. The OpenGL driver can optimize memory usage by creating a single memory mapping for your vertex data. You can also provide a hint as to how the data should be stored: cached or shared. The cached option specifies to cache vertex data in video memory. The shared option indicates that data should be mapped into a region of memory that allows the GPU to access the vertex data directly using DMA transfer. This option is best for dynamic data. If you use shared memory, you'll need to double buffer your data.\n\nYou can set up and use the vertex array range extension by following these steps:\n\nEnable the extension by calling glEnableClientState and supplying the GL_VERTEX_ARRAY_RANGE_APPLE constant.\n\nAllocate storage for the vertex data. You are responsible for maintaining storage for the data.\n\nDefine an array of vertex data by calling a function such as glVertexPointer. You need to supply a pointer to your data.\n\nOptionally set up a hint about handling the storage of the array data by calling the function glVertexArrayParameteriAPPLE.\n\nGLvoid glVertexArrayParameteriAPPLE(GLenum pname, GLint param);\n\npname must be VERTEX_ARRAY_STORAGE_HINT_APPLE.\n\nparam is a hint that specifies how your application expects to use the data. OpenGL uses this hint to optimize performance. You can supply either STORAGE_SHARED_APPLE or STORAGE_CACHED_APPLE. The default value is STORAGE_SHARED_APPLE, which indicates that the vertex data is dynamic and that OpenGL should use optimization and flushing techniques suitable for this kind of data. If you expect the supplied data to be static, use STORAGE_CACHED_APPLE so that OpenGL can optimize appropriately.\n\nCall the OpenGL function glVertexArrayRangeAPPLE to establish the data set.\n\nvoid glVertexArrayRangeAPPLE(GLsizei length, GLvoid *pointer);\n\nlength specifies the length of the vertex array range. The length is typically the number of unsigned bytes.\n\n*pointer points to the base of the vertex array range.\n\nDraw with the vertex data using standard OpenGL vertex array commands.\n\nIf you need to modify the vertex data, set a fence object after you’ve submitted all the drawing commands. See Use Fences for Finer-Grained Synchronization\n\nPerform other work so that the GPU has time to process the drawing commands that use the vertex array.\n\nCall glFinishFenceAPPLE to gain access to the vertex array.\n\nModify the data in the vertex array.\n\nCall glFlushVertexArrayRangeAPPLE.\n\nvoid glFlushVertexArrayRangeAPPLE(GLsizei length, GLvoid *pointer);\n\nlength specifies the length of the vertex array range, in bytes.\n\n*pointer points to the base of the vertex array range.\n\nFor dynamic data, each time you change the data, you need to maintain synchronicity by calling glFlushVertexArrayRangeAPPLE. You supply as parameters an array size and a pointer to an array, which can be a subset of the data, as long as it includes all of the data that changed. Contrary to the name of the function, glFlushVertexArrayRangeAPPLE doesn't actually flush data like the OpenGL function glFlush does. It simply makes OpenGL aware that the data has changed.\n\nListing 10-5 shows code that sets up and uses the vertex array range extension with dynamic data. It overwrites all of the vertex data during each iteration through the drawing loop. The call to the glFinishFenceAPPLE command guarantees that the CPU and the GPU don't access the data at the same time. Although this example calls the glFinishFenceAPPLE function almost immediately after setting the fence, in reality you need to separate these calls to allow parallel operation of the GPU and CPU. To see how that's done, read Use Double Buffering to Avoid Resource Conflicts.\n\nListing 10-5  Using the vertex array range extension with dynamic data\n\n//  To set up the vertex array range extension\n\n\nglVertexArrayParameteriAPPLE(GL_VERTEX_ARRAY_STORAGE_HINT_APPLE, GL_STORAGE_SHARED_APPLE);\n\n\nglVertexArrayRangeAPPLE(buffer_size, my_vertex_pointer);\n\n\nglEnableClientState(GL_VERTEX_ARRAY_RANGE_APPLE);\n\n\n \n\n\nglEnableClientState(GL_VERTEX_ARRAY);\n\n\nglVertexPointer(3, GL_FLOAT, 0, my_vertex_pointer);\n\n\nglSetFenceAPPLE(my_fence);\n\n\n \n\n\n//  When you want to draw using the vertex data\n\n\ndraw_loop {\n\n\n    glFinishFenceAPPLE(my_fence);\n\n\n    GenerateMyDynamicVertexData(my_vertex_pointer);\n\n\n    glFlushVertexArrayRangeAPPLE(buffer_size, my_vertex_pointer);\n\n\n    PerformDrawing();\n\n\n    glSetFenceAPPLE(my_fence);\n\n\n}\n\nListing 10-6 shows code that uses the vertex array range extension with static data. Unlike the setup for dynamic data, the setup for static data includes using the hint for cached data. Because the data is static, it's unnecessary to set a fence.\n\nListing 10-6  Using the vertex array range extension with static data\n\n//  To set up the vertex array range extension\n\n\nGenerateMyStaticVertexData(my_vertex_pointer);\n\n\nglVertexArrayParameteriAPPLE(GL_VERTEX_ARRAY_STORAGE_HINT_APPLE, GL_STORAGE_CACHED_APPLE);\n\n\nglVertexArrayRangeAPPLE(array_size, my_vertex_pointer);\n\n\nglEnableClientState(GL_VERTEX_ARRAY_RANGE_APPLE);\n\n\n \n\n\nglEnableClientState(GL_VERTEX_ARRAY);\n\n\nglVertexPointer(3, GL_FLOAT, stride, my_vertex_pointer);\n\n\n \n\n\n//  When you want to draw using the vertex data\n\n\ndraw_loop {\n\n\n    PerformDrawing();\n\n\n}\n\nFor detailed information on this extension, see the OpenGL specification for the vertex array range extension.\n\nVertex Array Object\n\nLook at the DrawUsingVertexBuffers function in Listing 10-4. It configures buffer pointers for position, color, and indexing before calling glDrawElements. A more complex vertex structure may require additional buffer pointers to be enabled and changed before you can finally draw your geometry. If your application swaps frequently between multiple configurations of elements, changing these parameters adds significant overhead to your application. The APPLE_vertex_array_object extension allows you to combine a collection of buffer pointers into a single OpenGL object, allowing you to change all the buffer pointers by binding a different vertex array object.\n\nTo use this extension, follow these steps during your application’s initialization routines:\n\nGenerate a vertex array object for a configuration of pointers you wish to use together.\n\nvoid glGenVertexArraysAPPLE(sizei n, const uint *arrays);\n\nn is the number of arrays you wish to create identifiers for.\n\narrays specifies a pointer to memory to store the array names.\n\nglGenVertexArraysAPPLE(1,&myArrayObject);\n\nBind the vertex array object you want to configure.\n\nvoid glBindVertexArrayAPPLE(uint array);\n\narray is the identifier for an array that you received from glGenVertexArraysAPPLE.\n\nglBindVertexArrayAPPLE(myArrayObject);\n\nCall the pointer routines (glColorPointer and so forth.) that you would normally call inside your rendering loop. When a vertex array object is bound, these calls change the currently bound vertex array object instead of the default OpenGL state.\n\n    glBindBuffer(GL_ARRAY_BUFFER, staticBuffer);\n\n\n    glEnableClientState(GL_VERTEX_ARRAY);\n\n\n    glVertexPointer(2, GL_FLOAT, sizeof(vertexStatic), (void*)offsetof(vertexStatic,position));\n\n\n...\n\nRepeat the previous steps for each configuration of vertex pointers.\n\nInside your rendering loop, replace the calls to configure the array pointers with a call to bind the vertex array object.\n\nglBindVertexArrayAPPLE(myArrayObject);\n\n\nglDrawArrays(...);\n\nIf you need to get back to the default OpenGL behavior, call glBindVertexArrayAPPLE and pass in 0.\n\nglBindVertexArrayAPPLE(0);\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Techniques for Scene Antialiasing",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_fsaa/opengl_fsaa.html#//apple_ref/doc/uid/TP40001987-CH405-SW7",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nTechniques for Scene Antialiasing\n\nAliasing is the bane of the digital domain. In the early days of the personal computer, jagged edges and blocky graphics were accepted by the user simply because not much could be done to correct them. Now with faster hardware and higher-resolution displays, there are several antialiasing techniques that can smooth edges to achieve a more realistic scene.\n\nOpenGL supports antialiasing that operates at the level of lines and polygons as well as at the level of the full scene. This chapter discusses techniques for full scene antialiasing (FSAA). If your application needs point or line antialiasing instead of full scene antialiasing, use the built in OpenGL point and line antialiasing functions. These are described in Section 3.4.2 in the OpenGL Specification.\n\nThe three antialiasing techniques in use today are multisampling, supersampling, and alpha channel blending:\n\nMultisampling defines a technique for sampling pixel content at multiple locations for each pixel. This is a good technique to use if you want to smooth polygon edges.\n\nSupersampling renders at a much higher resolution than what's needed for the display. Prior to drawing the content to the display, OpenGL scales and filters the content to the appropriate resolution. This is a good technique to use when you want to smooth texture interiors in addition to polygon edges.\n\nAlpha channel blending uses the alpha value of a fragment to control how to blend the fragment with the pixel values that are already in the framebuffer. It's a good technique to use when you want to ensure that foreground and background images are composited smoothly.\n\nThe ARB_multisample extension defines a specification for full scene antialiasing. It describes multisampling and alpha channel sampling. The specification does not specifically mention supersampling but its wording doesn't preclude supersampling. The antialiasing methods that are available depend on the hardware and the actual implementation depends on the vendor. Some graphics cards support antialiasing using a mixture of multisampling and supersampling. The methodology used to select the samples can vary as well. Your best approach is to query the renderer to find out exactly what is supported. OpenGL lets you provide a hint to the renderer as to which antialiasing technique you prefer. Hints are available as renderer attributes that you supply when you create a pixel format object.\n\nA smaller subset of renderers support the EXT_framebuffer_blit and EXT_framebuffer_multisample extensions. These extensions allow your application to create multisampled offscreen frame buffer objects, render detailed scenes to them, with precise control over when the multisampled renderbuffer is resolved to a single displayable color per pixel.\n\nGuidelines\n\nKeep the following in mind when you set up full scene antialiasing:\n\nAlthough a system may have enough VRAM to accommodate a multisample buffer, a large buffer can affect the ability of OpenGL to maintain a properly working texture set. Keep in mind that the buffers associated with the rendering context—depth and stencil—increase in size by a factor equal to number of samples per pixel.\n\nThe OpenGL driver allocates the memory needed for the multisample buffer; your application should not allocate this memory.\n\nAny antialiasing algorithm that operates on the full scene requires additional computing resources. There is a tradeoff between performance and quality. For that reason, you may want to provide a user interface that allows the user to enable and disable FSAA, or to choose the level of quality for antialiasing.\n\nThe commands glEnable(GL_MULTISAMPLE) and glDisable(GL_MULTISAMPLE) are ignored on some hardware because some graphics cards have the feature enabled all the time. That doesn't mean that you should not call these commands because you'll certainly need them on hardware that doesn't ignore them.\n\nA hint as to the variant of sampling you want is a suggestion, not a command. Not all hardware supports all types of antialiasing. Other hardware mixes multisampling with supersampling techniques. The driver dictates the type of antialiasing that's actually used in your application.\n\nThe best way to find out which sample modes are supported is to call the CGL function CGLDescribeRenderer with the renderer property kCGLRPSampleModes or kCGLRPSampleAlpha. You can also determine how many samples the renderer supports by calling CGLDescribeRenderer with the renderer property kCGLRPMaxSamples.\n\nGeneral Approach\n\nThe general approach to setting up full scene antialiasing is as follows:\n\nCheck to see what's supported. Not all renderers support the ARB multisample extension, so you need to check for this functionality (see Detecting Functionality).\n\nTo find out what type of antialiasing a specific renderer supports, call the function CGLDescribeRenderer. Supply the renderer property kCGLRPSampleModes to find out whether the renderer supports multisampling and supersampling. Supply kCGLRPSampleAlpha to see whether the renderer supports alpha sampling.\n\nYou can choose to exclude unsupported hardware from the pixel format search by specifying only the hardware that supports multisample antialiasing. Keep in mind that if you exclude unsupported hardware, the unsupported displays will not render anything. If you include unsupported hardware, OpenGL uses normal aliased rendering to the unsupported displays and multisampled rendering to supported displays.\n\nInclude these buffer attributes in the attributes array:\n\nThe appropriate sample buffer attribute constant (NSOpenGLPFASampleBuffers or kCGLPFASampleBuffers) along with the number of multisample buffers. At this time the specification allows only one multisample buffer.\n\nThe appropriate samples constant (NSOpenGLPFASamples or kCGLPFASamples) along with the number of samples per pixel. You can supply 2, 4, 6, or more depending on what the renderer supports and the amount of VRAM available. The value that you supply affects the quality, memory use, and speed of the multisampling operation. For fastest performance, and to use the least amount of video memory, specify 2 samples. When you need more quality, specify 4 or more.\n\nThe no recovery attribute ( NSOpenGLPFANoRecovery or kCGLPFANoRecovery). Although enabling this attribute is not mandatory, it's recommended to prevent OpenGL from using software fallback as a renderer. Multisampled antialiasing performance is slow in the software renderer.\n\nOptionally provide a hint for the type of antialiasing you want—multisampling, supersampling, or alpha sampling. See Hinting for a Specific Antialiasing Technique.\n\nEnable multisampling with the following command:\n\nglEnable(GL_MULTISAMPLE);\n\nRegardless of the enabled state, OpenGL always uses the multisample buffer if you supply the appropriate buffer attributes when you set up the pixel format object. If you haven't supplied the appropriate attributes, enabling multisampling has no effect.\n\nWhen multisampling is disabled, all coverage values are set to 1, which gives the appearance of rendering without multisampling.\n\nSome graphics hardware leaves multisampling enabled all the time. However, don't rely on hardware to have multisampling enabled; use glEnable to programmatically turn on this feature.\n\nOptionally provide hints for the rendering algorithm. You perform this optional step only if you want OpenGL to compute coverage values by a method other than uniformly weighting samples and averaging them.\n\nSome hardware supports a multisample filter hint through an OpenGL extension—GL_NV_multisample_filter_hint. This hint allows an OpenGL implementation to use an alternative method of resolving the color of multisampled pixels.\n\nYou can specify that OpenGL uses faster or nicer rendering by calling the OpenGL function glHint, passing the constant GL_MULTISAMPLE_FILTER_HINT_NV as the target parameter and GL_FASTEST or GL_NICEST as the mode parameter. Hints allow the hardware to optimize the output if it can. There is no performance penalty or returned error for issuing a hint that's not supported.\n\nFor more information, see the OpenGL extension registry for NV_multisample_filter_hint.\n\nHinting for a Specific Antialiasing Technique\n\nWhen you set up your renderer and buffer attributes for full scene antialiasing, you can specify a hint to prefer one antialiasing technique over the others. If the underlying renderer does not have sufficient resources to support what you request, OpenGL ignores the hint. If you do not supply the appropriate buffer attributes when you create a pixel format object, then the hint does nothing. Table 13-1 lists the hinting constants available for the NSOpenGLPixelFormat class and CGL.\n\nTable 13-1  Antialiasing hints\n\nMultisampling\n\n\t\n\nSupersampling\n\n\t\n\nAlpha blending\n\n\n\n\nNSOpenGLPFAMultisample\n\n\t\n\nNSOpenGLPFASupersample\n\n\t\n\nNSOpenGLPFASampleAlpha\n\n\n\n\nkCGLPFAMultisample\n\n\t\n\nkCGLPFASupersample\n\n\t\n\nkCGLPFASampleAlpha\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Drawing to a Window or View",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_drawing/opengl_drawing.html#//apple_ref/doc/uid/TP40001987-CH404-SW8",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nDrawing to a Window or View\n\nThe OpenGL programming interface provides hundreds of drawing commands that drive graphics hardware. It doesn't provide any commands that interface with the windowing system of an operating system. Without a windowing system, the 3D graphics of an OpenGL program are trapped inside the GPU. Figure 2-1 shows a cube drawn to a Cocoa view.\n\nFigure 2-1  OpenGL content in a Cocoa view\n\nThis chapter shows how to display OpenGL drawing onscreen using the APIs provided by OS X. (This chapter does not show how to use GLUT.) The first section describes the overall approach to drawing onscreen and provides an overview of the functions and methods used by each API.\n\nGeneral Approach\n\nTo draw your content to a view or a layer, your application uses the NSOpenGL classes from within the Cocoa application framework. While the CGL API is used by your applications only to create full-screen content, every NSOpenGLContext object contains a CGL context object. This object can be retrieved from the NSOpenGLContext when your application needs to reference it directly. To show the similarities between the two, this chapter discusses both the NSOpenGL classes and the CGL API.\n\nTo draw OpenGL content to a window or view using the NSOpenGL classes, you need to perform these tasks:\n\nSet up the renderer and buffer attributes that support the OpenGL drawing you want to perform.\n\nEach of the OpenGL APIs in OS X has its own set of constants that represent renderer and buffer attributes. For example, the all-renderers attribute is represented by the NSOpenGLPFAAllRenderers constant in Cocoa and the kCGLPFAAllRenderers constant in the CGL API.\n\nRequest, from the operating system, a pixel format object that encapsulates pixel storage information and the renderer and buffer attributes required by your application. The returned pixel format object contains all possible combinations of renderers and displays available on the system that your program runs on and that meets the requirements specified by the attributes. The combinations are referred to as virtual screens. (See Virtual Screens.)\n\nThere may be situations for which you want to ensure that your program uses a specific renderer. Choosing Renderer and Buffer Attributes discusses how to set up an attributes array that guarantees the system passes back a pixel format object that uses only that renderer.\n\nIf an error occurs, your application may receive a NULL pixel format object. Your application must handle this condition.\n\nCreate a rendering context and bind the pixel format object to it. The rendering context keeps track of state information that controls such things as drawing color, view and projection matrices, characteristics of light, and conventions used to pack pixels.\n\nYour application needs a pixel format object to create a rendering context.\n\nRelease the pixel format object. Once the pixel format object is bound to a rendering context, its resources are no longer needed.\n\nBind a drawable object to the rendering context. For a windowed context, this is typically a Cocoa view.\n\nMake the rendering context the current context. The system sends OpenGL drawing to whichever rendering context is designated as the current one. It's possible for you to set up more than one rendering context, so you need to make sure that the one you want to draw to is the current one.\n\nPerform your drawing.\n\nThe specific functions or methods that you use to perform each of the steps are discussed in the sections that follow.\n\nDrawing to a Cocoa View\n\nThere are two ways to draw OpenGL content to a Cocoa view. If your application has modest drawing requirements, then you can use the NSOpenGLView class. See Drawing to an NSOpenGLView Class: A Tutorial.\n\nIf your application is more complex and needs to support drawing to multiple rendering contexts, you may want to consider subclassing the NSView class. For example, if your application supports drawing to multiple views at the same time, you need to set up a custom NSView class. See Drawing OpenGL Content to a Custom View.\n\nDrawing to an NSOpenGLView Class: A Tutorial\n\nThe NSOpenGLView class is a lightweight subclass of the NSView class that provides convenience methods for setting up OpenGL drawing. An NSOpenGLView object maintains an NSOpenGLPixelFormat object and an NSOpenGLContext object into which OpenGL calls can be rendered. It provides methods for accessing and managing the pixel format object and the rendering context, and handles notification of visible region changes.\n\nAn NSOpenGLView object does not support subviews. You can, however, divide the view into multiple rendering areas using the OpenGL function glViewport.\n\nThis section provides step-by-step instructions for creating a simple Cocoa application that draws OpenGL content to a view. The tutorial assumes that you know how to use Xcode and Interface Builder. If you have never created an application using the Xcode development environment, see Getting Started with Tools.\n\nCreate a Cocoa application project named Golden Triangle.\n\nAdd the OpenGL framework to your project.\n\nAdd a new file to your project using the Objective-C class template. Name the file MyOpenGLView.m and create a header file for it.\n\nOpen the MyOpenGLView.h file and modify the file so that it looks like the code shown in Listing 2-1 to declare the interface.\n\n\n\n\n\nListing 2-1  The interface for MyOpenGLView\n\n#import <Cocoa/Cocoa.h>\n\n\n \n\n\n@interface MyOpenGLView : NSOpenGLView\n\n\n{\n\n\n}\n\n\n- (void) drawRect: (NSRect) bounds;\n\n\n@end\n\nSave and close the MyOpenGLView.h file.\n\nOpen the MyOpenGLView.m file and include the gl.h file, as shown in Listing 2-2.\n\n\n\n\n\nListing 2-2  Include OpenGL/gl.h\n\n#import \"MyOpenGLView.h\"\n\n\n#include <OpenGL/gl.h>\n\n\n \n\n\n@implementation MyOpenGLView\n\n\n@end\n\nImplement the drawRect: method as shown in Listing 2-3, adding the code after the @implementation statement. The method sets the clear color to black and clears the color buffer in preparation for drawing. Then, drawRect: calls your drawing routine, which you’ll add next. The OpenGL command glFlush draws the content provided by your routine to the view.\n\n\n\n\n\nListing 2-3  The drawRect: method for MyOpenGLView\n\n-(void) drawRect: (NSRect) bounds\n\n\n{\n\n\n    glClearColor(0, 0, 0, 0);\n\n\n    glClear(GL_COLOR_BUFFER_BIT);\n\n\n    drawAnObject();\n\n\n    glFlush();\n\n\n}\n\nAdd the code to perform your drawing. In your own application, you'd perform whatever drawing is appropriate. But for the purpose of learning how to draw OpenGL content to a view, add the code shown in Listing 2-4. This code draws a 2D, gold-colored triangle, whose dimensions are not quite the dimensions of a true golden triangle, but good enough to show how to perform OpenGL drawing.\n\nMake sure that you insert this routine before the drawRect: method in the MyOpenGLView.m file.\n\n\n\n\n\nListing 2-4  Code that draws a triangle using OpenGL commands\n\nstatic void drawAnObject ()\n\n\n{\n\n\n    glColor3f(1.0f, 0.85f, 0.35f);\n\n\n    glBegin(GL_TRIANGLES);\n\n\n    {\n\n\n        glVertex3f(  0.0,  0.6, 0.0);\n\n\n        glVertex3f( -0.2, -0.3, 0.0);\n\n\n        glVertex3f(  0.2, -0.3 ,0.0);\n\n\n    }\n\n\n    glEnd();\n\n\n}\n\nOpen the MainMenu.xib in Interface Builder.\n\nChange the window’s title to Golden Triangle.\n\nDrag an NSOpenGLView object from the Library to the window. Resize the view to fit the window.\n\nChange the class of this object to MyOpenGLView.\n\nOpen the Attributes pane of the inspector for the view, and take a look at the renderer and buffer attributes that are available to set. These settings save you from setting attributes programmatically.\n\nOnly those attributes listed in the Interface Builder inspector are set when the view is instantiated. If you need additional attributes, you need to set them programmatically.\n\nBuild and run your application. You should see content similar to the triangle shown in Figure 2-2.\n\nFigure 2-2  The output from the Golden Triangle program\n\nThis example is extremely simple. In a more complex application, you'd want to do the following:\n\nReplace the immediate-mode drawing commands with commands that persist your vertex data inside OpenGL. See OpenGL Application Design Strategies.\n\nIn the interface for the view, declare a variable that indicates whether the view is ready to accept drawing. A view is ready for drawing only if it is bound to a rendering context and that context is set to be the current one.\n\nCocoa does not call initialization routines for objects created in Interface Builder. If you need to perform any initialization tasks, do so in the awakeFromNib method for the view. Note that because you set attributes in the inspector, there is no need to set them up programmatically unless you need additional ones. There is also no need to create a pixel format object programmatically; it is created and loaded when Cocoa loads the nib file.\n\nYour drawRect: method should test whether the view is ready to draw into. You need to provide code that handles the case when the view is not ready to draw into.\n\nOpenGL is at its best when doing real-time and interactive graphics. Your application needs to provide a timer or support user interaction. For more information about creating animation in your OpenGL application, see Synchronize with the Screen Refresh Rate.\n\nDrawing OpenGL Content to a Custom View\n\nThis section provides an overview of the key tasks you need to perform to customize the NSView class for OpenGL drawing. Before you create a custom view for OpenGL drawing, you should read Creating a Custom View in View Programming Guide.\n\nWhen you subclass the NSView class to create a custom view for OpenGL drawing, you override any Quartz drawing or other content that is in that view. To set up a custom view for OpenGL drawing, subclass NSView and create two private variables—one which is an NSOpenGLContext object and the other an NSOpenGLPixelFormat object, as shown in Listing 2-5.\n\nListing 2-5  The interface for a custom OpenGL view\n\n@class NSOpenGLContext, NSOpenGLPixelFormat;\n\n\n \n\n\n@interface CustomOpenGLView : NSView\n\n\n{\n\n\n  @private\n\n\n    NSOpenGLContext*     _openGLContext;\n\n\n    NSOpenGLPixelFormat* _pixelFormat;\n\n\n}\n\n\n+ (NSOpenGLPixelFormat*)defaultPixelFormat;\n\n\n- (id)initWithFrame:(NSRect)frameRect pixelFormat:(NSOpenGLPixelFormat*)format;\n\n\n- (void)setOpenGLContext:(NSOpenGLContext*)context;\n\n\n- (NSOpenGLContext*)openGLContext;\n\n\n- (void)clearGLContext;\n\n\n- (void)prepareOpenGL;\n\n\n- (void)update;\n\n\n- (void)setPixelFormat:(NSOpenGLPixelFormat*)pixelFormat;\n\n\n- (NSOpenGLPixelFormat*)pixelFormat;\n\n\n@end\n\nIn addition to the usual methods for the private variables (openGLContext, setOpenGLContext:, pixelFormat, and setPixelFormat:) you need to implement the following methods:\n\n+ (NSOpenGLPixelFormat*) defaultPixelFormat\n\nUse this method to allocate and initialize the NSOpenGLPixelFormat object.\n\n- (void) clearGLContext\n\nUse this method to clear and release the NSOpenGLContext object.\n\n- (void) prepareOpenGL\n\nUse this method to initialize the OpenGL state after creating the NSOpenGLContext object.\n\nYou need to override the update and initWithFrame: methods of the NSView class.\n\nupdate calls the update method of the NSOpenGLContext class.\n\ninitWithFrame:pixelFormat retains the pixel format and sets up the notification NSViewGlobalFrameDidChangeNotification. See Listing 2-6.\n\nListing 2-6  The initWithFrame:pixelFormat: method\n\n- (id)initWithFrame:(NSRect)frameRect pixelFormat:(NSOpenGLPixelFormat*)format\n\n\n{\n\n\n    self = [super initWithFrame:frameRect];\n\n\n    if (self != nil) {\n\n\n        _pixelFormat   = [format retain];\n\n\n    [[NSNotificationCenter defaultCenter] addObserver:self\n\n\n                     selector:@selector(_surfaceNeedsUpdate:)\n\n\n                     name:NSViewGlobalFrameDidChangeNotification\n\n\n                     object:self];\n\n\n    }\n\n\n    return self;\n\n\n}\n\n\n \n\n\n- (void) _surfaceNeedsUpdate:(NSNotification*)notification\n\n\n{\n\n\n   [self update];\n\n\n}\n\nIf the custom view is not guaranteed to be in a window, you must also override the lockFocus method of the NSView class. See Listing 2-7. This method makes sure that the view is locked prior to drawing and that the context is the current one.\n\nListing 2-7  The lockFocus method\n\n- (void)lockFocus\n\n\n{\n\n\n    NSOpenGLContext* context = [self openGLContext];\n\n\n \n\n\n    [super lockFocus];\n\n\n    if ([context view] != self) {\n\n\n        [context setView:self];\n\n\n    }\n\n\n    [context makeCurrentContext];\n\n\n}\n\nThe reshape method is not supported by the NSView class. You need to update bounds in the drawRect: method, which should take the form shown in Listing 2-8.\n\nListing 2-8  The drawRect method for a custom view\n\n-(void) drawRect\n\n\n{\n\n\n    [context makeCurrentContext];\n\n\n    //Perform drawing here\n\n\n    [context flushBuffer];\n\n\n}\n\nThere may be other methods that you want to add. For example, you might consider detaching the context from the drawable object when the custom view is moved from the window, as shown in Listing 2-9.\n\nListing 2-9  Detaching the context from a drawable object\n\n-(void) viewDidMoveToWindow\n\n\n{\n\n\n    [super viewDidMoveToWindow];\n\n\n    if ([self window] == nil)\n\n\n        [context clearDrawable];\n\n\n}\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Drawing Offscreen",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_offscreen/opengl_offscreen.html#//apple_ref/doc/uid/TP40001987-CH403-SW2",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nDrawing Offscreen\n\nOpenGL applications may want to use OpenGL to render images without actually displaying them to the user. For example, an image processing application might render the image, then copy that image back to the application and save it to disk. Another useful strategy is to create intermediate images that are used later to render additional content. For example, your application might want to render an image and use it as a texture in a future rendering pass. For best performance, offscreen targets should be managed by OpenGL. Having OpenGL manage offscreen targets allows you to avoid copying pixel data back to your application, except when this is absolutely necessary.\n\nOS X offers two useful options for creating offscreen rendering targets:\n\nFramebuffer objects. The OpenGL framebuffer extension allows your application to create fully supported offscreen OpenGL framebuffers. Framebuffer objects are fully supported as a cross-platform extension, so they are the preferred way to create offscreen rendering targets. See Rendering to a Framebuffer Object.\n\nPixel buffer drawable objects. Pixel buffer drawable objects are an Apple-specific technology for creating an offscreen target. Each of the Apple-specific OpenGL APIs provides routines to create an offscreen hardware accelerated pixel buffer. Pixel buffers are recommended for use only when framebuffer objects are not available. See Rendering to a Pixel Buffer.\n\nRendering to a Framebuffer Object\n\nThe OpenGL framebuffer extension (GL_EXT_framebuffer_object) allows applications to create offscreen rendering targets from within OpenGL. OpenGL manages the memory for these framebuffers.\n\nNote: Extensions are available on a per-renderer basis. Before you use framebuffer objects you must check each renderer to make sure that it supports the extension. See Detecting Functionality for more information.\n\nA framebuffer object (FBO) is similar to a drawable object, except a drawable object is a window-system-specific object, whereas a framebuffer object is a window-agnostic object that's defined in the OpenGL standard. After drawing to a framebuffer object, it is straightforward to read the pixel data to the application, or to use it as source data for other OpenGL commands.\n\nFramebuffer objects offer a number of benefits:\n\nThey are window-system independent, which makes porting code easier.\n\nThey are easy to set up and save memory. There is no need to set up attributes and obtain a pixel format object.\n\nThey are associated with a single OpenGL context, whereas each pixel buffer must be bound to a context.\n\nYou can switch between them faster since there is no context switch as with pixel buffers. Because all commands are rendered by a single context, no additional serialization is required.\n\nThey can share depth buffers; pixel buffers cannot.\n\nYou can use them for 2D pixel images and texture images.\n\nCompleteness is a key concept to understanding framebuffer objects. Completeness is a state that indicates whether a framebuffer object meets all the requirements for drawing. You test for this state after performing all the necessary setup work. If a framebuffer object is not complete, it cannot be used as the destination for rendering operations and as a source for read operations.\n\nCompleteness is dependent on many factors that are not possible to condense into one or two statements, but these factors are thoroughly defined in the OpenGL specification for the framebuffer object extension. The specification describes the requirements for internal formats of images attached to the framebuffer, how to determine if a format is color-, depth-, and stencil-renderable, as well as other requirements.\n\nPrior to using framebuffer objects, read the OpenGL specification, which not only defines the framebuffer object API, but provides detailed definitions of all the terms necessary to understand their use and shows several code examples.\n\nThe remainder of this section provides an overview of how to use a framebuffer as either a texture or an image. The functions used to set up textures and images are slightly different. The API for images uses the renderbuffer terminology defined in the OpenGL specification. A renderbuffer image is simply a 2D pixel image. The API for textures uses texture terminology, as you might expect. For example, one of the calls for setting up a framebuffer object for a texture is glFramebufferTexture2DEXT, whereas the call for setting up a framebuffer object for an image is glFramebufferRenderbufferEXT. You'll see how to set up a simple framebuffer object for each type of drawing, starting first with textures.\n\nUsing a Framebuffer Object as a Texture\n\nThese are the basic steps needed to set up a framebuffer object for drawing a texture offscreen:\n\nMake sure the framebuffer extension (GL_EXT_framebuffer_object) is supported on the system that your code runs on. See Determining the OpenGL Capabilities Supported by the Renderer.\n\nCheck the renderer limits. For example, you might want to call the OpenGL function glGetIntegerv to check the maximum texture size (GL_MAX_TEXTURE_SIZE) or find out the maximum number of color buffers you can attach to the framebuffer object(GL_MAX_COLOR_ATTACHMENTS_EXT).\n\nGenerate a framebuffer object name by calling the following function:\n\nvoid glGenFramebuffersEXT (GLsizei n, GLuint *ids);\n\nn is the number of framebuffer object names that you want to create.\n\nOn return, *ids points to the generated names.\n\nBind the framebuffer object name to a framebuffer target by calling the following function:\n\nvoid glBindFramebufferEXT(GLenum target, GLuint framebuffer);\n\ntarget should be the constant GL_FRAMEBUFFER_EXT.\n\nframebuffer is set to an unused framebuffer object name.\n\nOn return, the framebuffer object is initialized to the state values described in the OpenGL specification for the framebuffer object extension. Each attachment point of the framebuffer is initialized to the attachment point state values described in the specification. The number of attachment points is equal to GL_MAX_COLOR_ATTACHMENTS_EXT plus 2 (for depth and stencil attachment points).\n\nWhenever a framebuffer object is bound, drawing commands are directed to it instead of being directed to the drawable associated with the rendering context.\n\nGenerate a texture name.\n\nvoid glGenTextures(GLsizei n, GLuint *textures);\n\nn is the number of texture object names that you want to create.\n\nOn return, *textures points to the generated names.\n\nBind the texture name to a texture target.\n\nvoid glBindTexture(GLenum target, GLuint texture);\n\ntarget is the type of texture to bind.\n\ntexture is the texture name you just created.\n\nSet up the texture environment and parameters.\n\nDefine the texture by calling the appropriate OpenGL function to specify the target, level of detail, internal format, dimensions, border, pixel data format, and texture data storage.\n\nAttach the texture to the framebuffer by calling the following function:\n\nvoid glFramebufferTexture2DEXT (GLenum target, GLenum attachment,\n\n\n                                 GLenum textarget, GLuint texture,\n\n\n                                 GLint level);\n\ntarget must be GL_FRAMEBUFFER_EXT.\n\nattachment must be one of the attachment points of the framebuffer: GL_STENCIL_ATTACHMENT_EXT, GL_DEPTH_ATTACHMENT_EXT, or GL_COLOR_ATTACHMENTn_EXT, where n is a number from 0 to GL_MAX_COLOR_ATTACHMENTS_EXT-1.\n\ntextarget is the texture target.\n\ntexture is an existing texture object.\n\nlevel is the mipmap level of the texture image to attach to the framebuffer.\n\nCheck to make sure that the framebuffer is complete by calling the following function:\n\nGLenum glCheckFramebufferStatusEXT(GLenum target);\n\ntarget must be the constant GL_FRAMEBUFFER_EXT.\n\nThis function returns a status constant. You must test to make sure that the constant is GL_FRAMEBUFFER_COMPLETE_EXT. If it isn't, see the OpenGL specification for the framebuffer object extension for a description of the other constants in the status enumeration.\n\nRender content to the texture. You must make sure to bind a different texture to the framebuffer object or disable texturing before you render content. If you render to a framebuffer object texture attachment with that same texture currently bound and enabled, the result is undefined.\n\nTo draw the contents of the texture to a window, make the window the target of all rendering commands by calling the function glBindFramebufferEXT and passing the constant GL_FRAMEBUFFER_EXT and 0. The window is always specified as 0.\n\nUse the texture attachment as a normal texture by binding it, enabling texturing, and drawing.\n\nDelete the texture.\n\nDelete the framebuffer object by calling the following function:\n\nvoid  glDeleteFramebuffersEXT (GLsizei n, const GLuint *framebuffers);\n\nn is the number of framebuffer objects to delete.\n\n*framebuffers points to an array that contains the framebuffer object names.\n\nListing 5-1 shows code that performs these tasks. This example creates and draws to a single framebuffer object.\n\nListing 5-1  Setting up a framebuffer for texturing\n\nGLuint framebuffer, texture;\n\n\nGLenum status;\n\n\nglGenFramebuffersEXT(1, &framebuffer);\n\n\n// Set up the FBO with one texture attachment\n\n\nglBindFramebufferEXT(GL_FRAMEBUFFER_EXT, framebuffer);\n\n\nglGenTextures(1, &texture);\n\n\nglBindTexture(GL_TEXTURE_2D, texture);\n\n\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);\n\n\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);\n\n\nglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA8, TEXWIDE, TEXHIGH, 0,\n\n\n                GL_RGBA, GL_UNSIGNED_BYTE, NULL);\n\n\nglFramebufferTexture2DEXT(GL_FRAMEBUFFER_EXT, GL_COLOR_ATTACHMENT0_EXT,\n\n\n                GL_TEXTURE_2D, texture, 0);\n\n\nstatus = glCheckFramebufferStatusEXT(GL_FRAMEBUFFER_EXT);\n\n\nif (status != GL_FRAMEBUFFER_COMPLETE_EXT)\n\n\n                // Handle error here\n\n\n// Your code to draw content to the FBO\n\n\n// ...\n\n\n// Make the window the target\n\n\nglBindFramebufferEXT(GL_FRAMEBUFFER_EXT, 0);\n\n\n//Your code to use the contents of the FBO\n\n\n// ...\n\n\n//Tear down the FBO and texture attachment\n\n\nglDeleteTextures(1, &texture);\n\n\nglDeleteFramebuffersEXT(1, &framebuffer);\nUsing a Framebuffer Object as an Image\n\nThere is a lot of similarity between setting up a framebuffer object for drawing images and setting one up to draw textures. These are the basic steps needed to set up a framebuffer object for drawing a 2D pixel image (a renderbuffer image) offscreen:\n\nMake sure the framebuffer extension (EXT_framebuffer_object) is supported on the renderer that your code runs on.\n\nCheck the renderer limits. For example, you might want to call the OpenGL function glGetIntegerv to find out the maximum number of color buffers (GL_MAX_COLOR_ATTACHMENTS_EXT).\n\nGenerate a framebuffer object name by calling the function glGenFramebuffersEXT.\n\nBind the framebuffer object name to a framebuffer target by calling the function glBindFramebufferEXT.\n\nGenerate a renderbuffer object name by calling the following function:\n\nvoid  glGenRenderbuffersEXT (GLsizei n, GLuint *renderbuffers );\n\nn is the number of renderbuffer object names to create.\n\n*renderbuffers points to storage for the generated names.\n\nBind the renderbuffer object name to a renderbuffer target by calling the following function:\n\nvoid glBindRenderbufferEXT (GLenum target, GLuint renderbuffer);\n\ntarget must be the constant GL_RENDERBUFFER_EXT.\n\nrenderbuffer is the renderbuffer object name generated previously.\n\nCreate data storage and establish the pixel format and dimensions of the renderbuffer image by calling the following function:\n\nvoid glRenderbufferStorageEXT (GLenum target, GLenum internalformat,\n\n\n                                GLsizei width, GLsizei height);\n\ntarget must be the constant GL_RENDERBUFFER_EXT.\n\ninternalformat is the pixel format of the image. The value must be RGB, RGBA, DEPTH_COMPONENT, STENCIL_INDEX, or one of the other formats listed in the OpenGL specification.\n\nwidth is the width of the image, in pixels.\n\nheight is the height of the image, in pixels.\n\nAttach the renderbuffer to a framebuffer target by calling the function glFramebufferRenderbufferEXT.\n\nvoid glFramebufferRenderbufferEXT(GLenum target, GLenum attachment,\n\n\n                             GLenum renderbuffertarget, GLuint renderbuffer);\n\ntarget must be the constant GL_FRAMEBUFFER_EXT.\n\nattachment should be one of the attachment points of the framebuffer: GL_STENCIL_ATTACHMENT_EXT, GL_DEPTH_ATTACHMENT_EXT, or GL_COLOR_ATTACHMENTn_EXT, where n is a number from 0 to GL_MAX_COLOR_ATTACHMENTS_EXT–1.\n\nrenderbuffertarget must be the constant GL_RENDERBUFFER_EXT.\n\nrenderbuffer should be set to the name of the renderbuffer object that you want to attach to the framebuffer.\n\nCheck to make sure that the framebuffer is complete by calling the following function:\n\nenum  glCheckFramebufferStatusEXT(GLenum target);\n\ntarget must be the constant GL_FRAMEBUFFER_EXT.\n\nThis function returns a status constant. You must test to make sure that the constant is GL_FRAMEBUFFER_COMPLETE_EXT. If it isn't, see the OpenGL specification for the framebuffer object extension for a description of the other constants in the status enumeration.\n\nRender content to the renderbuffer.\n\nTo access the contents of the renderbuffer object, bind the framebuffer object and then use OpenGL functions such as glReadPixels or glCopyTexImage2D.\n\nDelete the framebuffer object with its renderbuffer attachment.\n\nListing 5-2 shows code that sets up and draws to a single renderbuffer object. Your application can set up more than one renderbuffer object if it requires them.\n\nListing 5-2  Setting up a renderbuffer for drawing images\n\nGLuint framebuffer, renderbuffer;\n\n\nGLenum status;\n\n\n// Set the width and height appropriately for your image\n\n\nGLuint imageWidth = 1024,\n\n\n       imageHeight = 1024;\n\n\n//Set up a FBO with one renderbuffer attachment\n\n\nglGenFramebuffersEXT(1, &framebuffer);\n\n\nglBindFramebufferEXT(GL_FRAMEBUFFER_EXT, framebuffer);\n\n\nglGenRenderbuffersEXT(1, &renderbuffer);\n\n\nglBindRenderbufferEXT(GL_RENDERBUFFER_EXT, renderbuffer);\n\n\nglRenderbufferStorageEXT(GL_RENDERBUFFER_EXT, GL_RGBA8, imageWidth, imageHeight);\n\n\nglFramebufferRenderbufferEXT(GL_FRAMEBUFFER_EXT, GL_COLOR_ATTACHMENT0_EXT,\n\n\n                 GL_RENDERBUFFER_EXT, renderbuffer);\n\n\nstatus = glCheckFramebufferStatusEXT(GL_FRAMEBUFFER_EXT);\n\n\nif (status != GL_FRAMEBUFFER_COMPLETE_EXT)\n\n\n                // Handle errors\n\n\n//Your code to draw content to the renderbuffer\n\n\n// ...\n\n\n//Your code to use the contents\n\n\n// ...\n\n\n// Make the window the target\n\n\nglBindFramebufferEXT(GL_FRAMEBUFFER_EXT, 0);\n\n\n// Delete the renderbuffer attachment\n\n\nglDeleteRenderbuffersEXT(1, &renderbuffer);\nRendering to a Pixel Buffer\n\nThe OpenGL extension string GL_APPLE_pixel_buffer provides hardware-accelerated offscreen rendering to a pixel buffer. A pixel buffer is typically used as a texture source. It can also be used for remote rendering.\n\nImportant: Pixel buffers are deprecated starting with OS X v10.7 and are not supported by the OpenGL 3.2 Core profile; use framebuffer objects instead.\n\nYou must create a rendering context for each pixel buffer. For example, if you want to use a pixel buffer as a texture source, you create one rendering context attached to the pixel buffer and a second context attached to a window or view.\n\nThe first step in using a pixel buffer is to create it. The Apple-specific OpenGL APIs each provide a routine for this purpose:\n\nThe NSOpenGLPixelBuffer method initWithTextureTarget:textureInternalFormat:textureMaxMipMapLevel:pixelsWide:pixelsHigh:\n\nThe CGL function CGLCreatePBuffer\n\nEach of these routines requires that you provide a texture target, an internal format, a maximum mipmap level, and the width and height of the texture.\n\nThe texture target must be one of these OpenGL texture constants: GL_TEXTURE_2D for a 2D texture, GL_TEXTURE_RECTANGLE_ARB for a rectangular (not power-of-two) texture, or GL_TEXTURE_CUBE_MAP for a cube map texture.\n\nThe internal format specifies how to interpret the data for texturing operations. You can supply any of these options: GL_RGB (each pixel is a three-component group), GL_RGBA (each pixel is a four-component group), or GL_DEPTH_COMPONENT (each pixel is a single depth component).\n\nThe maximum mipmap level should be 0 for a pixel buffer that does not have a mipmap. The value that you supply should not exceed the actual maximum number of mipmap levels that can be represented with the given width and height.\n\nNote that none of the routines that create a pixel buffer allocate the storage needed. The storage is allocated by the system at the time that you attach the pixel buffer to a rendering context.\n\nSetting Up a Pixel Buffer for Offscreen Drawing\n\nAfter you create a pixel buffer, the general procedure for using a pixel buffer for drawing is similar to the way you set up windows and views for drawing:\n\nSpecify renderer and buffer attributes.\n\nObtain a pixel format object.\n\nCreate a rendering context and make it current.\n\nAttach a pixel buffer to the context using the appropriate Apple OpenGL attachment function:\n\nThe setPixelBuffer:cubeMapFace:mipMapLevel:currentVirtualScreen: method of the NSOpenGLContext class instructs the receiver to render into a pixel buffer.\n\nThe CGL function CGLSetPBuffer attaches a CGL rendering context to a pixel buffer.\n\nDraw, as you normally would, using OpenGL.\n\nUsing a Pixel Buffer as a Texture Source\n\nPixel buffers let you perform direct texturing without incurring the cost of extra copies. After drawing to a pixel buffer, you can create a texture by following these steps:\n\nGenerate a texture name by calling the OpenGL function glGenTextures.\n\nBind the named texture to a target by calling the OpenGL function glBindTexture.\n\nSet the texture parameters by calling OpenGL function glTexEnvParameter.\n\nSet up the pixel buffer as the source for the texture by calling one of the following Apple OpenGL functions:\n\nThe setTextureImageToPixelBuffer:colorBuffer: method of the NSOpenGLContext class attaches the image data in the pixel buffer to the texture object currently bound by the receiver.\n\nThe CGL function CGLTexImagePBuffer binds the contents of a CGL pixel buffer as the data source for a texture object.\n\nThe context that you attach to the pixel buffer is the target rendering context: the context that uses the pixel buffer as the source of the texture data. Each of these routines requires a source parameter, which is an OpenGL constant that specifies the source buffer to texture from. The source parameter must be a valid OpenGL buffer, such as GL_FRONT, GL_BACK, or GL_AUX0, and should be compatible with the buffer attributes used to create the OpenGL context associated with the pixel buffer. This means that the pixel buffer must possess the buffer in question for texturing to succeed. For example, if the buffer attribute used with the pixel buffer is only single buffered, then texturing from the GL_BACK buffer will fail.\n\nIf you modify content of any pixel buffer that contains mipmap levels, you must call the appropriate Apple OpenGL function again (setTextureImageToPixelBuffer:colorBuffer: or CGLTexImagePBuffer) before drawing with the pixel buffer to ensure that the content is synchronized with OpenGL. To synchronize the content of pixel buffers without mipmaps, simply rebind to the texture object using glBind.\n\nDraw primitives using the appropriate texture coordinates. (See \"The Red book\"—OpenGL Programming Guide—for details.)\n\nCall glFlush to cause all drawing commands to be executed.\n\nWhen you no longer need the texture object, call the OpenGL function glDeleteTextures.\n\nSet the current context to NULL using one of the Apple OpenGL routines:\n\nThe makeCurrentContext method of the NSOpenGLContext class\n\nThe CGL function CGLSetCurrentContext\n\nDestroy the pixel buffer by calling CGLDestroyPBuffer.\n\nDestroy the context by calling CGLDestroyContext.\n\nDestroy the pixel format by calling CGLDestroyPixelFormat.\n\nYou might find these guidelines useful when using pixel buffers for texturing:\n\nYou cannot make OpenGL texturing calls that modify pixel buffer content (such as glTexSubImage2D or glCopyTexImage2D) with the pixel buffer as the destination. You can use texturing commands to read data from a pixel buffer, such as glCopyTexImage2D, with the pixel buffer texture as the source. You can also use OpenGL functions such as glReadPixels to read the contents of a pixel buffer directly from the pixel buffer context.\n\nTexturing can fail to produce the intended results without reporting an error. You must make sure that you enable the proper texture target, set a compatible filter mode, and adhere to other requirements described in the OpenGL specification.\n\nYou are not required to set up context sharing when you texture from a pixel buffer. You can have different pixel format objects and rendering contexts for both the pixel buffer and the target drawable object, without sharing resources, and still texture using a pixel buffer in the target context.\n\nRendering to a Pixel Buffer on a Remote System\n\nFollow these steps to render to a pixel buffer on a remote system. The remote system does not need to have a display attached to it.\n\nWhen you set the renderer and buffer attributes, include the remote pixel buffer attribute kCGLPFARemotePBuffer.\n\nLog in to the remote machine using the ssh command to ensure security.\n\nRun the application on the target system.\n\nRetrieve the content.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Determining the OpenGL Capabilities Supported by the Renderer",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_general/opengl_gen_tasks.html#//apple_ref/doc/uid/TP40001987-CH211-SW7",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nPrevious\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nDetermining the OpenGL Capabilities Supported by the Renderer\n\nOne of the benefits of using OpenGL is that it is extensible. An extension is typically introduced by one or more vendors and then later is accepted by the OpenGL Working Group. Some extensions are promoted from a vendor-specific extension to one shared by more than one vendor, sometimes even being incorporated into the core OpenGL API. Extensions allow OpenGL to embrace innovation, but require you to verify that the OpenGL functionality you want to use is available.\n\nBecause extensions can be introduced at the vendor level, more than one extension can provide the same basic functionality. There might also be an ARB-approved extension that has functionality similar to that of a vendor-specific extension. Your application should prefer core functionality or ARB-approved extensions over those specific to a particular vendor, when both are offered by the same renderer. This makes it easier to transparently support new renderers from other vendors.\n\nAs particular functionality becomes widely adopted, it can be moved into the core OpenGL API by the ARB. As a result, functionality that you want to use could be included as an extension, as part of the core API, or both. For example, the ability to combine texture environments is supported through the GL_ARB_texture_env_combine and the GL_EXT_texture_env_combine extensions. It's also part of the core OpenGL version 1.3 API. Although each has similar functionality, they use a different syntax. You may need to check in several places (core OpenGL API and extension strings) to determine whether a specific renderer supports functionality that you want to use.\n\nDetecting Functionality\n\nOpenGL has two types of commands—those that are part of the core API and those that are part of an extension to OpenGL. Your application first needs to check for the version of the core OpenGL API and then check for the available extensions. Keep in mind that OpenGL functionality is available on a per-renderer basis. For example, a software renderer might not support fog effects even though fog effects are available in an OpenGL extension implemented by a hardware vendor on the same system. For this reason, it's important that you check for functionality on a per-renderer basis.\n\nRegardless of what functionality you are checking for, the approach is the same. You need to call the OpenGL function glGetString twice. The first time pass the GL_VERSION constant. The function returns a string that specifies the version of OpenGL. The second time, pass the GL_EXTENSIONS constant. The function returns a pointer to an extension name string. The extension name string is a space-delimited list of the OpenGL extensions that are supported by the current renderer. This string can be rather long, so do not allocate a fixed-length string for the return value of the glGetString function. Use a pointer and evaluate the string in place.\n\nPass the extension name string to the function gluCheckExtension along with the name of the extension you want to check for. The gluCheckExtension function returns a Boolean value that indicates whether or not the extension is available for the current renderer.\n\nIf an extension becomes part of the core OpenGL API, OpenGL continues to export the name strings of the promoted extensions. It also continues to support the previous versions of any extension that has been exported in earlier versions of OS X. Because extensions are not typically removed, the methodology you use today to check for a feature works in future versions of OS X.\n\nChecking for functionality, although fairly straightforward, involves writing a large chunk of code. The best way to check for OpenGL functionality is to implement a capability-checking function that you call when your program starts up, and then any time the renderer changes. Listing 8-1 shows a code excerpt that checks for a few extensions. A detailed explanation for each line of code appears following the listing.\n\nListing 8-1  Checking for OpenGL functionality\n\nGLint maxRectTextureSize;\n\n\nGLint myMaxTextureUnits;\n\n\nGLint myMaxTextureSize;\n\n\nconst GLubyte * strVersion;\n\n\nconst GLubyte * strExt;\n\n\nfloat myGLVersion;\n\n\nGLboolean isVAO, isTexLOD, isColorTable, isFence, isShade,\n\n\n          isTextureRectangle;\n\n\nstrVersion = glGetString (GL_VERSION); \n// 1\n\n\nsscanf((char *)strVersion, \"%f\", &myGLVersion);\n\n\nstrExt = glGetString (GL_EXTENSIONS); \n// 2\n\n\nglGetIntegerv(GL_MAX_TEXTURE_UNITS, &myMaxTextureUnits); \n// 3\n\n\nglGetIntegerv(GL_MAX_TEXTURE_SIZE, &myMaxTextureSize); \n// 4\n\n\nisVAO =\n\n\n    gluCheckExtension ((const GLubyte*)\"GL_APPLE_vertex_array_object\",strExt); \n// 5\n\n\nisFence = gluCheckExtension ((const GLubyte*)\"GL_APPLE_fence\", strExt); \n// 6\n\n\nisShade =\n\n\n     gluCheckExtension ((const GLubyte*)\"GL_ARB_shading_language_100\", strExt); \n// 7\n\n\nisColorTable =\n\n\n     gluCheckExtension ((const GLubyte*)\"GL_SGI_color_table\", strExt) ||\n\n\n             gluCheckExtension ((const GLubyte*)\"GL_ARB_imaging\", strExt); \n// 8\n\n\nisTexLOD =\n\n\n     gluCheckExtension ((const GLubyte*)\"GL_SGIS_texture_lod\", strExt) ||\n\n\n                                  (myGLVersion >= 1.2); \n// 9\n\n\nisTextureRectangle = gluCheckExtension ((const GLubyte*)\n\n\n                                 \"GL_EXT_texture_rectangle\", strExt);\n\n\nif (isTextureRectangle)\n\n\n      glGetIntegerv (GL_MAX_RECTANGLE_TEXTURE_SIZE_EXT, &maxRectTextureSize);\n\n\nelse\n\n\n     maxRectTextureSize = 0; \n// 10\n\nHere is what the code does:\n\nGets a string that specifies the version of OpenGL.\n\nGets the extension name string.\n\nCalls the OpenGL function glGetIntegerv to get the value of the attribute passed to it which, in this case, is the maximum number of texture units.\n\nGets the maximum texture size.\n\nChecks whether vertex array objects are supported.\n\nChecks for the Apple fence extension.\n\nChecks for support for version 1.0 of the OpenGL shading language.\n\nChecks for RGBA-format color lookup table support. In this case, the code needs to check for the vendor-specific string and for the ARB string. If either is present, the functionality is supported.\n\nChecks for an extension related to the texture level of detail parameter (LOD). In this case, the code needs to check for the vendor-specific string and for the OpenGL version. If the vendor string is present or the OpenGL version is greater than or equal to 1.2, the functionality is supported.\n\nGets the OpenGL limit for rectangle textures. For some extensions, such as the rectangle texture extension, it may not be enough to check whether the functionality is supported. You may also need to check the limits. You can use glGetIntegerv and related functions (glGetBooleanv, glGetDoublev, glGetFloatv) to obtain a variety of parameter values.\n\nYou can extend this example to make a comprehensive functionality-checking routine for your application. For more details, see the GLCheck.c file in the Cocoa OpenGL sample application.\n\nThe code in Listing 8-2 shows one way to query the current renderer. It uses the CGL API, which can be called from Cocoa applications. In reality, you need to iterate over all displays and all renderers for each display to get a true picture of the OpenGL functionality available on a particular system. You also need to update your functionality snapshot each time the list of displays or display configuration changes.\n\nListing 8-2  Setting up a valid rendering context to get renderer functionality information\n\n#include <OpenGL/OpenGL.h>\n\n\n#include <ApplicationServices/ApplicationServices.h>\n\n\nCGDirectDisplayID display = CGMainDisplayID (); \n// 1\n\n\nCGOpenGLDisplayMask myDisplayMask =\n\n\n                CGDisplayIDToOpenGLDisplayMask (display); \n// 2\n\n\n \n\n\n{ // Check capabilities of display represented by display mask\n\n\n    CGLPixelFormatAttribute attribs[] = {kCGLPFADisplayMask,\n\n\n                             myDisplayMask,\n\n\n                             0}; \n// 3\n\n\n    CGLPixelFormatObj pixelFormat = NULL;\n\n\n    GLint numPixelFormats = 0;\n\n\n    CGLContextObj myCGLContext = 0;\n\n\n    CGLContextObj curr_ctx = CGLGetCurrentContext (); \n// 4\n\n\n    CGLChoosePixelFormat (attribs, &pixelFormat, &numPixelFormats); \n// 5\n\n\n    if (pixelFormat) {\n\n\n        CGLCreateContext (pixelFormat, NULL, &myCGLContext); \n// 6\n\n\n        CGLDestroyPixelFormat (pixelFormat); \n// 7\n\n\n        CGLSetCurrentContext (myCGLContext); \n// 8\n\n\n        if (myCGLContext) {\n\n\n            // Check for capabilities and functionality here\n\n\n        }\n\n\n    }\n\n\n    CGLDestroyContext (myCGLContext); \n// 9\n\n\n    CGLSetCurrentContext (curr_ctx); \n// 10\n\n\n}\n\nHere's what the code does:\n\nGets the display ID of the main display.\n\nMaps a display ID to an OpenGL mask.\n\nFills a pixel format attributes array with the display mask attribute and the mask value.\n\nSaves the current context so that it can be restored later.\n\nGets the pixel format object for the display. The numPixelFormats parameter specifies how many pixel formats are listed in the pixel format object.\n\nCreates a context based on the first pixel format in the list supplied by the pixel format object. Only one renderer will be associated with this context.\n\nIn your application, you would need to iterate through all pixel formats for this display.\n\nDestroys the pixel format object when it is no longer needed.\n\nSets the current context to the newly created, single-renderer context. Now you are ready to check for the functionality supported by the current renderer. See Listing 8-1 for an example of functionality-checking code.\n\nDestroys the context because it is no longer needed.\n\nRestores the previously saved context as the current context, thus ensuring no intrusion upon the user.\n\nGuidelines for Code That Checks for Functionality\n\nThe guidelines in this section ensure that your functionality-checking code is thorough yet efficient.\n\nDon't rely on what's in a header file. A function declaration in a header file does not ensure that a feature is supported by the current renderer. Neither does linking against a stub library that exports a function.\n\nMake sure that a renderer is attached to a valid rendering context before you check the functionality of that renderer.\n\nCheck the API version or the extension name string for the current renderer before you issue OpenGL commands.\n\nCheck only once per renderer. After you've determined that the current renderer supports an OpenGL command, you don't need to check for that functionality again for that renderer.\n\nMake sure that you are aware of whether a feature is being used as part of the Core OpenGL API or as an extension. When a feature is implemented both as part of the core OpenGL API and as an extension, it uses different constants and function names.\n\nOpenGL Renderer Implementation-Dependent Values\n\nThe OpenGL specification defines implementation-dependent values that define the limits of what an OpenGL implementation is capable of. For example, the maximum size of a texture and the number of texture units are both common implementation-dependent values that an application is expected to check. Each of these values provides a minimum value that all conforming OpenGL implementations are expected to support. If your application’s usage exceeds these minimums, it must check the limit first, and fail gracefully if the implementation cannot provide the limit desired. Your application may need to load smaller textures, disable a rendering feature, or choose a different implementation.\n\nAlthough the specification provides a comprehensive list of these limitations, a few stand out in most OpenGL applications. Table 8-1 lists values that applications should test if they require more than the minimum values in the specification.\n\nTable 8-1  Common OpenGL renderer limitations\n\nMaximum size of the texture\n\n\t\n\nGL_MAX_TEXTURE_SIZE\n\n\n\n\nNumber of depth buffer planes\n\n\t\n\nGL_DEPTH_BITS\n\n\n\n\nNumber of stencil buffer planes\n\n\t\n\nGL_STENCIL_BITS\n\nThe limit on the size and complexity of your shaders is a key area you need to test. All graphics hardware supports limited memory to pass attributes into the vertex and fragment shaders. Your application must either keep its usage below the minimums as defined in the specification, or it must check the shader limitations documented in Table 8-2 and choose shaders that are within those limits.\n\nTable 8-2  OpenGL shader limitations\n\nMaximum number of vertex attributes\n\n\t\n\nGL_MAX_VERTEX_ATTRIBS\n\n\n\n\nMaximum number of uniform vertex vectors\n\n\t\n\nGL_MAX_VERTEX_UNIFORM_COMPONENTS\n\n\n\n\nMaximum number of uniform fragment vectors\n\n\t\n\nGL_MAX_FRAGMENT_UNIFORM_COMPONENTS\n\n\n\n\nMaximum number of varying vectors\n\n\t\n\nGL_MAX_VARYING_FLOATS\n\n\n\n\nMaximum number of texture units usable in a vertex shader\n\n\t\n\nGL_MAX_VERTEX_TEXTURE_IMAGE_UNITS\n\n\n\n\nMaximum number of texture units usable in a fragment shader\n\n\t\n\nGL_MAX_TEXTURE_IMAGE_UNITS\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Glossary",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposer_Patch_PlugIn_ProgGuide/glossary/glossary.html#//apple_ref/doc/uid/TP40004787-CH6-DontLinkElementID_21",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer Custom Patch Programming Guide\nTable of Contents\nIntroduction\nThe Basics of Custom Patches\nWriting Processor Patches\nWriting Image Processing Patches\nWriting Consumer Patches\nGlossary\nRevision History\nNext\nPrevious\nGlossary\ncustom patch  \n\nA subclass of QCPlugIn that performs custom processing in the Quartz Composer development environment.\n\n\n\nplug-in  \n\nAn NSBundle used to package one or more custom patches.\n\n\n\nQCPlugIn  \n\nA class defined by the Quartz framework that provides the base class to subclass for writing custom patches.\n\n\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2010 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2010-03-24\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "About OpenGL for OS X",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_intro/opengl_intro.html#//apple_ref/doc/uid/TP40001987",
    "html": "Documentation Archive\nDeveloper\nSearch\nOpenGL Programming Guide for Mac\nTable of Contents\nIntroduction\nOpenGL on the Mac Platform\nDrawing to a Window or View\nOptimizing OpenGL for High Resolution\nDrawing to the Full Screen\nDrawing Offscreen\nChoosing Renderer and Buffer Attributes\nWorking with Rendering Contexts\nDetermining the OpenGL Capabilities Supported by the Renderer\nOpenGL Application Design Strategies\nBest Practices for Working with Vertex Data\nBest Practices for Working with Texture Data\nCustomizing the OpenGL Pipeline with Shaders\nTechniques for Scene Antialiasing\nConcurrency and OpenGL\nTuning Your OpenGL Application\nAppendix A: Legacy OpenGL Functionality by Version\nAppendix B: Updating an Application to Support the OpenGL 3.2 Core Specification\nAppendix C: Setting Up Function Pointers to OpenGL Routines\nRevision History\nGlossary\nNext\nRetired Document\n\nImportant: OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nAbout OpenGL for OS X\n\nImportant OpenGL was deprecated in macOS 10.14. To create high-performance code on GPUs, use the Metal framework instead. See Metal.\n\nOpenGL is an open, cross-platform graphics standard with broad industry support. OpenGL greatly eases the task of writing real-time 2D or 3D graphics applications by providing a mature, well-documented graphics processing pipeline that supports the abstraction of current and future hardware accelerators.\n\nAt a Glance\n\nOpenGL is an excellent choice for graphics development on the Macintosh platform because it offers the following advantages:\n\nReliable Implementation. The OpenGL client-server model abstracts hardware details and guarantees consistent presentation on any compliant hardware and software configuration. Every implementation of OpenGL adheres to the OpenGL specification and must pass a set of conformance tests.\n\nPerformance. Applications can harness the considerable power of the graphics hardware to improve rendering speeds and quality.\n\nIndustry acceptance. The specification for OpenGL is controlled by the Khronos Group, an industry consortium whose members include many of the major companies in the computer graphics industry, including Apple. In addition to OpenGL for OS X, there are OpenGL implementations for Windows, Linux, Irix, Solaris, and many game consoles.\n\nOpenGL Is a C-based, Platform-Neutral API\n\nBecause OpenGL is a C-based API, it is extremely portable and widely supported. As a C API, it integrates seamlessly with Objective-C based Cocoa applications. OpenGL provides functions your application uses to generate 2D or 3D images. Your application presents the rendered images to the screen or copies them back to its own memory.\n\nThe OpenGL specification does not provide a windowing layer of its own. It relies on functions defined by OS X to integrate OpenGL drawing with the windowing system. Your application creates an OS X OpenGL rendering context and attaches a rendering target to it (known as a drawable object). The rendering context manages OpenGL state changes and objects created by calls to the OpenGL API. The drawable object is the final destination for OpenGL drawing commands and is typically associated with a Cocoa window or view.\n\nRelevant Chapters: OpenGL on the Mac Platform\n\nDifferent Rendering Destinations Require Different Setup Commands\n\nDepending on whether your application intends to draw OpenGL content to a window, to draw to the entire screen, or to perform offscreen image processing, it takes different steps to create the rendering context and associate it with a drawable object.\n\nRelevant Chapters: Drawing to a Window or View, Drawing to the Full Screen and Drawing Offscreen\n\nOpenGL on Macs Exists in a Heterogenous Environment\n\nMacs support different types of graphics processors, each with different rendering capabilities, supporting versions of OpenGL from 1.x through OpenGL 3.2. When creating a rendering context, your application can accept a broad range of renderers or it can restrict itself to devices with specific capabilities. Once you have a context, you can configure how that context executes OpenGL commands.\n\nOpenGL on the Mac is not only a heterogenous environment, but it is also a dynamic environment. Users can add or remove displays, or take a laptop running on battery power and plug it into a wall. When the graphics environment on the Mac changes, the renderer associated with the context may change. Your application must handle these changes and adjust how it uses OpenGL.\n\nRelevant Chapters: Choosing Renderer and Buffer Attributes, Working with Rendering Contexts, and Determining the OpenGL Capabilities Supported by the Renderer\n\nOpenGL Helps Applications Harness the Power of Graphics Processors\n\nGraphics processors are massively parallelized devices optimized for graphics operations. To access that computing power adds additional overhead because data must move from your application to the GPU over slower internal buses. Accessing the same data simultaneously from both your application and OpenGL is usually restricted. To get great performance in your application, you must carefully design your application to feed data and commands to OpenGL so that the graphics hardware runs in parallel with your application. A poorly tuned application may stall either on the CPU or the GPU waiting for the other to finish processing.\n\nWhen you are ready to optimize your application’s performance, Apple provides both general-purpose and OpenGL-specific profiling tools that make it easy to learn where your application spends its time.\n\nRelevant Chapters: Optimizing OpenGL for High Resolution, OpenGL on the Mac Platform,OpenGL Application Design Strategies, Best Practices for Working with Vertex Data, Best Practices for Working with Texture Data, Customizing the OpenGL Pipeline with Shaders, and Tuning Your OpenGL Application\n\nConcurrency in OpenGL Applications Requires Additional Effort\n\nMany Macs ship with multiple processors or multiple cores, and future hardware is expected to add more of each. Designing applications to take advantage of multiprocessing is critical. OpenGL places additional restrictions on multithreaded applications. If you intend to add concurrency to an OpenGL application, you must ensure that the application does not access the same context from two different threads at the same time.\n\nRelevant Chapters: Concurrency and OpenGL\n\nPerformance Tuning Allows Your Application to Provide an Exceptional User Experience\n\nOnce you’ve improved the performance of your OpenGL application and taken advantage of concurrency, put some of the freed processing power to work for you. Higher resolution textures, detailed models, and more complex lighting and shading algorithms can improve image quality. Full-scene antialiasing on modern graphics hardware can eliminate many of the “jaggies” common on lower resolution images.\n\nRelevant Chapters: Customizing the OpenGL Pipeline with Shaders,Techniques for Scene Antialiasing\n\nHow to Use This Document\n\nIf you have never programmed in OpenGL on the Mac, you should read this book in its entirety, starting with OpenGL on the Mac Platform. Critical Mac terminology is defined in that chapter as well as in the Glossary.\n\nIf you already have an OpenGL application running on the Mac, but have not yet updated it for OS X v10.7, read Choosing Renderer and Buffer Attributes to learn how to choose an OpenGL profile for your application.\n\nTo find out how to update an existing OpenGL app for high resolution, see Optimizing OpenGL for High Resolution.\n\nOnce you have OpenGL content in your application, read OpenGL Application Design Strategies to learn fundamental patterns for implementing high-performance OpenGL applications, and the chapters that follow to learn how to apply those patterns to specific OpenGL problems.\n\nImportant: Although this guide describes how to create rendering contexts that support OpenGL 3.2, most code examples and discussion in the rest of the book describe the earlier legacy versions of OpenGL. See Updating an Application to Support the OpenGL 3.2 Core Specification for more information on migrating your application to OpenGL 3.2.\n\nPrerequisites\n\nThis guide assumes that you have some experience with OpenGL programming, but want to learn how to apply that knowledge to create software for the Mac. Although this guide provides advice on optimizing OpenGL code, it does not provide entry-level information on how to use the OpenGL API. If you are unfamiliar with OpenGL, you should read OpenGL on the Mac Platform to get an overview of OpenGL on the Mac platform, and then read the following OpenGL programming guide and reference documents:\n\nOpenGL Programming Guide, by Dave Shreiner and the Khronos OpenGL Working Group; otherwise known as \"The Red book.”\n\nOpenGL Shading Language, by Randi J. Rost, is an excellent guide for those who want to write programs that compute surface properties (also known as shaders).\n\nOpenGL Reference Pages.\n\nBefore reading this document, you should be familiar with Cocoa windows and views as introduced in Window Programming Guide and View Programming Guide.\n\nSee Also\n\nKeep these reference documents handy as you develop your OpenGL program for OS X:\n\nNSOpenGLView Class Reference, NSOpenGLContext Class Reference, NSOpenGLPixelBuffer Class Reference, and NSOpenGLPixelFormat Class Reference provide a complete description of the classes and methods needed to integrate OpenGL content into a Cocoa application.\n\nCGL Reference describes low-level functions that can be used to create full-screen OpenGL applications.\n\nOpenGL Extensions Guide provides information about OpenGL extensions supported in OS X.\n\nThe OpenGL Foundation website, http://www.opengl.org, provides information on OpenGL commands, the Khronos OpenGL Working Group, logo requirements, OpenGL news, and many other topics. It's a site that you'll want to visit regularly. Among the many resources it provides, the following are important reference documents for OpenGL developers:\n\nOpenGL Specification provides detailed information on how an OpenGL implementation is expected to handle each OpenGL command.\n\nOpenGL Reference describes the main OpenGL library.\n\nOpenGL GLU Reference describes the OpenGL Utility Library, which contains convenience functions implemented on top of the OpenGL API.\n\nOpenGL GLUT Reference describes the OpenGL Utility Toolkit, a cross-platform windowing API.\n\nOpenGL API Code and Tutorial Listings provides code examples for fundamental tasks, such as modeling and texture mapping, as well as for advanced techniques, such as high dynamic range rendering (HDRR).\n\nNext\n\n\n\n\n\nCopyright © 2004, 2018 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2018-06-04"
  },
  {
    "title": "Adding Compositions to Webpages and Widgets",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposer/qc_webkit/qc_webkit.html#//apple_ref/doc/uid/TP40001357-CH3-SW6",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer Programming Guide\nTable of Contents\nIntroduction\nUsing QCView to Create a Standalone Composition\nPublishing Ports and Binding Them to Controls\nUsing the QCRenderer Class to Play a Composition\nAdding Compositions to Webpages and Widgets\nRevision History\nNext\nPrevious\nRetired Document\n\nImportant: This document may not represent best practices for current development. Links to downloads and other resources may no longer be valid.\n\nAdding Compositions to Webpages and Widgets\n\nThe Quartz Composer WebKit plug-in, available in OS X v10.4.7 or later, lets you include Quartz Composer compositions in a webpage or Dashboard widget. The plug-in is a WebKit–style Internet plug-in, available to WebKit–based browsers, such as Safari, OmniWeb, and Shiira. It’s also available to Dashboard widgets, because Dashboard uses the WebKit to render the HTML, CSS, and JavaScript files that make widgets.\n\nThis chapter shows how to use the WebKit plug-in and how to manipulate a composition using JavaScript. It also describes the HTML attributes specific to the plug-in and discusses which Quartz Composer patches are not allowed in a composition rendered by a WebKit plug-in.\n\nIncluding a Composition in an HTML File\n\nTo add a Quartz Composer composition to a webpage or Dashboard widget, you need to include it in an HTML file using the <embed> tag, as shown in Listing 4-1.\n\nListing 4-1  The <embed> tag for a composition\n\n<embed type=\"application/x-quartzcomposer\"\n\n\n       src=\"my_composition.qtz\"\n\n\n       id=\"myComposition\"\n\n\n       width=\"300px\"\n\n\n       height=\"150px\"\n\n\n       opaque=\"false\">\n\n\n</embed>\n\nSome of the attributes included with the <embed> tag in Listing 4-1 are standard attributes found in many HTML elements. Two of the attributes in Listing 4-1 are necessary for the plug-in to be loaded properly:\n\ntype tells the browser loading this element that the content to be displayed is a Quartz Composer composition. To render a Quartz Composer composition, set this attribute to application/x-quartzcomposer. It is important to include this attribute because without it, a browser may try to use another plug-in to render your composition.\n\nsrc is the uniform resource locator, or URL, to your composition. The URL can specify either a path relative to the HTML file that contains the <embed> tag or a fully qualified path.\n\nThe opaque attribute is optional and specifies whether your composition uses transparency. By default, it is set to true. If your composition uses transparency (that is, the composition is not completely opaque), make sure that you set the attribute to false.\n\nSee HTML Attributes for the Quartz Composer WebKit Plug-in for a description of other HTML attributes that are available.\n\nNote: Certain Quartz Composer patches access local files, services, or hardware. Access to these items from the Quartz Composer WebKit plug-in is not allowed. To see which patches are allowed within a composition, read Allowed and Disallowed Patches.\n\nIf your composition is self-contained and doesn’t require programmatic interaction through JavaScript, you’re done after you include the composition in your HTML. If your composition has changeable parameters or needs to be controlled through JavaScript, read Manipulating a Composition Using JavaScript.\n\nManipulating a Composition Using JavaScript\n\nThe Quartz Composer WebKit plug-in allows for manipulating a composition using JavaScript, as shown in Listing 4-2.\n\nListing 4-2  Changing a composition’s values via JavaScript\n\nvar composition = document.getElementById(\"myComposition\");\n\n\nif (composition.playing() == true)\n\n\n{\n\n\n    composition.pause();\n\n\n    composition.setInputValue(\"aPublishedInput\", aValue);\n\n\n    composition.setInputValue(\"anotherPublishedInput\", anotherValue);\n\n\n    composition.play();\n\n\n}\n\nThe code in Listing 4-2 gets a composition from the DOM (Document Object Model) and queries its playback status. If the composition is playing, the code pauses playback. Then it modifies some of the composition input values. Finally, the code resumes playback.\n\nNote: It is not necessary to pause a composition to change its input values.\n\nYou need to get the composition from the DOM because it returns the composition object that you can manipulate. Note that the element identifier is the same as the id attribute value for the composition included in Listing 4-1. You perform the rest of these actions using JavaScript methods. To learn more about the JavaScript API for Quartz Composer WebKit plug-ins, read Quartz Composer WebKit Plug-in JavaScript Reference.\n\nHTML Attributes for the Quartz Composer WebKit Plug-in\n\nThe Quartz Composer WebKit plug-in provides HTML attributes that affect how a composition is rendered and triggers JavaScript functions for certain events.\n\nabove\n\nA Boolean value that specifies whether the composition should render above or below the window that displays a webpage. If above is set to true, no content on the page within the bounds of the composition is shown. If above is set to false, all content on the page within the composition’s bounds is shown over the composition.\n\nThis setting is most useful for Dashboard widgets, where a value of false lets you place elements over a composition.\n\nThe default value, used if this attribute is not specified, is true.\n\nautoplay\n\nA Boolean value that specifies whether the composition is played automatically when it’s loaded. If autoplay is set to true, the composition begins playback when it has completed loading. If autoplay is set to false, it doesn’t play automatically.\n\nThe default value, used if this attribute is not specified, is true.\n\nopaque\n\nA Boolean value that specifies whether the composition is opaque or transparent. If opaque is set to true, the composition is rendered with no alpha channel, meaning any transparent areas in the composition are rendered as solid and any elements placed under the composition are not visible. If opaque is set to false, the composition is rendered with an alpha channel, meaning that transparent areas in the composition are rendered using any transparency found in the composition and any elements placed under the composition are visible through transparent areas in the composition.\n\nThe default value, used if this attribute is not specified, is true.\n\nmaxfps\n\nAn integer value that specifies the maximum frame rate used when rendering the composition.\n\nThe default value, used if this attribute is not specified, is 30.\n\nonload\n\nA JavaScript event handler called when the composition is finished loading. The JavaScript function you provide for this event is not passed any arguments when it is called.\n\nonloading\n\nA JavaScript event handler called when the composition is in the process of loading. The JavaScript function you provide for this event is passed one argument when it is called; the argument is a float value between 0 and 1 that indicates loading progress.\n\nThe default loading animation will be used if this attribute is not specified.\n\nonerror\n\nA JavaScript event handler called if the composition fails to load. The JavaScript function you provide for this event is passed one argument when it is called; the argument contains a string representing the reason the composition failed to load.\n\nAllowed and Disallowed Patches\n\nThe Quartz Composer WebKit plug-in renders most, but not all, the patches available in Quartz Composer. You are not allowed to use patches that:\n\nAccess information from local files and folders, such as the Folder Images and Spotlight Images patches\n\nFetch information from services, such as the Bonjour Services, Host Info, RSS Feed, and Image Downloader patches\n\nCommunicate with certain hardware, such as the MIDI Clock, MIDI Controllers, MIDI Notes, and Audio Input patches\n\nFor detailed information about Quartz Composer patches and WebKit, see Technical Q&A QA1505 Availability of Quartz Composer Patches in Web Kit.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2013 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2013-04-23"
  },
  {
    "title": "Document Revision History",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposer_Patch_PlugIn_ProgGuide/RevisionHistory.html#//apple_ref/doc/uid/TP40004787-CH2-DontLinkElementID_16",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer Custom Patch Programming Guide\nTable of Contents\nIntroduction\nThe Basics of Custom Patches\nWriting Processor Patches\nWriting Image Processing Patches\nWriting Consumer Patches\nGlossary\nRevision History\nPrevious\nDocument Revision History\n\nThis table describes the changes to Quartz Composer Custom Patch Programming Guide.\n\nDate\tNotes\n2010-03-24\t\n\nUpdate example code to new initializer pattern.\n\n\n2007-12-11\t\n\nFixed a minor technical error.\n\n\n2007-10-31\t\n\nNew document that describes how to create custom patches for distribution.\n\n\n\nPrevious\n\n\n\n\n\nCopyright © 2010 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2010-03-24\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Tutorial: Creating a Composition",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposerUserGuide/qc_tutorial/qc_tutorial.html#//apple_ref/doc/uid/TP40005381-CH213-SW3",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer User Guide\nTable of Contents\nIntroduction\nQuartz Composer Basic Concepts\nThe Quartz Composer User Interface\nBasic and Advanced Tasks, Tips, and Tricks\nTutorial: Creating a Composition\nGlossary\nRevision History\nNext\nPrevious\nTutorial: Creating a Composition\n\nCreating a composition with Quartz Composer supplied in OS X v10.5 is easy. You need only to add patches to a workspace and connect them. What’s more, you can get instant feedback on your composition as you add patches, allowing you to tune your composition as you create it.\n\nThis chapter is a tutorial that provides step-by-step instructions for creating a glow filter and applying the filter to a rotating cube. After you create the composition, you’ll see how to use it as a screen saver and turn it into a QuickTime movie.\n\nCreating a Glow Filter\n\nThis section shows how to use Quartz Composer to create a composition that renders a textured, rotating cube and then to apply a glow effect to the cube. By following the steps in this section, you’ll learn to:\n\nCreate a hierarchical composition that uses macro patches\n\nSet up consumer patches so they evaluate in the correct order\n\nBefore you start creating the composition, you’ll learn what a glow filter is and how it can be implemented as a Quartz composition. Then, you’ll create your own glowing cube composition by following these steps:\n\nSetting Up a Rotating Cube\n\nAdding a Background Color\n\nCreating a Render in Image Macro\n\nApplying a Gaussian Blur\n\nIncreasing the Glow Effect\n\nGlow Filter Overview\n\nA glow filter creates a diffuse lighting effect around the edges of an object to make the object appear as if it is glowing. This standard effect appears in many image processing applications. Creating an effect like this in OS X v10.3 and earlier requires many lines of code. Quartz Composer doesn’t require any code to create this and other effects; you simply assemble and connect patches.\n\nA glow filter achieves its effect by adding together an image and a blurred copy of the image. The addition operation brightens the highlights in an image to create an area of diffuse brightening along the edges in the image. The beach scene on the right of Figure 4-1 is the result of adding the scene on the left to a blurred copy of the scene.\n\nFigure 4-1  A beach scene without (left) and with (right) the glow effect applied\n\nFigure 4-2 shows the Quartz composition for creating a glow effect on a bitmap image. The composition uses four patches:\n\nImage Importer generates a bitmap image from a JPEG, TIFF, PNG, BMP, GIF, or PDF file.\n\nGaussian Blur blurs an image by the amount specified by a radius.\n\nAddition adds the blurred image to the bitmap image and provides the result as an output parameter.\n\nBillboard renders the image.\n\nFigure 4-2  A composition that creates a glow effect on an image\n\nNow that you know how a glow filter is implemented, it’s time to launch Quartz Composer and get started creating your own composition. Quartz Composer is located the following directory:\n\n/Developer/Applications/\n\nYou’ll need a texture file to complete the composition described in the rest of this chapter. If you don’t have one, you can use one of the image files in /Library/User Pictures.\n\nSetting Up a Rotating Cube\n\nQuartz Composer provides a Rotating Cube clip that renders a cube using an input image and a duration value. The image you supply will appear on the six faces of the cube. The duration controls the period of the cube’s rotation.\n\nFollow these steps to set up the rotating cube.\n\nLaunch Quartz Composer. Choose the Blank Composition template.\n\nIf you’ve turned off the option to choose a template, then choose File > New Blank.\n\nClick the Patch Creator button and type Rotating Cube in the search field.\n\nPress Return to add the Rotating Cube clip to the workspace as shown in Figure 4-3.\n\nWhen you add a clip to the workspace, Quartz Composer makes a copy of the clip and adds the copy to the workspace. This means that you can modify the added clip without affecting what’s in the Patch Creator.\n\nFigure 4-3  The Rotating Cube clip in the workspace\n\nIn the Finder, locate an image file that contains the texture to use for the faces of the cube. Then drag the image directly from the Finder to the workspace.\n\nThis example uses a brick image as a texture, but you can supply any image you’d like to appear on the faces of the cube.\n\nQuartz Composer automatically creates an instance of an Image Importer patch, copies the texture to the patch, and renames the patch to have the name of the image file.\n\nTip:  Make sure images are not larger than necessary. For example, avoid using a 1024 x 1024 texture when rendering to a destination that is smaller, such as 320 x 240.\n\nConnect the Image output to the Image input of the Rotating Cube as shown in Figure 4-4.\n\nTo create a connection, click the output port of one patch and then click the input port of another patch. (You’ll notice that you can’t create a connection in the opposite direction.)\n\nFigure 4-4  The Image connected to the Rotating Cube\n\nOpen the Input Parameters pane of the inspector for the Rotating Cube.\n\nThe default period value is 10, which means that the cube completes a full rotation every 10 seconds. Change the value to 1 and look at the viewer window to observe how the cube rotation speeds up. Then, change the value back to 10. As you can see, it is easy to change input parameter values.\n\nClick Viewer to bring it to the front.\n\nA brick-faced cube (see Figure 4-5) rotates. The background appears smeared. To correct that, you’ll need to clear the viewer prior to rendering each frame of the animation. You’ll do that next, in Adding a Background Color.\n\nFigure 4-5  The background smears\nAdding a Background Color\n\nSo far the composition is a rotating cube; Quartz Composer is rendering nothing except the cube. Because Quartz Composer doesn’t erase the viewing area between frames, you see smearing. In this section you’ll add a Clear patch so that you can not only eliminate the smearing, but you can also control the background color of the rendering destination.\n\nColors in Quartz Composer have an alpha component that lets you specify the opacity of the color. A fully transparent color has an opacity of 0 and a fully opaque one has an opacity of 1.\n\nNote: Opacity is also referred to as the alpha value, which is the term you’ll see if you use the Quartz 2D programming interface.\n\nFollow these steps to add a background color to the composition:\n\nUse the Patch Creator to find and add a Clear patch to the workspace.\n\nNotice in Figure 4-6 that Clear is numbered 2 and Rotating Cube is numbered 1. This means that the Rotating Cube is rendered first and then the rendering area is cleared, which is the wrong order for these operations. If you leave these patches in this rendering order, all you’ll see is the solid clear color.\n\nFigure 4-6  The number in the title bar of a consumer patch indicates its rendering order\n\nTo change the rendering order, click the number on the Clear patch and choose Layer 1 from the pop-up menu, as shown in Figure 4-7.\n\nFigure 4-7  The rendering layer for the Clear patch\n\nLook at the viewer window.\n\nThe cube rotates over a black background. The default color is black with an alpha value of 0.0. (An alpha value of 0.0 means 0% opacity or completely transparent.) You can change the color and the opacity in the Input Parameters pane of the inspector for the Clear patch by clicking the color well.\n\nCreating a Render in Image Macro\n\nThe Render in Image patch is a macro that renders what’s in it (that is, its subpatches) to an image. By rendering part of your composition into an image, you can then use that image as you would any image in Quartz Composer.\n\nTip: How do you tell the difference between a patch that is a macro and one that isn’t? Macro patches have square corners instead of rounded corners.\n\nThe steps in this section show how to use the Render in Image macro to convert the rotating cube into a reusable image. You’ll then be able to apply a glow effect to that image.\n\nUse the Patch Creator to find and add a Render in Image macro (see Figure 4-8) to the workspace.\n\nFigure 4-8  The Render in Image macro\n\nIn the Input Parameters pane of the inspector for the Render in Image macro, make sure the Image Dimensions are set to 0 x 0 pixels.\n\nThis is the default setting. When image dimensions are set to 0 x 0, Quartz Composer renders the image so that it has the same dimensions as the rendering destination. When you set the image dimensions to specific values, Quartz Composer forces the image to render to those dimensions.\n\nSelect all the patches except the Render in Image macro and choose Edit > Cut.\n\nDouble-click the Render in Image macro and choose Edit > Paste.\n\nYou’ve just moved all the patches into the Render in Image macro, creating another level in the composition.\n\nTake a look at the viewer window.\n\nNote that the composition no longer has a patch in the root macro patch level that draws to a rendering destination.\n\nClick Edit Parent to move up to the root macro patch level of your composition.\n\nUse the Patch Creator to find and add a Billboard patch (see Figure 4-9) to the root macro patch level of the workspace.\n\nThe Billboard patch renders a simple quad that takes the perspective of facing the viewer.\n\nFigure 4-9  A Billboard patch\n\nConnect the output port of the Render in Image macro to the Image input port of the Billboard patch.\n\nAt this point, you might notice that your composition does not fill the viewer window. In the next step, you’ll remedy that.\n\nLook at the Input Parameters pane of the inspector for the Billboard patch. Notice that the Width is set to 1.\n\nThe Width parameter specifies the width of the quad. Quartz Composer automatically computes the height of the quad based on the aspect ratio. Recall that the coordinate space is 2.0 units wide (see Figure 1-8). Because the current width is 1.0, the billboard quad uses half the width of the rendering area.\n\nFor this composition, you will make the billboard use the full width of the Quartz Composer coordinate space.\n\nIn the Input Parameters pane of the inspector for the Billboard patch, enter 2 in the Width text box.\n\nThe composition should now render to the full width of the viewer window.\n\nTake a look at the Viewer. You should see the rotating cube over a black background.\n\nApplying a Gaussian Blur\n\nA Gaussian Blur patch combines neighboring pixels to effectively produce a blurred image. (For those who want to know the details, the convolution kernel used to compute the blue represents the shape of a Gaussian distribution (also known as a bell curve.)\n\nRecall that the glow effect results from adding a blurred copy of an image to the original image. (See Figure 4-1.) To achieve the glow effect, you’ll use a Gaussian Blur patch to blur a copy of the rotating cube image. Then, you’ll use an Addition compositing filter patch to combine the two images.\n\nAdd the Gaussian Blur and Addition patches to the root macro patch level of the workspace.\n\nConnect the Image output port from the Render in Image macro to the Image input port of the Gaussian Blur patch and make another connection to the Background Image input port of the Addition patch.\n\nTip: To make multiple connections from the Image output port, hold the Command key as you click the output port and then each of the input ports.\n\nConnect the Image output port of the Gaussian Blur patch to the Image input port of the Addition patch.\n\nFinally, connect the Image output port of the Addition patch to the Image input port of the Billboard patch.\n\nThe composition should look as shown in Figure 4-10.\n\nFigure 4-10  A composition that produces a glowing, rotating cube\n\nTake a look at the composition in the Viewer.\n\nThe glow effect around the edges of the cube is subtle. The next section shows how to increase the effect.\n\nIncreasing the Glow Effect\n\nWith older video tube technology it was possible to turn up the brightness adjustment so much that the images on the screen would get little halos around them. Although this sort of behavior is undesirable when you want to view nice crisp images, it is exactly what you need to achieve a glow effect. In this section you’ll set up the digital equivalent of “turning up the brightness” by adding a Gamma Adjust patch.\n\nThe Gamma Adjust patch is usually used to compensate for the nonlinear effects of displays by adjusting midtone brightness. Adjusting the gamma effectively changes the slope of the transition between black and white.\n\nUsing the Gamma Adjust patch in this composition has nothing to do with display adjustment. It’s really a hack, but one that works quite well. When the gamma-adjusted image is then blurred, the rotating cube attains a halo that results in a more pleasing glow effect.\n\nFollow these steps to add the Gamma Adjust patch.\n\nUse the Patch Creator to find and add a Gamma Adjust patch (see Figure 4-11) to the root macro patch level of the workspace.\n\nFigure 4-11  The Gamma Adjust patch\n\nInsert the Gamma Adjust patch into the composition as shown in Figure 4-12.\n\nFigure 4-12  The Gamma Adjust patch added to the composition\n\nIn the Input Parameters pane of the inspector for the Gamma Adjust patch, move the Power slider until you achieve a pleasing glow effect.\n\nPublishing Ports\n\nPublished ports are available at the top level of a composition and, as you’ll see, provide easy access for manually changing their values. They become even more important as a way to programmatically control a composition. Later, in Making a Screen Saver and Turning a Composition into a QuickTime Movie, you’ll see how publishing ports makes your composition interactive. See Quartz Composer Programming Guide for information on how to programmatically change the values of published ports.\n\nYou’ll revise the composition so that it has two published input ports at the root macro patch level—one that controls the clear color and the other that controls the gamma value.\n\nDouble-click the Render in Image macro so that you can see the Clear and Rotating Cube patches.\n\nNotice that the Clear subpatch has a Clear Color input port. The default clear color is black, which is why the cube appears on a black background. It is difficult to change the color during runtime because the color input is buried in the subpatch.\n\nControl-click the Clear patch and choose Published Inputs > Clear Color, as shown in Figure 4-13.\n\nFigure 4-13  Publishing the Clear Color input port\n\nPress Return.\n\nNotice that the Clear Color input port appears gray to indicate it is published.\n\nSwitch to the root macro patch level.\n\nNotice that the Render in Image patch has an input parameter it didn’t have earlier—Clear Color. This input parameter is the one you just published from the Clear patch. When the Clear Color parameter for Render in Image is set, the color value is passed to the Clear patch.\n\nParameters published from lower levels are not available outside the composition unless you also publish them from the root macro patch. You’ll do that next.\n\nControl-click the Gamma Adjust patch and choose Published Inputs > Power.\n\nPress Return to keep the assigned name.\n\nControl-click the Render in Image patch and choose Published Inputs > Clear Color.\n\nPress Return to keep the assigned name.\n\nIn the viewer window, click Input Parameters.\n\nThe input ports that are published to the root macro patch level appear as a sheet in the viewer window, as shown in Figure 4-14. Published input ports are are also referred to as the input parameters of the composition. You can manipulate the controls to change the color and power values in real time, just as the background color in the figure was changed to gray by using the published color well.\n\nFigure 4-14  Published input ports as they appear in Quartz Composer\n\nIn the editor window, hover the pointer over the Clear Color input port for the Render in Image patch and note the published key name, as shown in Figure 4-15. Do the same thing for the Power input for the Gamma Adjust patch.\n\nFigure 4-15  The help tag for the Clear Color input port\n\nSave the composition and quit Quartz Composer.\n\nYou’ve successfully created a composition. Next you’ll see how to use it as a screen saver and then how to make it into a QuickTime movie.\n\nMaking a Screen Saver\n\nThere are two steps required for making a Quartz Composer a screen saver:\n\nCreate a composition.\n\nPut the composition in one of these folders:\n\n/Library/Screen Savers\n\n~/Library/Screen Savers\n\nThe composition appears as a screen saver in the Desktop & Screen Saver pane of System Preferences, as shown in Figure 4-16.\n\nFigure 4-16  A composition can be used as a screen saver\n\nIf the screen saver composition has input parameters, clicking the Options button displays a window for setting the parameter values. Those values are saved as user preferences.\n\nTip:  To open a screen saver composition in Quartz Composer from within System Preferences, select the screen saver composition, then click the Options button while pressing the Option key.\n\nTurning a Composition into a QuickTime Movie\n\nTurning a composition into a QuickTime Movie is a quick and easy way to let users view and interact with a composition. Follow these steps to make a QuickTime movie from a composition:\n\nWith the editor window active, choose File > Export as QuickTime Movie.\n\nClick Save and then, in the window that appears, modify the dimensions and duration if you need to.\n\nClick Export.\n\nWhen you open the exported movie in the Finder, QuickTime Movie Player launches.\n\nThe generated QuickTime movie does not contain a prerendered version of the composition but the composition itself, which is then rendered in real time by QuickTime. Such QuickTime movies can be only opened in OS X v10.5 or later.\n\nNote: If your composition uses resources that come from the Internet, such as an RSS feed, you won't be able to access those resources from within a QuickTime movie.\n\nAnother way to create a QuickTime movie from a Quartz composition is to open it in QuickTime Player and save it as a movie. When creating a QuickTime movie in this way, you can’t specify custom dimensions and duration; default values are used. You can change those defaults with the defaults command-line tool. For example, to specify a width of 1024, a height of 768, and a duration of 60 minutes, you enter these commands in the Terminal window:\n\ndefaults write NSGlobalDomain QuartzComposerDefaultMovieDuration 60\n\ndefaults write NSGlobalDomain QuartzComposerDefaultMovieWidth 1024\n\ndefaults write NSGlobalDomain QuartzComposerDefaultMovieHeight 768\n\nThe NSGlobalDomain defaults apply to all applications.\n\nIf you want to set default values for a specific application (for example, iMovie), you would enter command similar to the following:\n\ndefaults write com.apple.iMovie3 QuartzComposerDefaultMovieDuration 300\n\ndefaults write com.apple.iMovie3 QuartzComposerDefaultMovieWidth 640\n\ndefaults write com.apple.iMovie3 QuartzComposerDefaultMovieHeight 480\n\nTip:  In OS X v10.4 and later, any QuickTime-compatible application can open and play a composition. Your composition performs best if the opening application uses the QuickTime Kit and HIMovieView functions, both of which are available in OS X v10.4 and later.\n\nNext Steps\n\nIf you know how to write code, you may want to read the following:\n\nQuartz Composer Programming Guide shows how to use the Quartz Composer programming interface to load, play, and control compositions. It also provides instructions on how to use bindings in Interface Builder to create a standalone composition.\n\nQuartz Composer Custom Patch Programming Guide describes how to create your own patches that you can use in the Quartz Composer development environment.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2007 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2007-07-17\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Bitmap Images and Image Masks",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_images/dq_images.html#//apple_ref/doc/uid/TP30001066-CH212-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nBitmap Images and Image Masks\n\nBitmap images and image masks are like any drawing primitive in Quartz. Both images and image masks in Quartz are represented by the CGImageRef data type. As you’ll see later in this chapter, there are a variety of functions that you can use to create an image. Some of them require a data provider or an image source to supply bitmap data. Other functions create an image from an existing image either by copying the image or by applying an operation to the image. No matter how you create a bitmap image in Quartz, you can draw the image to any flavor of graphics context. Keep in mind that a bitmap image is an array of bits at a specific resolution. If you draw a bitmap image to a resolution-independent graphics context (such as a PDF graphics context) the bitmap is limited by the resolution at which you created it.\n\nThere is one way to create a Quartz image mask—by calling the function CGImageMaskCreate. You’ll see how to create one in Creating an Image Mask. Applying an image mask is not the only way to mask drawing. The sections Masking an Image with Color, Masking an Image with an Image Mask, and Masking an Image by Clipping the Context discuss all the masking methods available in Quartz.\n\nAbout Bitmap Images and Image Masks\n\nA bitmap image (or sampled image) is an array of pixels (or samples). Each pixel represents a single point in the image. JPEG, TIFF, and PNG graphics files are examples of bitmap images. Application icons are bitmap images. Bitmap images are restricted to rectangular shapes. But with the use of the alpha component, they can appear to take on a variety of shapes and can be rotated and clipped, as shown in Figure 11-1.\n\nFigure 11-1  Bitmap images\n\nEach sample in a bitmap contains one or more color components in a specified color space, plus one additional component that specifies the alpha value to indicate transparency. Each component can be from 1 to as many as 32 bits. In Mac OS X, Quartz also provides support for floating-point components. The supported formats in Mac OS X and iOS are described in “Pixel formats supported for bitmap graphics contexts”. ColorSync provides color space support for bitmap images.\n\nQuartz also supports image masks. An image mask is a bitmap that specifies an area to paint, but not the color. In effect, an image mask acts as a stencil to specify where to place color on the page. Quartz uses the current fill color to paint an image mask. An image mask can have a depth of 1 to 8 bits.\n\nBitmap Image Information\n\nQuartz supports a wide variety of image formats and has built-in knowledge of several popular formats. In iOS, the formats include JPEG, GIF, PNG, TIF, ICO, GMP, XBM, and CUR. Other bitmap image formats or proprietary formats require that you specify details about the image format to Quartz in order to ensure that images are interpreted correctly. The image data you supply to the function CGImageCreate must be interleaved on a per pixel, not a per scan line, basis. Quartz does not support planar data.\n\nThis section describes the information associated with a bitmap image. When you create and work with Quartz images (which use the CGImageRef data type), you’ll see that some Quartz image-creation functions require you to specify all this information, while other functions require a subset of this information. What you provide depends on the encoding used for the bitmap data, and whether the bitmap represents an image or an image mask.\n\nNote: For the best performance when working with raw image data, use the vImage framework. You can import image data to vImage from a CGImageRef reference with the vImageBuffer_InitWithCGImage function. For details, see Accelerate Release Notes.\n\nQuartz uses the following information when it creates a bitmap image (CGImageRef):\n\nA bitmap data source, which can be a Quartz data provider or a Quartz image source. Data Management in Quartz 2D describes both and discusses the functions that provide a source of bitmap data.\n\nAn optional decode array (Decode Array).\n\nAn interpolation setting, which is a Boolean value that specifies whether Quartz should apply an interpolation algorithm when resizing the image.\n\nA rendering intent that specifies how to map colors that are located within the destination color space of a graphics context. This information is not needed for image masks. See Setting Rendering Intent for more information.\n\nThe image dimensions.\n\nThe pixel format, which includes bits per component, bits per pixel, and bytes per row (Pixel Format).\n\nFor images, color spaces and bitmap layout (Color Spaces and Bitmap Layout) information to describe the location of alpha and whether the bitmap uses floating-point values. Image masks don’t require this information.\n\nDecode Array\n\nA decode array maps the image color values to other color values, which is useful for such tasks as desaturating an image or inverting the colors. The array contains a pair of numbers for each color component. When Quartz renders the image, it applies a linear transform to map the original component value to a relative number within the designated range appropriate for the destination color space. For example, the decode array for an image in the RGB color space contains six entries, one pair for each red, green, and blue color component.\n\nPixel Format\n\nThe pixel format consists of the following information:\n\nBits per component, which is the number of bits in each individual color component in a pixel. For an image mask, this value is the number of significant masking bits in a source pixel. For example, if the source image is an 8-bit mask, specify 8 bits per component.\n\nBits per pixel, which is the total number of bits in a source pixel. This value must be at least the number of bits per component times the number of components per pixel.\n\nBytes per row. The number of bytes per horizontal row in the image.\n\nColor Spaces and Bitmap Layout\n\nTo ensure that Quartz correctly interprets the bits of each pixel, you must specify:\n\nWhether a bitmap contains an alpha channel. Quartz supports RGB, CMYK, and gray color spaces. It also supports alpha, or transparency, although alpha information is not available in all bitmap image formats. When it is available, the alpha component can be located in either the most significant bits of a pixel or the least significant bits.\n\nFor bitmaps that have an alpha component, whether the color components are already multiplied by the alpha value. Premultiplied alpha describes a source color whose components are already multiplied by an alpha value. Premultiplying speeds up the rendering of an image by eliminating an extra multiplication operation per color component. For example, in an RGB color space, rendering an image with premultiplied alpha eliminates three multiplication operations (red times alpha, green times alpha, and blue times alpha) for each pixel in the image.\n\nThe data format of the samples—integer or floating-point values.\n\nWhen you create an image using the function CGImageCreate, you supply a bitmapInfo parameter, of type CGImageBitmapInfo, to specify bitmap layout information. The following constants specify the location of the alpha component and whether the color components are premultiplied:\n\nkCGImageAlphaLast—the alpha component is stored in the least significant bits of each pixel, for example, RGBA.\n\nkCGImageAlphaFirst—the alpha component is stored in the most significant bits of each pixel, for example, ARGB.\n\nkCGImageAlphaPremultipliedLast—the alpha component is stored in the least significant bits of each pixel, and the color components have already been multiplied by this alpha value.\n\nkCGImageAlphaPremultipliedFirst—the alpha component is stored in the most significant bits of each pixel, and the color components have already been multiplied by this alpha value.\n\nkCGImageAlphaNoneSkipLast—there is no alpha component. If the total size of the pixel is greater than the space required for the number of color components in the color space, the least significant bits are ignored.\n\nkCGImageAlphaNoneSkipFirst—there is no alpha component. If the total size of the pixel is greater than the space required for the number of color components in the color space, the most significant bits are ignored.\n\nkCGImageAlphaNone—equivalent to kCGImageAlphaNoneSkipLast.\n\nYou use the constant kCGBitmapFloatComponents to indicate a bitmap format that uses floating-point values. For floating-point formats, you logically OR this constant with the appropriate constant from the previous list. For example, for a 128 bits per pixel floating-point format that uses premultiplied alpha, with the alpha located in the least significant bits of each pixel, you supply the following information to Quartz:\n\nkCGImageAlphaPremultipliedLast|kCGBitmapFloatComponents\n\nFigure 11-2 visually depicts how pixels are represented in CMYK and RGB color spaces that use 16- or 32-bit integer formats. The 32-bit integer pixel formats use 8 bits per component. The 16-bit integer format uses 5 bits per component. Quartz 2D also supports 128-bit floating-point pixel formats that use 32 bits per component. The 128-bit formats are not shown in the figure.\n\nFigure 11-2  32-bit and 16-bit pixel formats for CMYK and RGB color spaces in Quartz 2D\nCreating Images\n\nTable 11-1 lists the functions that Quartz provides to create CGImage objects. The choice of image creation function depends on the source of the image data. The most flexible function is CGImageCreate. It creates an image from any kind of bitmap data. However, it’s the most complex function to use because you must specify all bitmap information. To use this function, you need to be familiar with the topics discussed in Bitmap Image Information.\n\nIf you want to create a CGImage object from an image file that uses a standard image format such as PNG or JPEG, the easiest solution is to call the function CGImageSourceCreateWithURL to create an image source and then call the function CGImageSourceCreateImageAtIndex to create an image from the image data at a specific index in the image source. If the original image file contains only one image, then provide 0 as the index. If the image file format supports files that contain multiple images, you need to supply the index to the appropriate image, keeping in mind that the index values start at 0.\n\nIf you’ve drawn content to a bitmap graphics context and want to capture that drawing to a CGImage object, call the function CGBitmapContextCreateImage.\n\nSeveral functions are utilities that operate on existing images, either to make a copy, create a thumbnail, or create an image from a portion of a larger one. Regardless of how you create a CGImage object, you use the function CGContextDrawImage to draw the image to a graphics context. Keep in mind that CGImage objects are immutable. When you no longer need a CGImage object, release it by calling the function CGImageRelease.\n\nTable 11-1  Functions for creating images\n\nFunction\n\n\t\n\nDescription\n\n\n\n\nCGImageCreate\n\n\t\n\nA flexible function for creating an image. You must specify all the bitmap information that is discussed in Bitmap Image Information.\n\n\n\n\nCGImageSourceCreateImageAtIndex\n\n\t\n\nCreates an image from an image source. Image sources can contain more than one image. See Data Management in Quartz 2D for information on creating an image source.\n\n\n\n\nCGImageSourceCreateThumbnailAtIndex\n\n\t\n\nCreates a thumbnail image of an image that is associated with an image source. See Data Management in Quartz 2D for information on creating an image source.\n\n\n\n\nCGBitmapContextCreateImage\n\n\t\n\nCreates an image by copying the bits from a bitmap graphics context.\n\n\n\n\nCGImageCreateWithImageInRect\n\n\t\n\nCreates an image from the data contained within a sub-rectangle of an image.\n\n\n\n\nCGImageCreateCopy\n\n\t\n\nA utility function that creates a copy of an image.\n\n\n\n\nCGImageCreateCopyWithColorSpace\n\n\t\n\nA utility function that creates a copy of an image and replaces its color space.\n\nThe sections that follow discuss how to create:\n\nA subimage from an existing image\n\nAn image from a bitmap graphics context\n\nYou can consult these sources for additional information:\n\nData Management in Quartz 2D discusses how to read and write image data.\n\nCGImage Reference, CGImageSource Reference, and CGBitmapContext Reference for further information on the functions listed in Table 11-1 and their parameters.\n\nCreating an Image From Part of a Larger Image\n\nThe function CGImageCreateWithImageInRect lets you create a subimage from an existing Quartz image. Figure 11-3 illustrates extracting an image that contains the letter “A” from a larger image by supplying a rectangle that specifies the location of the letter “A”.\n\nFigure 11-3  A subimage created from a larger image\n\nThe image returned by the function CGImageCreateWithImageInRect retains a reference to the original image, which means you can release the original image after calling this function.\n\nFigure 11-4 shows another example of extracting a portion of an image to create another image. In this case, the rooster’s head is extracted from the larger image, and then drawn to a rectangle that’s larger than the subimage, effectively zooming in on the image.\n\nListing 11-1 shows code that creates and then draws the subimage. The rectangle that the function CGContextDrawImage draws the rooster’s head to has dimensions that are twice the dimensions of the extracted subimage. The listing is a code fragment. You’d need to declare the appropriate variables, create the rooster image, and dispose of the rooster image and the rooster head subimage. Because the code is a fragment, it does not show how to create the graphics context that the image is drawn to. You can use any flavor of graphics context that you’d like. For examples of how to create a graphics context, see Graphics Contexts.\n\nFigure 11-4  An image, a subimage taken from it and drawn so it’s enlarged\n\nListing 11-1  Code that creates a subimage and draws it enlarged\n\nmyImageArea = CGRectMake (rooster_head_x_origin, rooster_head_y_origin,\n\n\n                            myWidth, myHeight);\n\n\nmySubimage = CGImageCreateWithImageInRect (myRoosterImage, myImageArea);\n\n\nmyRect = CGRectMake(0, 0, myWidth*2, myHeight*2);\n\n\nCGContextDrawImage(context, myRect, mySubimage);\nCreating an Image from a Bitmap Graphics Context\n\nTo create an image from an existing bitmap graphics context, you call the function CGBitmapContextCreateImage as follows:\n\nCGImageRef myImage;\n\n\nmyImage = CGBitmapContextCreateImage (myBitmapContext);\n\nThe CGImage object returned by the function is created by a copy operation. Therefore any subsequent changes you make to the bitmap graphics context do not affect the contents of the returned CGImage object. In some cases the copy operation actually follows copy-on-write semantics, so that the actual physical copy of the bits occurs only if the underlying data in the bitmap graphics context is modified. You may want to use the resulting image and release it before you perform additional drawing into the bitmap graphics context so that you can avoid the actual physical copy of the data.\n\nFor an example that shows how to create a bitmap graphics context, seeCreating a Bitmap Graphics Context.\n\nCreating an Image Mask\n\nA Quartz bitmap image mask is used the same way an artist uses a silkscreen. A bitmap image mask determines how color is transferred, not which colors are used. Each sample value in the image mask specifies the amount that the current fill color is masked at a specific location. The sample value specifies the opacity of the mask. Larger values represent greater opacity and specify locations where Quartz paints less color. You can think of the sample value as an inverse alpha value. A value of 1 is transparent and 0 is opaque.\n\nImage masks are 1, 2, 4, or 8 bits per component. For a 1-bit mask, a sample value of 1 specifies sections of the mask that block the current fill color. A sample value of 0 specifies sections of the mask that show the current fill color of the graphics state when the mask is painted. You can think of a 1-bit mask as black and white; samples either completely block paint or completely allow paint.\n\nImage masks that have 2, 4, or 8 bits per component represent grayscale values. Each component maps to a range of 0 to 1 using the following formula:\n\nFor example, a 4-bit mask has values that range from 0 to 1 in increments of 1/15 . Component values that are 0 or 1 represent the extremes—completely block paint and completely allow paint. Values between 0 and 1 allow partial painting using the formula 1 – MaskSampleValue. For example, if the sample value of an 8-bit mask scales to 0.7, color is painted as if it had an alpha value of (1 – 0.7), which is 0.3.\n\nThe function CGImageMaskCreate creates a Quartz image mask from bitmap image information that you supply and that is discussed in Bitmap Image Information. The information you supply to create an image mask is the same as what you supply to create an image, except that you do not supply color space information, a bitmap information constant, or a rendering intent, as you can see by looking at the function prototype in Listing 11-2.\n\nListing 11-2  The prototype for the function CGImageMaskCreate\n\nCGImageRef CGImageMaskCreate (\n\n\n        size_t width,\n\n\n        size_t height,\n\n\n        size_t bitsPerComponent,\n\n\n        size_t bitsPerPixel,\n\n\n        size_t bytesPerRow,\n\n\n        CGDataProviderRef provider,\n\n\n        const CGFloat decode[],\n\n\n        bool shouldInterpolate\n\n\n);\nMasking Images\n\nMasking techniques can produce many interesting effects by controlling which parts of an image are painted. You can:\n\nApply an image mask to an image. You can also use an image as a mask to achieve an effect that’s opposite from applying an image mask.\n\nUse color to mask parts of an image, which includes the technique referred to as chroma key masking.\n\nClip a graphics context to an image or image mask, which effectively masks an image (or any kind of drawing) when Quartz draws the content to the clipped context.\n\nMasking an Image with an Image Mask\n\nThe function CGImageCreateWithMask returns the image that’s created by applying an image mask to an image. This function takes two parameters:\n\nThe image you want to apply the mask to. This image can’t be an image mask or have a masking color (see Masking an Image with Color) associated with it.\n\nAn image mask created by calling the function CGImageMaskCreate. It’s possible to provide an image instead of an image mask, but that gives a much different result. See Masking an Image with an Image.\n\nSource samples of an image mask act as an inverse alpha value. An image mask sample value (S):\n\nEqual to 1 blocks painting the corresponding image sample.\n\nEqual to 0 allows painting the corresponding image sample at full coverage.\n\nGreater than 0 and less 1 allows painting the corresponding image sample with an alpha value of (1 – S).\n\nFigure 11-5 shows an image created with one of the Quartz image-creation functions and Figure 11-6 shows an image mask created with the function CGImageMaskCreate. Figure 11-7 shows the image that results from calling the function CGImageCreateWithMask to apply the image mask to the image.\n\nFigure 11-5  The original image\nFigure 11-6  An image mask\n\nNote that the areas in the original image that correspond to the black areas of the mask show through in the resulting image (Figure 11-7). The areas that correspond to the white areas of the mask aren’t painted. The areas that correspond to the gray areas in the mask are painted using an intermediate alpha value that’s equal to 1 minus the image mask sample value.\n\nFigure 11-7  The image that results from applying the image mask to the original image\nMasking an Image with an Image\n\nYou can use the function CGImageCreateWithMask to mask an image with another image, rather than with an image mask. You would do this to achieve an effect opposite of what you get when you mask an image with an image mask. Instead of passing an image mask that’s created using the function CGImageMaskCreate, you supply an image created from one of the Quartz image-creation functions.\n\nSource samples of an image that is used as a mask (but is not a Quartz image mask) operate as alpha values. An image sample value (S):\n\nEqual to 1 allows painting the corresponding image sample at full coverage.\n\nEqual to 0 blocks painting the corresponding image sample.\n\nGreater than 0 and less 1 allows painting the corresponding image sample with an alpha value of S.\n\nFigure 11-8 shows the image that results from calling the function CGImageCreateWithMask to apply the image shown in Figure 11-6 to the image shown in Figure 11-5. In this case, assume that the image shown in Figure 11-6 is created using one of the Quartz image-creation functions, such as CGImageCreate. Compare Figure 11-8 with Figure 11-7 to see how the same sample values, when used as image samples instead of image mask samples, achieve the opposite effect.\n\nThe areas in the original image that correspond to the black areas of the image aren’t painted in the resulting image (Figure 11-8). The areas that correspond to the white areas are painted. The areas that correspond to the gray areas in the mask are painted using an intermediate alpha value that’s equal to the masking image sample value.\n\nFigure 11-8  The image that results from masking the original image with an image\nMasking an Image with Color\n\nThe function CGImageCreateWithMaskingColors creates an image by masking one color or a range of colors in an image supplied to the function. Using this function, you can perform chroma key masking similar to what’s shown in Figure 11-9 or you can mask a range of colors, similar to what’s shown in Figure 11-11, Figure 11-12, and Figure 11-13.\n\nThe function CGImageCreateWithMaskingColors takes two parameters:\n\nAn image that is not an image mask and that is not the result of applying an image mask or masking color to another image.\n\nAn array of color components that specify a color or a range of colors for the function to mask in the image.\n\nFigure 11-9  Chroma key masking\n\nThe number of elements in the color component array must be equal to twice the number of color components in the color space of the image. For each color component in the color space, supply a minimum value and a maximum value that specifies the range of colors to mask. To mask only one color, set the minimum value equal to the maximum value. The values in the color component array are supplied in the following order:\n\n{min[1], max[1], ... min[N], max[N]}, where N is the number of components.\n\nIf the image uses integer pixel components, each value in the color component array must be in the range [0 .. 2^bitsPerComponent - 1] . If the image uses floating-point pixel components, each value can be any floating-point number that is a valid color component.\n\nAn image sample is not painted if its color values fall in the range:\n\n{c[1], ... c[N]}\n\nwhere min[i] <= c[i] <= max[i] for 1 <= i <= N\n\nAnything underneath the unpainted samples, such as the current fill color or other drawing, shows through.\n\nThe image of two tigers, shown in Figure 11-10, uses an RGB color space that has 8 bits per component. To mask a range of colors in this image, you supply minimum and maximum color component values in the range of 0 to 255.\n\nFigure 11-10  The original image\n\nListing 11-3 shows a code fragment that sets up a color components array and supplies the array to the function CGImageCreateWithMaskingColors to achieve the result shown in Figure 11-11.\n\nListing 11-3  Masking light to mid-range brown colors in an image\n\nCGImageRef myColorMaskedImage;\n\n\nconst CGFloat myMaskingColors[6] = {124, 255,  68, 222, 0, 165};\n\n\nmyColorMaskedImage = CGImageCreateWithMaskingColors (image,\n\n\n                                        myMaskingColors);\n\n\nCGContextDrawImage (context, myContextRect, myColorMaskedImage);\nFigure 11-11  An image with light to midrange brown colors masked out\n\nListing 11-4 shows another code fragment that operates on the image shown in Figure 11-10 to get the results shown in Figure 11-12. This example masks a darker range of colors.\n\nListing 11-4  Masking shades of brown to black\n\nCGImageRef myMaskedImage;\n\n\nconst CGFloat myMaskingColors[6] = { 0, 124, 0, 68, 0, 0 };\n\n\nmyColorMaskedImage = CGImageCreateWithMaskingColors (image,\n\n\n                                        myMaskingColors);\n\n\nCGContextDrawImage (context, myContextRect, myColorMaskedImage);\nFigure 11-12  A image after masking colors from dark brown to black\n\nYou can mask colors in an image as well as set a fill color to achieve the effect shown in Figure 11-13 in which the masked areas are replaced with the fill color. Listing 11-5 shows the code fragment that generates the image shown in Figure 11-13.\n\nListing 11-5  Masking a range of colors and setting a fill color and\n\nCGImageRef myMaskedImage;\n\n\nconst CGFloat myMaskingColors[6] = { 0, 124, 0, 68, 0, 0 };\n\n\nmyColorMaskedImage = CGImageCreateWithMaskingColors (image,\n\n\n                                        myMaskingColors);\n\n\nCGContextSetRGBFillColor (myContext, 0.6373,0.6373, 0, 1);\n\n\nCGContextFillRect(context, rect);\n\n\nCGContextDrawImage(context, rect, myColorMaskedImage);\nFigure 11-13  An image drawn after masking a range of colors and setting a fill color\nMasking an Image by Clipping the Context\n\nThe function CGContextClipToMask maps a mask into a rectangle and intersects it with the current clipping area of the graphics context. You supply the following parameters:\n\nThe graphics context you want to clip.\n\nA rectangle to apply the mask to.\n\nAn image mask created by calling the function CGImageMaskCreate. You can supply an image instead of an image mask to achieve an effect opposite of what you get by supplying an image mask. The image must be created with a Quartz image creation function, but it cannot be the result of applying a mask or masking color to another image.\n\nThe resulting clipped area depends on whether you provide an image mask or an image to the function CGContextClipToMask. If you supply an image mask, you get results similar to those described in Masking an Image with an Image Mask, except that the graphics context is clipped. If you supply an image, the graphics context is clipped similar to what’s described in Masking an Image with an Image.\n\nTake a look at Figure 11-14. Assume it is an image mask created by calling the function CGImageMaskCreate and then the mask is supplied as a parameter to the function CGContextClipToMask. The resulting context allows painting to the black areas, does not allow painting to the white areas, and allows painting to the gray area with an alpha value of 1–S, where S is the sample value of the image masks. If you draw an image to the clipped context using the function CGContextDrawImage, you’ll get a result similar to that shown in Figure 11-15.\n\nFigure 11-14  A masking image\nFigure 11-15  An image drawn to a context after clipping the content with an image mask\n\nWhen the masking image is treated as an image, you get the opposite result, as shown in Figure 11-16.\n\nFigure 11-16  An image drawn to a context after clipping the content with an image\nUsing Blend Modes with Images\n\nYou can use Quartz 2D blend modes (see Setting Blend Modes) to composite two images or to composite an image over any content that’s already drawn to the graphic context. This section discusses compositing an image over a background drawing.\n\nThe general procedure for compositing an image over a background is as follows:\n\nDraw the background.\n\nSet the blend mode by calling the function CGContextSetBlendMode with one of the blend mode constants. (The blend modes are based upon those defined in the PDF Reference, Fourth Edition, Version 1.5, Adobe Systems, Inc.)\n\nDraw the image you want to composite over the background by calling the function CGContextDrawImage.\n\nThis code fragment composites one image over a background using the “darken” blend mode:\n\nCGContextSetBlendMode (myContext, kCGBlendModeDarken);\n\n\nCGContextDrawImage (myContext, myRect, myImage2);\n\nThe rest of this section uses each of the blend modes available in Quartz to draw the image shown on the right side of Figure 11-17 over the background that consists of the painted rectangles shown on the left side of the figure. In all cases, the rectangles are first drawn to the graphics context. Then, a blend mode is set by calling the function CGContextSetBlendMode, passing the appropriate blend mode constant. Finally, the image of the jumper is drawn to the graphics context.\n\nFigure 11-17  Background drawing (left) and foreground image (right)\nNormal Blend Mode\n\nNormal blend mode paints source image samples over background image samples. Normal blend mode is the default blend mode in Quartz. You only need to explicitly set normal blend mode if you are currently using another blend mode and want to switch to normal blend mode. You can set normal blend mode by passing the constant kCGBlendModeNormal to the function CGContextSetBlendMode or by restoring the graphics state (assuming the previous graphics state used normal blend mode) using the function CGContextRestoreGState.\n\nFigure 11-18 shows the result of using normal blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure. In this example, the image is drawn using an alpha value of 1.0, so the background is completely obscured by the image.\n\nFigure 11-18  Drawing an image over a background using normal blend mode\nMultiply Blend Mode\n\nMultiply blend mode multiplies source image samples with background image samples. The colors in the resulting image are at least as dark as either of the two contributing sample colors.\n\nYou specify multiply blend mode by passing the constant kCGBlendModeMultiply to the function CGContextSetBlendMode. Figure 11-19 shows the result of using multiply blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-19  Drawing an image over a background using multiply blend mode\nScreen Blend Mode\n\nScreen blend mode multiplies the inverse of the source image samples with the inverse of the background image samples to obtain colors that are at least as light as either of the two contributing sample colors.\n\nYou specify screen blend mode by passing the constant kCGBlendModeScreen to the function CGContextSetBlendMode. Figure 11-20 shows the result of using screen blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-20  Drawing an image over a background using screen blend mode\nOverlay Blend Mode\n\nOverlay blend mode either multiplies or screens the source image samples with the background image samples, depending on the color of the background samples. The result is to overlay the existing image samples while preserving the highlights and shadows of the background. The background color mixes with the source image to reflect the lightness or darkness of the background.\n\nYou specify overlay blend mode by passing the constant kCGBlendModeOverlay to the function CGContextSetBlendMode. Figure 11-21 shows the result of using overlay blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-21  Drawing an image over a background using overlay blend mode\nDarken Blend Mode\n\nDarken blend mode creates composite image samples by choosing the darker samples from the source image or the background. Source image samples that are darker than the background image samples replace the corresponding background samples.\n\nYou specify darken blend mode by passing the constant kCGBlendModeDarken to the function CGContextSetBlendMode. Figure 11-22 shows the result of using darken blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-22  Drawing an image over a background using darken blend mode\nLighten Blend Mode\n\nLighten blend mode creates composite image samples by choosing the lighter samples from the source image or the background. Source image samples that are lighter than the background image samples replace the corresponding background samples.\n\nYou specify lighten blend mode by passing the constant kCGBlendModeLighten to the function CGContextSetBlendMode. Figure 11-23 shows the result of using lighten blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-23  Drawing an image over a background using lighten blend mode\nColor Dodge Blend Mode\n\nColor dodge blend mode brightens background image samples to reflect the source image samples. Source image sample values that specify black remain unchanged.\n\nYou specify color dodge blend mode by passing the constant kCGBlendModeColorDodge to the function CGContextSetBlendMode. Figure 11-24 shows the result of using color dodge blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-24  Drawing an image over a background using color dodge blend mode\nColor Burn Blend Mode\n\nColor burn blend mode darkens background image samples to reflect the source image samples. Source image sample values that specify white remain unchanged.\n\nYou specify color burn blend mode by passing the constant kCGBlendModeColorBurn to the function CGContextSetBlendMode. Figure 11-25 shows the result of using color burn blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-25  Drawing an image over a background using color burn blend mode\nSoft Light Blend Mode\n\nSoft light blend mode either darkens or lightens colors, depending on the source image sample color. If the source image sample color is lighter than 50% gray, the background lightens, similar to dodging. If the source image sample color is darker than 50% gray, the background darkens, similar to burning. If the source image sample color is equal to 50% gray, the background does not change.\n\nImage samples that are equal to pure black or pure white produce darker or lighter areas, but do not result in pure black or white. The overall effect is similar to what you achieve by shining a diffuse spotlight on the source image.\n\nYou specify soft light blend mode by passing the constant kCGBlendModeSoftLight to the function CGContextSetBlendMode. Figure 11-26 shows the result of using soft light blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-26  Drawing an image over a background using soft light blend mode\nHard Light Blend Mode\n\nHard light blend mode either multiplies or screens colors, depending on the source image sample color. If the source image sample color is lighter than 50% gray, the background is lightened, similar to screening. If the source image sample color is darker than 50% gray, the background is darkened, similar to multiplying. If the source image sample color is equal to 50% gray, the source image does not change. Image samples that are equal to pure black or pure white result in pure black or white. The overall effect is similar to what you achieve by shining a harsh spotlight on the source image.\n\nYou specify hard light blend mode by passing the constant kCGBlendModeHardLight to the function CGContextSetBlendMode. Figure 11-27 shows the result of using hard light blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-27  Drawing an image over a background using hard light blend mode\nDifference Blend Mode\n\nDifference blend mode subtracts either the source image sample color from the background image sample color, or the reverse, depending on which sample has the greater brightness value. Source image sample values that are black produce no change; white inverts the background color values.\n\nYou specify difference blend mode by passing the constant kCGBlendModeDifference to the function CGContextSetBlendMode. Figure 11-28 shows the result of using difference blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-28  Drawing an image over a background using difference blend mode\nExclusion Blend Mode\n\nExclusion blend mode produces a lower-contrast version of the difference blend mode. Source image sample values that are black don’t produce a change; white inverts the background color values.\n\nYou specify exclusion blend mode by passing the constant kCGBlendModeExclusion to the function CGContextSetBlendMode. Figure 11-29 shows the result of using exclusion blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-29  Drawing an image over a background using exclusion blend mode\nHue Blend Mode\n\nHue blend mode uses the luminance and saturation values of the background with the hue of the source image. You specify hue blend mode by passing the constant kCGBlendModeHue to the function CGContextSetBlendMode. Figure 11-30 shows the result of using hue blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-30  Drawing an image over a background using hue blend mode\nSaturation Blend Mode\n\nSaturation blend mode uses the luminance and hue values of the background with the saturation of the source image. Pure gray areas don’t produce a change. You specify saturation blend mode by passing the constant kCGBlendModeSaturation to the function CGContextSetBlendMode. Figure 11-31 shows the result of using saturation blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-31  Drawing an image over a background using saturation blend mode\nColor Blend Mode\n\nColor blend mode uses the luminance values of the background with the hue and saturation values of the source image. This mode preserves the gray levels in the image. You specify color blend mode by passing the constant kCGBlendModeColor to the function CGContextSetBlendMode. Figure 11-32 shows the result of using color blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-32  Drawing an image over a background using color blend mode\nLuminosity Blend Mode\n\nLuminosity blend mode uses the hue and saturation of the background with the luminance of the source image to create an effect that is inverse to the effect created by the color blend mode.\n\nYou specify luminosity blend mode by passing the constant kCGBlendModeLuminosity to the function CGContextSetBlendMode. Figure 11-33 shows the result of using luminosity blend mode to paint the image shown in Figure 11-17 over the rectangles shown in the same figure.\n\nFigure 11-33  Drawing an image over a background using luminosity blend mode\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Writing Processor Patches",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposer_Patch_PlugIn_ProgGuide/WritingProcessorPatches/WritingProcessorPatches.html#//apple_ref/doc/uid/TP40004787-CH4-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer Custom Patch Programming Guide\nTable of Contents\nIntroduction\nThe Basics of Custom Patches\nWriting Processor Patches\nWriting Image Processing Patches\nWriting Consumer Patches\nGlossary\nRevision History\nNext\nPrevious\nWriting Processor Patches\n\nA custom processor patch is true to its name. It processes data in response to changes in the values of its input parameters. This chapter shows how to write a custom patch that processes strings and another that processes numeric values. You’ll use Objective-C 2.0 properties to define the input and output parameters. Then you’ll see how to modify the numeric value processor so that it uses internal settings. By using the template provided in Xcode, you’ll package each custom patch as a plug-in that the Quartz Composer development tool can recognize. Any custom patch included in a plug-in shows up in the Quartz Composer Patch Creator.\n\ndotCom: Creating Domain Names\n\nThe dotCom custom patch takes any string as input and appends .com to it. For example, if the input string is patch then the output string is patch.com. When packaged as a plug-in that’s loaded into the Quartz Composer development tool, the resulting custom patch looks like what’s shown in Figure 2-1. By creating this simple patch first, you’ll learn the critical parts of a custom patch and how to bundle them together.\n\nFigure 2-1  The dotCom custom patch\n\nFollow these steps to create the dotCom custom patch:\n\nOpen Xcode and choose File > New Project.\n\nIn the New Project window, choose Standard Apple Plug-ins > Quartz Composer Plug-in and click Next.\n\nEnter dotCom in the Project Name text field and click Finish.\n\nThe project opens with these files.\n\nOpen the dotComPlugin.h file.\n\nThe plug-in template automatically subclasses QCPlugIn. Declare two string properties to use as the input and output parameters. Recall that input parameter keys must start with input and output parameter keys must start with output. Your code should look as follows:\n\n#import <Quartz/Quartz.h>\n\n\n \n\n\n@interface dotComPlugIn : QCPlugIn\n\n\n{\n\n\n}\n\n\n@property(assign) NSString* inputString;\n\n\n@property(assign) NSString* outputString;\n\n\n@end\n\nClose the dotComPlugIn.h file.\n\nOpen the dotComPlugIn.m file.\n\nJust after the @implementation statement, add the following directives so that Quartz Composer handles the implementation of the parameters.\n\n@dynamic inputString, outputString;\n\nSupply a description for the custom patch.\n\nThis description is what the user will see in Quartz Composer. Modify the description to look as follows:\n\n#define    kQCPlugIn_Description  @\"Appends \\\".com\\\" to any string.\"\n\nNote that Xcode automatically defines the custom patch name based on the project name you supplied. In this case, the name is dotCom.\n\n#define    kQCPlugIn_Name  @\"dotCom\"\n\nYou can change the name if you’d like. This name shows up in the Patch Creator as the patch name. Although the name does not need to be unique, it’s best for the user if the patch name is both descriptive and unique.\n\nNext you’ll write the methods needed to implement the dotComPlugIn subclass. You do not need to modify the default attributes method supplied in the template, which should look as follows:\n\n+ (NSDictionary*) attributes\n\n\n{\n\n\n    return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n                kQCPlugIn_Name,QCPlugInAttributeNameKey,\n\n\n                kQCPlugIn_Description,QCPlugInAttributeDescriptionKey,\n\n\n                nil];\n\n\n}\n\nModify the attributesForPropertyPortWithKey: so that it returns a dictionary for each input and output parameter. Each dictionary must contain a value followed by its port attribute name key. If the port has a default value, the dictionary should contain the value followed by the default value key.\n\nThe port attribute key name is what appears as a label for the custom patch port in Quartz Composer. Essentially what you are doing is mapping the QCPlugIn subclass parameter keys to custom patch port names. You’ll also want to define a reasonable default value.\n\nFor the dotCom plug-in, modify the method so it looks as follows:\n\n+ (NSDictionary*) attributesForPropertyPortWithKey:(NSString*)key\n\n\n{\n\n\n    if([key isEqualToString:@\"inputString\"])\n\n\n        return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            @\"Name\", QCPortAttributeNameKey,\n\n\n            @\"mydomain\",  QCPortAttributeDefaultValueKey,\n\n\n            nil];\n\n\n    if([key isEqualToString:@\"outputString\"])\n\n\n        return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            @\"Name.com\", QCPortAttributeNameKey,\n\n\n            nil];\n\n\n    return nil;\n\n\n}\n\nMake sure the executionMode method returns kQCPlugInExecutionModeProcessor.\n\nThis is a processor patch—it takes input values, processes them, and outputs a value.\n\n+ (QCPlugInExecutionMode) executionMode\n\n\n{\n\n\n    return kQCPlugInExecutionModeProcessor;\n\n\n}\n\nMake sure the timeMode method returns kQCPlugInTimeModeNone.\n\nThe dotCom plug-in needs to execute only when the input string changes.\n\n+ (QCPlugInTimeMode) timeMode\n\n\n{\n\n\n    return kQCPlugInTimeModeNone;\n\n\n}\n\nThe execution method is where the processing takes place. For the dotCom custom patch, it is fairly straightforward. The method needs to append the string .com to whatever string it passed to the patch. The code should look as follows:\n\n- (BOOL) execute:(id<QCPlugInContext>)context\n\n\n            atTime:(NSTimeInterval)time\n\n\n            withArguments:(NSDictionary*)arguments\n\n\n{\n\n\n    self.outputString = [self.inputString stringByAppendingString:@\".com\"];\n\n\n \n\n\n    return YES;\n\n\n}\n\nNote that you use self.<propertyname> to access property values. Recall that you can only read the values of input parameters, but you can read and write the values of output parameters.\n\nSave and close the dotComPlugIn.m file.\n\nOpen the Info.plist file.\n\nNotice that Xcode automatically adds the following entry, which is required, in the dictionary.\n\n<key>QCPlugInClasses</key>\n\n\n<array>\n\n\n    <string>dotComPlugIn</string>\n\n\n</array>\n\nIf you add another custom patch to the project, you need to add the name of the QCPlugIn subclass as an entry here. However, you don’t need to add anything for the dotCom custom patch.\n\nIf you want, you can customize the bundle identifier before saving and closing the file.\n\nUnder Targets, choose Build & Copy. Then, click Build “Build & Copy” and Start from the Action pop-up menu.\n\nWhen you build using this option, Xcode copies the successfully built plug-in to ~/Library/Graphics/Quartz Composer Plug-Ins and launches the Quartz Composer development tool.\n\nAfter Quartz Composer launches, create a blank composition. Click Patch Creator and type dot in the Search field.\n\nDouble-click the dotCom name to create an instance in the Editor window. Then add instances of the Image With String, Clear, and Billboard patches to the editor window. Connect them as shown and make sure that the dotCom custom patch works as it should.\n\nThe output in the Viewer should look like this:\n\nMIDI2Color: Mapping MIDI Values to Colors\n\nMIDI (Musical Instrument Digital Interface) is a communication standard that allows electronic musical instruments to send signals to each other for the purpose of controlling, monitoring, and editing musical events (note on, note off, volume, synthesizer voice, and so on). The MIDI2Color custom patch that you’ll create in this section maps a numerical value, in the range of 0 to 127, to a color. Many MIDI values fall in the 0 to 127 range, such as MIDI note number and volume.\n\nThe idea behind the MIDI2Color custom patch is to map a particular pitch (C, C#/Db, D, D#/Eb,E, F, and so on) to a color and to map the octave that the pitch resides in to an alpha value. Low pitches produce the most opaque colors and high pitches produce the most transparent colors. In this way, you can create a composition that uses MIDI information to drive the colors used for graphical output. Figure 2-2 shows the spectrum of colors that the MIDI2Color custom patch produces. The lowest octaves are on the left, the highest on the right. The pitches range from C to B, starting at the bottom and increasing towards the top.\n\nFigure 2-2  The spectrum of RGBA colors produced by the MIDI2Color custom patch\n\nYou’ll see that many of the steps needed to create the MIDI2Color custom patch are the same as those used to create the dotCom custom patch.\n\nTo create the MIDI2Color custom patch, follow these steps:\n\nOpen Xcode and choose File > New Project.\n\nIn the New Project window, choose Standard Apple Plug-ins > Quartz Composer Plug-in and click Next.\n\nEnter MIDI2Color in the Project Name text field and click Finish.\n\nOpen the MIDI2ColorPlugin.h file.\n\nThe Xcode template automatically subclasses QCPlugIn. You need to add property declarations for an input parameter that is a numerical value and an output parameter that is a color. Recall from Table 1-1 that ports that take a numerical value are declared as properties whose data type is double. Ports that are for colors require a property whose data type is a CGColorRef.\n\nNote: CGColorRef is defined in the Quartz 2D programming interface (see CGColor Reference). If you are unfamiliar with Quartz colors and color spaces, you may want to take a look at the reference and read the chapter on colors in Quartz 2D Programming Guide.\n\nYou should also add an internal variable for a color space. Later, you’ll create a color space and save it in this variable to avoid creating and releasing the color space each time you need to output a different color. Although the color output by your custom patch may change, the color space remains the same for each instance of the patch.\n\nWhen you are done modifying the interface file, it should look as follows:\n\n#import <Quartz/Quartz.h>\n\n\n \n\n\n@interface MIDI2ColorPlugIn : QCPlugIn\n\n\n{\n\n\n    CGColorSpaceRef myColorSpace;\n\n\n}\n\n\n// Declare a property input port of type Index and with the key inputValue\n\n\n@property NSUInteger inputValue;\n\n\n// Declare a property input port of type \"Color\" and with the key outputColor\n\n\n@property(assign) CGColorRef outputColor;\n\n\n@end\n\nSave and close the MIDI2ColorPlugIn.h file.\n\nOpen the MIDI2ColorPlugin.m file.\n\nJust after the @implementation statement, add the following directives. Quartz Composer will handle their implementation.\n\n@dynamic inputValue, outputColor;\n\nDefine a name and description for the custom patch.\n\nThe name is already provided for you. You don’t need to change it. Modify the description as shown:\n\n#define    kQCPlugIn_Name @\"MIDI2Color\"\n\n\n#define    kQCPlugIn_Description  @\"Converts a MIDI value to a color with transparency.\"\n\nNext you’ll write the methods needed to implement the MIDI2ColorPlugIn subclass, starting with the attributes method. The implementation provided by the template should look as follows and doesn’t need any modification:\n\n+ (NSDictionary*) attributes\n\n\n{\n\n\n    return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n                kQCPlugIn_Name,QCPlugInAttributeNameKey,\n\n\n                kQCPlugIn_Description,QCPlugInAttributeDescriptionKey,\n\n\n                nil];\n\n\n}\n\nModify the attributesForPropertyPortWithKey: so that it returns a dictionary for each input and output parameter. Each dictionary must contain a value followed by its port attribute name key. If the port has a default value, the dictionary should contain the value followed by the default value key.\n\nIf the input values need to be within a certain range, you should provide maximum and minimum values. MIDI values should range from 0 to 127, inclusive, so this example provides an opportunity for you to add these values.\n\nFor the MIDI2Color plug-in, modify the method so it looks as follows.\n\n+ (NSDictionary*) attributesForPropertyPortWithKey:(NSString*)key\n\n\n{\n\n\n    if([key isEqualToString:@\"inputValue\"])\n\n\n        return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            @\"Input Value\", QCPortAttributeNameKey,\n\n\n            [NSNumber numberWithUnsignedInteger:64],  QCPortAttributeDefaultValueKey,\n\n\n            [NSNumber numberWithUnsignedInteger:127],  QCPortAttributeMaximumValueKey,\n\n\n            [NSNumber numberWithUnsignedInteger:0],  QCPortAttributeMinimumValueKey,\n\n\n            nil];\n\n\n    if([key isEqualToString:@\"outputColor\"])\n\n\n        return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            @\"Output Color\", QCPortAttributeNameKey,\n\n\n            nil];\n\n\n    return nil;\n\n\n}\n\nMake sure the executionMode method returns kQCPlugInExecutionModeProcessor\n\n+ (QCPlugInExecutionMode) executionMode\n\n\n{\n\n\n    return kQCPlugInExecutionModeProcessor;\n\n\n}\n\nMake sure the timeMode method returns kQCPlugInTimeModeNone.\n\nThe MIDI2Color custom patch needs to execute only when the input value changes; it does not depend on time.\n\n+ (QCPlugInTimeMode) timeMode\n\n\n{\n\n\n    return kQCPlugInTimeModeNone;\n\n\n}\n\nModify the startExecution: method to create a color space object.\n\nFor this example, you can use the startExecution: method to create and initialize a Quartz color space. The color space should remain the same throughout the life of the custom patch instance. You’ll create the color space when the patch starts executing, store it in the colorSpace instance variable you created previously, and then release it when the custom patch instance is no longer executing.\n\n- (BOOL) startExecution:(id<QCPlugInContext>)context\n\n\n{\n\n\n   myColorSpace = CGColorSpaceCreateWithName(kCGColorSpaceGenericRGB);\n\n\n   return YES;\n\n\n}\n\nModify the stopExecution: method to release the color space object.\n\n- (void) stopExecution:(id<QCPlugInContext>)context\n\n\n{\n\n\n    CGColorSpaceRelease(myColorSpace);\n\n\n}\n\nWrite the execution method for the MIDI2Color custom patch.\n\nThe method converts a value that’s in the range of 0 to 127 to an RGBA color (red, green, blue, alpha). First, the code figures out which octave the input value resides in. Then it finds out which pitch class the value represents. The RGB color values are determined by the pitch class, while the alpha value is determined by the octave. The lower the octave, the more opaque the color.\n\nNote: A pitch class represents the pitches that have the same note name, regardless of octave. There are twelve pitch classes—C, C#/Db, D, D#/Eb, E, F, F#/Gb, G, G#/Ab, A, A#/Bb, and B. C# and Db are enharmonic equivalents—different labels for the same thing—as are D# and Eb, F# and Gb, and so on.\n\nModify the execute:atTime:withArguments: method to look as follows:\n\n- (BOOL) execute:(id<QCPlugInContext>)context\n\n\n            atTime:(NSTimeInterval)time\n\n\n            withArguments:(NSDictionary*)arguments\n\n\n{\n\n\n    static float color[4];\n\n\n \n\n\n    // Use a Quartz color\n\n\n    CGColorRef myColor;\n\n\n    int pitch, octave;\n\n\n    float alpha;\n\n\n    octave = floor(self.inputValue/12);\n\n\n    pitch = (int) (self.inputValue - (octave * 12));\n\n\n \n\n\n    // Set the RGB values according to pitch: C, C#/Db, D, E, and so on\n\n\n    switch (pitch) {\n\n\n        case 0: color[0] = 1.0; color[1] = 0.0; color[2] = 0.0; break; // C\n\n\n        case 1: color[0] = 1.0; color[1] = 0.5; color[2] = 0.0; break; // C#/Db\n\n\n        case 2: color[0] = 1.0; color[1] = 0.75; color[2] = 0.0; break; // D\n\n\n        case 3: color[0] = 1.0; color[1] = 1.0; color[2] = 0.0; break; //D#/Eb\n\n\n        case 4: color[0] = 0.5; color[1] = 1.0; color[2] = 0.0; break; //E\n\n\n        case 5: color[0] = 0.0; color[1] = 1.0; color[2] = 0.0; break; //F\n\n\n        case 6: color[0] = 0.0; color[1] = 0.5; color[2] = 0.5; break; //F#/Gb\n\n\n        case 7: color[0] = 0.0; color[1] = 0.0; color[2] = 1.0; break; //G\n\n\n        case 8: color[0] = 0.25; color[1] = 0.0; color[2] = 0.75; break; //G#/Ab\n\n\n        case 9: color[0] = 0.3; color[1] = 0.0; color[2] = 0.5; break; // A\n\n\n        case 10: color[0] = 0.4; color[1] = 0.0; color[2] = 0.75; break; //A#/Bb\n\n\n        case 11: color[0] = 0.5; color[1] = 0.0; color[2] = 1.0; break;// B\n\n\n        default: color[0] = 0.5; color[1] = 0.5; color[2] = 0.5;\n\n\n    }\n\n\n    // Set the alpha value, based on octave\n\n\n    alpha = 1.0 - ((float)octave/11.0);\n\n\n    color[3] = alpha;\n\n\n    // Create a Quartz color object using the previously created color space\n\n\n    myColor = CGColorCreate(myColorSpace, color);\n\n\n    // Set the color on the output (this also retains the color)\n\n\n    self.outputColor = myColor;\n\n\n    // Release the color object since it is now stored in the output parameter\n\n\n    CGColorRelease(myColor);\n\n\n    return YES;\n\n\n}\n\nSave and close the MIDI2ColorPlugIn.m file.\n\nOpen the Info.plist file and make sure the following key is an entry in the dictionary:\n\n<key>QCPlugInClasses</key>\n\n\n<array>\n\n\n    <string>MIDI2ColorPlugIn</string>\n\n\n</array>\n\nIf you want, customize the bundle identifier, then save and close the file.\n\nUnder Targets, choose Build & Copy. Then, click Build Build & Copy from the Action pop-up menu.\n\nWhen you build using this option, Xcode copies the successfully built plug-in to ~/Library/Graphics/Quartz Composer Plug-Ins.\n\nOpen the Quartz Composer development environment and search for the MIDI2Color custom patch in the Patch Creator.\n\nMake sure the MIDI2Color patch works by creating a composition that uses it.\n\nYou can use an interpolation patch to provide values that include the 0 to 127 range. It’s best to extend the range to make sure the edge cases are processed correctly. Set the start and end value of the interpolation patch to -20 and 150, respectively. Then set the repeat mode to Mirrored Loop and the duration to 25 or higher. Connect the output port of the interpolation patch to the input port of the MIDI2Color patch. After setting up a Clear patch, drag a Particle System patch to the editor and connect the output of the MIDI2Color patch to the Color port of the Particle System patch.\n\nDrag an image directly to the editor widow. Then connect the image output port to the Image input port of the Particle System patch. (If you don’t have any images readily available, you might choose one of the images located in ~Pictures/iChat Icons/Flowers.) Your test application should look similar to this:\n\nYou might want to make a few adjustments to the input parameters of the Particle System patch to get a result that is aesthetically pleasing to you, but you don’t need to. Because the images produced by the Particle System have a lifetime, you’ll be able to more easily see the colors and observe not only how the hues change but how the opacity changes.\n\nNumber2Color: Extending MIDI2Color\n\nThe MIDI2Color custom patch has a major shortcoming; it processes input values that are in the range of 0 to 127. Although MIDI electronic instruments use those values to designate pitch and other aspects of music, many of the built-in Quartz Composer MIDI custom patches provide normalized output values (0.0 to 1.0) instead of raw MIDI values. In addition, the MIDI2Color patch can operate on any numerical values, not just those provided by MIDI, as you saw with the test composition that used the Interpolation patch. Its use does not need to be restricted to MIDI input, so a name change is in order.\n\nIn this section you’ll see how to improve the MIDI2Color patch by writing a similar custom patch that accepts any range of values. You’ll add two parameters that will be available on the Settings pane of the inspector for the patch so that the user can set the range. You’ll write an execution method that maps any range of values to over ten “octaves” of \"pitch” values. You’ll name the patch Number2Color to indicate that the patch can be used for any numeric value.\n\nFollow these steps to create a Number2Color patch:\n\nOpen Xcode and choose File > New Project.\n\nIn the New Project window, choose Standard Apple Plug-ins > Quartz Composer Plug-in for Objective C With Internal Settings And User Interface. Then click click Next.\n\nThis template provides the nib file that you’ll modify for the user interface of the Settings pane.\n\nEnter Number2Color in the Project Name text field and click Finish.\n\nOpen the Number2ColorPlugin.h file.\n\nModify the interface file so that it has two dynamic Objective-C properties and two instance variable properties. The dynamic Objective-C properties—inputValue and outputColor—are the input and output parameters for the input and output ports of the patch. The instance variable properties—minValue and maxValue—are the internal parameters that will be available on the Settings pane in the inspector for the patch. You also need to add a variable to keep track of the color space, just as you did for the MIDI2Color custom patch.\n\n#import <Quartz/Quartz.h>\n\n\n \n\n\n@interface Number2ColorPlugIn : QCPlugIn\n\n\n{\n\n\n   CGColorSpaceRef myColorSpace;\n\n\n \n\n\n}\n\n\n// Declare a property input port of type Number and with the key inputValue\n\n\n@property double inputValue;\n\n\n// Declare a property input port of type Color and with the key outputColor\n\n\n@property CGColorRef outputColor;\n\n\n \n\n\n// Declare internal settings as properties of type double\n\n\n@property double minValue;\n\n\n@property double maxValue;\n\n\n@end\n\nSave and close the Number2ColorPlugIn.h file.\n\nOpen the Number2ColorPlugin.m file.\n\nJust after the @implementation statement, add the following directives. Quartz Composer will handle their implementation.\n\n@dynamic inputValue, outputColor;\n\n\n@synthesize minValue, maxValue;\n\nAdd a description for the custom patch by modifying the appropriate #define statement.\n\n#define    kQCPlugIn_Name @\"Number2Color\"\n\n\n#define    kQCPlugIn_Description  @\"Converts a value to a color with transparency. You can define a range of values to use for the color mapping.\"\n\nNext you’ll write the methods needed to implement the Number2ColorPlugIn subclass. The attributes method should already look as follows:\n\n+ (NSDictionary*) attributes\n\n\n{\n\n\n    return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n                kQCPlugIn_Name,QCPlugInAttributeNameKey,\n\n\n                kQCPlugIn_Description,QCPlugInAttributeDescriptionKey,\n\n\n                nil];\n\n\n}\n\nModify the attributesForPropertyPortWithKey: so that it returns a dictionary for each input and output parameter. Do not include the maxValue and minValue properties here. These require different setup work because those are patch settings, not input parameters.\n\nThe method should look as follows:\n\n+ (NSDictionary*) attributesForPropertyPortWithKey:(NSString*)key\n\n\n{\n\n\n    if([key isEqualToString:@\"inputValue\"])\n\n\n        return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            @\"Value\", QCPortAttributeNameKey,\n\n\n            [NSNumber numberWithUnsignedInteger:64],  QCPortAttributeDefaultValueKey,\n\n\n            nil];\n\n\n    if([key isEqualToString:@\"outputColor\"])\n\n\n        return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            @\"Color\", QCPortAttributeNameKey,\n\n\n            nil];\n\n\n    return nil;\n\n\n}\n\nMake sure the executionMode method returns kQCPlugInExecutionModeProcessor.\n\n+ (QCPlugInExecutionMode) executionMode\n\n\n{\n\n\n    return kQCPlugInExecutionModeProcessor;\n\n\n}\n\nMake sure the timeMode method returns kQCPlugInTimeModeNone.\n\nSimilar to MIDI2Color, the Number2Color plug-in executes only when the input value changes; it does not depend on time.\n\n+ (QCPlugInTimeMode) timeMode\n\n\n{\n\n\n    return kQCPlugInTimeModeNone;\n\n\n}\n\nNext you’ll implement methods that are required when you use internal parameters. First you need to write an init method to set the initial values of the minValue and maxValue parameters to a default value.\n\n (id) init\n\n\n{\n\n\n    self = [super init];\n\n\n    if (self) {\n\n\n        self.minValue = 0.0;\n\n\n        self.maxValue = 127.0;\n\n\n    }\n\n\n \n\n\n    return self;\n\n\n}\n\nWrite a dealloc method. If any of the internal parameters are objects, such as a color, you would release the object in this method (for example self.foo = nil;). But because none of the internal parameters are objects, the dealloc method provided by the template is okay as is.\n\n- (void) dealloc\n\n\n{\n\n\n    [super dealloc];\n\n\n}\n\nImplement the plugInKeys method so that it returns the keys that represent the internal parameters for the plug-in. This list is used to serialize the settings automatically. It’s also used by the QCPlugInViewController object to allow editing the values for these keys in the user interface. Make sure that you terminate the list with nil.\n\n+ (NSArray*) plugInKeys\n\n\n{\n\n\n    return [NSArray arrayWithObjects:@\"minValue\", @\"maxValue\", nil];\n\n\n}\n\nProvide a createViewController method so that you can provide support in the user interface for viewing and setting the internal parameters. The viewNibName string must match the name of the nib file.\n\nThis method is is already included as part of the template. It should look like this:\n\n- (QCPlugInViewController*) createViewController\n\n\n{\n\n\n    return [[QCPlugInViewController alloc]\n\n\n            initWithPlugIn:self\n\n\n            viewNibName:@\"Number2ColorSettings\"];\n\n\n}\n\nModify the startExecution: method to create a color space object.\n\nFor this example, you can use the startExecution: method to create and initialize a Quartz color space. The color space should remain the same throughout the life of the custom patch instance. You’ll create the color space when the patch starts to execute, store it in the instance variable you created previously, and then release the color space when the custom patch instance is no longer executing.\n\n- (BOOL) startExecution:(id<QCPlugInContext>)context\n\n\n{\n\n\n   myColorSpace = CGColorSpaceCreateWithName(kCGColorSpaceGenericRGB);\n\n\n   return YES;\n\n\n}\n\nModify the stopExecution: method to release the color space object.\n\n- (void) stopExecution:(id<QCPlugInContext>)context\n\n\n{\n\n\n   CGColorSpaceRelease(myColorSpace);\n\n\n}\n\nImplement the execution method. Similar to the MIDI2Color plug-in, this is where the processing takes place. You’ll notice that the Number2Color execution method is similar to the MIDI2Color execution method except that Number2Color uses the minimum and maximum values to map the input value to the specified range.\n\n- (BOOL) execute:(id<QCPlugInContext>)context\n\n\n            atTime:(NSTimeInterval)time\n\n\n            withArguments:(NSDictionary*)arguments\n\n\n{\n\n\n    static float color[4];\n\n\n \n\n\n    CGColorRef myColor;\n\n\n    int pitch, octave;\n\n\n    double convertedInputValue;\n\n\n    float alpha;\n\n\n \n\n\n    // Make sure there is a range of values\n\n\n    if (self.maxValue == self.minValue)\n\n\n        // If not, execution fails.\n\n\n        return NO;\n\n\n    // Use the internal settings to scale the input value\n\n\n    convertedInputValue = (self.inputValue - self.minValue)/(self.maxValue - self.minValue) * 127.0);\n\n\n    // The remaining code is the same as that used for MIDI2Color\n\n\n    octave = floor(convertedInputValue/12);\n\n\n    pitch = (int) (convertedInputValue - (octave * 12));\n\n\n \n\n\n    switch (pitch) {\n\n\n        case 0: color[0] = 1.0; color[1] = 0.0; color[2] = 0.0; break;\n\n\n        case 1: color[0] = 1.0; color[1] = 0.5; color[2] = 0.0; break;\n\n\n        case 2: color[0] = 1.0; color[1] = 0.75; color[2] = 0.0; break;\n\n\n        case 3: color[0] = 1.0; color[1] = 1.0; color[2] = 0.0; break;\n\n\n        case 4: color[0] = 0.5; color[1] = 1.0; color[2] = 0.0; break;\n\n\n        case 5: color[0] = 0.0; color[1] = 1.0; color[2] = 0.0; break;\n\n\n        case 6: color[0] = 0.0; color[1] = 0.5; color[2] = 0.5; break;\n\n\n        case 7: color[0] = 0.0; color[1] = 0.0; color[2] = 1.0; break;\n\n\n        case 8: color[0] = 0.25; color[1] = 0.0; color[2] = 0.75; break;\n\n\n        case 9: color[0] = 0.3; color[1] = 0.0; color[2] = 0.5; break;\n\n\n        case 10: color[0] = 0.4; color[1] = 0.0; color[2] = 0.75; break;\n\n\n        case 11: color[0] = 0.5; color[1] = 0.0; color[2] = 1.0; break;\n\n\n        default: color[0] = 0.5; color[1] = 0.5; color[2] = 0.5;\n\n\n    }\n\n\n    alpha = 1.0 - ((float)octave/11.0);\n\n\n    color[3] = alpha;\n\n\n    myColor = CGColorCreate(myColorSpace, color);\n\n\n    self.outputColor = myColor;\n\n\n    CGColorRelease(myColor);\n\n\n    return YES;\n\n\n}\n\nOpen the Info.plist file and make sure the following key is an entry in the dictionary:\n\n<key>QCPlugInClasses</key>\n\n\n<array>\n\n\n    <string>Number2ColorPlugIn</string>\n\n\n</array>\n\nIf you want, customize the bundle identifier, then save and close the file.\n\nNext you’ll use Interface Builder to create the user interface for the Settings pane.\n\nDouble click Number2ColorSettings.nib.\n\nInterface Builder launches with a View window.\n\nDrag a Text Field (NSTextField) from the Library to the View window.\n\nDrag a Label from the Library, place it next to the text field, and label it Minimum value:.\n\nWith the text field selected, open the Bindings inspector.\n\nClick the disclosure triangle next to Value, click “Bind to”, and choose File’s Owner.\n\nEnter plugIn.minValue in the Model Key Path text field.\n\nRecall that the model key path is plugIn.XXX, where XXX is the corresponding key for the internal setting.\n\nDrag a text field to the view.\n\nDrag a label next to the text field and label it Maximum value:.\n\nThe user interface should now look like this:\n\nWith the text field selection, open the Bindings inspector.\n\nBind the text field to the File’s owner and enter plugIn.maxValue as the Model Key Path.\n\nControl-drag from File’s Owner icon in the Nib document window to the view. Then click view in the heads-up display that appears.\n\nSave the file and quit Interface Builder.\n\nUnder Targets, choose Build & Copy. Then, click Build “Build & Copy” from the Action pop-up menu.\n\nWhen you build using this option, Xcode copies the successfully built plug-in to ~/Library/Graphics/Quartz Composer Plug-Ins.\n\nOpen the Quartz Composer development and search for the Number2Color custom patch in the Patch Creator.\n\nDrag the patch to the editor. Then open the inspector to the Settings pane. The pane should look similar to the following.\n\nTest the composition with a variety of ranges of input values.\n\nNote: If you create a plug-in that uses variables in the Settings pane whose values do not conform to the NSCoding protocol, then you must implement serializedValueForKey: and setSerializedValue:forKey:.\n\nPackaging Two Custom Patches in a Plug-in\n\nAlthough, for testing purposes, you might want to develop each custom patch separately, you can combine several patches into one plug-in when you are ready to distribute the patches. This section shows you how to package the MIDI2Color and Number2Color custom patches together. First you’ll add the files for the MIDI2Color custom patch to the Number2Color project. Then you’ll modify the property list file to include the MIDI2ColorPlugIn class.\n\nOpen the Number2Color Xcode project.\n\nChoose Project > Add to Project.\n\nNavigate to the MIDI2ColorPlugin.h and MIDI2ColorPlugin.m files, select them, and click Add.\n\nIn the sheet that appears, click Add.\n\nMake sure that the Number2Color target is selected.\n\nOpen the Info.plist file.\n\nEnter the MIDI2ColorPlugIn class so that the QCPlugInClasses entry looks like this:\n\n<key>QCPlugInClasses</key>\n\n\n<array>\n\n\n  <string>Number2ColorPlugIn</string>\n\n\n  <string>MIDI2ColorPlugIn</string>\n\n\n</array>\n\nSave the project.\n\nUnder Targets, choose Build & Copy. Then, click Build Build & Copy from the Action pop-up menu.\n\nAfter following these instructions, the name of the plug-in remains Number2Color. You might want to rename it to MyColorGenerators or some other name that indicates the plug-in contains more than one custom patch. If you want, you can add additional custom patches to the plug-in.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2010 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2010-03-24\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "The Basics of Custom Patches",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposer_Patch_PlugIn_ProgGuide/plugin_1/plugin_1.html#//apple_ref/doc/uid/TP40004787-CH3-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer Custom Patch Programming Guide\nTable of Contents\nIntroduction\nThe Basics of Custom Patches\nWriting Processor Patches\nWriting Image Processing Patches\nWriting Consumer Patches\nGlossary\nRevision History\nNext\nPrevious\nThe Basics of Custom Patches\n\nA Quartz Composer patch is similar to a routine in a traditional programming language—a patch is a base processing unit that executes and produces a result. A custom patch is one that you write by subclassing the QCPlugIn class. To make a custom patch available in the Quartz Composer patch library, you need to package it as a plug-in—an NSBundle object—and install in the appropriate directory. If you’d like, you can package more than one custom patch in a plug-in. You don’t need to create a user interface for a custom patch. When Quartz Composer loads your plug-in, it creates the user interface automatically for each custom patch in the plug-in so that custom patches have the same look in the workspace as the patches that are built in to Quartz Composer.\n\nThis chapter provides a discussion of custom patches and gives an overview of the tasks you need to perform to create a custom patch. You’ll see which aspects of your code give rise to the user interface for a custom patch. By the end of the chapter you should have a fairly good idea of how to get started writing your own custom patches. Then you can read Writing Processor Patches and Writing Consumer Patches to find out how to write specific kinds of patches.\n\nA Close Look at the Patch User Interface\n\nPatches in Quartz Composer all have the same basic look and feel, as you can see in Figure 1-1. The patch name is at the top of the patch, in the patch title bar. Input ports are on the left side of the patch and output ports are on the right side. Many provider patches don’t have input ports because they get data from an outside source, such as a mouse, keyboard, tablet, or MIDI controller. Consumer patches don’t have output ports because they render data to a destination. Processor patches have one or more input ports and one or more output ports.\n\nFigure 1-1  A variety of Quartz Composer patches\n\nThe inspector provides access to the patch parameters. The Input Parameters pane contains the same parameters represented by the input ports. It provides the option to manually adjust input values instead of supplying values through the input ports. Compare the Teapot patch shown in Figure 1-1 with Figure 1-2. You’ll see that the ports match up with the input parameters.\n\nFigure 1-2  The Input Parameters pane for the Teapot patch\n\nSome patches also have a Settings pane, as shown in Figure 1-3, that provides support for configuring settings whose values either can’t be represented by one of the standard port data types (see Table 1-1) or that control advanced options.\n\nFigure 1-3  The Settings pane for the Date Formatter patch\n\nFrom Custom Patch Code to Patch User Interface\n\nA custom patch is a subclass of the QCPlugIn class. Quartz Composer creates the input and output ports on a custom patch from dynamic Objective-C properties of the subclass. Properties whose name begins with input (and are one of the supported types) will appear as an input port on the patch. Properties whose name begins with output (and are one of the supported types) will appear as an output port on the patch. Listing 1-1 shows code that creates a subclass of QCPlugIn and declares one input and one output parameter. As you read this section you’ll see how that code relates to the resulting custom patch in Quartz Composer.\n\nNote: Objective-C properties are represented syntactically as identifiers. Accessing declared properties is equivalent to invoking an accessor method directly and is more convenient than using key-value coding methods. If you are unfamiliar with properties, see The Objective-C Programming Language.\n\nListing 1-1  The subclass and properties for a custom string-processing patch\n\n@interface iPatchPlugIn : QCPlugIn\n\n\n{\n\n\n}\n\n\n \n\n\n@property(assign) NSString* inputString;\n\n\n@property(assign) NSString* outputString;\n\n\n \n\n\n@end\n\nYou can implement the attributesForPropertyPortWithKey: method to map the name of each property (such as inputString or outputString) to a key-value pair for the corresponding patch parameter. Listing 1-2 shows an implementation of this method for the properties declared in Listing 1-1. For each property, the method returns a dictionary that contains the port name and its default value, if any. This example returns the port name Name for the property that’s named inputString. It returns iName for the property that’s named outputString.\n\nListing 1-2  A routine that returns attributes for property ports\n\n+ (NSDictionary*) attributesForPropertyPortWithKey:(NSString*)key\n\n\n{\n\n\n    if([key isEqualToString:@\"inputString\"])\n\n\n        return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n                @\"Name\", QCPortAttributeNameKey,\n\n\n                @\"Pod\", QCPortAttributeDefaultValueKey,\n\n\n                nil];\n\n\n    if([key isEqualToString:@\"outputString\"])\n\n\n        return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n                @\"iName\", QCPortAttributeNameKey,\n\n\n                nil];\n\n\n \n\n\n    return nil;\n\n\n}\n\nFigure 1-4 shows the patch that Quartz Composer creates as a result of the property declarations and the attributesForPropertyPortWithKey: method. Note that input values are read only—your custom patch code reads the input values and processes them in some way. Your custom patch code can write to output ports as well as read their current values.\n\nFigure 1-4  The resulting patch\n\nIf you don’t use properties, or if your custom patch needs to change the number of ports at runtime, you can use the QCPlugIn methods to dynamically add and remove input and output ports, and the associated methods to set and retrieve values. These methods are described in QCPlugIn Class Reference:\n\naddInputPortWithType:forKey:withAttributes:\n\naddOutputPortWithType:forKey:withAttributes:\n\nremoveInputPortForKey:\n\nremoveOutputPortForKey:\n\nsetValue:forOutputKey:\n\nvalueForInputKey:\n\nYou can define a string that specifies a practical name for the custom patch. This is the name that appears as the patch title. You should also define a description string that briefly tells what the custom patch does. For example:\n\n#define    kQCPlugIn_Name @\"iPatch\"\n\n\n#define    kQCPlugIn_Description @\"Converts any name to an \\\"iName\\\"\"\n\nThen you need to implement the attributes method so that your custom patch returns a dictionary that contains the name and description. Listing 1-3 shows the attributes method for the iPatch custom patch.\n\nListing 1-3  A routine that returns the custom patch name and description\n\n+ (NSDictionary*) attributes\n\n\n{\n\n\n    return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n                 @\"Name\", QCPlugInAttributeNameKey,\n\n\n                @\"Convert any name to an \\\"iName\\\"\", QCPlugInAttributeDescriptionKey,\n\n\n                nil];\n\n\n}\n\nThe description appears in the Quartz Composer when the patch is selected in the Patch Creator and when the user hovers the pointer over the patch title bar in the workspace.\n\nYou must establish an execution mode for the custom patch by implementing the executionMode method. The method returns the appropriate execution mode constant, which represents a Quartz Composer patch type—provider, processor, or consumer.\n\nkQCPlugInExecutionModeProvider specifies to execute when the output values are needed but at most once per frame. This mode is for custom patches that pull data from an external source such as video, the mouse, a MIDI device, an RSS feed, and so on.\n\nkQCPlugInExecutionModeProcessor specifies to execute when the output values are needed and when input values change.\n\nkQCPlugInExecutionModeConsumer specifies to execute every frame. This type of custom patch pulls data from others and renders it to a destination.\n\nA custom patch must establish a time dependency by implementing the timeMode method. The method returns one of the following time mode constants:\n\nkQCPlugInTimeModeNone does not depend on time.\n\nkQCPlugInTimeModeIdle does not depend on time, but needs to give the system some time to process.\n\nkQCPlugInTimeModeTimeBase has a time base defined by the system and the custom patch uses time in its computations for the result.\n\nIf a custom patch uses the time base mode, the patch will have an option that allows the user to set the time base to parent, local, or external, as shown in Figure 1-5.\n\nFigure 1-5  The time base setting for the Interpolation patch\n\nProperty and Port Data Types\n\nObjective-C 2.0 properties must be one of the data types listed in Table 1-1. Quartz Composer maps the property data type to the appropriate port type. The type constants for ports that your custom patch creates at runtime are listed in the Custom Port Type column of the table, next to the Objective-C class associated with the port value. If your custom patch requires data that can’t be captured by one of the data types below, see Internal Settings.\n\nTable 1-1  Data type mappings\n\nPort\n\n\t\n\nObjective-C 2.0 property type\n\n\t\n\nCustom port type\n\n\t\n\nObjective-C class\n\n\n\n\nBoolean\n\n\t\n\nBOOL\n\n\t\n\nQCPortTypeBoolean\n\n\t\n\nNSNumber\n\n\n\n\nIndex\n\n\t\n\nNSUInteger\n\n\t\n\nQCPortTypeIndex\n\n\t\n\nNSNumber\n\n\n\n\nNumber\n\n\t\n\ndouble\n\n\t\n\nQCPortTypeNumber\n\n\t\n\nNSNumber\n\n\n\n\nString\n\n\t\n\nNSString *\n\n\t\n\nQCPortTypeString\n\n\t\n\nNSString\n\n\n\n\nColor\n\n\t\n\nCGColorRef\n\n\t\n\nQCPortTypeColor\n\n\t\n\nCGColorRef\n\n\n\n\nStructure\n\n\t\n\nNSDictionary *\n\n\t\n\nQCPortTypeStructure\n\n\t\n\nNSDictionary\n\n\n\n\nImage (input)\n\n\t\n\nid<QCPlugInInputImageSource>\n\n\t\n\nQCPortTypeImage\n\n\t\n\n(id)<QCPlugInInputImageSource>\n\n\n\n\nImage (output)\n\n\t\n\nid <QCPlugInOutputImageProvider>\n\n\t\n\nQCPortTypeImage\n\n\t\n\n(id)<QCPlugInOutputImageProvider>\n\nImages in Quartz Composer are opaque objects that conform to protocols. Using protocols avoids the restrictions of a particular image type as well as type mismatches. It also gets the best performance because Quartz Composer defers pixel computation until it is needed.\n\nThe supported pixel formats for images are:\n\nARGB8—8 bits alpha, 8 bits red, 8 bits green, 8 bits blue, unsigned integer\n\nBGRA8—8 bits blue, 8 bits green, 8 bits red, 8 bits alpha, unsigned integer\n\nRGBAf— 32 bits red, 32 bits green, 32 bits blue, 32 bits alpha, floating-point\n\nl8—8 bits luminance, unsigned integer\n\nlf—32 bits luminance, floating-point\n\nInput images are opaque source objects that conform to the QCPlugInInputImageSource protocol. To use an image as an input parameter to your custom patch, declare it as a property:\n\n@property(assign) id<QCPlugInInputImageSource> inputImage;\n\nOutput images are opaque provider objects that conform to the QCPlugInOutputImageProvider protocol. To use an image as an output parameter for your custom patch, declare it as a property:\n\n@property(assign) id<QCPlugInOutputImageProvider> outputImage;\n\nWriting Image Processing Patches shows how to define the methods of the QCPlugInInputImageSource and QCPlugInOutputImageProvider protocols. See also QCPlugInInputImageSource Protocol Reference and QCPlugInOutputImageProvider Protocol Reference.\n\nInternal Settings\n\nCustom patch parameters that are not suitable as input ports can be added to the Settings pane of the inspector for the patch. A few reasons why you might want to use internal settings are:\n\nThe parameter can’t be represented by one of the data types listed in Table 1-1. See, for example, the interpolation curve shown in Figure 1-6.\n\nThe default value of the parameter works in most cases and should be modified only by a knowledgeable user.\n\nIt doesn’t make sense to animate the parameter.\n\nFigure 1-6  Two sample Settings panes\n\nFor such cases, your custom patch needs to provide a user interface for editing these values. Quartz Composer inserts your custom user interface into the Settings pane of the inspector for the patch. Internal settings are accessible through key-value coding. The simplest way to implement them are as Objective-C 2.0 properties. Listing 1-4 shows two typical declarations for property instance variables.\n\nListing 1-4  Code that declares property instance variables\n\n@property(copy) NSColor* systemColor;\n\n\n@property(copy) MyConfiguration* systemConfiguration;\n\nYour QCPlugIn subclass must implement the plugInKeys method so that it returns a list of keys for the internal settings. The plugInKeys method for the systemColor and systemConfiguration properties is shown in Listing 1-5. Make sure to terminate the list with nil. (See Key-Value Coding Programming Guide.)\n\nListing 1-5  An implementation of the plugInKeys method\n\n+ (NSArray*) plugInKeys\n\n\n{\n\n\n    return [NSArray arrayWithObjects: @“systemColor”,\n\n\n                @“systemConfiguration”,\n\n\n                nil]\n\n\n}\n\nYou use Interface Builder to create the user interface for editing internal settings. The interface is a view object (NSView or NSView subclass) that is managed through an instance of QCPlugInViewController. This instance acts as a controller between the custom patch instance and the view that contains the controls. In order for the nib to load properly, your plug-in class must implement the createViewController method of QCPlugIn and return an instance of QCPlugInViewController initialized with the correct nib name. In this example shown in Listing 1-6, the name is MyPlugIn.\n\nListing 1-6  Code that implements a view controller\n\n- (QCPlugInViewController*) createViewController\n\n\n{\n\n\n  return [[QCPlugInViewController alloc] initWithPlugIn:self\n\n\n         viewNibName:@”MyPlugIn”];\n\n\n}\n\nUsing Cocoa bindings for the controls in your custom interface requires no code. Just use plugIn.XXX as the model key path, where XXX is the corresponding key for the internal setting. If you prefer, you can subclass QCPlugInViewController to implement the usual target-action communication model. (You also need to make sure that you do not autorelease the controller. Then you need to connect the controls in the nib to the owner controller.)\n\nTip: In Xcode, use the Quartz Composer Plug-in With Internal Setting And User Interface template. This template includes a nib file for the Setting pane. See The QCPlugIn Template in Xcode.\n\nWhen you read or write a composition file that uses the custom patch, the internal settings are serialized. Serialization is automatic for any setting whose class conforms to the NSCoding protocol, such as NSColor. For example, the systemColor property defined in Listing 1-4 does not require any action on your part; the system serializes it automatically.\n\nFor settings that don’t conform to the NSCoding protocol, you need to override the following methods:\n\nserializedValueForKey: converts a value to its serialized representation.\n\nsetSerializedValue:forKey: converts from a serialized representation back to the original value.\n\nFor example, the systemConfiguration property defined in Listing 1-4 is serialized as shown in Listing 1-7. A serialized value must be a property list class such as NSString, NSNumber, NSDate, NSArray, or NSDictionary or nil.\n\nListing 1-7  Code that overrides serialization methods for system configuration data\n\n- (id) serializedValueForKey:(NSString*)key\n\n\n {\n\n\n    if([key isEqualToString:@\"systemConfiguration\"])\n\n\n        return [self.systemConfiguration data];\n\n\n    // Ensure this has a data method\n\n\n    return [super serializedValueForKey:key];\n\n\n}\n\n\n \n\n\n- (void) setSerializedValue:(id)serializedValue\n\n\n                     forKey:(NSString*)key\n\n\n{\n\n\n    // System config is subclass of NSObject.\n\n\n    // It's up to you to keep track of the version.\n\n\n    if([key isEqualToString:@\"systemConfiguration\"])\n\n\n        self.systemConfiguration\n\n\n           = [MyConfiguration configurationWithData:serializedValue];\n\n\n    else\n\n\n        [super setSerializedValue:serializedValue forKey:key];\n\n\n}\nCustom Patch Execution\n\nQuartz Composer assumes that the following statements are true for your QCPlugIn subclass:\n\nThere can be multiple instances of a custom patch. Quartz Composer creates one instance of the QCPlugIn subclass each time the custom patch appears in a composition.\n\nThe custom patch works correctly even when it is not executed on the main thread.\n\nThe custom patch does not require a run loop to be present and running. (If you need a run loop, then you must set up the run loop on a secondary thread and communicate with it.)\n\nThe execute:atTime:withArguments: method of QCPlugIn is at the heart of the custom patch. The method typically:\n\nReads the values from the input ports or gets data from a source\n\nPerforms computations, taking into account time, if necessary\n\nEither writes the result to the output ports or renders the content to a destination\n\nYour execute:atTime:withArguments: method should access its property ports only when necessary. When you can, cache values in loops rather than repeatedly read them from the port.\n\nWhen the Quartz Composer engine renders, it calls methods related to executing the custom patch. Quartz Composer passes an opaque object—that is, an execution context that conforms to the QCPlugInContext protocol—to these methods. You should neither retain the context nor use it outside of the execution methods. Make sure you don’t write values to the input ports (input ports are read only).\n\nThe QCPlugInContext protocol contains a number of useful methods for getting information about the rendering destination, including:\n\nbounds returns the bounds, expressed in Quartz Composer units (–1.0 to 1.0).\n\ncolorSpace returns the output color space.\n\nCGLContextObj returns the CGL context to perform rendering to.\n\nThe protocol also provides utilities for logging messages and getting a dictionary of user information. The user information dictionary is shared with all instances of the custom patch in the same plug-in context environment. The dictionary was designed that way to allowing sharing information between custom patch instances. For more information on these methods, see QCPlugInContext Protocol Reference.\n\nThe QCPlugIn Template in Xcode\n\nXcode provides two templates that makes it straightforward to write and package custom patches. One template is for custom patches that do not require a Settings pane. The other template includes a nib file for a Setting pane. For each template, Xcode provides the skeletal files and methods that are needed and names the files appropriately. Figure 1-7 shows the files automatically created by Xcode for a Quartz Composer plug-in project named iPatch.\n\nFigure 1-7  The files in a Quartz Composer plug-in project\n\nXcode automatically names files using the project name that you supply. These are the default files provided by Xcode.\n\n<ProjectName>PlugIn.m is the implementation file for the custom patch. You need to modify this file.\n\n<ProjectName>PlugIn.h is the interface file for the custom patch. You need to modify this file.\n\nInfo.plist contains properties of the plug-in, such as development region, bundle identifier, product name, and a QCPlugInClasses key that lists classes for each custom patch in the plug-in. You need to modify the Info.plist and make sure that the value associated with the QCPlugInClasses key is correct. The default value for this key is based on the project name that you supply, so you shouldn’t need to modify it unless you plan to include more than one custom patch in the bundle.\n\nThe interface file declares a subclass of QCPlugIn. Xcode automatically names the subclass <ProjectName>PlugIn. For example, if you supply Number2Color as the project name, the interface file uses Number2ColorPlugIn as the subclass name.\n\nThe implementation file contains these methods of the QCPlugIn class that you need to modify for your purposes:\n\nattributes\n\nattributesForPropertyPortWithKey:\n\nexecutionMode\n\ntimeMode\n\nexecute:atTime:withArguments:\n\nThere are other methods of QCPlugIn provided in the template that you can implement if appropriate. (See also QCPlugIn Class Reference.)\n\nThe implementation file provided by Xcode contains two important statements that you should not modify and one that you need to modify. This statement should not be modified or deleted:\n\n#import <OpenGL/CGLMacro.h>\n\nUsing CGLMacro.h improves performance. The inclusion of this statement allows Quartz Composer to optimize its use of OpenGL by providing a local context variable and caching the current renderer in that variable. You should keep this statement even if your custom patch does not contain OpenGL code itself. If your custom patch contains OpenGL code, you need to declare a local variable and set the current context to it. For example:\n\n#import <OpenGL/CGLMacro.h>\n\nCGLContext cgl_ctx = myContext;\n\nwhere myContext is the current context.\n\nSee Using OpenGL in a Custom Patch and OpenGL Programming Guide for Mac for more information.\n\nThe statement that defines the custom patch name is automatically filled-in by Xcode based on the project name that you supply. To help the user distinguish patches, it’s best if the patch name is unique in the context of the Quartz Composer Patch Creator. If the name isn’t unique, your patch will be difficult to use. So, change this statement.\n\n#define kQCPlugIn_Name @\"<ProjectName>\"\n\nXcode provides a default definition for the custom patch description:\n\n#define kQCPlugIn_Description @\"Converts any name to an \\\"iName\\\"\"\n\nYou need to modify this string so that it describes the custom patch. If the patch name is not unique, it’s best to describe how your patch differs from another patch of the same name. You’ll also want to provide localized strings. (For more information on localization, see Strings Files.)\n\nPackaging, Installing, and Testing Custom Patches\n\nYou need to package a Quartz Composer custom patch as a standard Cocoa bundle. You can package more than one custom patch in the bundle. The QCPlugIn template makes it trivial to package custom patches; see The QCPlugIn Template in Xcode.\n\nThe bundle Info.plist file must have an entry for each QCPlugIn subclass that’s in the bundle. Listing 1-8 shows a property list entry for a bundle that contains two custom patches: MyColorGenerator and MyNumberCruncher.\n\nListing 1-8  Entries in the Info.plist file\n\n<key>QCPlugInClasses</key>\n\n\n <array>\n\n\n  <string>MyColorGenerator</string>\n\n\n  <string>MyNumberCruncher</string>\n\n\n </array>\n\nWhen you build the bundles, you should target 32-bit, 64-bit, PowerPC, and Intel architectures.\n\nYou can make a custom patch available to any application that uses Quartz Composer by installing the plug-in that contains the custom patch in /Library/Graphics/Quartz Composer Plug-Ins or ~/Library/Graphics/Quartz Composer Plug-Ins. When Quartz Composer launches, it automatically loads the plug-in so that all the custom patches contained in the plug-in show up in the Patch Creator.\n\nYou can choose instead to include custom patch code in an application bundle. You might want to do this either to control the use of the custom patches or because they are useful only to the application you are embedding them in. To make a custom patch available to the application, register the subclass of the QCPlugIn class using the registerPlugInClass: method. If you want to restrict access to a plug-in that contains one or more custom patches, you can load the plug-in from any location by calling the method loadPlugInAtPath:.\n\nNote: When you use a custom patch in a Quartz composition, the custom patch is not embedded in the composition. If you move the composition to another computer, you must install the plug-in that contains the custom patch on the same computer.\n\nYou should make sure that your custom patch works properly by using it in a Quartz Composer application. If you use the Build & Copy target, Xcode automatically copies the built plug-in to ~/Library/Graphics/Quartz Composer Plug-Ins and then launches Quartz Composer.\n\nIf you do any of the following, the custom patches in your plug-in will not appear as patches in the Patch Creator:\n\nFail to include the class name for the QCPlugInClasses key or misspell the name. The class name in the Info.plist file must match the name of the QCPlugIn subclass.\n\nImproperly declare properties. Properties that represent ports must use one of the supported types. (See Table 1-1.)\n\nChoose a plug-in name that conflicts with another Quartz Composer plug-in bundle. The plug-in name must be unique. However, custom patch names are not required to be unique. But for usability, it’s a good idea to choose unique, descriptive custom patch names.\n\nYou can check the system log in Console for error messages from Quartz Composer.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2010 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2010-03-24\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Writing Consumer Patches",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposer_Patch_PlugIn_ProgGuide/WritingConsumerPatches/WritingConsumerPatches.html#//apple_ref/doc/uid/TP40004787-CH5-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer Custom Patch Programming Guide\nTable of Contents\nIntroduction\nThe Basics of Custom Patches\nWriting Processor Patches\nWriting Image Processing Patches\nWriting Consumer Patches\nGlossary\nRevision History\nNext\nPrevious\nWriting Consumer Patches\n\nA consumer patch pulls (or consumes) data from processor and provider patches, operates on the data, and renders the result to a destination, typically a screen but it can be another destination. This chapter discusses some best practices for using OpenGL in a Quartz Composer custom patch and then shows how to create a custom consumer patch that renders an OpenGL texture to a quad.\n\nFigure 4-1  Consumer patches render to a destination\n\nYou need to know OpenGL programming to understand this chapter. If you don’t know OpenGL, but want to learn, these books provide a good starting point:\n\nOpenGL Programming Guide, by the OpenGL Architecture Review Board; otherwise known as \"The Redbook.”\n\nOpenGL Reference Manual, by the OpenGL Architecture Review Board; otherwise known as \"The Bluebook.”\n\nAlthough these books provide a solid foundation in OpenGL, you’ll also need to read OpenGL Programming Guide for Mac for details on how to get the best performance on the Macintosh platform using the most recent OpenGL extensions and the OS X OpenGL frameworks.\n\nUsing OpenGL in a Custom Patch\n\nUsing OpenGL in a custom patch requires a bit more setup work than what’s needed for patches that don’t use OpenGL, but not much. You need to set the OpenGL context to the execution context of your custom patch. The setup requires two steps:\n\nInclude the CGLMacro.h file.\n\nSet the OpenGL context to the execution context of the custom patch using this line of code:\n\nCGLContextObj cgl_ctx = [context CGLContextObj];\n\nThe first step is done for you when you use an Xcode project template for Quartz Composer plug-ins. When you include the CGL macro header file, you can then use a local context variable to cache the current renderer. The local cache eliminates the need for OpenGL to perform a global context and renderer lookup for each command it executes, thereby reducing overhead and improving performance.\n\nThe CGLContextObj method of the QCPlugInContext protocol gets an OpenGL context that you can draw to from within the execution method of your custom patch. (See QCPlugInContext Protocol Reference.) After setting the OpenGL context (cgl_ctx), Quartz Composer sends all OpenGL commands to the OpenGL execution context of the custom patch.\n\nThe OpenGL code in your custom patch benefits from any programming techniques that improve the performance of any OpenGL code. In other words, there are no special requirements for OpenGL code that’s part of a custom patch. However, you’ll want to save and restore all state changes except the ones that are part of GL_CURRENT_BIT (RGBA color, color index, normal vector, texture coordinates, and so forth). You may also want to take a look at the chapter “Improving Performance” in OpenGL Programming Guide for Mac.\n\nRotating Square: Rendering a Texture to a Quad\n\nThe Rotating Square custom patch renders a texture to an OpenGL quad that you can animate. It has eight input parameters:\n\nEnable is a port that Quartz Composer automatically creates for a consumer patch.\n\nThe next three—X Position, Y Position, and Z Position—control the location of the center of the square.\n\nRotation Angle determines the angle of rotation of the quad.\n\nColor is the background color for the quad.\n\nImage is converted to a texture and scaled to fit the quad. See Getting Images From an Input Port.\n\nThe resulting patch is shown in Figure 4-2.\n\nFigure 4-2  The Rotating Square custom patch\n\nFollow these steps to create the Rotating Square custom patch:\n\nOpen Xcode and choose File > New Project.\n\nIn the New Project window, choose Standard Apple Plug-ins > Quartz Composer Plug-in and click Next.\n\nEnter RotatingSquare in the Project Name text field and click Next.\n\nOpen the RotatingSquarePlugin.h file.\n\nModify the interface file so that it has four dynamic Objective-C properties: x and y values, a color, and an input image.\n\n#import <Quartz/Quartz.h>\n\n\n \n\n\n@interface RotatingSquarePlugIn : QCPlugIn\n\n\n{\n\n\n}\n\n\n// Declare four property input ports of type Number with the\n\n\n// keys inputX, inputY, inputZ, and inputAngle\n\n\n@property double inputX;\n\n\n@property double inputY;\n\n\n@property double inputZ;\n\n\n@property double inputAngle;\n\n\n// Declare a property input port of type Color with the key inputColor\n\n\n@property(assign) CGColorRef inputColor;\n\n\n// Declare a property input port of type Image with the key inputImage\n\n\n@property(assign) id<QCPlugInInputImageSource> inputImage;\n\n\n@end\n\nSave and close the RotatingSquarePlugIn.h file.\n\nOpen the RotatingPlugin.m file. To ensure the best performance with OpenGL, make sure the file contains the following statement:\n\n#import <OpenGL/CGLMacro.h>\n\nYou’ll set up the OpenGL context later, in the execute method.\n\nImmediately after the implementation statement, add this directive so that Quartz Composer handles the implementation of the properties:\n\n@dynamic inputX, inputY, inputZ, inputAngle, inputColor, inputImage;\n\nAdd a space to separate the words in the name. Then modify the description for the custom patch.\n\nWhen you are done, the two #define statements to look as follows:\n\n#define    kQCPlugIn_Name @\"Rotating Square\"\n\n\n#define    kQCPlugIn_Description  @\"Renders a colored square that you can animate.\"\n\nNext you’ll write the methods needed to implement the RotatingSquarePlugIn subclass. The attributes method provided in the template should already look like this:\n\n+ (NSDictionary*) attributes\n\n\n{\n\n\n    return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n                kQCPlugIn_Name,QCPlugInAttributeNameKey,\n\n\n                kQCPlugIn_Description,QCPlugInAttributeDescriptionKey,\n\n\n                nil];\n\n\n}\n\nModify the attributesForPropertyPortWithKey: method so that it returns a dictionary for each input parameter. The port attribute key name is what appears in Quartz Composer as a label for the custom patch port.\n\nThe method should look as follows:\n\n+ (NSDictionary*) attributesForPropertyPortWithKey:(NSString*)key\n\n\n{\n\n\n    if([key isEqualToString:@\"inputX\"])\n\n\n        return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n                @\"X Position\", QCPortAttributeNameKey,\n\n\n                nil];\n\n\n    if([key isEqualToString:@\"inputY\"])\n\n\n        return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n                @\"Y Position\", QCPortAttributeNameKey,\n\n\n                nil];\n\n\n    if([key isEqualToString:@\"inputZ\"])\n\n\n        return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n                @\"Z Position\", QCPortAttributeNameKey,\n\n\n                nil];\n\n\n    if([key isEqualToString:@\"inputAngle\"])\n\n\n        return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n                @\"Rotation Angle\", QCPortAttributeNameKey,\n\n\n                nil];\n\n\n    if([key isEqualToString:@\"inputColor\"])\n\n\n        return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n                @\"Color\", QCPortAttributeNameKey,\n\n\n                nil];\n\n\n    if([key isEqualToString:@\"inputImage\"])\n\n\n        return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            @\"Image\", QCPortAttributeNameKey, nil];\n\n\n \n\n\n    return nil;\n\n\n}\n\nMake sure the executionMode method returns kQCPlugInExecutionModeConsumer to indicate that the custom patch is a consumer. Among other things, this causes Quartz Composer to add an Enable input port to the resulting patch.\n\n+ (QCPlugInExecutionMode) executionMode\n\n\n{\n\n\n    return kQCPlugInExecutionModeConsumer;\n\n\n}\n\nMake sure that the timeMode returns kQCPlugInTimeModeNone.\n\nThis is the default, so you should not need to make any changes to the provided code.\n\nThis custom patch executes only when the input values change.\n\n+ (QCPlugInTimeMode) timeMode\n\n\n{\n\n\n    return kQCPlugInTimeModeNone;\n\n\n}\n\nImplement the execution method for the execution context. This is where the OpenGL command stream is defined. You need to define the CGL context here.\n\n- (BOOL) execute:(id<QCPlugInContext>)context\n\n\n            atTime:(NSTimeInterval)time\n\n\n            withArguments:(NSDictionary*)arguments\n\n\n{\n\n\n    // Define a context and set it. This line causes OpenGL to use macros.\n\n\n    CGLContextObj     cgl_ctx = [context CGLContextObj];\n\n\n    id<QCPlugInInputImageSource>    image;\n\n\n    GLuint            textureName;\n\n\n    GLint             saveMode;\n\n\n    const CGFloat*    colorComponents;\n\n\n    GLenum      error;\n\n\n \n\n\n    if(cgl_ctx == NULL)\n\n\n        return NO;\n\n\n    // Copy the image on the input port to a local variable.\n\n\n    image = self.inputImage;\n\n\n \n\n\n    // Get a texture from the image in the context color space\n\n\n    if(image && [image lockTextureRepresentationWithColorSpace:([image shouldColorMatch] ? [context colorSpace] :\n\n\n                        [image imageColorSpace])\n\n\n               forBounds:[image imageBounds]])\n\n\n        textureName = [image textureName];\n\n\n    else\n\n\n        textureName = 0;\n\n\n \n\n\n    // Save and set the modelview matrix.\n\n\n    glGetIntegerv(GL_MATRIX_MODE, &saveMode);\n\n\n    glMatrixMode(GL_MODELVIEW);\n\n\n    glPushMatrix();\n\n\n    // Translate the matrix\n\n\n    glTranslatef(self.inputX, self.inputY, self.inputZ);\n\n\n    // Rotate the matrix\n\n\n    glRotatef(self.inputAngle, 0.0, 1.0, 0.0);\n\n\n \n\n\n    // Bind the texture to a texture unit\n\n\n    if(textureName) {\n\n\n        [image bindTextureRepresentationToCGLContext:cgl_ctx\n\n\n                                         textureUnit:GL_TEXTURE0\n\n\n                                normalizeCoordinates:YES];\n\n\n   }\n\n\n    // Get the color components (RGBA) from the input color port.\n\n\n    colorComponents = CGColorGetComponents(self.inputColor);\n\n\n    // Set the color.\n\n\n    glColor4f(colorComponents[0], colorComponents[1], colorComponents[2], colorComponents[3]);\n\n\n \n\n\n    // Render the textured quad by mapping the texture coordinates to the vertices\n\n\n    glBegin(GL_QUADS);\n\n\n        glTexCoord2f(1.0, 1.0);\n\n\n        glVertex3f(0.5, 0.5, 0); // upper right\n\n\n        glTexCoord2f(0.0, 1.0);\n\n\n        glVertex3f(-0.5, 0.5, 0); // upper left\n\n\n        glTexCoord2f(0.0, 0.0);\n\n\n        glVertex3f(-0.5, -0.5, 0); // lower left\n\n\n        glTexCoord2f(1.0, 0.0);\n\n\n        glVertex3f(0.5, -0.5, 0); // lower right\n\n\n    glEnd();\n\n\n \n\n\n    // Unbind the texture from the texture unit.\n\n\n    if(textureName)\n\n\n        [image unbindTextureRepresentationFromCGLContext:cgl_ctx\n\n\n                                             textureUnit: GL_TEXTURE0];\n\n\n \n\n\n    // Restore the modelview matrix.\n\n\n    glMatrixMode(GL_MODELVIEW);\n\n\n    glPopMatrix();\n\n\n    glMatrixMode(saveMode);\n\n\n \n\n\n    // Check for OpenGL errors and log them if there are errors.\n\n\n    if(error = glGetError())\n\n\n        [context logMessage:@\"OpenGL error %04X\", error];\n\n\n \n\n\n    // Release the texture.\n\n\n    if(textureName)\n\n\n        [image unlockTextureRepresentation];\n\n\n \n\n\n    return (error ? NO : YES);\n\n\n}\n\nOpen the Info.plist file and make sure the following key is an entry in the dictionary:\n\n<key>QCPlugInClasses</key>\n\n\n<array>\n\n\n    <string>RotatingSquarePlugIn</string>\n\n\n</array>\n\nIf you want, customize the bundle identifier. Then save and close the file.\n\nUnder Targets, choose Build & Copy. Then, click Build Build & Copy from the Action pop-up menu.\n\nWhen you build using this option, Xcode copies the successfully built plug-in to ~/Library/Graphics/Quartz Composer Plug-Ins.\n\nOpen Quartz Composer and search for the Rotating Square custom patch in the Patch Creator.\n\nCreate a composition to test the custom patch. Try setting the color and the positions. Add an interpolation patch to animate the square. To check the image input, you can drag an image directly to the editor and then connect the image to the patch, as shown below.\n\nKeep in mind that if the image is not square, the Rotating Square patch maps the coordinates to fit the image in the square, distorting the image if necessary.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2010 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2010-03-24\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Writing Image Processing Patches",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposer_Patch_PlugIn_ProgGuide/qc_image_proc/qc_image_proc.html#//apple_ref/doc/uid/TP40004787-CH7-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer Custom Patch Programming Guide\nTable of Contents\nIntroduction\nThe Basics of Custom Patches\nWriting Processor Patches\nWriting Image Processing Patches\nWriting Consumer Patches\nGlossary\nRevision History\nNext\nPrevious\nWriting Image Processing Patches\n\nThe Quartz Composer framework defines protocols for getting image data from input ports and providing image data to output ports. These protocols eliminate the need to use explicit image data types such as CIImage or CGImage objects. Using protocols ensures there won’t be any impedance mismatches between the image output port of one patch and the image input port of another.\n\nThis chapter describes how to implement the QCPlugInInputImageSource and QCPlugInOutputImageProvider protocols. Then it provides step-by-step instructions for creating a custom image processing patch that uses two image input ports and one image output port.\n\nGetting Images From an Input Port\n\nThe QCPlugInInputImageSource protocol converts an input image from whatever format it’s in to either a memory buffer or OpenGL texture representation. When you need to access the pixels in an image, you simply convert the image to a representation (texture or buffer) using one of the methods defined by the protocol. Use a texture representation to process image data on the GPU. Use a buffer representation to process image data on the CPU.\n\nTo create an image input port as an Objective-C 2.0 property, declare it as follows:\n\n@property(dynamic) id<QCPlugInInputImageSource> inputImage;\n\nTo create an image input port dynamically, use the type QCPortTypeImage:\n\n[self addInputPortWithType:QCPortTypeImage\n\n\n                    forKey:@\"inputImage\"\n\n\n            withAttributes:nil];\n\nYou convert input images to textures only when you plan to use OpenGL to process the data, as OpenGL is the software interface to the GPU. To use input images as textures you need to perform these steps:\n\nLock the texture representation using lockTextureRepresentationWithColorSpace:forBounds:. This method creates a read-only OpenGL texture from a subregion of the input image.\n\nBind the texture to a texture unit. You can perform this step entirely with OpenGL commands, but it is much easier to use the convenience method bindTextureRepresentationToCGLContext:textureUnit:normalizeCoordinates: which binds the texture to the provided texture unit (GL_TEXTURE0, and so on). It also loads the texture matrix onto the texture stack, scaling and flipping the coordinates if necessary, as long as you pass YES as the normalizeCoordinates argument.\n\nUse the texture in whatever way is appropriate for your application.\n\nWhen you no longer need the texture, unbind it from its texture unit. If you used the binding convenience method in step 2, then you must call the unbindTextureRepresentationFromCGLContext:textureUnit convenience method.\n\nRelease the OpenGL texture representation of the input image by calling the unlockTextureRepresentation method.\n\nIn addition to these steps, there are other tasks you should perform when using OpenGL. Writing Consumer Patches provides tips for using OpenGL in a custom patch and shows how to bind a texture and render it to a destination.\n\nYou convert images to a buffer representation only if your custom patch manipulates input images on the CPU. To use input image data in a buffer you need to perform these steps:\n\nCreate and lock a buffer representation of the image using the lockBufferRepresentationWithPixelFormat:colorSpace:forBounds: method, which creates a read-only memory buffer from a subregion of the input image. The pixel format and color space must be compatible.\n\nUse the image data. You can get the base address of the image buffer with the bufferBaseAddress method and the number of bytes per row with the bufferBytesPerRow method.\n\nWhen you no longer need the image buffer, call the unlockBufferRepresentation to release it.\n\nQCPlugInInputImageSource Protocol Reference provides more information on these methods as well as the methods that retrieve texture and image buffer information—such as the bounds, height, width, and color space. Histogram Operation: Modifying Color in an Image shows how to convert an input image to an image buffer.\n\nProviding Images for an Output Port\n\nThe QCPlugInOutputImageProvider protocol defines methods for rendering image data to an image buffer or to a texture. Quartz Composer calls the methods you implement only when the output image is needed. This “lazy” approach to supplying the output image is efficient and ensures the best performance possible.\n\nIf your custom patch has an image output port, you need to implement the appropriate methods for rendering image data and to supply information about the rendering destination and the image bounds.\n\nTo create an image output port as an Objective-C 2.0 property, declare it as follows:\n\n@property(assign) id<QCPlugInOutputImageProvider> outputImage;\n\nTo create an image input port dynamically use the type QCPortTypeImage:\n\n[self addOutputPortWithType:QCPortTypeImage\n\n\n                    forKey:@\"outputImage\"\n\n\n           withAttributes:nil];\n\nTo write images to that port, you need to perform these steps:\n\nCreate an internal class that represents the output image.\n\n@interface MyOutputImageProvider : NSObject <QCPlugInOutputImageProvider>\n\n\n{\n\n\n  // Declare instance variables, as appropriate\n\n\n}\n\nImplement the methods that provide information about the image—imageBounds imageColorSpace.\n\nImplement the methods that provide information about the rendering destination. If your custom patch renders to an image buffer, you must implement thesupportedBufferPixelFormats method. If it renders to a texture, you must implement thesupportedDrawablePixelFormats and canRenderWithCGLContext: methods.\n\nImplement one of the methods for rendering to a destination. If your custom patch renders to an image buffer, you must implement the renderToBuffer:withBytesPerRow:pixelFormat:forBounds: method. If your custom patch renders to a texture, you must implement the renderWithCGLContext:toDrawableWithPixelFormat:forBounds: method.\n\nNote: If you are using textures and you want to render to a framebuffer object or create a texture to use in an intermediate processing step, you can also implement the createRenderedTextureWithCGLContext:forBounds: method. If this method returns [0], Quartz Composer calls the renderWithCGLContext:toDrawableWithPixelFormat:forBounds: method.\n\nSee QCPlugInOutputImageProvider Protocol Reference for additional details on these methods.\n\nWhen Quartz Composer calls renderToBuffer:withBytesPerRow:pixelFormat:forBounds:, it passes your method a base address, the number of row bytes, the pixel format of the image data, and the bounds of the subregion. Your method then writes pixels to the supplied image buffer. Histogram Operation: Modifying Color in an Image provides an example of how to implement this method.\n\nWhen Quartz Composer calls renderWithCGLContext:toDrawableWithPixelFormat:forBounds:, it automatically sets the viewport to the dimensions of the image, and the projection and modelview matrices to the identity matrix. Prior to rendering, you must save all the OpenGL states that you plan to change except the ones defined by GL_CURRENT_BIT. When you are done rendering, you must restore the saved OpenGL states.\n\nHistogram Operation: Modifying Color in an Image\n\nThis section shows how to write a custom patch that has two image input ports and one image output port. The patch computes a histogram for one of the input images, and uses that histogram to modify the colors in the other input image. It outputs the color-modified image. This patch is a bit more complex than those described in the rest of the book. Before following the instructions in this section, make sure that you’ve read Writing Processor Patches and are familiar with the basic tasks for setting up and creating a custom patch.\n\nThe Histogram Operation custom patch described in this section is similar to, but not exactly like, the Histogram Operation sample project provided with the Developer Tools for OS X v10.5 in:\n\n/Developer/Examples/Quartz Composer/Plugins\n\nYou might also want to take a look at that sample project.\n\nAn RGBA histogram is a count of the values at each intensity level, for each pixel component (red, green, blue, and alpha). For an image with a bit depth of 8 bits, each component can have a value from 0 to 255. Figure 3-1 shows an RGB histogram for a daisy image. The intensity values are plotted on the x-axis and the number of pixels are on the y-axis. The image is opaque, so there is no need to show the alpha component in this figure.\n\nFigure 3-1  An RGB histogram for an image of a daisy\n\nThe image data for this patch is processed using the CPU, so you’ll see how to create an image buffer representation and render to an image buffer by defining a custom output image provider. (You can find out how to create a texture representation by reading Writing Consumer Patches.) You’ll also see how to use the Accelerate framework to compute a histogram.\n\nFigure 3-2 shows a composition that uses the Histogram Operation custom patch. By looking at the thumbnail images, you can get an idea of how the patch modifies the source image using the histogram image.\n\nFigure 3-2  A Quartz composition that uses the Histogram Operation custom patch\n\nThe Accelerate framework is a high-performance vector-accelerated library of routines. It’s ideal to use for custom patches that perform intensive computations. You’ll use the framework’s vImage library to compute a histogram. The function vImageHistogramCalculation_ARGB8888 calculates histograms for the alpha, red, green, and blue channels (see Listing 3-1). It takes an image buffer, an array to store histogram data, and a flag to indicate whether to turn off vImage internal tiling routines . (See vImage Reference Collection.)\n\nListing 3-1  Prototype for the vImage histogram function\n\nvImage_Error vImageHistogramCalculation_ARGB8888 (\n\n\n                      const vImage_Buffer *src,\n\n\n                      vImagePixelCount *histogram[4],\n\n\n                      vImage_Flags flags\n\n\n);\n\nThe steps for creating the Histogram Operation custom patch are in these sections:\n\nCreate the Xcode Project\n\nCreate the Interface\n\nModify the Methods for the PlugIn Class\n\nImplement the Methods for the Histogram Object\n\nWrite Methods for the Output Image Provider\n\nWrite the Execution Methods for the Plug-in Class\n\nCreate the Xcode Project\n\nTo create the Histogram Operation Xcode project, follow these steps.:\n\nOpen Xcode and choose File > New Project.\n\nIn the New Project window, choose Standard Apple Plug-ins > Quartz Composer Plug-in and click Next.\n\nEnter HistogramOperation in the Project Name text field and click Finish.\n\nChoose Project > Add to Project, navigate to the Accelerate Framework, and click Add.\n\nThis framework is in System/Library/Frameworks.\n\nIn the sheet that appears, click Add.\n\nCreate the Interface\n\nIf you created the custom patches in Writing Processor Patches, most of the steps in this section should be familiar to you.\n\nOpen the HistogramOperationPlugin.h file.\n\nAdd a statement to import the Accelerate framework.\n\n#import <Accelerate/Accelerate.h>\n\nDeclare two properties for image input ports—one for the source image that the custom patch modifies and another for an image used for the histogram. Declare a property for the output image port. Your code should look as follows:\n\n#import <Quartz/Quartz.h>\n\n\n#import <Accelerate/Accelerate.h>\n\n\n \n\n\n@interface HistogramOperationPlugIn : QCPlugIn\n\n\n{\n\n\n \n\n\n}\n\n\n@property(assign) id<QCPlugInInputImageSource> inputSourceImage;\n\n\n@property(assign) id<QCPlugInInputImageSource> inputHistogramImage;\n\n\n@property(assign) id<QCPlugInOutputImageProvider> outputResultImage;\n\n\n@end\n\nAdd the interface for a class that computes an RGBA histogram from an image.\n\nThe Histogram object holds the image source from which you’ll compute a histogram. In addition to an image source instance variable, you need to create four instance variables to store a count of the color component and alpha values—red, green, blue, and alpha.\n\nYou need to write a method that initializes the image instance variable. You’ll need another method to compute the histogram values. Declare these methods now; you’ll write them later.\n\nYour code should look similar to the following:\n\n@interface Histogram : NSObject\n\n\n{\n\n\n    id<QCPlugInInputImageSource>    _image;\n\n\n \n\n\n    vImagePixelCount                _histogramA[256];\n\n\n    vImagePixelCount                _histogramR[256];\n\n\n    vImagePixelCount                _histogramG[256];\n\n\n    vImagePixelCount                _histogramB[256];\n\n\n    CGColorSpaceRef                 _colorSpace;\n\n\n}\n\n\n- (id) initWithImageSource:(id<QCPlugInInputImageSource>)image colorSpace:(CGColorSpaceRef)colorSpace;\n\n\n- (BOOL) getRGBAHistograms:(vImagePixelCount**)histograms;\n\n\n@end\n\nIn the interface for the HistogramOperationPlugIn class, add an instance variable for a Histogram object. You’ll use this to cache the image histogram.\n\nThe interface should look like this:\n\n@interface HistogramOperationPlugIn : QCPlugIn\n\n\n{\n\n\n    Histogram*                        _cachedHistogram;\n\n\n}\n\nNote that the interface for the Histogram class must either be specified before the HistogramOperationPlugIn class or the class must be declared using:\n\n@class Histogram;\n\nAdd the interface for an internal class for the image provider, to represent the output image produced by the custom patch.\n\nThis class has two instance variable—the input image used to create a histogram, and the Histogram object that you’ll use to modify the source image. You also need to declare a method to initialize the image instance variable. You’ll write the initWithImageSource:histogram: method later. When done, your code should look like this:\n\n@interface HistogramImageProvider : NSObject <QCPlugInOutputImageProvider>\n\n\n{\n\n\n    id<QCPlugInInputImageSource>    _image;\n\n\n    Histogram*                      _histogram;\n\n\n}\n\n\n- (id) initWithImageSource:(id<QCPlugInInputImageSource>)image\n\n\n                 histogram:(Histogram*)histogram;\n\n\n@end\n\nClose the HistogramOperationPlugIn.h file.\n\nModify the Methods for the PlugIn Class\n\nNext you’ll modify the methods needed to implement the HistogramOperationPlugIn class.\n\nOpen the HistogramOperationPlugIn.m file.\n\nJust after the implementation statement for HistogramOperationPlugin, declare the input and output properties as dynamic. Quartz Composer will handle their implementation.\n\n@dynamic inputSourceImage, inputHistogramImage, outputResultImage;\n\nModify the description and name for the custom patch.\n\n#define    kQCPlugIn_Name  @\"Histogram Operation\"\n\n\n#define kQCPlugIn_Description @\"Alters a source image according to the histogram of another image.\"\n\nYou do not need to modify the default attributes method supplied in the template, which should look as follows:\n\n+ (NSDictionary*) attributes\n\n\n{\n\n\n    return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n                kQCPlugIn_Name,QCPlugInAttributeNameKey,\n\n\n                kQCPlugIn_Description,QCPlugInAttributeDescriptionKey,\n\n\n                nil];\n\n\n}\n\nModify the attributesForPropertyPortWithKey: so that it returns a dictionary for each input and output parameter.\n\n+ (NSDictionary*) attributesForPropertyPortWithKey:(NSString*)key\n\n\n{\n\n\n \n\n\n  if([key isEqualToString:@\"inputSourceImage\"])\n\n\n    return [NSDictionary dictionaryWithObjectsAndKeys:@\"Source Image\",\n\n\n                         QCPortAttributeNameKey, nil];\n\n\n  if([key isEqualToString:@\"inputHistogramImage\"])\n\n\n    return [NSDictionary dictionaryWithObjectsAndKeys:@\"Histogram Image\",\n\n\n                          QCPortAttributeNameKey, nil];\n\n\n  if([key isEqualToString:@\"outputResultImage\"])\n\n\n    return [NSDictionary dictionaryWithObjectsAndKeys:@\"Result Image\",\n\n\n                          QCPortAttributeNameKey, nil];\n\n\n  return nil;\n\n\n}\n\nMake sure the executionMode method returns kQCPlugInExecutionModeProcessor.\n\n+ (QCPlugInExecutionMode) executionMode\n\n\n{\n\n\n    return kQCPlugInExecutionModeProcessor;\n\n\n}\n\nMake sure the timeMode method returns kQCPlugInTimeModeNone.\n\n+ (QCPlugInTimeMode) timeMode\n\n\n{\n\n\n    return kQCPlugInTimeModeNone;\n\n\n}\nImplement the Methods for the Histogram Object\n\nBefore you implement the execution methods for HistogramOperationPlugIn, you’ll implement the methods needed for the Histogram class—one to initialize the object with an image, a method to release the image when it is no longer needed, and another method that creates an image buffer from an input image and uses vImage to compute a histogram.\n\nYou need to add the code in this section between these statements:\n\n@implementation Histogram\n\n\n@end\n\nWrite an initialize method that retains the image used to calculate the histogram.\n\n- (id) initWithImageSource:(id<QCPlugInInputImageSource>)image colorSpace:(CGColorSpaceRef)colorSpace\n\n\n{\n\n\n   // Make sure there is an image.\n\n\n  if(!image) {\n\n\n        [self release];\n\n\n        return nil;\n\n\n    }\n\n\n \n\n\n    // Keep the image and the processing color space around.\n\n\n    self = [super init];\n\n\n    if (self) {\n\n\n       _image = [(id)image retain];\n\n\n       _colorSpace = CGColorSpaceRetain(colorSpace);\n\n\n     }\n\n\n    return self;\n\n\n}\n\nWrite a method that releases the histogram image when it’s no longer needed.\n\n- (void) dealloc\n\n\n{\n\n\n    [(id)_image release];\n\n\n    CGColorSpaceRelease(_colorSpace);\n\n\n \n\n\n    [super dealloc];\n\n\n}\n\nWrite a method that gets and stores histogram data for each pixel component.\n\nIn this method, you need to get a buffer representation of the image on the histogram image input port.\n\n- (BOOL) getRGBAHistograms:(vImagePixelCount**)histograms\n\n\n{\n\n\n    vImage_Buffer                    buffer;\n\n\n    vImage_Error                    error;\n\n\n \n\n\n    if(_image) {\n\n\n        // Get a buffer representation from the image\n\n\n        if(![_image  lockBufferRepresentationWithPixelFormat:QCPlugInPixelFormatARGB8\n\n\n                     colorSpace:[_image imageColorSpace]\n\n\n                     forBounds:[_image imageBounds]])\n\n\n            return NO;\n\n\n \n\n\n        // Set up the vImage buffer\n\n\n        buffer.data = (void*)[_image bufferBaseAddress];\n\n\n        buffer.rowBytes = [_image bufferBytesPerRow];\n\n\n        buffer.width = [_image bufferPixelsWide];\n\n\n        buffer.height = [_image bufferPixelsHigh];\n\n\n        // Set up the vImage histogram array\n\n\n        histograms[0] = _histogramA;\n\n\n        histograms[1] = _histogramR;\n\n\n        histograms[2] = _histogramG;\n\n\n        histograms[3] = _histogramB;\n\n\n        // Call the vImage function to compute the histograms for the image data\n\n\n        error = vImageHistogramCalculation_ARGB8888(&buffer, histograms, 0);\n\n\n \n\n\n        // Now that you have the histogram, you can release the buffer\n\n\n        [_image unlockBufferRepresentation];\n\n\n        // Handle errors, if there are any\n\n\n        if(error != kvImageNoError)\n\n\n            return NO;\n\n\n \n\n\n        // You no longer need the histogram image, so release it\n\n\n        [(id)_image release];\n\n\n        _image = nil;\n\n\n    }\n\n\n \n\n\n    // Reverse the histogram data\n\n\n    histograms[0] = _histogramR;\n\n\n    histograms[1] = _histogramG;\n\n\n    histograms[2] = _histogramB;\n\n\n    histograms[3] = _histogramA;\n\n\n \n\n\n    return YES;\n\n\n}\nWrite the Execution Methods for the Plug-in Class\n\nTo make the code more readable, place the code in this section between these statements:\n\n@implementation HistogramOperationPlugIn (Execution)\n\n\n@end\n\nWrite the execute method for the plug-in class.\n\nThis method is invoked by Quartz Composer whenever either of the input ports change. The method updates the histogram image if it changed and creates a HistogramImageProvider object from the source image and the cached histogram.\n\n- (BOOL) execute:(id<QCPlugInContext>)context atTime:(NSTimeInterval)time withArguments:(NSDictionary*)arguments\n\n\n{\n\n\n    id<QCPlugInInputImageSource>    image;\n\n\n    HistogramImageProvider*         provider;\n\n\n    CGColorSpaceRef                 colorSpace;\n\n\n \n\n\n    // If the histogram input image changes, update the cached histogram.\n\n\n    if([self didValueForInputKeyChange:@\"inputHistogramImage\"]) {\n\n\n        [_cachedHistogram release];\n\n\n        if(image = self.inputHistogramImage) {\n\n\n           colorSpace = (CGColorSpaceGetModel([image imageColorSpace]) == kCGColorSpaceModelRGB ?\n\n\n               [image imageColorSpace] : [context colorSpace]);\n\n\n           _cachedHistogram = [[Histogram alloc]\n\n\n                    initWithImageSource:self.inputHistogramImage\n\n\n                    colorSpace:colorSpace];\n\n\n        }\n\n\n        else\n\n\n            _cachedHistogram = nil;\n\n\n    }\n\n\n \n\n\n    // Check for a histogram and a source image, if they both exist,\n\n\n    // create the provider and initialize it with the source image and histogram\n\n\n    if(_cachedHistogram && (image = self.inputSourceImage)) {\n\n\n        provider = [[HistogramImageProvider alloc]\n\n\n                        initWithImageSource:sourceImage\n\n\n                        histogram:_cachedHistogram];\n\n\n        // Bail out if the provider doesn't exist\n\n\n        if(provider == nil)\n\n\n          return NO;\n\n\n        // Otherwise, set the output image to the provider\n\n\n        self.outputResultImage = provider;\n\n\n        // Release the provider\n\n\n        [provider release];\n\n\n    }\n\n\n    else\n\n\n       // If the histogram and source image don't both exist,\n\n\n       // set the output image to nil\n\n\n       self.outputResultImage = nil;\n\n\n \n\n\n    return YES;\n\n\n}\n\nImplement the stop execution method.\n\nThis method is optional. But, for this custom patch, you need to release the cached histogram when the custom patch stops executing. So you must implement it.\n\n- (void) stopExecution:(id<QCPlugInContext>)context\n\n\n{\n\n\n    [_cachedHistogram release];\n\n\n    _cachedHistogram = nil;\n\n\n}\nWrite Methods for the Output Image Provider\n\nAll of the code in this section needs to be added between these statements:\n\n@implementation HistogramImageProvider\n\n\n@end\n\nThe output image provider does all the work to render the image. Recall that Quartz Composer invokes your render method only when an output image is needed, thereby avoiding unnecessary computations.\n\nWrite a method that initializes the image provider with a source image an a previously computed histogram.\n\n- (id) initWithImageSource:(id<QCPlugInInputImageSource>)image histogram:(Histogram*)histogram\n\n\n{\n\n\n    // Check to make sure  an image and a histogram exists.\n\n\n    if(!image || !histogram) {\n\n\n        [self release];\n\n\n        return nil;\n\n\n    }\n\n\n \n\n\n    // Keep the image and histogram around.\n\n\n    self = [super init];\n\n\n    if (self) {\n\n\n        _image = [(id)image retain];\n\n\n        _histogram = [histogram retain];\n\n\n    }\n\n\n \n\n\n    return self;\n\n\n}\n\nImplement a dealloc method that releases the image and the histogram.\n\n \n\n\n- (void) dealloc\n\n\n{\n\n\n    [(id)_image release];\n\n\n    [_histogram release];\n\n\n \n\n\n    [super dealloc];\n\n\n}\n\nImplement a method to inform Quartz Composer of the bounds of the image.\n\n- (NSRect) imageBounds\n\n\n{\n\n\n    // This image has the same bounds as the source image.\n\n\n    return [_image imageBounds];\n\n\n}\n\nImplement a method to inform Quartz Composer of the color space used by the image.\n\n- (CGColorSpaceRef) imageColorSpace\n\n\n{\n\n\n    // Preserve the original image color space.\n\n\n    return [_image imageColorSpace];\n\n\n}\n\nImplement a method to inform Quartz Composer of the supported pixel formats.\n\nYou need to support ARGB8, BGRA8 and RGBAf. Use the constants supplied in the Quartz Composer framework.\n\n- (NSArray*) supportedBufferPixelFormats\n\n\n{\n\n\n    /* Support for only ARGB8, BGRA8 and RGBAf */\n\n\n    return [NSArray arrayWithObjects:QCPlugInPixelFormatARGB8,\n\n\n                                      QCPlugInPixelFormatBGRA8,\n\n\n                                      QCPlugInPixelFormatRGBAf,\n\n\n                                      nil];\n\n\n}\n\nImplement the render to buffer method.\n\nQuartz Composer invokes this method whenever your custom patch needs to produce an output image. This happens when the image output port is connected to an image input port and when one of the input images change.\n\n- (BOOL) renderToBuffer:(void*)baseAddress\n\n\n        withBytesPerRow:(NSUInteger)rowBytes\n\n\n            pixelFormat:(NSString*)format\n\n\n              forBounds:(NSRect)bounds\n\n\n{\n\n\n    vImage_Buffer        inBuffer,\n\n\n                         outBuffer;\n\n\n    vImage_Error         error;\n\n\n    const vImagePixelCount*     histograms[4];\n\n\n    const vImagePixelCount*     temp;\n\n\n \n\n\n    // Retrieve histogram data. This triggers computation of the\n\n\n    // histogram if necessary.\n\n\n    if(![_histogram getRGBAHistograms:(vImagePixelCount**)histograms])\n\n\n         return NO;\n\n\n \n\n\n    // Get a buffer representation for the source image.\n\n\n    if(![_image lockBufferRepresentationWithPixelFormat:format\n\n\n                                             colorSpace:[_image imageColorSpace]\n\n\n                                              forBounds:bounds])\n\n\n         // Bail out if the buffer representation fails\n\n\n         return NO;\n\n\n \n\n\n    // Apply the previously computed histogram to the source image and\n\n\n    // render the result to the output buffer\n\n\n    inBuffer.data = (void*)[_image bufferBaseAddress];\n\n\n    inBuffer.rowBytes = [_image bufferBytesPerRow];\n\n\n    inBuffer.width = [_image bufferPixelsWide];\n\n\n    inBuffer.height = [_image bufferPixelsHigh];\n\n\n    outBuffer.data = baseAddress;\n\n\n    outBuffer.rowBytes = rowBytes;\n\n\n    outBuffer.width = [_image bufferPixelsWide];\n\n\n    outBuffer.height = [_image bufferPixelsHigh];\n\n\n    // Call the vImage histogram function that's appropriate\n\n\n    // for the pixel format.\n\n\n    if([format isEqualToString:QCPlugInPixelFormatRGBAf])\n\n\n        error = vImageHistogramSpecification_ARGBFFFF(&inBuffer, &outBuffer,\n\n\n                                                        NULL, histograms,\n\n\n                                                        256, 0.0, 1.0, 0);\n\n\n    else if([format isEqualToString:QCPlugInPixelFormatARGB8]) {\n\n\n        // You need to convert the histogram from RGBA to ARGB\n\n\n        temp = histograms[3];\n\n\n        histograms[3] = histograms[2];\n\n\n        histograms[2] = histograms[1];\n\n\n        histograms[1] = histograms[0];\n\n\n        histograms[0] = temp;\n\n\n        error = vImageHistogramSpecification_ARGB8888(&inBuffer, &outBuffer,\n\n\n                                                      histograms, 0);\n\n\n    }\n\n\n    else if([format isEqualToString:QCPlugInPixelFormatBGRA8]) {\n\n\n        // You need to convert the histogram from RGBA to BGRA\n\n\n        temp = histograms[0];\n\n\n        histograms[0] = histograms[2];\n\n\n        histograms[2] = temp;\n\n\n        error = vImageHistogramSpecification_ARGB8888(&inBuffer, &outBuffer,\n\n\n                                                      histograms, 0);\n\n\n    }\n\n\n    else\n\n\n        // This should never happen.\n\n\n        error = -1;\n\n\n \n\n\n    // Release the buffer representation.\n\n\n    [_image unlockBufferRepresentation];\n\n\n    // Check for vImage errors.\n\n\n    if(error != kvImageNoError)\n\n\n          return NO;\n\n\n    // Success!\n\n\n    return YES;\n\n\n}\nNext\nPrevious\n\n\n\n\n\nCopyright © 2010 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2010-03-24\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Document Revision History",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposerUserGuide/qc_revhistory/qc_revhistory.html#//apple_ref/doc/uid/TP40005381-CH205-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer User Guide\nTable of Contents\nIntroduction\nQuartz Composer Basic Concepts\nThe Quartz Composer User Interface\nBasic and Advanced Tasks, Tips, and Tricks\nTutorial: Creating a Composition\nGlossary\nRevision History\nPrevious\nDocument Revision History\n\nThis table describes the changes to Quartz Composer User Guide.\n\nDate\tNotes\n2007-07-17\t\n\nNew document that explains how to use the Quartz Composer development tool to create motion graphics compositions.\n\n\n \t\n\nSome of the content in this document was previously published in the OS X v10.4 version of Quartz Composer Programming Guide.\n\n\n\nPrevious\n\n\n\n\n\nCopyright © 2004, 2007 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2007-07-17\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Document Revision History",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposer/qc_revhistory/qc_revhistory.html#//apple_ref/doc/uid/TP40001357-CH205-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer Programming Guide\nTable of Contents\nIntroduction\nUsing QCView to Create a Standalone Composition\nPublishing Ports and Binding Them to Controls\nUsing the QCRenderer Class to Play a Composition\nAdding Compositions to Webpages and Widgets\nRevision History\nPrevious\nRetired Document\n\nImportant: This document may not represent best practices for current development. Links to downloads and other resources may no longer be valid.\n\nDocument Revision History\n\nThis table describes the changes to Quartz Composer Programming Guide.\n\nDate\tNotes\n2013-04-23\t\n\nMoved to Retired Documents Library.\n\n\n2008-10-15\t\n\nAdded description of \"onloading\" HTML attribute in Quartz Composer WebKit plug-in.\n\n\n2007-07-11\t\n\nUpdated for OS Xv10.5\n\n\n \t\n\nMoved information about using the Quartz Composer development tool to Quartz Composer User Guide.\n\n\n \t\n\nAdded information about Introduction to Quartz Composer Custom Patch Programming Guide.\n\n\n \t\n\nModified the instructions to use Interface Builder version 3.0 instead of version 2.5.\n\n\n2006-12-05\t\n\nAdded a chapter on using compositions in webpages and widgets.\n\n\n \t\n\nAdded a few cross references to make it easier to find related information.\n\n\n2006-07-24\t\n\nAdded additional information about where to get Quartz Composer.\n\n\n2005-11-09\t\n\nRemoved broken hyperlink and added information about the use of Internet resources in a QuickTime movie.\n\n\n2005-08-11\t\n\nMade revisions to chapter on QCRenderer.\n\n\n2005-07-07\t\n\nUpdated screenshots to reflect changes in the user interface for the Billboard patch and made changes to the QCRenderer code.\n\n\n2005-06-04\t\n\nFixed a pathname and improved the wording for a few instructions.\n\n\n2005-04-29\t\n\nUpdated for public release of OS X v10.4. First public version.\n\n\n \t\n\nChanged the title from Visual Computing With Quartz Composer to make it more consistent with the titles of similar documentation.\n\n\n \t\n\nCompletely revised to reflect changes in the Quartz Composer application.\n\n\n2004-06-29\t\n\nNew seed draft that describes the developmental tool, available with OS X v10.4, for processing and rendering graphical data.\n\n\n\nPrevious\n\n\n\n\n\nCopyright © 2004, 2013 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2013-04-23"
  },
  {
    "title": "Publishing Ports and Binding Them to Controls",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposer/qc_play_ib_input/qc_play_ib_input.html#//apple_ref/doc/uid/TP40001357-CH208-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer Programming Guide\nTable of Contents\nIntroduction\nUsing QCView to Create a Standalone Composition\nPublishing Ports and Binding Them to Controls\nUsing the QCRenderer Class to Play a Composition\nAdding Compositions to Webpages and Widgets\nRevision History\nNext\nPrevious\nRetired Document\n\nImportant: This document may not represent best practices for current development. Links to downloads and other resources may no longer be valid.\n\nPublishing Ports and Binding Them to Controls\n\nAny input or output port in a composition can be controlled through the use of Cocoa bindings by publishing the ports that you want to control. This chapter shows how to publish ports in a composition and then add controls to the user interface. When you make changes to the controls in Interface Builder, the changed values pass to the appropriate input ports that are in the composition.\n\nPerform these tasks to publish ports and bind them to controls:\n\nPublishing Ports\n\nSetting Up a Patch Controller\n\nBinding Controls to Input Ports\n\nNote: This chapter assumes that you are using Xcode and Interface Builder Version 3.0. Interface Builder Version 3.0 is a major update from the previous version of Interface Builder. For detailed information on how to use the new version, see Interface Builder User Guide.\n\nPublishing Ports\n\nYou’ll revise the MacEngraving composition provided with OS X v10.5 so that it has two published input ports at the root macro patch level—one that controls the clear color and the other that controls the text shown on the computer.\n\nOpen the MacEngraving composition located in Developer/Examples/Quartz Compositions/Interface Builder.\n\nNotice that the “Text” input port of the Image With String patch is gray, to indicate that this port is already published.\n\nControl-click the Clear Color input port on the Clear patch and choose Published Inputs > Clear Color. Then press Return.\n\nNotice that the Clear Color input port appears gray to indicate that it is published.\n\nIn the Viewer window, click Input Parameters.\n\nThe Text and Clear Color input ports appear as a sheet in the Viewer window. When you change the text and the clear color, you immediately see the results in the View window.\n\nClick the workspace to make sure that no patches are selected. Then open the Inspector to the Published Inputs & Outputs pane.\n\nYou should see the Text and Clear Color input port names and the key assigned to each. The key for the Text input port is text. The key for the Clear Color input port is Clear_Color. Make note of these, because you must use these keys in Interface Builder to set up the bindings. The keys you supply in Interface Builder must match exactly what you see in this pane; keys are case-sensitive.\n\nSave the composition and quit Quartz Composer.\n\nSetting Up a Patch Controller\n\nThe QCPatchController class establishes Cocoa bindings between the user interface controls and a composition. In the Cocoa model-view-controller (MVC) paradigm, QCPatchController acts as a controller between a composition (which is the model) and a QCView (which is the view). For more information on the MVC paradigm, see Cocoa Application Tutorial.\n\nFollow these steps to set up a patch controller:\n\nLaunch Interface Builder and choose a Cocoa Window template.\n\nDrag a Quartz Composer View from the Quartz Composer objects in the Library to the window.\n\nIf you don’t see Quartz Composer as an entry in the Library, you may need to add the Quartz Composer plug-in. See Add the Quartz Composer Plug-in.\n\nDrag a Quartz Composer Patch Controller (QCPatchController instance) from Library to the Nib document window.\n\nSelect the patch controller and open the Attributes inspector.\n\nClick “Load from Composition File” and choose the MacEngraving composition you modified in the previous section.\n\nSelect the QCView and then open the Bindings inspector.\n\nClick the disclosure triangle next to “Patch.”\n\nChoose Patch Controller from the “Bind to” pop-up menu.\n\nEnter patch in the Controller Key text field to bind the patch property to the patch key of the QCPatchController.\n\nThis binds the QCView to the composition that’s loaded in the QCPatchController so that the QCView renders this composition.\n\nChoose File > Simulate Interface.\n\nThe engraved computer appears in the window.\n\nQuit the interface test application.\n\nNote:  When you bind the patch property of a QCView, you can make sure that a composition is not loaded on the QCView by clicking the Unload button in the Attributes Inspector for the QCView.\n\nIn the next section you’ll add controls to the interface and set up bindings between them and the inputs of the composition root macro patch.\n\nBinding Controls to Input Ports\n\nThis section adds a color well and text field to the interface and binds them to the Clear Color and Power input ports published from the MacEngraving composition. (For information on color wells, see Choosing Colors With Color Wells and Color Panels.)\n\nDrag a Text Field from the Library to the window.\n\nThe easiest way to locate the field is to type text in the Search field of the Library.\n\nPlace the control below the QCView, on the left side.\n\nResize the text field to so it is half the width of the window.\n\nWith the text field selected, open the Bindings inspector and click the disclosure triangle next to Value.\n\nClick “Bind to” and then choose Patch Controller in the “Bind to” popup menu.\n\nEnter patch in the Controller Key text field.\n\nEnter text.value in the Model Key Path text field.\n\nThis binds the text field to the text input parameter of the composition that’s loaded in the QCPatchController. You retrieve the value on the port itself by appending .value to its unique key.\n\nNote that keys are case sensitive, so the Model Key Path field must reflect exactly the key listed in the Published Inputs & Outputs pane of the Inspector in the composition for that port. Recall that the key for the Text input port is text.\n\nDrag a color well control (NSColorWell instance) from the Library to the window.\n\nPlace the control below the right side of QCView, using the guides to align the color well properly in the window.\n\nWith the color well selected, open the Bindings inspector and click the disclosure triangle next to Value.\n\nClick “Bind to” and then choose Patch Controller in the “Bind to” popup menu.\n\nEnter patch in the Controller key text field.\n\nEnter Clear_Color.value in the Model Key Path text field.\n\nRecall that the key for the Clear Color input port is Clear_Color. The Model Key Path field must reflect exactly what’s specified in the composition for that port.\n\nThis binds the color well to the clear color input parameter of the composition that’s loaded in the QCPatchController. You retrieve the value on the port itself by appending .value to its unique key.\n\nChoose File > Simulate Interface.\n\nEnter text in the text field. Then click the color well control and change the background color.\n\nThe rendered view changes as the settings change.\n\nNote: If the controls don’t modify the composition, check the text that you typed into the Model Key Path text field. It must match exactly the key that you see in the Published Inputs & Outputs pane of the Inspector in the Quartz Composer development tool. Keys are case-sensitive.\n\nSee Also\n\n/Developer/Examples/Quartz Composer Compositions/Interface Builder Compositions contains other examples of nib files that bind controls in Interface Builder to input ports in a composition. You may want to look at these files before you bind controls to your own composition.\n\nCocoa Bindings Programming Topics contains more details and provides pointers to additional information.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2013 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2013-04-23"
  },
  {
    "title": "Text",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_text/dq_text.html#//apple_ref/doc/uid/TP30001066-CH213-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nText\n\nThis chapter previously described the basic text support provided by Quartz. However, the low-level support provided by Quartz has been deprecated and superceded by Core Text, an advanced low-level technology for laying out text and handing fonts. Core Text is designed for high performance and ease of use and allows you to draw Unicode text directly to a graphics context. If you are writing an application that needs precise control over how text is displayed, see Core Text Programming Guide.\n\nIf you are developing a text application for iOS, look first at Text Programming Guide for iOS, which describes text support in iOS. In particular, UIKit provides classes that implement common tasks, making it easy to add text to your application:\n\nIf you are developing a text application for Mac OS X, look first at Cocoa Text Architecture Guide, which describes the Cocoa text system. Cocoa provides full Unicode support, text input and editing, precise text layout and typesetting, font management, and many other advanced text-handling capabilities.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Using the QCRenderer Class to Play a Composition",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposer/qc_play_renderer/qc_play_renderer.html#//apple_ref/doc/uid/TP40001357-CH209-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer Programming Guide\nTable of Contents\nIntroduction\nUsing QCView to Create a Standalone Composition\nPublishing Ports and Binding Them to Controls\nUsing the QCRenderer Class to Play a Composition\nAdding Compositions to Webpages and Widgets\nRevision History\nNext\nPrevious\nRetired Document\n\nImportant: This document may not represent best practices for current development. Links to downloads and other resources may no longer be valid.\n\nUsing the QCRenderer Class to Play a Composition\n\nThe QCRenderer class is a simplified runtime object that can load and play a composition to an arbitrary OpenGL context. QCRenderer also provides an interface to pass data to the input ports or retrieve data from the output ports of the root macro patch of a composition.\n\nThis chapter shows how to create a QCRenderer object for a full screen OpenGL context and then load and render a Quartz Composer composition to that context. The sample code is part of a Cocoa application created using Xcode. Before reading this chapter, you may want to look at QCRenderer Class Reference.\n\nEach section in this chapter provides a code fragment and an explanation of the essential parts of the code fragment. To see how to put all the code together into a complete, working application, you will need to open, build, and run the Player sample application that’s provided with the OS X v10.5 developer tools. See Building and Running the Player Sample Project.\n\nThe following tasks are essential to creating a sample application that plays a full-screen composition using a QCRenderer object. Each is described in more detail in the rest of the chapter.\n\nDeclaring the Application Interface\n\nGetting a Composition File\n\nCapturing the Main Display\n\nSetting Up the OpenGL Context\n\nSetting Up Full Screen Display and Syncing\n\nCreating a QCRenderer Object\n\nCreating a Timer\n\nWriting the Rendering Routine\n\nOverriding the sendEvent Method\n\nDeclaring the Application Interface\n\nThis application is one of the rare ones that requires you to subclass NSApplication. You need to create a subclass so that you can override the sendEvent: method to catch user events while the application is displaying a full-screen OpenGL context, and also to set the NSApplication instance as its own delegate. See NSApplication Class Reference for more information.\n\nYou need to replace the principal class (NSApplication) by your custom subclass of NSApplication (in this example, PlayerApplication) in the Info.plist file. Open the Info.plist file from within Xcode, then find the following key-value pair:\n\n<key>NSPrincipalClass</key>\n\n\n<string>NSApplication</string>\n\nThen replace NSApplication with the name of the subclass. For this example, the revised key-value pair would look as follows:\n\n<key>NSPrincipalClass</key>\n\n\n<string>PlayerApplication</string>\n\nListing 3-1 shows the interface for PlayerApplication, which requires variables to create the OpenGL context, QCRenderer, and other objects that support rendering a composition.\n\nListing 3-1  The interface for PlayerApplication\n\n@interface PlayerApplication : NSApplication\n\n\n{\n\n\n    NSOpenGLContext* _openGLContext;\n\n\n    QCRenderer*      _renderer;\n\n\n    NSString*        _filePath;\n\n\n    NSTimer*         _renderTimer;\n\n\n    NSTimeInterval   _startTime;\n\n\n    NSSize           _screenSize;\n\n\n}\n\n\n@end\nGetting a Composition File\n\nA QCRenderer object requires an OpenGL context and a Quartz Composer file for its creation. If the user drags a composition to your application icon, implement this NSApplication delegate method to retain the file pathname for later use when you create the QCRenderer object.\n\n- (BOOL) application:(NSApplication*)theApplication\n\n\n                    openFile:(NSString*)filename\n\n\n{\n\n\n    _filePath = [filename retain];\n\n\n \n\n\n    return YES;\n\n\n}\n\nIf the user opens your application by double-clicking its icon, ask the user to specify a composition and then retain the file pathname by including the following code in the applicationDidFinishLaunching: delegate method.\n\nNSOpenPanel *openPanel;\n\n\n \n\n\nif(_filePath == nil)\n\n\n    {\n\n\n        openPanel = [NSOpenPanel openPanel];\n\n\n        [openPanel setAllowsMultipleSelection:NO];\n\n\n        [openPanel setCanChooseDirectories:NO];\n\n\n        [openPanel setCanChooseFiles:YES];\n\n\n        if([openPanel runModalForDirectory:nil\n\n\n                    file:nil\n\n\n                    types:[NSArray arrayWithObject:@\"qtz\"]] != NSOKButton)\n\n\n            {\n\n\n                NSLog(@\"No composition file specified\");\n\n\n                [NSApp terminate:nil];\n\n\n            }\n\n\n        _filePath = [[[openPanel filenames] objectAtIndex:0] retain];\n\n\n    }\nCapturing the Main Display\n\nThe Quartz Services programming interface provides functions that configure and control displays. Use its functions to capture the main screen and cache its dimensions. See Quartz Display Services Reference for more information on these functions.\n\nYou can capture the main display by using the code in Listing 3-2. A detailed explanation for each numbered line of code follows the listing.\n\nListing 3-2  Code that captures the main display\n\nCGDisplayCapture (kCGDirectMainDisplay);\n// 1\n\n\nCGDisplayHideCursor (kCGDirectMainDisplay);\n\n\n_screenSize.width = CGDisplayPixelsWide(kCGDirectMainDisplay);\n// 2\n\n\n_screenSize.height = CGDisplayPixelsHigh(kCGDirectMainDisplay);\n\nHere what the code does:\n\nCaptures the main display. Capturing the screen is important because later you’ll set the receiver of the OpenGL context to full screen mode. A captured display prevents contention from other applications and system services. In addition, applications are not notified of display changes, preventing them from repositioning their windows and the Finder from repositioning desktop icons.\n\nCaches the screen dimensions. The rendering method uses the screen dimensions to normalize the mouse coordinates. See Writing the Rendering Routine.\n\nSetting Up the OpenGL Context\n\nAn OpenGL context pixel format requires a pixel format that specifies the buffers (depth buffer, alpha buffer, stencil buffer, and accumulation buffer) as well as other attributes of a context. Listing 3-3 shows how to set up an OpenGL context. A detailed explanation for each numbered line of code follows the listing.\n\nListing 3-3  Setting up an OpenGL context\n\nNSOpenGLPixelFormat *format;\n// 1\n\n\nNSOpenGLPixelFormatAttribute attributes[] = {\n// 2\n\n\n                    NSOpenGLPFAFullScreen,\n\n\n                    NSOpenGLPFAScreenMask,\n\n\n                    CGDisplayIDToOpenGLDisplayMask(kCGDirectMainDisplay),\n\n\n                    NSOpenGLPFANoRecovery,\n\n\n                    NSOpenGLPFADoubleBuffer,\n\n\n                    NSOpenGLPFAAccelerated,\n\n\n                    NSOpenGLPFADepthSize,\n\n\n                    24,\n\n\n                    (NSOpenGLPixelFormatAttribute) 0\n\n\n                };\n\n\n \n\n\nformat = [[[NSOpenGLPixelFormat alloc] initWithAttributes:attributes]\n\n\n                                         autorelease];\n// 3\n\n\n_openGLContext = [[NSOpenGLContext alloc] \n// 4\n\n\n                    initWithFormat:format\n\n\n                    shareContext:nil];\n\n\nif(_openGLContext == nil) \n// 5\n\n\n    {\n\n\n        NSLog(@\"Cannot create OpenGL context\");\n\n\n        [NSApp terminate:nil];\n\n\n    }\n\nHere’s what the code does:\n\nDeclares storage for an NSOpenGLPixelFormat object. You specify a format when you create an OpenGL context.\n\nSets up the attributes for the pixel format. These attributes specify, among other things, a full-screen context and a depth buffer. NSOpenGLPixelFormat Class Reference provides a complete description of the available format attributes. At the very least, you must provide a color buffer and a depth buffer for the QCRenderer object.\n\nAllocates a pixel format object and initializes it with the pixel format attributes.\n\nAllocates an OpenGL context and initializes it with the pixel format object.\n\nChecks to make sure the OpenGL context is not nil. If it is, the application must terminate.\n\nSetting Up Full Screen Display and Syncing\n\nYou need to set the OpenGL context to full-screen mode and then set the swap interval to 1 to ensure that the buffers are swapped only during the vertical retrace of the monitor. If the buffers aren’t synchronized with the retrace, the composition could render with tearing artifacts. (For more information on swap intervals, see OpenGL Programming Guide for Mac.)\n\nlong    value = 1;\n\n\n \n\n\n[_openGLContext setFullScreen];\n\n\n[_openGLContext setValues:&value forParameter:kCGLCPSwapInterval];\nCreating a QCRenderer Object\n\nA QCRenderer object requires an OpenGL context, the OpenGL pixel format, and a file pathname. If for some reason the renderer can’t be created, the application must terminate. Use the following code to create the renderer and check for its creation.\n\n_renderer = [[QCRenderer alloc]\n\n\n            initWithOpenGLContext:_openGLContext\n\n\n            pixelFormat:format\n\n\n            file:_filePath];\n\n\nif(_renderer == nil)\n\n\n    {\n\n\n        NSLog(@\"Cannot create QCRenderer\");\n\n\n        [NSApp terminate:nil];\n\n\n    }\nCreating a Timer\n\nYou need to set up a timer to regularly render the composition. This timer set up in the following code is scheduled to fire 60 times per second. Each time it fires, it invokes the render routine that’s created in the next section. If the timer can’t be created, the application must terminate, so make sure you include code to check for the existence of the timer. For more information on times, see NSTimer Class Reference.\n\n#define kRendererFPS 60.0\n\n\n \n\n\n_renderTimer = [[NSTimer scheduledTimerWithTimeInterval:(1.0 /\n\n\n                                (NSTimeInterval)kRendererFPS)\n\n\n                        target:self\n\n\n                        selector:@selector(_render:)\n\n\n                        userInfo:nil\n\n\n                        repeats:YES]\n\n\n                        retain];\n\n\nif(_renderTimer == nil)\n\n\n    {\n\n\n        NSLog(@\"Cannot create NSTimer\");\n\n\n        [NSApp terminate:nil];\n\n\n    }\nWriting the Rendering Routine\n\nWhen the timer fires or when a user event needs to be processed, the renderWithEvent: method is invoked. Recall that for this application the timer is set to fire 60 times per second. Listing 3-4 shows the _render and renderWithEvent: methods. A detailed explanation for each numbered line of code follows the listing.\n\nListing 3-4  The rendering methods\n\n- (void) _render:(NSTimer*)timer\n\n\n{\n\n\n    [self renderWithEvent:nil];\n\n\n}\n\n\n \n\n\n- (void) renderWithEvent:(NSEvent*)event\n\n\n{\n\n\n    NSTimeInterval  time = [NSDate timeIntervalSinceReferenceDate];\n\n\n    NSPoint             mouseLocation;\n\n\n    NSMutableDictionary  *arguments;\n\n\n \n\n\n    if(_startTime == 0)\n// 1\n\n\n    {\n\n\n        _startTime = time;\n\n\n        time = 0;\n\n\n    }\n\n\n    else\n\n\n        time -= _startTime;\n\n\n \n\n\n    mouseLocation = [NSEvent mouseLocation];\n// 2\n\n\n    mouseLocation.x /= _screenSize.width;\n// 3\n\n\n    mouseLocation.y /= _screenSize.height;\n// 4\n\n\n    arguments = [NSMutableDictionary dictionaryWithObject:[NSValue \n// 5\n\n\n                    valueWithPoint:mouseLocation]\n\n\n                    forKey:QCRendererMouseLocationKey];\n\n\n    if(event)\n// 6\n\n\n        [arguments setObject:event forKey:QCRendererEventKey];\n\n\n    // Your code to set input port values \n// 7\n\n\n    if(![_renderer renderAtTime:time arguments:arguments])\n// 8\n\n\n                NSLog(@\"Rendering failed at time %.3fs\", time);\n\n\n    // Your code to get output port values\n// 9\n\n\n    [_openGLContext flushBuffer];\n// 10\n\n\n}\n\nHere’s what the code does:\n\nComputes the composition time as the difference between the current time and the time at which rendering started.\n\nGets the current mouse position, in screen coordinates. Mouse coordinates need to be normalized relative to the OpenGL context viewport ([0,1],x[0,1]) with the origin (0,0) at the lower-left corner.\n\nNormalizes the x mouse coordinate.\n\nNormalizes the y mouse coordinate.\n\nCreates a dictionary and writes the normalized mouse coordinates to it. Coordinates are specified as an NSPoint object stored in an NSValue object.\n\nIf there is a user event, adds it to the arguments dictionary.\n\nThis is where you could add code to set the value for an input parameter that’s published to the root macro patch of a composition. You use the method setValue:forInputKey:, making sure to pass a valid key.\n\nRenders a frame at the specified time, passing the arguments dictionary.\n\nThis is where you could add code to retrieve the value of a published output parameter. You use the method valueForOutputKey:, making sure to pass a valid key.\n\nFlushes the OpenGL context to display the rendered frame onscreen.\n\nOverriding the sendEvent Method\n\nRecall that this example subclasses NSApplication so that it could override the sendEvent: method to ensure that user events are processed while there is a full screen OpenGL context on screen. The sendEvent: method in Listing 3-5 checks for:\n\nAn Escape key press, and terminates the application if there is one.\n\nA meaningful event for the composition, and invokes the renderer immediately with such an event.\n\nNote:  Don’t copy and paste the #define statement from Listing 3-5 to an Xcode project; the formatting used in the listing prevents the code from compiling. Instead, use the complete Player Xcode project that’s provided with the Max OS X v10.5 developer tools. For more information, see Building and Running the Player Sample Project.\n\nListing 3-5  The overridden event-sending method\n\n#define kRendererEventMask (NSLeftMouseDownMask|NSLeftMouseDraggedMask |\n\n\n                    NSLeftMouseUpMask | NSRightMouseDownMask |\n\n\n                    NSRightMouseDraggedMask | NSRightMouseUpMask |\n\n\n                    NSOtherMouseDownMask | NSOtherMouseUpMask |\n\n\n                    NSOtherMouseDraggedMask | NSKeyDownMask |\n\n\n                    NSKeyUpMask | NSFlagsChangedMask |\n\n\n                    NSScrollWheelMask)\n\n\n \n\n\n- (void) sendEvent:(NSEvent*)event\n\n\n{\n\n\n    if(([event type] == NSKeyDown) && ([event keyCode] == 0x35))\n\n\n            [NSApp terminate:nil];\n\n\n \n\n\n    if(_renderer && (NSEventMaskFromType ([event type]) &\n\n\n                 kRendererEventMask))\n\n\n            [self renderWithEvent:event];\n\n\n    else\n\n\n    [       super sendEvent:event];\n\n\n}\nBuilding and Running the Player Sample Project\n\nThe best way to experiment with using the QCRenderer class is to build and run the Player sample application that’s supplied with the OS X v10.5 developer tools. After installing the developer tools, you can find the Player Xcode project in the following location:\n\n/Developer/Examples/Quartz Composer Sample Code\n\nAfter you compile the Player application, you can open a composition in two ways. Either launch the application and specify a composition to play or drag a composition onto the Player application icon. To support opening a composition by dragging it to the application icon, you need to change the Info.plist file. Listing 3-6 shows the Info.plist file for the Player sample project. You can view this file from within Xcode by double-clicking Info.plist in the file list. Note that the listing has a CFBundleTypeExtensions key followed by the qtz extension.\n\nListing 3-6  Specifying the qtz extension in the Info.plist file\n\n<key>CFBundleDocumentTypes</key>\n\n\n    <array>\n\n\n        <dict>\n\n\n            <key>CFBundleTypeExtensions</key>\n\n\n            <array>\n\n\n                <string>qtz</string>\n\n\n            </array>\n\n\n        </dict>\n\n\n    </array>\nSee Also\n\nNSApplication Class Reference discusses the class, its methods, and the cases for which you might want to subclass NSApplication.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2013 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2013-04-23"
  },
  {
    "title": "Using QCView to Create a Standalone Composition",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposer/qc_play_ib/qc_play_ib.html#//apple_ref/doc/uid/TP40001357-CH207-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer Programming Guide\nTable of Contents\nIntroduction\nUsing QCView to Create a Standalone Composition\nPublishing Ports and Binding Them to Controls\nUsing the QCRenderer Class to Play a Composition\nAdding Compositions to Webpages and Widgets\nRevision History\nNext\nPrevious\nRetired Document\n\nImportant: This document may not represent best practices for current development. Links to downloads and other resources may no longer be valid.\n\nUsing QCView to Create a Standalone Composition\n\nThis chapter shows how to create a standalone Quartz Composer composition using the QCView class. You don’t need to write any code to accomplish this task. You simply create a new Cocoa application, modify the nib file in Interface Builder so that the application window has a QCView in it, and then specify a composition to load. Launching the compiled Cocoa application causes the composition to play automatically.\n\nNote: This chapter assumes that you are using Xcode and Interface Builder Version 3.0. Interface Builder Version 3.0 is a major update from the previous version of Interface Builder. For detailed information on how to use the new version, see Interface Builder User Guide.\n\nFollow these steps to create a standalone composition:\n\nCreate a Cocoa Application\n\nModify the MainMenu.nib File.\n\nBuild the Cocoa Application\n\nCreate a Cocoa Application\n\nCreating a Cocoa application is easy. You don’t need to write any code, but you do need to include the Quartz framework in the application.\n\nFollow these steps:\n\nLaunch Xcode.\n\nChoose File > New Project.\n\nIn the New Project window, choose Cocoa Application and click Next.\n\nType a project name and click Finish.\n\nThe Xcode project window opens.\n\nChoose Project > Add to Project.\n\nNavigate to the Quartz.framework, choose it and click Add.\n\nIt’s located in System/Library/Frameworks/Quartz.framework. A Cocoa application does not automatically include the framework that contains the Quartz Composer programming interface, which is why you need to add it.\n\nClick Add in the Add to Targets sheet that appears.\n\nThe Quartz.framework appears in the list of file names on the right side of the project window and in the Groups and Files list on the left side. You may want to drag the Quartz.framework into the Frameworks folder in the Groups and Files list so that your project is well-organized.\n\nIn the Resources folder in the Groups & Files list, double-click MainMenu.nib.\n\nInterface Builder launches.\n\nIf the Library is not already open, choose Tools > Library.\n\nClick the disclosure triangle next to Library.\n\nIf you don’t see Quartz Composer listed in the Library, go to Add the Quartz Composer Plug-in.\n\nIf you see Quartz Composer, go to Modify the MainMenu.nib File.\n\nAdd the Quartz Composer Plug-in\n\nTo add the Quartz Composer plug-in to the Interface Builder library, follow these steps:\n\nChoose Interface Builder > Preferences.\n\nIn the Preferences pane, click Plug-ins.\n\nClick the plus (+) button and navigate to Developer > Extras > Palettes.\n\nChoose QuartzComposer.ibplugin.\n\nClose the Preference pane.\n\nModify the MainMenu.nib File\n\nTo modify the MainMenu.nib file so that it plays a composition, follow these steps:\n\nIn the Library, choose Quartz Composer.\n\nDrag a Quartz Composer View (QCView instance) from the library to the main window. Resize and position the view to suit your application.\n\nWith the QCView selected, open the Attributes inspector.\n\nClick Load and choose a .qtz file to associate with the QCView. This is the Quartz Composer composition that plays when the application runs.\n\nTo forward user events, such as mouse clicks, from the view to the composition, click Forward All Events in the Rendering Options. If you forward events, the QCView accepts First Responder status.\n\nEvents must be forwarded for compositions that can respond to user input. For example, if the position of an object depends on the location of the mouse, you need to forward events.\n\nChoose File > Simulate Interface to verify that the composition loads and plays.\n\nBuild the Cocoa Application\n\nNow that you’ve loaded a composition in a QCView, the “difficult part” of making a standalone composition is over. Follow these steps to complete the process:\n\nSwitch from Interface Builder to Xcode.\n\nIn the Xcode project window, click Build & Restart.\n\nAfter the project finishes building, a window appears, and you should see your composition playing in it. If you don’t see the composition, but you did see it when you simulated the interface in Interface Builder, then check whether the Quartz framework is added to your Cocoa application. Also check the Xcode run log for any information printed by Quartz Composer.\n\nFor more practice making standalone compositions, see Example: The Dancing Cube Composition.\n\nExample: The Dancing Cube Composition\n\nThe Dancing Cubes.qtz composition (available in the directory /Developer/Examples/Quartz Composer Compositions/Interactive/) tracks the mouse and renders a series of cubes based on the mouse position.\n\nTo modify a nib file so that it plays the Dancing Cubes composition, follow these steps:\n\nCreate a Cocoa application and add the Quartz framework to it.\n\nSee Create a Cocoa Application for details.\n\nDouble-click the MainMenu.nib file in the Xcode project window.\n\nThis launches Interface Builder with the main application window open.\n\nWith the window selected, choose Tools > Size Inspector .\n\nIn the Inspector, set the Content Size to 640 and the height to 480.\n\nDrag a Quartz Composer View (QCView instance) from the Quartz Composer objects in the Library to the main window. Then resize the view to fill the window.\n\nWith the QCView selected, choose Tools > Size Inspector.\n\nClick the inside set of lines in Autosizing so they appear as solid red lines.\n\nYou’ll see that the red box in the Autosizing animation grows as the containing window grows, which is the behavior you want for the view. (The red box represents the view.)\n\nChoose Tools > Attributes.,\n\nClick Load.\n\nChoose Dancing Cubes.qtz from the directory /Developer/Examples/Quartz Composer Compositions/Interactive/, then click Open.\n\nClick Forward All Events to make sure the composition receives all mouse events.\n\nChoose File > Simulate Interface. The window displays cubes that look similar to those in Figure 1-1.\n\nFigure 1-1  The Dancing Cubes composition\n\nQuit the interface simulation and save the nib file.\n\nSwitch to Xcode.\n\nBuild and Restart the application.\n\nWhen the application runs, move the pointer to the window. Then click and hold the mouse button and move the mouse. Cube movements should track the mouse movement when the mouse button is pressed.\n\nSee Also\n\nThe directory /Developer/Examples/Quartz Composer Compositions/Interface Builder contains additional nib files that use QCView. Take a look at them to see what other results you can achieve using Interface Builder and Quartz Composer.\n\nPublishing Ports and Binding Them to Controls describes how to use Cocoa binding to expose input parameters in the user interface of a standalone composition.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2013 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2013-04-23"
  },
  {
    "title": "The Quartz Composer User Interface",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposerUserGuide/qc_editor/qc_editor.html#//apple_ref/doc/uid/TP40005381-CH202-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer User Guide\nTable of Contents\nIntroduction\nQuartz Composer Basic Concepts\nThe Quartz Composer User Interface\nBasic and Advanced Tasks, Tips, and Tricks\nTutorial: Creating a Composition\nGlossary\nRevision History\nNext\nPrevious\nThe Quartz Composer User Interface\n\nThe Quartz Composer development tool supplied in OS X v 10.5 has two main windows: the editor and the viewer. The editor window is for assembling and connecting patches to create a composition. The viewer window shows the output produced by the composition, the rendering resources consumed by the composition, and debugging information.\n\nThis chapter describes the editor and viewer windows, the utility windows and views, the key menu items, and other aspects of the user interface for Quartz Composer. After reading this chapter, you’ll be ready to use the editor to create a composition.\n\nThe Editor Window\n\nThe main part of the Quartz Composer Editor window is the workspace—a grid for assembling and connecting patches (see Figure 2-1). The set of connected patches on the workspace is sometimes called a graph. In theory, the workspace is infinite in size. The size of the scrollers in the scroll bars provides a clue as to the size of the graph.\n\nThe toolbar at the top of the editor window contains controls that either open utility windows or modify what’s shown in the editor window.\n\nThe Patch Creator button opens the Patch Creator utility window. You can browse patch names and categories in this utility window, or you can use the search field to look for patches. For more details see Figure 2-4.\n\nFigure 2-1  The Editor window\n\nThe Zoom Levels controls come in handy for complex compositions that contain a lot of patches. You can zoom instantly to fit all patches in the visible area of the workspace, zoom out, return to the default magnification, or zoom in to read the text more easily.\n\nThe Create Macro and Edit Parent buttons support compositions that have nested patches (or macros). Clicking Create Macro adds an empty macro patch to the workspace. To see what’s inside or to edit a macro, simply double-click it. If you are in a macro, you can go up one level by clicking the Edit Parent button. Figure 2-2 shows the content of an audio spectrum macro. You can tell you are inside a macro by looking at the space just below the toolbar, which displays the breadcrumb, or path. You can also change levels by clicking an item in the path.\n\nFigure 2-2  A macro that draws an audio spectrum\n\nA clip contains a set of patches that perform a task, such as rendering a rotating cube or an animated background. Clips, like routines in a library for a traditional programming language, speed development. They are prepackaged “mini” compositions that you can drag into your composition and customize for your own use. Quartz Composer provides just a few clips but you can add your own by clicking the Create Clip button. See Adding a Clip to the Patch Creator.\n\nThe Patch Parameters button toggles a view that contains the input parameters of the selected patch. See Patch Parameter Pane.\n\nThe Patch inspector button opens a utility window that lets you set input parameters, view and edit patch information, change internal settings for patches that have them, and inspect published input and output ports if there are any. See The Patch Inspector.\n\nThe Viewer button opens a window that shows the rendered output from the composition. See The Viewer Window.\n\nControl-click the toolbar to open a sheet that allows you to customize the toolbar. You can drag items into and out of the toolbar, as well as rearrange them.\n\nFigure 2-3  The sheet for customizing the toolbar\nThe Patch Creator\n\nThe Patch Creator is a utility window for browsing and getting information about Quartz Composer patches and clips. Within this window is the Patch Browser, which lists patches and clips by name and category. If you are new to Quartz Composer, you can use the browser to familiarize yourself with the available patches. When you click a patch name, you can read its description. (See Figure 2-4.)\n\nFigure 2-4  The Patch Creator\n\nIf you already know the name of a patch, the fastest way to locate it is to start typing its name in the search field. As you type, the list narrows to those patches whose name contains the typed string. Figure 2-5 shows the list of patches after a search for the key word “time.”\n\nFigure 2-5  The search feature can help you to locate patches\n\nBecause Quartz Composer supports custom patches and clips, it’s possible for the list of patches in the Patch Creator to change. The best way to find out what’s available is to browse through the list. However, you might find it helpful to know more about the patch categories. The following sections describe the categories and highlight some of the more interesting and powerful patches.\n\nComposite Patches\n\nComposite patches take two input images and produce one output image. Quartz Composer provides patches for the commonly used compositing and blending operations (addition, source over, source in, source out, and so on). Figure 2-6 shows an addition patch that produces an image that’s rendered to a 2D billboard.\n\nFigure 2-6  The Addition patch is a Composite patch\nController Patches\n\nController patches produce one or more values that you can use to control the output of another patch. They can produce output values based on user input or as the result of mathematical operations, such as functions you supply, values from waveforms, and randomly generated values. Figure 2-7 shows the x and y position of the mouse used to control the position of a cylinder onscreen, while an Interpolation patch provides a value to control the rotation of the cylinder in the z plane.\n\nFigure 2-7  The Mouse and Interpolation patches are Controller patches\nEnvironment Patches\n\nEnvironment patches are typically macros that operate on a motion graphic to transform its look in some way. The Lighting and Fog patches in Figure 2-8 are two examples, but there are many more, including patches that transform 3D space and macro patches that replicate their contents.\n\nFigure 2-8  The Lighting and Fog patches are Environment patches\nFilter Patches\n\nFilter patches operate on the pixels in an input image to produce an output image. The Comic Effect patch in Figure 2-9 is a Filter patch that operates on a single image to make the image look as if it is published in a comic book. There are dozens of Filter patches, including Pixellate, Gloom, Unsharp Mask, and Edges.\n\nFigure 2-9  Comic Effect is an image processing filter patch\nGenerator Patches\n\nGenerator patches produce an image algorithmically. The Checkerboard patch shown in Figure 2-10 produces a checkerboard pattern whose colors, square size, and sharpness are based on input values. If there aren’t any external input values, the patch uses its default values. Other Generator patches include Image with String, Star Shine, and Random Generator.\n\nFigure 2-10  Checkerboard is a Generator patch\nGradient Patches\n\nGradient patches produces an image based on Gaussian, linear, or radial algorithms. Note that Quartz Composer also has a patch named “Gradient” in the render category that renders a gradient to a destination. Patches in the Gradient category, by contrast, must produce an output image that can then be used as input to another patch. The Gradient category also has Linear Gradient and Gaussian Gradient patches.\n\nFigure 2-11  Radial Gradient is a Gradient patch\nModifier Patches\n\nModifier patches take input values or objects (such as images), change or transform the input in some way, and then output the transformed value or object. The Color Transformation and Image Crop patches in Figure 2-12 are Modifier patches. There are also patches for modifying strings (such as String Case and String Truncate), inspecting structures (such as Structure Index Member and Structure Key Member), and modifying textures (Image Texturing Properties).\n\nFigure 2-12  Image Crop and Color Transformation are Modifier patches\nNetwork Patches\n\nNetwork patches send, receive, or operate on network data. Figure 2-13 shows three examples: Network Receiver, Network Synchronizer, and Network Broadcaster.\n\nFigure 2-13  Some sample Network patches\nNumeric Patches\n\nNumeric patches produce values based on a mathematical expression that you supply or that is built in to the patch. The center patch in Figure 2-14 is one that allows you to define an expression. Quartz Composer automatically creates the input ports based on the expression. There are many more helpful Numeric patches to explore, such as Counter, Conditional, and Round.\n\nFigure 2-14  Some sample Numeric patches\nPlug-in Patches\n\nThe Plug-in category includes custom patches packaged as a Quartz Composer plug-in. For information on creating them, see Quartz Composer Custom Patch Programming Guide. You won’t find the MIDI2Color and iPatch patches shown in Figure 2-15 unless you build them yourself, which you can do by following the instructions in Quartz Composer Custom Patch Programming Guide.\n\nFigure 2-15  Two custom patches\nProgramming Patches\n\nProgramming patches (see Figure 2-16) provide a text field, accessible in the inspector, for adding a routine that the patch executes. The JavaScript patch requires you to use JavaScript. The Core Image Filter patch requires that you provide a kernel routine written using the Core Image Kernel Language (see Core Image Kernel Language Reference). The GLSL Shader patch requires that you supply a shader written using OpenGL Shading Language. For more details, see Using Programming Patches.\n\nFigure 2-16  Some example Programming patches\nRenderer Patches\n\nRenderer patches render to a destination—the viewer window. Figure 2-17 shows three of them—Clear, Billboard, and Particle System. The Clear patch clears the rendering destination, optionally using a color you supply. The Billboard patch renders a 2D image. The Particle System patch renders a set of particles, each of which have a lifetime. There are several other interesting renderers, including Cube, Psychedelic, Sprite, Sphere, and Teapot.\n\nFigure 2-17  A few Renderer patches\nSource Patches\n\nSource patches provide data (such as a file list, RSS feed, composition) from a source. Figure 2-18 shows Directory Scanner, RSS Downloader, and Composition Loader. But there are also patches for getting audio input, images from folders, and information about the computer.\n\nFigure 2-18  Some sample Source patches\nTool Patches\n\nTool patches perform tasks that are useful for creating a composition, such as the Anchor Position and Demultiplexer patches shown in Figure 2-19. (The title of the Demultiplexer patch in the figure is in quotes to show that the title was created dynamically. The title changes according to the output port type you choose.) There are many other tool patches including OpenGL Info, Image Dimensions, Input Splitter, Date Formatter, Structure Count, and System Time.\n\nFigure 2-19  Two Tool patches\nTransition Patches\n\nTransition patches produce an effect between a source image and a destination image—the sort of effect you would use in a slideshow. Ripple, Page Curl, and Dissolve, shown in Figure 2-20, are three such patches. There are others, including Flash, Swipe, Mod, Copy Machine, and Disintegrate with Mask.\n\nFigure 2-20  Three Transition patches\nClip Patches\n\nClip patches are those that you create by selecting a group of connected patches and clicking Create Clip. Quartz Composer provides a few clips, such as the Discs Background shown in Figure 2-21 and the Rotating Cube clip, which you’ll use if you follow the tutorial in Creating a Glow Filter. Quartz Composer doesn’t provide the Company Logo clip, also shown in Figure 2-21; a logo or other graphic that you repeatedly use is a good candidate for a clip.\n\nFigure 2-21  Two Clip patches\nPatch Parameter Pane\n\nThe Patch Parameters button on the editor window toolbar toggles the patch parameters pane shown to the right of the workspace in Figure 2-22. This pane provides convenient access for editing input parameters\n\nThe Macro Patch field at the top of the pane shows the input parameters for any published ports for that level. (You can find out more about published ports in Publishing Ports.) The pane also shows the input parameters for the patch, or patches, currently selected in the workspace. When you select another patch, the pane changes to the input parameters for that patch. If you select more than one patch, the input parameters for all selected patches appear in the pane, making it easy to change parameters in different patches.\n\nFigure 2-22  The patch parameter pane\nThe Patch Inspector\n\nThe Patch inspector is a utility window that has one to three panes, depending on the patch that’s currently selected in the workspace. All patches have an Input Parameters pane. The Input Parameters pane (see Figure 2-23) allows you to edit the input parameters for a patch. It’s equivalent to the patch parameter pane, which you view within the editor window.\n\nFigure 2-23  The Input Parameters pane\n\nSome patches also have a Settings pane. You’ll see it only if the patch has settings that are advanced or that aren’t typically animated. Figure 2-24 shows an advanced option for clearing the depth buffer along with an explanation of why you might use the option. It also provides an option for setting the number of gradient points.\n\nFigure 2-24  The Settings pane\n\nQuartz Composer allows you to publish the input and output ports for a patch. Publishing allows you to access values for patches that are nested within a composition (see The Patch Hierarchy). You can inspect the published ports using the Published Inputs & Outputs pane of the inspector. Figure 2-25 shows the published input and output ports for a macro patch from a rather complex, hierarchical composition. Publishing makes these ports available to the parent level of this patch.\n\nFigure 2-25  The published ports for a macro patch\n\nPorts published at the topmost level of a composition are available outside the composition, as you’ll see in Publishing Ports. You can inspect the ports published at the topmost level (that is, the root macro patch) by opening the inspector and clicking the workspace (make sure that no patches are selected). Then, the inspector provides information about the entire level of the composition rather than about a single patch.\n\nFigure 2-26  The published ports for the root macro patch, or top level of a composition\nThe Viewer Window\n\nThe viewer window has its own toolbar. You can start and stop rendering, switch to full-screen rendering, change the rendering mode (see Rendering Modes), view composition input parameters, and switch to the editor window.\n\nTip: To exit from full-screen mode, press the Escape key.\n\nFigure 2-27  The Quartz Composer viewer window\n\nThe viewer window shows the average frame rate at the bottom right of the window and the image size at the bottom center. The pop-up menu at the bottom left lets you constrain the viewer to a fixed size or aspect ratio, as shown in Figure 2-28.\n\nTip: You can manage viewer sizes and aspect ratios in Quartz Composer Preferences. See Setting Preferences.\n\nFigure 2-28  The aspect ratio pop-up menu\nRendering Modes\n\nThe viewer window has three rendering modes available:\n\nPerformance mode is the normal rendering mode and provides optimal performance. When you enable full-screen mode, Quartz Composer always uses performance rendering mode.\n\nProfile mode gathers statistics that are displayed in a drawer to the right of the viewer window (see Figure 2-29).\n\nDebug mode displays a log of debug information that is displayed in a drawer below the viewer window (see Figure 2-30) and color codes each patch to indicate its execution state. Notice that the viewer in the figure isn’t rendering content. The content in the drawer provides clues as to the underlying problem.\n\nFigure 2-29  Profile rendering mode\n\nProfile rendering mode analyzes each frame that is rendered and graphs information that you can use to optimize the rendering performance of a composition. You can obtain the following information:\n\nActive Patches, the number of patches that are currently active.\n\nTraversed Patches, the number of patches that are “touched” during the patch tree traversal, but not necessarily executed.\n\nExecuted Patches, the number of patches executed for the analyzed frame.\n\nExecution Time, an estimate of the length of time in milliseconds used by Quartz Composer to compute the analyzed frame.\n\nRendering Time, an estimate of the length of time in milliseconds used by Quartz Composer and OpenGL to render the analyzed frame.\n\nFigure 2-30  Debug rendering mode\n\nIn addition to displaying debugging information posted by Quartz Composer, debug rendering mode shades each patch in the editor window with a color, similar to the shading shown in Figure 2-31.\n\nGreen shading indicates that the patch is enabled and running. For example, the Clear and Billboard patches in the figure are running.\n\nRed shading indicates that the patch is not enabled. The Mouse patch in the figure is not enabled.\n\nOrange indicates that a patch is enabled but not running. The Image Importer and LFO patches are enabled but not running.\n\nYou can examine the data flow as a composition runs in debugging mode. Patches change colors as they move from one state to another.\n\nFigure 2-31  The debug rendering mode color-codes patches\nMenus\n\nAlthough you will want to explore all the menu items available in Quartz Composer, there are several menu items that you won’t want to overlook:\n\nFile > Export as QuickTime Movie saves a composition as a QuickTime movie. See Turning a Composition into a QuickTime Movie for details.\n\nEdit > Undo and Edit > Redo.\n\nFile > Compare Compositions opens a window that lets you compare compositions in a number of interesting ways. See Comparing Compositions.\n\nEditor > Edit Information allows you to add information about the composition, such as the copyright and a description.\n\nViewer > Save Snapshot saves an image of the composition that’s rendering in the viewer.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2007 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2007-07-17\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Basics of Using the Image Kit",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageKitProgrammingGuide/ImageKitComponents/ImageKitComponents.html#//apple_ref/doc/uid/TP40004907-CH3-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nImageKit Programming Guide\nTable of Contents\nIntroduction\nBasics of Using the Image Kit\nViewing, Editing, and Saving Images in an Image View\nBrowsing Images\nShowing Slides\nTaking Snapshots and Setting Pictures\nBrowsing Filters and Setting Input Parameters\nGlossary\nRevision History\nNext\nPrevious\nBasics of Using the Image Kit\n\nAs consumers accumulate more and more digital media, image applications are faced with handling large amounts of data in an efficient manner. Consumers not only want to open, view, and organize images, but they often need to crop images, adjust brightness, apply effects, view metadata, or perform a number of other image editing operations. The Image Kit framework is a bundle of image handling services that supports these tasks and more. It is designed to operate efficiently while providing a user interface with the look and feel of OS X. By using the Image Kit to perform the image handling tasks that most digital media applications need, you’ll be able to focus your code writing efforts on the parts of your application that distinguish it from other applications.\n\nThis chapter introduces the tasks supported by Image Kit classes and discusses how to set up Xcode and Interface Builder so that you can successfully build an application using the Image Kit. By reading this chapter, you’ll get an idea of what each of the the Image Kit classes can do. To use Image Kit classes effectively, you’ll need to read the chapters that describe how to implement the tasks required by your application.\n\nTasks Supported by the Image Kit\n\nThe Image Kit is a high-level Objective-C framework. It is built on a number of other OS X graphics technologies, including Quartz 2D, Core Image, Core Animation, and OpenGL. When you use the Image Kit, you can read any image data that Quartz 2D and the Image I/O frameworks support.\n\nThere are eight major categories of tasks that the Image Kit supports, the most basic of which is to view an image. Table 1-1 summarizes the tasks you can support with the Image Kit and lists the classes and protocols that you use for each task. Like the NSImageView class, the IKImageView class displays a single image in a frame and optionally can allow a user to drag an image to the view. Unlike the NSImageView class, the IKImageView class supports any image file format that Quartz supports.\n\nThe IKImageView class provides methods for zooming and for setting tool modes for moving, selecting, cropping, rotating, and annotating. With the appropriate tool mode selection, the view automatically displays a selection rectangle, a cropping rectangle, or an annotation oval. (Your application has to implement code that performs the actual data manipulation for these three tool modes.)\n\nThe IKImageEditPanel class allows users to view image metadata and to adjust digital images by:\n\nChanging the exposure\n\nSetting the white and black points\n\nAdjusting the gamma, saturation, contrast, and brightness\n\nSharpening\n\nApplying color effects: black and white, sepia, antique, fade color, boost color, blur, and invert\n\nThe IKImageView class has built-in support for the Image Edit panel (IKImageEditPanel class). After making the appropriate setting in your application, a user simply clicks an image and the Image Edit panel opens. No other action is needed on your part. The image editing panel can be used independently of the IKImageView class by performing the appropriate setup work and implementing the IKImageEditPanelDataSource protocol.\n\nNote: The user term for Image Edit panel is Image Edit window. See Terminology for Users for more information about the terminology that your application should adopt for strings in the user interface, including help tags and help.\n\nThe Image Kit provides the IKImageBrowserView class for displaying and arranging images in a grid in a way similar to the grid you see in iPhoto. The image browser can display large numbers of images, icons, movies, Quartz Composer compositions, and PDF documents. Users can drag images to the browser, select images, and move them. The Image Kit achieves smooth animation when images are moved in the browser.\n\nThe options for saving an image vary depending on the file format of the image. For that reason, the Image Kit provides the IKSaveOptions class. Save options appear as an accessory view (pane) in an NSSavePanel object.\n\nA slideshow is a popular way for consumers to view digital images. The IKSlideshow class and the IKSlideshowDataSource protocol provide an easy way for your application to support slideshows of images, PDF documents, and other image data. You can start, pause, and stop slideshows, export slideshows, access a specific item in the show, and perform a number of other tasks.\n\nMail and iChat are two of the many applications that allow users to supply an icon or photo that represents their identity. The IKPictureTaker class provides a lightweight panel for choosing and cropping an image or for taking a snapshot with a digital camera. The panel keeps track of recent pictures, allowing the user to choose from among them as an alternative to navigating to an image or taking a snapshot.\n\nThe IKFilterBrowserView and IKFilterBrowserPanel classes provide a user interface for browsing Core Image image processing filters and previewing their effects.\n\nThe IKFilterUIView class provides a user interface for Core Image filters, making it easy for you to support image processing with the more than one hundred filters supplied by the system. You can choose which parameters are available to set and the size of the controls. If you want to supply a custom user interface for a filter that you write, you can use the IKFilterCustomUIProvider class.\n\nTable 1-1  Tasks and the classes that support them\n\nTask\n\n\t\n\nClasses and protocols\n\n\n\n\nView and edit images\n\n\t\n\nIKImageView\n\n\n\n\nAdjust images, apply color effects, view metadata\n\n\t\n\nUse the IKImageEditPanel and IKImageEditPanelDataSource classes only when you are not using the IKImageView class, which has built-in support for the Image Edit panel.\n\n\n\n\nDisplay and arrange large numbers of images\n\n\t\n\nIKImageBrowserView, IKImageBrowserDataSource, IKImageBrowserDelegate, and IKImageBrowserItem\n\n\n\n\nRun slideshows\n\n\t\n\nIKSlideshow and IKSlideshowDataSource Protocol\n\n\n\n\nChoose an icon-sized picture from a directory or take a snapshot with an iSight or other digital camera\n\n\t\n\nIKPictureTaker\n\n\n\n\nSave images in a variety of file formats with options appropriate for the format\n\n\t\n\nIKSaveOptions\n\n\n\n\nBrowse Core Image filters and preview their effects\n\n\t\n\nIKFilterBrowserView and IKFilterBrowserPanel\n\n\n\n\nView and adjust the input parameters of a Core Image filter\n\n\t\n\nIKFilterUIView and IKFilterCustomUIProvider\n\nUsing the Image Kit in Xcode\n\nTo use the Image Kit framework in Xcode, you need to import the Quartz and Quartz Core frameworks. The Quartz framework contains Image Kit. The Quartz Core framework contains the Core Image classes needed by the IKFilterBrowserPanel and IKFilterBrowserView classes.\n\nTo import these frameworks in Xcode:\n\nOpen Xcode and create a Cocoa application.\n\nChoose Project > Add to Project.\n\nNavigate to System/Library/Frameworks, choose the Quartz.framework and QuartzCore.framework, and click Add.\n\nIn the sheet that appears, click Add.\n\nSave the project.\n\nAfter importing the frameworks, make sure that you add #import <Quartz/Quartz.h> into the appropriate files.\n\nUsing the Image Kit with Interface Builder 3.0\n\nThe examples in this document use Interface Builder 3.0. If you’ve never used Interface Builder before, you may want to skip this section and instead read the first chapter of Interface Builder User Guide. If you have used previous versions of Interface Builder, you’ll notice some substantial changes. This section points out a few of the changes that you’ll encounter when following the instructions in this document to create applications that use Image Kit classes.\n\nBecause of the new integration between Xcode 3.0 and Interface Builder 3.0, the actions and outlets that you declare in the interface file for a class are synchronously updated in Interface Builder. This will become apparent to you when you begin to make connections. When you drag from a controller in the nib document window to a view or other user interface element, a connections panel appears. The connections panel is a tool that lets you examine and create connections between objects. Figure 1-1 shows a connections panel for a window that has the instance variable mwindow as an outlet. This example is simple; some objects have several outlets as well as received and sent actions associated with it. For a complete description of the connections panel, see Interface Builder User Guide.\n\nFigure 1-1  An outlet for a window, shown in a connections panel\n\nThe Interface Builder library (shown in Figure 1-2) replaces the palette. The library contains the objects and resources you can use in your nib file. You can browse objects in the library by plug-in—Cocoa, IB, and so on—or use the search field to find an object. You can also create custom groups and smart groups to organize objects the way you want.\n\nFigure 1-2  The Interface Builder library\n\nThe media pane of the library shows images, sounds, and other resource that are available in your Xcode project. To use objects or media from the library, simply select what you want and drop them where you want them in your nib document.\n\nOne task you’ll perform repeatedly as you create Image Kit applications is to set springs and struts that control the autosizing behavior in the user interface. The Interface Builder size inspector now includes an animation that shows how the springs and struts affects autosizing behavior.\n\nFigure 1-3   Autosizing in the Interface Builder size inspector\n\nPerhaps more noticeable than the new features in Interface Builder are the absence of features and the tasks associated with them. Seasoned Interface Builder users will notice fewer tasks for wiring the user interface. Gone is the need to drag header files to the nib document window or to parse files.\n\nTerminology for Users\n\nStandard terminology helps users to come up to speed quickly using your application. Familiar terminology doesn’t get in the way of learning all the impressive features that you put into your application. The Image Kit introduces a number of new classes and protocols, named using terms familiar to developers. When you provide labels in the user interface for your application, write help tags, or provide documentation, your users will be best served if you adopt standard user terminology rather than developer terminology. Table 1-2 provides the user terms for the most common developer terms used in the Image Kit. You can find out more information about providing users with a consistent visual and behavioral experience by reading OS X Human Interface Guidelines.\n\nTable 1-2  Developer and user terms for the Image Kit\n\nDeveloper term\n\n\t\n\nUser term\n\n\n\n\naccessory view\n\n\t\n\npane\n\n\n\n\nimage browser view\n\n\t\n\nimage browser\n\n\n\n\nImage Edit panel\n\n\t\n\nImage Edit window\n\n\n\n\nOpen panel\n\n\t\n\nOpen dialog\n\n\n\n\npicture taker panel\n\n\t\n\npicture taker\n\n\n\n\nSave As panel\n\n\t\n\nSave As dialog\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2008 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2008-06-09\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Basic and Advanced Tasks, Tips, and Tricks",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposerUserGuide/qc_tips_moves/qc_tips_moves.html#//apple_ref/doc/uid/TP40005381-CH203-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer User Guide\nTable of Contents\nIntroduction\nQuartz Composer Basic Concepts\nThe Quartz Composer User Interface\nBasic and Advanced Tasks, Tips, and Tricks\nTutorial: Creating a Composition\nGlossary\nRevision History\nNext\nPrevious\nBasic and Advanced Tasks, Tips, and Tricks\n\nThis chapter describes how to use the editor to add patches to the workspace and connect them, inspect the values of input and output ports, and publish ports. You’ll learn tips and shortcuts for getting the Quartz Composer development environment supplied in OS X v10.5 to support your working style. You’ll see how to use some of the advanced features including templates and patches that process code.\n\nAdding Patches\n\nThere are three ways to add a patch from the Patch Creator:\n\nPress Return to add the patch whose name is selected.\n\nDouble-click a patch name.\n\nDrag a patch name to the workspace.\n\nTo duplicate a patch that’s already in the workspace, select the patch and press Command-D.\n\nTip: Instead of adding an Image Importer patch, you can drag an image from the Finder directly to the workspace. Quartz Composer automatically creates an instance of the Image Importer patch and uses the name of the image as the title of the patch. Quartz Composer provides a similar convenience for movies. Instead of adding a Movie Loader patch, simply drag a QuickTime move from the Finder to the workspace.\n\nMaking Connections Between Patches\n\nConnections between input and output ports of different patches establish a data flow in a composition. Simply click the output port, then click the input port you want to connect—you don’t need to hold down the mouse button while you move to the input port. Note that you can establish a connection only between ports that have compatible types—they must have the same type or a type conversion must be possible.\n\nTo break a connection between patches, either double-click the input port or drag the connector and release it. The connector disappears. Figure 3-1 shows some typical connections between patches.\n\nTip: You can redirect an existing connection by clicking the connection at the input port and then clicking another compatible input port. This action is similar to dragging, but you can release the mouse button while you move to the input port.\n\nFigure 3-1  Patch connections\n\nThe output from a patch can connect to the input of more than one patch. For example, the Result output port of the LFO patch (Figure 3-1) provides input to the Math patch and to the Sprite patch.\n\nTip: You can connect the output of a port to multiple input ports by holding down the Option key while clicking on each of the input ports.\n\nInspecting Port Values\n\nHover the pointer over a port to view a short description of the port, its data type, and the current value. (See Figure 3-2.) If the input port is not an object port, you can edit the port value by double-clicking the circle associated with the port, entering the new value in the text editor, and then pressing the Return key to validate the value or the Escape key to cancel. Note that input ports have a default value, which is always null for object ports.\n\nFigure 3-2  A help tag for an input port\n\nYou can also inspect and set the values for input ports by opening the inspector and displaying the Input Parameters pane or by clicking the Patch Parameters button in the editor to toggle the input parameters pane.\n\nYou can’t set the value for output ports, but you can view them. Image ports show a thumbnail representation of the image, as shown in Figure 3-3.\n\nFigure 3-3  Image ports display a thumbnail image\nFinding Out What a Patch Does\n\nWhen you select a patch in the Patch Creator, its description appears in the Description pane. But if you are working with a patch in the workspace, you can hover the pointer over the patch title bar to see a help tag that contains the name, category, and patch description, as shown in Figure 3-4.\n\nFigure 3-4  The patch description appears in a help tag\nSetting Preferences\n\nQuartz Composer lets you set preferences for the editor, the workspace, and the viewer, and manage clips. By default, when you launch Quartz Composer it displays templates that you can choose from. If you prefer, you can turn off that option in the General pane of Quartz Composer preferences and choose from among the options shown in Figure 3-5. You can also set whether the editor and viewer are visible when you open a composition and enable autosaving.\n\nFigure 3-5  The General pane of Quartz Composer preferences\n\nEditor preferences include options that affect the appearance of the workspace (colors, shadows, grid) and the behavior of the Patch Creator.\n\nYou can set and define aspect ratios in the Viewer pane. You can also set up options for debugging, full-screen mode, and the rendering frame rate.\n\nThe Clips pane lets you remove and edit clips, and view and edit clip information.\n\nKeyboard Shortcuts\n\nTable 3-1 summarizes the keyboard shortcuts for many of the actions available in the editor window.\n\nTable 3-1  Shortcuts for common actions in the editor window\n\nAction\n\n\t\n\nShortcut\n\n\n\n\nAdd a note\n\n\t\n\nControl-Click the workspace\n\n\n\n\nFast zoom in or out\n\n\t\n\nPress Command as you use the scroll wheel\n\n\n\n\nView all temporarily\n\n\t\n\nPress and hold the = key. When you release the key, the view snaps back. This is a good way to get your bearings in a complex composition.\n\n\n\n\nShow the Patch Creator\n\n\t\n\nCommand-Return\n\n\n\n\nCompare compositions\n\n\t\n\nOption-Command-O\n\n\n\n\nCreate macro\n\n\t\n\nShift-Command-M\n\n\n\n\nShow hierarchy browser\n\n\t\n\nCommand-B\n\n\n\n\nShow input parameters\n\n\t\n\nCommand-T\n\n\n\n\nCreate sticky inspector\n\n\t\n\nCommand-double-click\n\n\n\n\nShow inspector\n\n\t\n\nCommand-I\n\n\n\n\nEdit information\n\n\t\n\nOption-Command-I\n\n\n\n\nEdit protocol conformance\n\n\t\n\nOption-Command-P\n\n\n\n\nCustomize the toolbar\n\n\t\n\nCommand-click the toolbar and chose Customize Toolbar\n\n\n\n\nShow viewer\n\n\t\n\nShift-Command-V\n\n\n\n\nShow editor\n\n\t\n\nShift-Command-E\n\nTable 3-2 summarizes the keyboard shortcuts for the actions available in the viewer window.\n\nTable 3-2  Shortcuts for common actions in the viewer window\n\nAction\n\n\t\n\nShortcut\n\n\n\n\nRun the viewer\n\n\t\n\nCommand-R\n\n\n\n\nStop rendering\n\n\t\n\nCommand-.\n\n\n\n\nSave snapshot\n\n\t\n\nShift-Command-C\n\n\n\n\nEnable full-screen mode\n\n\t\n\nCommand-F\n\n\n\n\nPerformance rendering mode\n\n\t\n\nShift-Command-R\n\n\n\n\nDebug rendering mode\n\n\t\n\nShift-Command-D\n\n\n\n\nProfile rendering mode\n\n\t\n\nShift-Command-L\n\n\n\n\nCustomize the toolbar\n\n\t\n\nControl-click the toolbar and chose Customize Toolbar\n\nCommenting a Composition\n\nCommenting code is good practice, whether it’s in a program that uses a traditional coding language or in a composition. Quartz Composer provides three items that support comments for all patches:\n\nThe properties list. You can view and change items in this list by choosing Editor > Editor Information.\n\nThe patch title. You can change the default patch title to a more meaningful one for your composition by double-clicking the title.\n\nNotes. You can add a note to the workspace by Control-clicking an empty area. A note that you can resize and type text into appears. Figure 3-6 show part of a complex composition that uses two notes. You type the text in the opaque area of the note, and the translucent area gives an indication of which patches the note pertains to. Control-click the note to view the contextual menu that lets you set the color. The example in Figure 3-6 shows short notes, but you can enter as much text as you like—several paragraphs or more. The opaque area expands with the amount of text.\n\nFigure 3-6  Two notes used to comment a complex composition\n\nYou can provide comments for programming patches directly in patch, along with the code that you provide.\n\nComparing Compositions\n\nThere are several reasons why you might want to compare two compositions. The most typical is to see how two versions of a composition differ. Choosing File > Compare Compositions opens a window that provides several comparison options. You can compare patch connections, parameters, and settings by clicking the appropriate controls at the top right of the window. (See Figure 3-7). You can compare patch parameter views for each composition in the bottom portion of the window.\n\nWhat’s unique about this window is that you can also compare the compositions visually, and in a variety of interesting ways. In the lower part of the window, you can select an overlay method or whether to view the compositions side by -side. You can also choose to view the patch parameters.\n\nFigure 3-7  Comparing compositions\nChecking Patch Compatibility and Security\n\nStaring in OS X v10.5, Quartz Composer can provide information on whether a patch is compatible with OS X v10.4. You can also find out whether a patch could compromise security. Security information is important if you plan to embed a composition in a webpage or widget, or export it as a QuickTime movie. Webpages, widgets, and QuickTime movies won’t render compositions that contain unsafe patches. (For more information on embedding compositions in webpages and widgets, see Quartz Composer Programming Guide.)\n\nTo check the compatibility of patches in a composition, choose Editor > Display 10.4 Compatibility Information. Patches that are not compatible with OS X v10.4 display a small red icon with a white “x”. Patches that display a yellow icon with an exclamation point identify patches that might not be compatible because the patch was revised in some way, such as with additional input ports or changed settings. If you hover the pointer over the small icon, the tool tip for the patch provides more information about compatibility, such as what’s changed or that the patch is new.\n\nTo check whether any patches in a composition are unsafe, choose Editor > Indicate Unsafe Patches. Patches that are not safe display a small key icon.\n\nFigure 3-8 shows a variety of patches after choosing the compatibility and unsafe patches commands. The Image Importer shows a caution icon because the latest version has an additional setting that is not available in OS X v10.4. The Area Histogram and Grep patches are new in OS X v10.5. The Parallelogram Tile patch does not display any icons because it is compatible with OS X v10.4 and it is a secure patch. The Bonjour and MIDI Clock Receiver patches are unsafe patches. Note that the MIDI Clock Receiver patch also has a caution icon because its settings changed since OS X v10.4.\n\nFigure 3-8  Compatibility and security information\nImproving Rendering Performance\n\nTo reduce execution time and improve rendering performance:\n\nLimit image size to the dimensions you need.\n\nDon’t use RGBA images for masks.\n\nMake sure that the Render In Image patch is configured appropriately. The information in the Settings pane of the Patch inspector can help you choose the appropriate settings.\n\nSet the Enable input of consumer patches to false whenever possible.\n\nWhile in debug mode, minimize the number of patches that you use.\n\nDon’t draw unnecessarily. For example, don’t draw 200 sprites when 50 would suffice.\n\nDesign your composition to adapt to hardware features.\n\nIf you intend for your composition to run on a variety of hardware, you might want to avoid Core Image effects. These effects perform best when Core Image uses the GPU. The CPU fallback, if needed, can cause your composition to render less optimally.\n\nAdding a Clip to the Patch Creator\n\nIf you find yourself assembling and connecting the same set of patches repeatedly, you may want to create a clip.\n\nFollow these steps to add a clip to the Patch Creator:\n\nIn the editor window of your composition, select the set of patches that you want to create a clip from.\n\nChoose Editor > Create Clip.\n\nEnter the clip name into the sheet that appears, along with copyright information and a description.\n\nClick Done.\n\nYour new clip is now available in the Patch Creator. The name you entered in step 3 shows in the Patch Name list and the description shows in the Description box.\n\nNote: Quartz Composer stores clips in /Developer/Library/Quartz Composer/Clips.\n\nUsing Templates\n\nA template is a handy way to start from a preconfigured composition.\n\nTo make a composition available as a reusable template from within Quartz Composer, copy the composition file to this directory:\n\n/Developer/Library/Quartz Composer/Templates\n\nTo use the template later, choose File > New From Template.\n\nQuartz Composer provides a number of templates that conform to protocols (see Composition Repository). The templates you create are not required to conform to a protocol unless you want compositions created with your template to reside in the composition repository.\n\nUsing Programming Patches\n\nAlthough you can use Quartz Composer to create powerful motion graphics without any code, the ability to write code within programming patches provides additional flexibility for Core Image, JavaScript, and OpenGL programmers.\n\nThe Core Image Filter Patch\n\nThe Core Image Filter programming patch is extremely useful for creating custom image processing filters. To use this patch effectively, you need to understand Core Image filters and be familiar with the Core Image kernel language, which is an extension to the OpenGL Shading Language. Two good sources of information on writing Core Image filters are Core Image Programming Guide and Core Image Kernel Language Reference. Image Unit Tutorial provides additional details on writing kernel routines and contains several kernel examples.\n\nFigure 3-9 shows the Settings window for the Core Image Filter programming patch. You enter code for a kernel directly into this window. You can provide more than one kernel routine if you like, but it must be written using the Core Image Kernel Language.\n\nFigure 3-9  The Settings window for the Core Image Filter patch\n\nQuartz Composer automatically creates input ports for the patch based on the prototype of the kernel function that you supply in this window. Kernel input parameters whose type are float,vec2, vec3, and vec4 become number input ports. A __color data type becomes a color input port. A sampler data type becomes an image port. If you change the name of a kernel function parameter, you’ll break the connections to the related input port. The patch has a single output image that represents the result produced by the kernel.\n\nSelecting Show Advanced Input Sampler Options adds two input ports to the patch—one for specifying a wrapping mode (transparent or clamp) and another for specifying whether to use linear or nearest neighbor sampling.\n\nWhen you select Edit Filter Function, a text field appears at the bottom of the Settings pane, as shown in Figure 3-10. You can use this to define an ROI function (if one is needed) and a main function. You use JavaScript for the code in this window and wrapper functions provided by Quartz Composer. The main function must return an image (__image) and must follow this form:\n\nfunction __image main([arg_type_0] arg_0, [arg_type_1] arg_1, ...)\n\nwhere arguments are of any of the following types:\n\n__image, __vec2, __vec3, __vec4, __color, __number, __index\n\nFigure 3-10  The filter editing window\n\nThis patch supports Core Image filters of varying complexity, including:\n\nSingle kernel, single pass filters for which the region of interest (ROI) and domain of definition (DOD) coincide. For this type of filter, you need only to provide a kernel function in the top window, such as the multiplyEffect kernel shown in Figure 3-10.\n\nFilters for which one or more kernels require you to define the region of interest. In this case, you use the bottom window to define an ROI function, and then supply a main routine in the bottom window to apply to the kernel function defined in the top window.\n\nMultipass filters. You use a main function in the bottom window to apply multiple kernels or Core Image filters. You can apply any of the kernel functions that you define in the top window as well as any Core Image filter. Quartz Composer supplies wrappers for CIFilterShape and NSAffineTransform. See Listing 3-1.\n\nListing 3-1  A multipass filter that uses Core Image filters\n\n// Assume myKernel is a routine defined in the top window.\n\n\n// This code is the main function defined in the bottom window.\n\n\nfunction __image main (__image image, __vec4 color){\n\n\n    var image1, image2;\n\n\n    var transform;\n\n\n \n\n\n    // Apply the kernel that's defined in the top window.\n\n\n    image1 = myKernel.apply(image.extent, null, image);\n\n\n    // Apply the Core Image Sepia filter.\n\n\n    image2 = Filter.sepiaTone(image1, 1.0);\n\n\n    // Set up and apply a transform\n\n\n    transform = new AffineTransform().scale(0.5);\n\n\n    return Filter.affineTransform(image2,transform);\n\n\n}\nThe GLSL Patch\n\nThe GLSL patch (see Figure 3-11) uses the vertex and fragment shaders you provide to render a result. You need to use the OpenGL Shading Language to write the shaders.\n\nYou can find several examples of compositions that use the GLSL patch in:\n\n/Developer/Examples/Quartz Composer Compositions/GLSL\n\nFigure 3-11  The GLSL programming patch\nThe JavaScript Patch\n\nThe JavaScript patch (see Figure 3-12) implements the function that you provide in the Settings pane of the patch. Quartz Composer automatically creates input and output ports for the patch based on the prototype of the function. You must use these keywords to define input and output keys for your JavaScript function: __number, __index, __string, __image, __structure, and __boolean.\n\nFigure 3-12  The JavaScript programming patch\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2007 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2007-07-17\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Document Revision History",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageKitProgrammingGuide/RevisionHistory.html#//apple_ref/doc/uid/TP40004907-CH2-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nImageKit Programming Guide\nTable of Contents\nIntroduction\nBasics of Using the Image Kit\nViewing, Editing, and Saving Images in an Image View\nBrowsing Images\nShowing Slides\nTaking Snapshots and Setting Pictures\nBrowsing Filters and Setting Input Parameters\nGlossary\nRevision History\nPrevious\nDocument Revision History\n\nThis table describes the changes to ImageKit Programming Guide.\n\nDate\tNotes\n2008-06-09\t\n\nAdded information about support for dragging image views.\n\n\n2007-12-04\t\n\nRevised a figure and changed an instance variable name.\n\n\n2007-07-20\t\n\nNew document that explains how to support browsing, viewing, and editing images and browsing and controlling Core Image filters.\n\n\n\nPrevious\n\n\n\n\n\nCopyright © 2008 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2008-06-09\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Introduction to Quartz Composer Custom Patch Programming Guide",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposer_Patch_PlugIn_ProgGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40004787",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer Custom Patch Programming Guide\nTable of Contents\nIntroduction\nThe Basics of Custom Patches\nWriting Processor Patches\nWriting Image Processing Patches\nWriting Consumer Patches\nGlossary\nRevision History\nNext\nIntroduction to Quartz Composer Custom Patch Programming Guide\n\nImportant: This document is no longer being updated. For the latest information about Apple SDKs, visit the documentation website.\n\nA patch is one of the basic elements of the Quartz Composer development tool. Similar to routines in traditional programming languages, patches are base processing units. They execute and produce a result. In OS X v10.4, all patches were built-in to Quartz Composer. Starting in OS X v10.5, you can create custom patches and package them as a Quartz Composer plug-in. After a plug-in is installed in the appropriate directory, the patches contained in it are available to use in the Quartz Composer workspace and by most Quartz Composer clients, and can be used in the same manner that you use built-in patches.\n\nThis document shows how to create custom patches and package them as Quartz Composer plug-ins. You’ll see how to code a variety of patches from a simple string-processing patch to one that renders using OpenGL.\n\nAnyone who uses the Quartz Composer development tool and wants to create a custom patch should read this document. To get the most out of this document, you’ll need to be familiar with Quartz Composer User Guide. You’ll also need to know how to use Xcode to create an Objective-C project. Although Quartz Composer uses OpenGL when it renders, you don’t need to know OpenGL to write a custom patch unless you want to create a custom patch that renders on the GPU. This document shows how to write both non-rendering and rendering custom patches.\n\nYou can use the properties feature of Objective-C 2.0 when creating a custom patch. If you are unfamiliar with properties, you’ll want to read about them before you start writing custom patches. This feature is a time saver that eliminates the need to write accessor methods. All the examples in this document use properties. See The Objective-C Programming Language.\n\nOrganization of This Document\n\nThis document is organized into the following chapters:\n\nThe Basics of Custom Patches describes how the patches that appear in Quartz Composer relate to the code that generates a custom patch. It provides an overview of the tasks needed to create a custom patch and package it as a plug-in. It also describes the Xcode templates that you can use to write custom patches.\n\nWriting Processor Patches shows how to write three patches that process data—one that processes a string, another that converts a numeric value to a color, and another that shows how to configure a parameter that can’t be represented by one of the standard port data types.\n\nWriting Image Processing Patches describes how to use input and output image protocols to create a patch that produces an image by operating on two input images.\n\nWriting Consumer Patches discusses how to use OpenGL in a custom patch and provides instructions for writing a patch that renders a quad that you can animate.\n\nSee Also\n\nThe following resources are valuable to anyone writing a custom patch and packaging it as a Quartz Composer plug-in:\n\nSeveral of the sample code projects in /Developer/Examples/Quartz Composer/Plugins are custom patch projects.\n\nQuartz Composer User Guide describes the development tool and how to use it to create compositions.\n\nQuartz Composer Programming Guide shows how to perform programming tasks using the Quartz Composer framework.\n\nQuartz Composer Reference Collection describes all the classes and protocols in the Quartz Composer API. You’ll need to refer to this documentation as you write Quartz Composer custom patches.\n\nKey-Value Coding Programming Guide contains valuable information for anyone who is unfamiliar this mechanism for getting and setting values.\n\nNext\n\n\n\n\n\nCopyright © 2010 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2010-03-24\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Quartz Composer Basic Concepts",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposerUserGuide/qc_concepts/qc_concepts.html#//apple_ref/doc/uid/TP40005381-CH212-SW9",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer User Guide\nTable of Contents\nIntroduction\nQuartz Composer Basic Concepts\nThe Quartz Composer User Interface\nBasic and Advanced Tasks, Tips, and Tricks\nTutorial: Creating a Composition\nGlossary\nRevision History\nNext\nPrevious\nQuartz Composer Basic Concepts\n\nQuartz Composer is a development tool provided with OS X v10.5 for processing and rendering graphical data. Its visual programming environment is suited for:\n\nDeveloping graphics processing modules without writing a single line of code\n\nExploring the visual technologies available in OS X without needing to learn the programming interface for that technology\n\nAfter installing the developer tools provided with OS X v10.5, you can find the Quartz Composer development tool in:\n\n/Developer/Applications\n\nQuartz Composer brings together a rich set of graphical and nongraphical technologies, including Quartz 2D, Core Image, Core Video, OpenGL, QuickTime, Core MIDI Services, RSS (Really Simple Syndication), XML, and more.\n\nThis chapter describes the basic concepts you need to understand Quartz Composer. It defines the basic elements (compositions and patches), describes the flow of data in a composition, illustrates the coordinate system, and introduces the composition repository.\n\nCompositions\n\nYou use the Quartz Composer editor to create Quartz compositions, which are procedural motion graphics programs created by assembling preexisting modules (called patches) in a workflow for data processing and rendering. Figure 1-1 shows a simple composition.\n\nFigure 1-1  A Quartz composition\n\nCompositions can have input parameters and produce output results. The output produced by the composition shown in Figure 1-1 is a rotating cube whose faces show video captured by a camera connected to the computer (see Figure 1-2).\n\nFigure 1-2  Motion graphics produced by a composition\n\nA composition can operate autonomously, but it’s also possible for any Mac app to communicate with the composition and to integrate the composition into its existing work flow. (See Quartz Composer Programming Guide for details on integrating compositions into applications.) Compositions are stored as Quartz Composer files with the .qtz extension.\n\nPatches\n\nThe basic elements of Quartz Composer are patches. Similar to routines in traditional programming languages, patches are base processing units. They execute and produce a result. Patches are equivalent to the following:\n\nResult = function (time, {0 or more input parameters})\n\nUnlike traditional routines, patches are visual entities (see Figure 1-3) that you add to a visual programming environment. Circles on a patch represent ports, with input ports on the left side of a patch and output ports on the right side. Ports pass data through them—you can think of ports as parameters.\n\nFigure 1-3  Sample patches\n\nLike routines, not all patches take input parameters. Figure 1-3 shows three patches that demonstrate the various configurations that ports can have. The Low Frequency Oscillator (LFO) patch has both input and output ports. The six input ports—Type, Period, Phase, Amplitude, Offset, and PWM Ratio—provide data that is used to calculate the amplitude of an oscillation at a specific time. The calculated amplitude value is available on the output port of the patch.\n\nThe Image Importer and Quartz Composer Info patches don’t have any input ports, but each has an output port. The Image Importer patch produces an image, while the Quartz Composer Info patch produces a value that specifies the version of Quartz Composer running on the system. The Sprite patch, has many input ports, but no output port. Instead, the Sprite patch renders its result to a destination. Connections between the ports define how data flows when the composition runs.\n\nExecution Modes\n\nPatches are divided into groups that designate their execution mode—consumer, processor, or provider. A consumer renders a result to a destination. The Cube patch in Figure 1-1 is an example of a consumer. It draws a textured cube to the screen. A consumer patch has these behaviors:\n\nExecutes if its Enable input is set to True.\n\nExecutes in a defined order. Note the number in the top-right of the Cube patch. This designates the execution order (also called the rendering layer) with respect to other consumer patches. Consumer patches are executed in numerical order from lowest to highest.\n\nPulls data from processors and providers.\n\nA processor processes data at specified intervals or in response to changing input values. The Interpolation patch in Figure 1-1 is an example of a processor patch. It returns a value calculated by interpolating between a starting and ending value for a given time. The Interpolation patch updates its output value when input values or the time changes. In this case, the output value changes based on the duration of the interpolation and the repeat mode.\n\nA provider supplies data from an outside source to a composition. This type of patch executes on demand—that is, whenever data is requested of it, but at most once per frame. The Video Input patch in Figure 1-1 is an example of a provider patch. It supplies images captured from an external video source.\n\nThe title bars of patches are color coded to indicate their execution mode. Processors are green, providers are blue, and consumers are pink. Simply by looking at the color, you can determine the execution mode of each patch in Figure 1-1.\n\nThe Patch Hierarchy\n\nA Quartz composition is similar to any complex C or Objective-C program that has a main routine and many subroutines. The root macro patch is similar in concept to a main routine. A macro patch (or simply macro) is similar to a subroutine in a traditional program. Like subroutines, a macro can use (or call) another macro, which means macros can be nested, forming a patch hierarchy in a composition.\n\nQuartz Composer provides several macro patches that require you to add subpatches to them. For example, the Lighting patch is a macro. To use this patch to illuminate an object, you place, inside the Lighting patch, the patches that create the object that you want to illuminate. You can also create custom macros. Packaging patch collections as macros keeps complex compositions manageable and easy to read. Macro patches look different from other patches. Macros have squared corners while other patches have rounded corners, as you can see by looking at the assortment of patches in Figure 1-4.\n\nFigure 1-4  Macro patches have squared corners; other patches have rounded corners\nThe Evaluation Path\n\nThe Quartz Composer evaluation path determines when and how often each patch in a composition executes. When Quartz Composer executes a composition, it traverses the patch hierarchy, from the root macro patch level to lower levels, and attempts to execute the macro patches. Figure 1-5 shows the evaluation path for a composition with multiple levels. Evaluation begins at the root level with the macro patch. Quartz Composer must move to level 1 to obtain the data it needs for the macro patch at the root level. Level 1 contains a macro that must be evaluated, so evaluation moves to level 2. That level contains a macro, so evaluation moves to level 3. Level 3 does not contain any macros, so evaluation begins. After level 3 is evaluated, Quartz Composer moves to level 2 to complete that evaluation and then moves to level 1 to complete that evaluation.\n\nFigure 1-5  The evaluation path for a hierarchical composition\n\nFrom within a macro patch, Quartz Composer executes consumers first, beginning with the consumer whose rendering layer is lowest. Processor and provider patches execute when consumer patches pull data from them. Figure 1-6 shows the contents of a macro patch that renders a sprite. The mouse position controls the position of the sprite. A low-frequency oscillation controls the width and height of the sprite.\n\nFigure 1-6  The contents of a macro patch that renders a sprite\n\nFigure 1-7 shows the evaluation order of the macro shown in Figure 1-6. There are two consumer patches—Clear and Sprite. Recall that consumers evaluate in numerical order, from lowest to highest. For example, in Figure 1-7, Clear evaluates first, then Sprite. This means that Quartz Composer clears the viewing area before rendering a sprite. But in order for the Sprite patch to complete its execution, the patch pulls data first from the Mouse patch, then from the LFO patch, and finally from the Math patch. After the Sprite patch has all the data it needs, it can then render its result.\n\nFigure 1-7  Evaluation order\nThe Coordinate System\n\nQuartz Composer uses a three-dimensional homogeneous coordinate system, as shown in Figure 1-8. The origin is at the center of the screen. The x axis is horizontal and the y axis is vertical. The z axis is orthogonal to the x and y axes, so that it comes out of the screen, towards the viewer. The left and right borders of the screen have coordinates –1.0 and +1.0, respectively. (See the x axis in the figure.) The coordinates of the top and bottom borders (the y axis in the figure) depend on the screen aspect ratio (AR). In the case of a 4:3 aspect ratio, the values at the borders are +1.0 / AR = +0.75 and –1.0 / AR = –0.75, respectively.\n\nIt’s also possible to have an aspect ratio for which the top and bottom borders of the screen have coordinates –1.0 and +1.0, respectively, while the left and right borders are +1.0 / AR and –1.0 / AR, respectively. For example, a 3:4 aspect ratio.\n\nFigure 1-8  The Quartz Composer coordinate system\n\nThe Quartz Composer coordinate system maps to the rendering destination. The full width of a rendering destination is 2.0 units—one unit in the positive x axis added to one unit in the negative x axis. You can set the size of the graphics that are rendered. A size of 2.0 specifies to use the full width of the rendering destination. A size less than 2.0 units specifies to use a proportion of the rendering destination (1.0 specifies to use half, 1.5 specifies to use 75%, and so forth).\n\nBy default, the untransformed coordinate system is identical to the eye coordinate system. That is, the projection of a 3D object onto the 2D rendering destination is from the perspective of a viewer positioned directly in front of the monitor, with the z axis traveling perpendicular to the monitor.\n\nComposition Repository\n\nA Quartz composition provides a standard way to express motion graphics in OS X, whether it’s for animation or effects processing. For that reason, OS X v10.5 includes a composition repository—a central location for storing compositions. Any application can, using the Quartz Composer framework, query the repository for specific types of compositions or simply browse the repository to see what’s available. The repository resides across these folders:\n\n/System/Library/Compositions\n\n/Library/Compositions\n\n~/Library/Compositions\n\nAny composition stored in the repository must conform to one of the protocols listed in Table 1-1. Quartz Composer provides templates for each of these protocols, which are available when you startup Quartz Composer (see Figure 1-9) or by choosing File > New From Template. When you choose a template in the Quartz Composer user interface, you’ll see a detailed description for that protocol along with information about the required and optional parameters. After modifying a template, you can store the composition in the repository to allow other applications to use your composition.\n\nTable 1-1  Composition protocols\n\nProtocol\n\n\t\n\nRequired input parameters\n\n\t\n\nOptional input parameters\n\n\t\n\nOutput parameters\n\n\n\n\nGraphic animation\n\n\t\n\nNone\n\n\t\n\nPrimary color, secondary color, pace, and preview mode\n\n\t\n\nNone required, but must render to the screen\n\n\n\n\nGraphic transition\n\n\t\n\nSource image, destination image\n\n\t\n\nPreview mode\n\n\t\n\nNone required\n\n\n\n\nImage filter\n\n\t\n\nImage\n\n\t\n\nPosition of the center of the effect, preview mode\n\n\t\n\nImage\n\n\n\n\nMusic visualizer\n\n\t\n\nAudio peak, audio spectrum\n\n\t\n\nTrack info, track position, track signal\n\n\t\n\nNone required\n\n\n\n\nRSS visualizer\n\n\t\n\nURL for an RSS feed\n\n\t\n\nDisplay duration\n\n\t\n\nNone required\n\n\n\n\nScreen saver\n\n\t\n\nNone\n\n\t\n\nScreen image, preview mode\n\n\t\n\nNone required, webpage URL optional\n\nThe Quartz Composer programming interface provides Objective-C classes (QCCompositionRepository and QCComposition) that allow developers to access the repository programmatically and provide support within an application for browsing and choosing compositions. You do not need to know Objective-C to create a composition for the repository, but you must know Objective-C to access the repository programmatically. The Quartz Composer programming interface makes it easy for developers to support the sort of motion graphics that application like iMovie and iDVD use. For more information, see Quartz Composer Reference Collection.\n\nFigure 1-9  Templates available in Quartz Composer\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2007 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2007-07-17\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Introduction to Quartz Composer Programming Guide",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposer/qc_intro/qc_intro.html#//apple_ref/doc/uid/TP40001357",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer Programming Guide\nTable of Contents\nIntroduction\nUsing QCView to Create a Standalone Composition\nPublishing Ports and Binding Them to Controls\nUsing the QCRenderer Class to Play a Composition\nAdding Compositions to Webpages and Widgets\nRevision History\nNext\nRetired Document\n\nImportant: This document may not represent best practices for current development. Links to downloads and other resources may no longer be valid.\n\nIntroduction to Quartz Composer Programming Guide\n\nImportant: This document may not represent best practices for current development. Links to downloads and other resources may no longer be valid.\n\nThe Quartz Composer framework defines classes and protocols that work with compositions built using the Quartz Composer development tool. This book describes how to use the QCView and QCRenderer classes, and how to include compositions in webpages and widgets.\n\nYou should read this document if you are a developer who wants to load, play, and control compositions programmatically from a Cocoa application. This document assumes that you are familiar with the Quartz Composer development tool and the information in Quartz Composer User Guide. If you want to learn how to use the QCPlugIn class to create custom patches that you can use from within the Quartz Composer development tool, see Quartz Composer Custom Patch Programming Guide.\n\nOrganization of This Document\n\nThis document is organized as follows:\n\nUsing QCView to Create a Standalone Composition discusses how to render a composition to a QCView object using Interface Builder but no code.\n\nPublishing Ports and Binding Them to Controls shows how to add controls to the user interface that are bound to one or more published ports in a composition.\n\nUsing the QCRenderer Class to Play a Composition describes the QCRenderer class and shows how to use it to load and play a composition programmatically.\n\nAdding Compositions to Webpages and Widgets describes how to include a composition in a webpage or Dashboard widget.\n\nSee Also\n\nThese resources are essential for anyone wanting to program using the Quartz Composer framework:\n\nQuartz Composer Reference Collection provides documentation for the Objective-C programming interface for Quartz Composer.\n\n/Developer/Examples/Quartz Composer Sample Code contains a variety of Quartz Composer compositions.\n\nThe Quartz Composer development mailing list (quartzcomposer-dev) is an excellent place to discuss programming issues or topics with other developers.\n\nNext\n\n\n\n\n\nCopyright © 2004, 2013 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2013-04-23"
  },
  {
    "title": "Glossary",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposerUserGuide/qc_glossary/qc_glossary.html#//apple_ref/doc/uid/TP40005381-CH211-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer User Guide\nTable of Contents\nIntroduction\nQuartz Composer Basic Concepts\nThe Quartz Composer User Interface\nBasic and Advanced Tasks, Tips, and Tricks\nTutorial: Creating a Composition\nGlossary\nRevision History\nNext\nPrevious\nGlossary\nclip  \n\nPrepackaged “mini” compositions that you can drag into a composition and customize for your own use.\n\n\n\ncomposition  \n\nA collection of interconnected patches that describe a data flow.\n\n\n\ncomposition repository  \n\nA central location for storing compositions. Any application can, using the Quartz Composer framework, query the repository for specific types of compositions or browse the repository to see what’s available.\n\n\n\nconsumer  \n\nA patch that renders a result to a destination.\n\n\n\nCore Image filter  \n\nAn image processing routine provided by the Core Image framework. Core Image filters are automatically read into Quartz Composer and made available as patches.\n\n\n\ndebug rendering mode  \n\nA view that displays an animation of the data flow in a composition that can help track down issues. In this mode, patches in the workspace change colors as they move from one state to another. A drawer below the view displays log messages.\n\n\n\ngraph  \n\nA set of connected patches on the workspace.\n\n\n\nhierarchical browser  \n\nThe area in the Quartz Composer window used to view and navigate from one level to another in the patch hierarchy.\n\n\n\nmacro patch  \n\nA patch that contains other patches. A macro is similar to a subroutine in a traditional program. A macro can nest other macros within it. A macro is visually distinguished from a nonmacro patch by its shape—macros have squared-off corners and other patches have rounded corners.\n\n\n\nOpenGL  \n\nAn open source graphics library. For more information see http://www.opengl.org/.\n\n\n\npatch  \n\nThe base processing unit in a composition, which executes and produces a result. Patches are similar to routines in traditional programming languages. See also macro patch.\n\n\n\npatch hierarchy  \n\nThe levels in a composition created when macro patches are used. See also macro patch.\n\n\n\nport  \n\nThe mechanism by which patches communicate. Ports can represent input or output parameters. Connections between input and output ports of different patches establish a data flow in a composition.\n\n\n\nprocessor  \n\nA patch that processes data at specified intervals or in response to changing input values.\n\n\n\nprofile rendering mode  \n\nA view that displays an analysis of each rendered frame in a composition; the analysis can help you to optimize performance.\n\n\n\nprovider  \n\nA patch that supplies data from an outside source to a composition.\n\n\n\nroot macro patch  \n\nThe main routine in a composition; the evaluation of a composition begins at the root macro patch. All patches are nested, at one level or another, within the root macro patch. Ports that you publish at the root macro patch are accessible externally.\n\n\n\nRSS  \n\nReally Simple Syndication, a lightweight XML format.\n\n\n\nsubpatch  \n\nA patch that is contained in a macro.\n\n\n\ntemplate  \n\nA composition file that contains a basic set of patches for a particular purpose.\n\n\n\nworkspace  \n\nThe area in the Quartz Composer development tool used to assemble patches.\n\n\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2007 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2007-07-17\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Core Graphics Layer Drawing",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_layers/dq_layers.html#//apple_ref/doc/uid/TP30001066-CH219-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nCore Graphics Layer Drawing\n\nCGLayer objects (CGLayerRef data type) allow your application to use layers for drawing.\n\nLayers are suited for the following:\n\nHigh-quality offscreen rendering of drawing that you plan to reuse. For example, you might be building a scene and plan to reuse the same background. Draw the background scene to a layer and then draw the layer whenever you need it. One added benefit is that you don’t need to know color space or device-dependent information to draw to a layer.\n\nRepeated drawing. For example, you might want to create a pattern that consists of the same item drawn over and over. Draw the item to a layer and then repeatedly draw the layer, as shown in Figure 12-1. Any Quartz object that you draw repeatedly—including CGPath, CGShading, and CGPDFPage objects—benefits from improved performance if you draw it to a CGLayer. Note that a layer is not just for onscreen drawing; you can use it for graphics contexts that aren’t screen-oriented, such as a PDF graphics context.\n\nBuffering. Although you can use layers for this purpose, you shouldn’t need to because the Quartz Compositor makes buffering on your part unnecessary. If you must draw to a buffer, use a layer instead of a bitmap graphics context.\n\nFigure 12-1  Repeatedly painting the same butterfly image\n\nCGLayer objects and transparency layers are parallel to CGPath objects and paths created by CGContext functions. In the case of a CGLayer or CGPath object, you paint to an abstract destination and can then later draw the complete painting to another destination, such as a display or a PDF. When you paint to a transparency layer or use the CGContext functions that draw paths, you draw directly to the destination represented by a graphics context. There is no intermediate abstract destination for assembling the painting.\n\nHow Layer Drawing Works\n\nA layer, represented by the CGLayerRef data type, is engineered for optimal performance. When possible, Quartz caches a CGLayer object using a mechanism appropriate to the type of Quartz graphics context it is associated with. For example, a graphics context associated with a video card might cache the layer on the video card, which makes drawing the content that’s in a layer much faster than rendering a similar image that’s constructed from a bitmap graphics context. For this reason a layer is typically a better choice for offscreen drawing than a bitmap graphics context is.\n\nAll Quartz drawing functions draw to a graphics context. The graphics context provides an abstraction of the destination, freeing you from the details of the destination, such as its resolution. You work in user space, and Quartz performs the necessary transformations to render the drawing correctly to the destination. When you use a CGLayer object for drawing, you also draw to a graphics context. Figure 12-1 illustrates the necessary steps for layer drawing.\n\nFigure 12-2  Layer drawing\n\nAll layer drawing starts with a graphics context from which you create a CGLayer object using the function CGLayerCreateWithContext. The graphics context used to create a CGLayer object is typically a window graphics context. Quartz creates a layer so that it has all the characteristics of the graphics context—its resolution, color space, and graphics state settings. You can provide a size for the layer if you don’t want to use the size of the graphics context. In Figure 12-2, the left side shows the graphics context used to create the layer. The gray portion of the box on the right side, labeled CGLayer object, represents the newly created layer.\n\nBefore you can draw to the layer, you must obtain the graphics context that’s associated with the layer by calling the function CGLayerGetContext. This graphics context is the same flavor as the graphics context used to create the layer. As long as the graphics context used to create the layer is a window graphics context, then the CGLayer graphics context is cached to the GPU if at all possible. The white portion of the box on the right side of Figure 12-2 represents the newly created layer graphics context.\n\nYou draw to the layer’s graphics context just as you would draw to any graphics context, passing the layer’s graphic context to the drawing function. Figure 12-2 shows a leaf shape drawn to the layer context.\n\nWhen you are ready to use the contents of the layer, you can call the functions CGContextDrawLayerInRect or CGContextDrawLayerAtPoint, to draw the layer into a graphics context. Typically you would draw to the same graphics context that you used to create the layer object, but you are not required to. You can draw the layer to any graphics context, keeping in mind that layer drawing has the characteristics of the graphics context used to create the layer object, which could impose certain constraints (performance or resolution, for example). For example, a layer associated with the screen may be cached in video hardware. If the destination context is a printing or PDF context, it may need to be fetched from the graphics hardware to memory, resulting in poor performance.\n\nFigure 12-2 shows the contents of the layer—the leaf—drawn repeatedly to the graphics context used to create the layer object. You can reuse the drawing that’s in a layer as many times as you’d like before releasing the CGLayer object.\n\nTip:  Use transparency layers when you want to composite parts of a drawing to achieve such effects as shadowing a group of objects. (See Transparency Layers.) Use CGLayer objects when you want to draw offscreen or when you need to repeatedly draw the same thing.\n\nDrawing with a Layer\n\nYou need to perform the tasks described in the following section to draw using a CGLayer object:\n\nCreate a CGLayer Object Initialized with an Existing Graphics Context\n\nGet a Graphics Context for the Layer\n\nDraw to the CGLayer Graphics Context\n\nDraw the Layer to the Destination Graphics Context\n\nSee Example: Using Multiple CGLayer Objects to Draw a Flag for a detailed code example.\n\nCreate a CGLayer Object Initialized with an Existing Graphics Context\n\nThe function CGLayerCreateWithContext returns a layer that is initialized with an existing graphics context. The layer inherits all the characteristics of the graphics context, including the color space, size, resolution, and pixel format. Later, when you draw the layer to a destination, Quartz automatically color matches the layer to the destination context.\n\nThe function CGLayerCreateWithContext takes three parameters:\n\nThe graphics context to create the layer from. Typically you pass a window graphics context so that you can later draw the layer onscreen.\n\nThe size of the layer relative to the graphics context. The layer can be the same size as the graphics context or smaller. If you need to retrieve the layer size later, you can call the function CGLayerGetSize.\n\nAn auxiliary dictionary. This parameter is currently unused, so pass NULL.\n\nGet a Graphics Context for the Layer\n\nQuartz always draws to a graphics context. Now that you have a layer, you must create a graphics context associated with the layer. Anything you draw into the layer graphics context is part of the layer.\n\nThe function CGLayerGetContext takes a layer as a parameter and returns a graphics context associated with the layer.\n\nDraw to the CGLayer Graphics Context\n\nAfter you obtain the graphics context associated with a layer, you can perform any drawing you’d like to the layer graphics context. You can open a PDF file or an image file and draw the file contents to the layer. You can use any of the Quartz 2D functions to draw rectangles, lines, and other drawing primitives. Figure 12-3 shows an example of drawing rectangles and lines to a layer.\n\nFigure 12-3  A layer that contains two rectangles and a series of lines\n\nFor example, to draw a filled rectangle to a CGLayer graphics context, you call the function CGContextFillRect, supplying the graphics context you obtained from the function CGLayerGetContext. If the graphics context is named myLayerContext, the function call looks like this:\n\nCGContextFillRect (myLayerContext, myRect)\n\nDraw the Layer to the Destination Graphics Context\n\nWhen you are ready to draw the layer to its destination graphics context you can use either of the following functions:\n\nCGContextDrawLayerInRect, which draws a layer to a graphics context in the rectangle specified.\n\nCGContextDrawLayerAtPoint, which draws the layer to a graphics context at the point specified.\n\nTypically the destination graphics context you supply is a window graphics context and it is the same graphics context you use to create the layer. Figure 12-4 shows the result of repeatedly drawing the layer drawing shown in Figure 12-3. To achieve the patterned effect, you call either of the layer drawing functions repeatedly—CGContextDrawLayerAtPoint or CGContextDrawLayerInRect—changing the offset each time. For example you can call the function CGContextTranslateCTM to change the origin of the coordinate space each time you draw the layer.\n\nFigure 12-4  Drawing a layer repeatedly\n\nNote:  You are not required to draw a layer to the same graphics context that you use to initialize the layer. However, if you draw the layer to another graphics context, any limitations of the original graphics context are imposed on your drawing.\n\nExample: Using Multiple CGLayer Objects to Draw a Flag\n\nThis section shows how to use two CGLayer objects to draw the flag shown in Figure 12-5 onscreen. First you’ll see how to reduce the flag to simple drawing primitives, then you’ll look at the code needed to accomplish the drawing.\n\nFigure 12-5  The result of using layers to draw the United States flag\n\nFrom the perspective of drawing it onscreen, the flag has three parts:\n\nA pattern of red and white stripes. You can reduce the pattern to a single red stripe because, for onscreen drawing, you can assume a white background. You create a single red rectangle, then repeatedly draw the rectangle at various offsets to create the seven red stripes necessary for the U.S. flag. A layer is ideal for repeated drawing. You draw the red rectangle to a layer, then draw the layer onscreen seven times.\n\nA blue rectangle. You need the blue rectangle once, so using a layer is of no benefit. When it comes time to draw the blue rectangle, draw it directly onscreen.\n\nA pattern of 50 white stars. Like the red stripe, a layer is ideal for drawing the stars. You create a path that outlines a star shape, and then fill the path with white. Draw one star to a layer, then draw the layer 50 times, adjusting the offset each time to get the appropriate spacing.\n\nThe code in Figure 12-2 produces the output shown in Figure 12-5. A detailed explanation for each numbered line of code appears following the listing. The listing is rather long, so you might want to print the explanation so that you can read it as you look at the code. The myDrawFlag routine is called from within a Cocoa application. The application passes a window graphics context and a rectangle that specifies the size of the view associated with the window graphics context.\n\nNote: Before you call this or any routine that uses CGLayer objects, you must check to make sure that the system is running Mac OS X v10.4 or later and has a graphics card that supports using CGLayer objects.\n\nListing 12-1  Code that uses layers to draw a flag\n\nvoid myDrawFlag (CGContextRef context, CGRect* contextRect)\n\n\n{\n\n\n    int          i, j,\n\n\n                 num_six_star_rows = 5,\n\n\n                 num_five_star_rows = 4;\n\n\n    CGFloat      start_x = 5.0,\n// 1\n\n\n                 start_y = 108.0,\n// 2\n\n\n                 red_stripe_spacing = 34.0,\n// 3\n\n\n                 h_spacing = 26.0,\n// 4\n\n\n                 v_spacing = 22.0;\n// 5\n\n\n    CGContextRef myLayerContext1,\n\n\n                 myLayerContext2;\n\n\n    CGLayerRef   stripeLayer,\n\n\n                 starLayer;\n\n\n    CGRect       myBoundingBox,\n// 6\n\n\n                 stripeRect,\n\n\n                 starField;\n\n\n // ***** Setting up the primitives *****\n\n\n    const CGPoint myStarPoints[] = {{ 5, 5},   {10, 15},\n// 7\n\n\n                                    {10, 15},  {15, 5},\n\n\n                                    {15, 5},   {2.5, 11},\n\n\n                                    {2.5, 11}, {16.5, 11},\n\n\n                                    {16.5, 11},{5, 5}};\n\n\n \n\n\n    stripeRect  = CGRectMake (0, 0, 400, 17); // stripe\n// 8\n\n\n    starField  =  CGRectMake (0, 102, 160, 119); // star field\n// 9\n\n\n \n\n\n    myBoundingBox = CGRectMake (0, 0, contextRect->size.width, \n// 10\n\n\n                                      contextRect->size.height);\n\n\n \n\n\n     // ***** Creating layers and drawing to them *****\n\n\n    stripeLayer = CGLayerCreateWithContext (context, \n// 11\n\n\n                            stripeRect.size, NULL);\n\n\n    myLayerContext1 = CGLayerGetContext (stripeLayer);\n// 12\n\n\n \n\n\n    CGContextSetRGBFillColor (myLayerContext1, 1, 0 , 0, 1);\n// 13\n\n\n    CGContextFillRect (myLayerContext1, stripeRect);\n// 14\n\n\n \n\n\n    starLayer = CGLayerCreateWithContext (context,\n\n\n                            starField.size, NULL);\n// 15\n\n\n    myLayerContext2 = CGLayerGetContext (starLayer);\n// 16\n\n\n    CGContextSetRGBFillColor (myLayerContext2, 1.0, 1.0, 1.0, 1);\n// 17\n\n\n    CGContextAddLines (myLayerContext2, myStarPoints, 10);\n// 18\n\n\n    CGContextFillPath (myLayerContext2);    \n// 19\n\n\n \n\n\n     // ***** Drawing to the window graphics context *****\n\n\n    CGContextSaveGState(context);    \n// 20\n\n\n    for (i=0; i< 7;  i++)   \n// 21\n\n\n    {\n\n\n        CGContextDrawLayerAtPoint (context, CGPointZero, stripeLayer);\n// 22\n\n\n        CGContextTranslateCTM (context, 0.0, red_stripe_spacing);\n// 23\n\n\n    }\n\n\n    CGContextRestoreGState(context);\n// 24\n\n\n \n\n\n    CGContextSetRGBFillColor (context, 0, 0, 0.329, 1.0);\n// 25\n\n\n    CGContextFillRect (context, starField);\n// 26\n\n\n \n\n\n    CGContextSaveGState (context);              \n// 27\n\n\n    CGContextTranslateCTM (context, start_x, start_y);      \n// 28\n\n\n    for (j=0; j< num_six_star_rows;  j++)   \n// 29\n\n\n    {\n\n\n        for (i=0; i< 6;  i++)\n\n\n        {\n\n\n            CGContextDrawLayerAtPoint (context,CGPointZero,\n\n\n                                            starLayer);\n// 30\n\n\n            CGContextTranslateCTM (context, h_spacing, 0);\n// 31\n\n\n        }\n\n\n        CGContextTranslateCTM (context, (-i*h_spacing), v_spacing); \n// 32\n\n\n    }\n\n\n    CGContextRestoreGState(context);\n\n\n \n\n\n    CGContextSaveGState(context);\n\n\n    CGContextTranslateCTM (context, start_x + h_spacing/2, \n// 33\n\n\n                                 start_y + v_spacing/2);\n\n\n    for (j=0; j< num_five_star_rows;  j++)  \n// 34\n\n\n    {\n\n\n        for (i=0; i< 5;  i++)\n\n\n        {\n\n\n        CGContextDrawLayerAtPoint (context, CGPointZero,\n\n\n                            starLayer);\n// 35\n\n\n            CGContextTranslateCTM (context, h_spacing, 0);\n// 36\n\n\n        }\n\n\n        CGContextTranslateCTM (context, (-i*h_spacing), v_spacing);\n// 37\n\n\n    }\n\n\n    CGContextRestoreGState(context);\n\n\n \n\n\n    CGLayerRelease(stripeLayer);\n// 38\n\n\n    CGLayerRelease(starLayer);        \n// 39\n\n\n}\n\nHere’s what the code does:\n\nDeclares a variable for the horizontal location of the first star.\n\nDeclares a variable for the vertical location of the first star.\n\nDeclares a variable for the spacing between the red stripes on the flag.\n\nDeclares a variable for the horizontal spacing between the stars on the flag.\n\nDeclares a variable for the vertical spacing between the stars on the flag.\n\nDeclares rectangles that specify where to draw the flag to (bounding box), the stripe layer, and the star field.\n\nDeclares an array of points that specify the lines that trace out one star.\n\nCreates a rectangle that is the shape of a single stripe.\n\nCreates a rectangle that is the shape of the star field.\n\nCreates a bounding box that is the same size as the window graphics context passed to the myDrawFlag routine.\n\nCreates a layer that is initialized with the window graphics context passed to the myDrawFlag routine.\n\nGets the graphics context associated with that layer. You’ll use this layer for the stripe drawing.\n\nSets the fill color to opaque red for the graphics context associated with the stripe layer.\n\nFills a rectangle that represents one red stripe.\n\nCreates another layer that is initialized with the window graphics context passed to the myDrawFlag routine.\n\nGets the graphics context associated with that layer. You’ll use this layer for the star drawing.\n\nSets the fill color to opaque white for the graphics context associated with the star layer.\n\nAdds the 10 lines defined by the myStarPoints array to the context associated with the star layer.\n\nFills the path, which consists of the 10 lines you just added.\n\nSaves the graphics state of the windows graphics context. You need to do this because you’ll draw the same stripe repeatedly, but in different locations.\n\nSets up a loop that iterates 7 times, once for each red stripe on the flag.\n\nDraws the stripe layer (which consists of a single red stripe).\n\nTranslates the current transformation matrix so that the origin is positioned at the location where the next red stripe must be drawn.\n\nRestores the graphics state to what is was prior to drawing the stripes.\n\nSets the fill color to the appropriate shade of blue for the star field. Note that this color has an opacity of 1.0. Although all the colors in this example are opaque, they don’t need to be. You can create nice effects with layered drawing by using partially transparent colors. Recall that an alpha value of 0.0 specifies a transparent color.\n\nFills the star field rectangle with blue. You draw this rectangle directly to the window graphics context. Don’t use layers if you are drawing something only once.\n\nSaves the graphics state for the window graphics context because you’ll be transforming the CTM to position the stars properly.\n\nTranslates the CTM so that the origin lies in the star field, positioned for the first star (left side) in the first (bottom) row.\n\nThis and the next for loop sets up the code to repeatedly draw the star layer so the five odd rows on the flag each contain six stars.\n\nDraws the star layer to the window graphics context. Recall that the star layer contains one white star.\n\nPositions the CTM so that the origin is moved to the right in preparation for drawing the next star.\n\nPositions the CTM so that the origin is moved upward in preparation for drawing the next row of stars.\n\nTranslates the CTM so that the origin lies in the star field, positioned for the first star (left side) in the second row from the bottom. Note that the even rows are offset with respect to the odd rows.\n\nThis and the next for loop sets up the code to repeatedly draw the star layer so the four even rows on the flag each contain five stars.\n\nDraws the star layer to the window graphics context.\n\nPositions the CTM so that the origin is moved to the right in preparation for drawing the next star.\n\nPositions the CTM so that the origin is down and to the left in preparation for drawing the next row of stars.\n\nReleases the stripe layer.\n\nReleases the star layer.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Writing Kernels",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageUnitTutorial/WritingKernels/WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nImage Unit Tutorial\nTable of Contents\nIntroduction\nAn Image Unit and Its Parts\nWriting Kernels\nWriting the Objective-C Portion\nPreparing an Image Unit for Distribution\nRevision History\nNext\nPrevious\nWriting Kernels\n\nThe heart of any image processing filter is the kernel file. A kernel file contains one or more kernel routines and any required subroutines. A kernel routine gets called once for each pixel for the destination image. The routine must return a vec4 data type. Although this four-element vector typically contains pixel data, the vector is not required to represent a pixel. However, the kernel routines in this chapter produce only pixel data because that’s the most common data returned by a kernel routine.\n\nA kernel routine can:\n\nFabricate the data. For example, a routine can produce random pixel values, generate a solid color, or produce a gradient. It can generate patterns, such as stripes, a checkerboard, a starburst, or color bars.\n\nModify a single pixel from a source image. A kernel routine can adjust hue, exposure, white point values, replace colors, and so on.\n\nSample several pixels from a source image to produce the output pixel. Stylize filters such as edge detection, pixellate, pointillize, gloom, and bloom use this technique.\n\nUse location information from one or more pixels in a source image to produce the output pixel. Distortion effects, such as bump, pinch, and hole distortions are created this way.\n\nProduce the output pixel by using data from a mask, texture, or other source to modify one or more pixels in a source image. The Core Image stylize filters—height field from mask, shaded material, and the disintegrate with mask transition—are examples of filters that use this technique.\n\nCombine the pixels from two images to produce the output pixel. Blend mode, compositing, and transition filters work this way.\n\nThis chapter shows how to write a variety of kernel routines. First you’ll see what the programming constraints, or rules, are. Then you’ll learn how to write a simple filter that operates on one input pixel to produce one output pixel. As the chapter progresses, you’ll learn how to write more complex kernel routines, including those used for a multipass filter.\n\nAlthough the kernel routine is where the per-pixel processing occurs, it is only one part of an image unit. You also need to write code that provides input data to the kernel routine and performs a number of other tasks as described in Writing the Objective-C Portion. Then you’ll need to bundle all the code by following the instructions in Preparing an Image Unit for Distribution.\n\nBefore continuing in this chapter, see Core Image Kernel Language Reference for a description of the language you use to write kernel routines. Make sure you are familiar with the constraints discussed in Kernel Routine Rules.\n\nWriting Simple Kernel Routines\n\nA kernel routine that operates on the color of a source pixel at location (x, y) to produce a pixel at the same location in the destination image is fairly straightforward to write. In general, a kernel routine that operates on color follows these steps:\n\nGets the pixel from the source image that is at the same location as the pixel you want to produce in the output image. The Core Image Kernel Language function sample returns the pixel value produced by the specified sampler at a specified point. To get the specified point, use the function samplerCoord, which returns the position, in sampler space, that is associated with the current output pixel after any transformation matrix associated with the sampler is applied. That means if the image is transformed in some way (for example, rotation or scaling), the sampler ensures that the transformation is reflected in the sample it fetches.\n\nOperates on the color values.\n\nNote: Pixels in Core Image are comprised of red, green, blue, and alpha components whose floating-point values can range from 0.0 (component absent) to 1.0 (component present at 100%).\n\nReturns the modified pixel.\n\nEquations for this sort of filter take the following form:\n\nDepending on the operation, you may need to unpremultiply the color values prior to operating on them and the premultiply the color values before returning the modified pixel. The Core Image Kernel Language provides the unpremultiply and premultiply functions for this purpose.\n\nColor Inversion\n\nColor component values for pixels in Core Image are floating-point values that range from 0.0 (color component absent) to 1.0 (color component present at 100%). Inverting a color is accomplished by reassigning each color component of value of 1.0 – component_value, such that:\n\nred_value = 1.0 - red_value\n\n\nblue_value = 1.0 - blue_value\n\n\ngreen_value = 1.0 - green_value\n\nFigure 2-1 shows a grid of pixels. If you invert the color of each pixel by applying these equations, you get the resulting grid of pixels shown in Figure 2-2.\n\nFigure 2-1  A grid of pixels\nFigure 2-2  A grid of pixels after inverting the color\n\nTake a look at the kernel routine in Listing 2-1 to see how to implement color inversion. A detailed explanation for each numbered line of code appears following the listing. You’ll see how to write the Objective-C portion that packages this routine as an image unit by reading Creating a Color Inversion Image Unit.\n\nListing 2-1  A kernel routine that inverts color\n\nkernel vec4 _invertColor(sampler source_image) \n// 1\n\n\n{\n\n\n    vec4 pixValue; \n// 2\n\n\n    pixValue = sample(source_image, samplerCoord(source_image)); \n// 3\n\n\n    unpremultiply(pixValue); \n// 4\n\n\n    pixValue.r = 1.0 - pixValue.r; \n// 5\n\n\n    pixValue.g = 1.0 - pixValue.g;\n\n\n    pixValue.b = 1.0 - pixValue.b;\n\n\n    return premultiply(pixValue); \n// 6\n\n\n}\n\nHere’s what the code does:\n\nTakes a sampler object as an input parameter. Recall (see Kernel Routine Rules) that kernel routines do not take images as input parameters. Instead, the sampler object is in charge of accessing image data and providing it to the kernel routine.\n\nA routine that modifies a single pixel value will always have a sampler object as an input parameter. The sampler object for this category of kernel routine is passed in from a CISampler object created in the Objective-C portion of an image unit. (See Division of Labor.) The sampler object simply retrieves pixel values from the a source image. You can think of the sampler object as a data source.\n\nDeclares a vec4 data type to hold the red, green, blue, and alpha component values of a pixel. A four-element vector provides a convenient way to store pixel data.\n\nFetches a pixel value from the source image. Let’s take a closer look at this statement, particularly the sample and samplerCoord functions provided by the Core Image Kernel Language. The samplerCoord function returns the position, in sampler space, associated with the current destination pixel after any transformations associated with the image source or the sampler are applied to the image data. As the kernel routine has no way of knowing whether any transformations have been applied, it’s best to use the samplerCoord function to retrieve the the position. When you read Writing the Objective-C Portion you’ll see that it is possible, and often necessary, to apply transformations in the Objective-C portion of an image unit.\n\nThe sample function returns the pixel value obtained by the sampler object from the specified position. This function assumes the position is in sampler space, which is why you need to nest the call to samplerCoord to retrieve the position.\n\nUnpremultiplies the pixel data. If your routine operates on color data that could have an alpha value other then 1.0 (fully opaque), you need to call the Core Image Kernel Language function unpremultiply (or take similar steps—see the advanced tip below) prior to operating on the color values\n\nNote: Core Image always works in an RGB colorspace. Data, such as YUV, must first be converted to RGB. Such conversion is at a much higher level than the kernel routine. However, Core Image performs the conversion of YUV texture data automatically for you. Keep in mind that the data provided to your kernel routine by Core Image is always RGB based.\n\nInverts the red color component. The next two lines invert the green and blue color components. Note that you can access the individual components of a pixel by using .r, .g, .b, and .a instead of a numerical index. That way, you never need to concern yourself with the order of the components. (You can also use .x, .y, .z, and .w as field accessors.)\n\nPremultiplies the data and returns a vec4 data type that contains inverted values for the color components of the destination pixel. The function premultiply is defined by the Core Image Kernel Language.\n\nAdvanced Tip: The following is a more efficient way to write Listing 2-1, and faster to execute. The code simply subtracts the red, green, and blue values from the alpha value. For any value of alpha, this has the same result as Listing 2-1. The pixValue.aaa notation might look a bit odd, but it is valid, and represents a three-element vector made up of three identical values (the alpha value).\n\nkernel vec4 _invertColor(sampler source_image)\n\n\n{\n\n\n    vec4 pixValue;\n\n\n    pixValue = sample(source_image, samplerCoord(source_image));\n\n\n    pixValue.rgb = pixValue.aaa - pixValue.rgb;\n\n\n    return pixValue;\n\n\n}\n\nWhen you apply a color inversion kernel routine to the image shown in Figure 2-3 you get the result shown in Figure 2-4.\n\nFigure 2-3  An image of a gazelle\nFigure 2-4  A gazelle image after inverting the colors\nColor Component Rearrangement\n\nListing 2-2 shows another simple kernel routine that modifies the color values of a pixel by rearranging the color component values. The red channel is assigned the green values. The green channel is assigned the blue values. The blue channel is assigned the red values. Applying this filter to the image shown in Figure 2-5 results in the image shown in Figure 2-6.\n\nFigure 2-5  An image of a ladybug\nFigure 2-6  A ladybug image after rearranging pixel components\n\nAs you can see, the routine in Listing 2-2 is very similar to Listing 2-1. This kernel routine, however, uses two vectors, one for the pixel provided from the source image and the other to hold the modified values. The alpha value remains unchanged, but the red, green, and blue values are shifted.\n\nListing 2-2  A kernel routine that places RGB values in the GBR channels\n\nkernel vec4 RGB_to_GBR(sampler source_image)\n\n\n{\n\n\n    vec4 originalColor, twistedColor;\n\n\n \n\n\n    originalColor = sample(source_image, samplerCoord(source_image));\n\n\n    twistedColor.r = originalColor.g;\n\n\n    twistedColor.g = originalColor.b;\n\n\n    twistedColor.b = originalColor.r ;\n\n\n    twistedColor.a = originalColor.a;\n\n\n    return twistedColor;\n\n\n}\nColor Multiplication\n\nColor multiplication is true to its name; it multiplies each pixel in a source image by a specified color. Figure 2-7 shows the effect of applying a color multiply filter to the image shown in Figure 2-5.\n\nFigure 2-7  A ladybug image after applying a multiply filter\n\nListing 2-3 shows the kernel routine used to produce this effect. So that you don’t get the idea that a kernel can take only one input parameter, note that this routine takes two input parameters—one a sampler object and the other a __color data type. The __color data type is one of two data types defined by the Core Image kernel language; the other data type is sampler, which you already know about. These two data types are not the only ones that you can use input parameters to a kernel routine. You can also use these data types which are defined by the Open GL Shading Language (glslang)—float, vec2, vec3, vec4.\n\nImportant: Keep in mind, however, that in the Objective-C portion of the filter, all data types passed to the kernel routine must be packaged Objective-C objects. See Kernel Routine Rules for details.\n\nThe color supplied to a kernel routine will be matched by Core Image to the working color space of the Core Image context associated with the kernel. There is nothing that you need to do regarding color in the kernel. Just keep in mind that, to the kernel routine, __color is a vec4 data type in premultiplied RGBA format, just as the pixel values fetched by the sampler are.\n\nListing 2-3 points out an important aspect of kernel calculations—the use of vector math. The sample fetched by the Core Image Kernel Language function sample is a four-element vector. Because it is a vector, you can multiply it directly by multiplyColor; there is no need to access each component separately.\n\nBy now you should be used to seeing the samplerCoord function nested with the sample function!\n\nListing 2-3  A kernel routine that produces a multiply effect\n\nkernel vec4 multiplyEffect (sampler image_source, __color multiplyColor)\n\n\n{\n\n\n  return sample (image_source, samplerCoord (image_source)) * multiplyColor;\n\n\n}\nA Kernel Challenge\n\nNow that you’ve seen how to write kernel routines that operate on a single pixel, it’s time to challenge yourself. Write a kernel routine that produces a monochrome image similar to what’s shown in Figure 2-8. The filter should take two input parameters, a sampler and a __color. Use Quartz Composer to test and debug your routine. You can find a solution in Solution to the Kernel Challenge.\n\nFigure 2-8  A monochrome image of a ladybug\nTesting Kernel Routines in Quartz Composer\n\nThe kernel routine, as you know, is one part of a Core Image filter. There is still a good deal of code that you need to write at a level higher than the kernel routine and a bit more work beyond that to package your image processing code as an image unit. Fortunately, you don’t need to write this additional code to test simple kernel routines. You can instead use Quartz Composer.\n\nQuartz Composer is a development tool for processing and rendering graphical data. It’s available on any computer that has the Developer tools installed. You can find it in /Developer/Applications. The following steps will show you how to use Quartz Composer to test each of the kernel routines you’ve read about so far.\n\nNote: Before you follow these steps, you’ll need to familiarize yourself with Quartz Composer by reading Quartz Composer User Guide. The time that you spend learning to use Quartz Composer will save you a lot of time debugging and testing many of the kernel routines that your write.\n\nLaunch Quartz Composer by double-clicking its icon in /Developer/Applications.\n\nIn the sheet that appears, choose Blank Composition.\n\nOpen the Patch Creator and search for Core Image Filter.\n\nAdd the Core Image Filter patch to the workspace.\n\nUse the search field to locate the Billboard patch, then add that patch to the workspace.\n\nIn a similar manner, locate the Image Importer patch and add it to the workspace.\n\nConnect the Image output port on the Image Importer patch to the Image input port on the Core Image Filter patch.\n\nConnect the Image output port on the Core Image Filter patch to the Image input port on the Billboard.\n\nClick the Image Importer patch and then click the Inspector button on the toolbar.\n\nChoose Settings from the inspector pop-up menu. Then click Import From File and choose an image.\n\nClick Viewer in the toolbar to make sure that the Viewer window is visible.\n\nYou should see the image that you imported rendered to the Viewer window.\n\nClick the Core Image Filter patch and open the inspector to the Settings pane.\n\nNote that there is already a kernel routine entered for a multiply effect. If you want to see how that works, choose Input Parameters from the inspector pop-up menu and then click the color well to set a color. You immediately see the results in the Viewer window.\n\nCopy the kernel routine shown in Listing 2-1 and replace the multiply effect routine that’s in the Settings pane of the Core Image Kernel patch.\n\nNotice that not only does the image on the Viewer window change (its color should be inverted), but the Core Image Kernel patch automatically changes to match the input parameters of the kernel routine. The invert color kernel has only one input parameter, whereas the multiply effect supplied as a default had two parameters.\n\nFollow the same procedure to test the kernel routine shown in Listing 2-2.\n\nWriting Advanced Kernel Routines\n\nUp to now you’ve seen how to write several simple kernel routines that operate on a pixel from a source image to produce a pixel in a destination image that’s at the same working-space coordinate as the pixel from the source image. Some of the most interesting and useful filters, however, do not use this one-to-one mapping. These more advanced kernel routines are what you’ll learn about in this section.\n\nRecall from An Image Unit and Its Parts that kernel routines that do not use a one-to-one mapping require a region-of-interest method that defines the area from which a sampler object can fetch pixels. The kernel routine knows nothing about this method. The routine simply takes the data that is passed to it, operates on it, and computes the vec4 data type that the kernel routine returns. As a result, this section doesn’t show you how to set up the ROI method. You’ll see how to accomplish that task in Writing the Objective-C Portion. For now, assume that each sampler passed to a kernel routine supplies data from the appropriate region of interest.\n\nAn example of an image produced by an advanced kernel routine is shown in Figure 2-9. The figure shows a grid of pixels produced by a “color block” kernel routine. You’ll notice that the blocks of color are 4 pixels wide and 4 pixels high. The pixels marked with “S” denote the location of the pixel in a source image from which the 4 by 4 block inherits its color. As you can see, the kernel routine must perform a one-to-many mapping. This is just the sort of operation that the the pixellate kernel routine discussed in detail in Pixellate performs.\n\nFigure 2-9  Colored blocks of pixels\n\nYou’ll see how to write two other advanced kernel routines in Edge Enhancement and Fun House Mirror Distortion.\n\nPixellate\n\nA pixellate filter uses a limited number of pixels from a source image to produce the destination image, as described in previously. Compare Figure 2-3 with Figure 2-10. Notice that the processed image looks blocky; it is made up of dots of a solid color. The size of the dots are determined by a scaling factor that’s passed to the kernel routine as an input parameter.\n\nFigure 2-10  A gazelle image after pixellation\n\nThe trick to any pixellate routine is to use a modulus operator on the coordinates to divide the coordinates into discrete steps. This causes your code to read the same source pixel until your output coordinate has incremented beyond the threshold of the scale, producing an effect similar to that shown in Figure 2-10. The code shown in Listing 2-4 creates round dots instead of squares by creating an anti-aliased edge that produces the dot effect shown in Figure 2-11. Notice that each 4-by-4 block represents a single color, but the alpha component varies from 0.0 to 1.0. Anti-aliasing effects are used in a number of filters, so it is worthwhile to study the code, shown in Listing 2-4, that accomplishes this.\n\nFigure 2-11  Colored blocks of pixels with opacity added\n\nThe pixellate kernel routine takes two input parameters: a sampler object for fetching samples from the source image and a floating-point value that specifies the diameter of the dots. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 2-4  A kernel routine that pixellates\n\nkernel vec4 roundPixellate(sampler src, float scale)\n// 1\n\n\n{\n\n\n    vec2    positionOfDestPixel, centerPoint; \n// 2\n\n\n    float   d, radius;\n\n\n    vec4    outValue;\n\n\n    float   smooth = 0.5;\n\n\n \n\n\n    radius = scale / 2.0;\n\n\n    positionOfDestPixel = destCoord();\n// 3\n\n\n    centerPoint = positionOfDestPixel;\n\n\n    centerPoint.x = centerPoint.x - mod(positionOfDestPixel.x, scale) + radius; \n// 4\n\n\n    centerPoint.y = centerPoint.y - mod(positionOfDestPixel.y, scale) + radius; \n// 5\n\n\n    d = distance(centerPoint, positionOfDestPixel); \n// 6\n\n\n \n\n\n    outValue = sample(src, samplerTransform(src, centerPoint)); \n// 7\n\n\n    outValue.a = outValue.a * smoothstep((d - smooth), d, radius); \n// 8\n\n\n \n\n\n    return premultiply(outValue);  \n// 9\n\n\n \n\n\n}\n\nHere’s what the code does:\n\nTakes a sampler and a scaling value. Note the scaling value is declared as a float data type here, but when you write the Objective-C portion of the filter, you must pass the float as an NSNumber object. Otherwise, the filter will not work. See Kernel Routine Rules.\n\nDeclares two vec2 data types. The centerPoint variable holds the coordinate of the pixel that determines the color of a block; it is the “S” shown in Figure 2-11. The positionOfDestPixel variable holds the position of the destination pixel.\n\nGets the position, in working-space coordinates, of the pixel currently being computed. The function destCoord is defined by the Core Image Kernel Language. (See Core Image Kernel Language Reference.)\n\nCalculates the x-coordinate for the pixel that determines the color of the destination pixel.\n\nCalculates the y-coordinate for the pixel that determines the color of the destination pixel.\n\nCalculates how far the destination pixel is from the center point (“S”). This distance determines the value of the alpha component.\n\nNote: The distance function is defined in the OpenGL Shading Language specification. It returns the distance between two points.\n\nfloat distance (genType p0, genType p1);\n\nFetches a pixel value from the source image, at the location specified by the centerPoint vector.\n\nRecall that the sample function returns the pixel value produced by the sampler at the specified position. This function assumes the position is in sampler space, which is why you need to nest the call to samplerCoord to retrieve the position.\n\nCreates an anti-aliased edge by multiplying the alpha component of the destination pixel by the smoothstep function defined by glslang. (See the OpenGL Shading Language specification.)\n\nNote: The smoothstep function returns 0.0 if x <= edge0 and if x >= edge1. Otherwise, it interpolates between 0 and 1.\n\ngenType smoothstep (float edge0, float edge1, genType x);\n\nPremultiplies the result before returning the value.\n\nYou’ll see how to write the Objective-C portion that packages this routine as an image unit by reading Creating a Pixellate Image Unit.\n\nEdge Enhancement\n\nThe edge enhancement kernel routine discussed in this section performs two tasks. It detects the edges in an image using a Sobel template. It also enhances the edges. Although the kernel routine operates on all color components, you can get an idea of what is does by comparing Figure 2-12 with Figure 2-13.\n\nNote: There are many ways to computationally detect edges in an image. One approach is to use a template as a model of the ideal edge. An edge-detection template approximates the gradient at the pixel that corresponds to the center of the template. A Sobel template is a commonly used template because it provides a very good edge model while remaining small and thus not too costly from a computational standpoint. The template used here is implemented as a set of convolution masks whose weights on the diagonal elements are less than those on the horizontal and vertical elements. For details, see one of the image processing books suggested in Further Reading.\n\nFigure 2-12  A checkerboard pattern before edge enhancement\nFigure 2-13  A checkerboard pattern after edge enhancement\n\nThe _EdgyFilter kernel is shown in Listing 2-5. It takes two parameters, a sampler for fetching image data from a source image and a power parameter that’s used to brighten or darken the image. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 2-5  A kernel routine that enhances edges\n\nkernel vec4 _EdgyFilter(sampler image, float power) \n// 1\n\n\n{\n\n\n    const vec2 xy = destCoord(); \n// 2\n\n\n    vec4  p00,p01,p02, p10,p12, p20,p21,p22; \n// 3\n\n\n    vec4  sumX, sumY, computedPixel;  \n// 4\n\n\n    float edgeValue;\n\n\n \n\n\n    p00 = sample(image, samplerTransform(image, xy+vec2(-1.0, -1.0))); \n// 5\n\n\n    p01 = sample(image, samplerTransform(image, xy+vec2( 0.0, -1.0)));\n\n\n    p02 = sample(image, samplerTransform(image, xy+vec2(+1.0, -1.0)));\n\n\n    p10 = sample(image, samplerTransform(image, xy+vec2(-1.0,  0.0)));\n\n\n    p12 = sample(image, samplerTransform(image, xy+vec2(+1.0,  0.0)));\n\n\n    p20 = sample(image, samplerTransform(image, xy+vec2(-1.0, +1.0)));\n\n\n    p21 = sample(image, samplerTransform(image, xy+vec2( 0.0, +1.0)));\n\n\n    p22 = sample(image, samplerTransform(image, xy+vec2(+1.0, +1.0)));\n\n\n \n\n\n    sumX = (p22+p02-p20-p00) + 2.0*(p12-p10); \n// 6\n\n\n    sumY = (p20+p22-p00-p02) + 2.0*(p21-p01); \n// 7\n\n\n \n\n\n    edgeValue = sqrt(dot(sumX,sumX) + dot(sumY,sumY)) * power; \n// 8\n\n\n \n\n\n    computedPixel = sample(image, samplerCoord(image)); \n// 9\n\n\n    computedPixel.rgb = computedPixel.rgb * edgeValue; \n// 10\n\n\n    return computedPixel; \n// 11\n\n\n}\n\nHere’s what the code does:\n\nTakes a sampler and a power value. Note that the power value is declared as a float data type here, but when you write the Objective-C portion of the filter, you must pass the float as an NSNumber object. Otherwise, the filter will not work. See Kernel Routine Rules.\n\nGets the position, in working-space coordinates, of the pixel currently being computed. The function destCoord is defined by the Core Image Kernel Language. (See Core Image Kernel Language Reference.)\n\nDeclares 8 four-element vectors. These vectors will hold the values of the 8 pixels that are neighbors to the destination pixel that the kernel routine is computing.\n\nDeclares vectors to hold the intermediate results and the final computed pixel.\n\nThis and the following seven lines of code fetch 8 neighboring pixels.\n\nComputes the sum of the x values of the neighboring pixels, weighted by the Sobel template.\n\nComputes the sum of the y values of the neighboring pixels, weighted by the Sobel template.\n\nComputes the magnitude, then scales by the power parameter. The magnitude provides the edge detection/enhancement portion of the filter, and the power has a brightening (or darkening) effect.\n\nNote: The dot function provided by OpenGL Shading Language returns the dot product of two vectors. If x and y are two four-element vectors that represent pixels, the result returned is:\n\nresult = x.r * y.r + x.g * y.g + x.b * y.b + x.a * y.a\n\nGets the pixel whose destination value needs to be computed.\n\nModifies the color of the destination pixel by the edge value.\n\nReturns the computed pixel.\n\nWhen you apply the _EdgyFilter kernel to the image shown in Figure 2-3, you get the resulting image shown in Figure 2-14.\n\nFigure 2-14  A gazelle image after edge enhancement\nFun House Mirror Distortion\n\nThe fun house mirror distortion kernel routine is provided as the default kernel routine for the image unit template in Xcode. (You’ll learn how to use the image unit template in Writing the Objective-C Portion.) Similar to a mirror in a carnival fun house, this filter distorts an image by stretching and magnifying a vertical strip of the image. Compare Figure 2-15 with Figure 2-3.\n\nFigure 2-15  A gazelle image distorted by a fun house mirror routine\n\nThe fun house kernel routine shown in Listing 2-6 takes the following parameters:\n\nsrc is the sampler that fetches image data from a source image.\n\ncenter_x is the x coordinate that defines the center of the vertical strip in which the warping takes place.\n\ninverse_radius is the inverse of the radius. You can avoid a division operation in the kernel routine by performing this calculation outside the routine, in the Objective-C portion of the filter.\n\nradius is the extent of the effect.\n\nscale specifies the amount of warping.\n\nThe mySmoothstep routine in Listing 2-6 is a custom smoothing function to ensure that the pixels at the edge of the effect blend with the rest of the image.\n\nListing 2-6  Code that creates a fun house mirror distortion\n\nfloat mySmoothstep(float x)\n\n\n{\n\n\n    return (x * -2.0 + 3.0) * x * x;\n\n\n}\n\n\n \n\n\nkernel vec4 funHouse(sampler src, float center_x, float inverse_radius,\n\n\n            float radius, float scale) \n// 1\n\n\n{\n\n\n    float distance;\n\n\n    vec2 myTransform1, adjRadius;\n\n\n \n\n\n    myTransform1 = destCoord(); \n// 2\n\n\n    adjRadius.x = (myTransform1.x - center_x) * inverse_radius; \n// 3\n\n\n    distance = clamp(abs(adjRadius.x), 0.0, 1.0); \n// 4\n\n\n    distance = mySmoothstep(1.0 - distance) * (scale - 1.0) + 1.0; \n// 5\n\n\n    myTransform1.x = adjRadius.x * distance * radius + center_x; \n// 6\n\n\n    return sample(src, samplerTransform(src, myTransform1)); \n// 7\n\n\n}\n\nHere’s what the code does:\n\nTakes a sampler and four float values as parameters. Note that when you write the Objective-C portion of the filter, you must pass each float value as an NSNumber object. Otherwise, the filter will not work. See Kernel Routine Rules.\n\nFetches the position, in working-space coordinates, of the pixel currently being computed.\n\nComputes an x coordinate that’s adjusted for it’s distance from the center of the effect.\n\nComputes a distance value based on the adjusted x coordinate and that varies between 0 and 1. Essentially, this normalizes the distance.\n\nAdjusts the normalized distance value so that is varies along a curve. The scale value determines the height of the curve. The radius value used previously to calculate the distance determines the width of the curve.\n\nComputes a transformation vector.\n\nReturns the pixel located at the position in the coordinate space after the coordinate space is transformed by the myTransform1 vector.\n\nTake a look at the default image unit in Xcode to see what’s required for the Objective-C portion of the image unit. (See The Image Unit Template in Xcode.) You’ll see that a region-of-interest method is required. You’ll also notice that the inverse radius calculation is computed in the outputImage method.\n\nWriting Kernel Routines for a Detective Lens\n\nThis section describes a more sophisticated use of kernel routines. You’ll see how to create two kernel routines that could stand on their own, but later, in Creating a Detective Lens Image Unit, you’ll see how to combine them to create a filter that, to the user, will look similar to a physical magnification lens, as shown in Figure 2-16.\n\nFigure 2-16  The ideal detective lens\n\nTo create this effect, it’s necessary to perform tasks outside the kernel routine. You’ll need Objective-C code to set up region-of-interest routines, to set up input parameters for each kernel routine, and to pass the output image produced by each kernel routine to a compositing filter. You’ll see how to accomplish these tasks later. After a discussion of the problem and the components of a detective lens, you’ll see how to write each of the kernel routines.\n\nThe Problem: Why a Detective Lens?\n\nThe resolution of images taken by today’s digital cameras have outpaced the resolution of computer displays. Images are typically downsampled by image editing applications to allow the user to see the entire image onscreen. Unfortunately, the downsampling hides the details in the image. One solution is to show the image using a 1:1 ratio between the screen resolution and the image resolution. This solution is not ideal, because only a portion of the image is displayed onscreen, causing the user to scroll in order to view other parts of the image.\n\nThis is where the detective lens filter comes in. The filter allows the user to inspect the details of part of a high resolution image, similar to what’s shown in Figure 2-17. The filter does not actually magnify the original image. Instead, it displays a downsampled image for the pixels that are not underneath the lens and fetches pixels from the unscaled original image for the pixels that are underneath the lens.\n\nFigure 2-17  A detective lens that enlarges a portion of a high-resolution image\n\nNote: Why a detective lens? Although the filter could just as easily be called a loupe filter, “detective” more readily calls to mind the act of searching for details.\n\nDetective Lens Anatomy\n\nBefore writing any kernel routine, it’s helpful to understand the parameters that control the image processing effect you want to achieve. Figure 2-18 shows a diagram of the top view of the lens. The lens, has a center and a diameter. The lens holder has a width. The lens also has:\n\nAn opacity. Compare the colors underneath the lens with those outside the lens in Figure 2-17.\n\nReflectivity, which can cause a shiny spot on the lens if the lens does not have a modern reflection-free coating.\n\nFigure 2-18  The parameters of a detective lens\n\nFigure 2-19 shows another characteristic of the lens that influences its effect—roundness. This lens is convex, but the height shown in the diagram (along with the lens diameter) controls how curved the lens is.\n\nFigure 2-19  A side view of a detective lens\n\nThe lens holder has additional characteristics as you can see in Figure 2-20. This particular lens holder has a fillet. A fillet is a strip of material that rounds off an interior angle between two sections. By looking at the cross section, you’ll see that the lens holder can have three parts to it—an inner sloping section, an outer sloping section, and a flat top. The fillet radius determines whether there is a flat top, as shown in the figure. If the fillet radius is half the lens holder width, there is no flat top. If the fillet radius is less than half the lens holder width, there will be a flattened section as shown in the figure.\n\nFigure 2-20  A cross section of the lens holder\n\nNext you’ll take a look at the kernel routines needed to produce the lens and lens holder characteristics.\n\nThe Lens Kernel Routine\n\nThe lens kernel routine must produce an effect similar to a physical lens. Not only should the routine appear to magnify what’s underneath it, but it should be slightly opaque and reflect some light. Figure 2-21 shows a lens with those characteristics.\n\nFigure 2-21  A checkerboard pattern magnified by a lens filter\n\nIn previous sections, you’ve seen how to write routines that require only one sampler. The kernel routine that produced the effect shown in Figure 2-21 requires three sampler objects for fetching pixels from:\n\nThe high-resolution image. Recall that the purpose of the lens is to allow the user to inspect details in an image that’s too large to fit onscreen. This sampler object fetches pixels to show underneath the lens—the part of the image that will appear magnified. Depending on the amount of magnification desired by the filter client, the sampler might need to downsample the high resolution image.\n\nA downsampled version of the high-resolution image. This sampler object fetches pixels to show outside the lens—the part of the image that will not appear to be magnified.\n\nThe highlight image. These samples are used to generate highlights in the lens to give the appearance of the lens being reflective. Figure 2-22 shows the highlight image. The highlights are so subtle, that to reproduce the image for this document, transparent pixels are represented as black. White pixels are opaque. The highlight shown in the figure is exaggerated so that it can be seen here.\n\nRecall that the setup work for sampler objects is done in the Objective-C portion of an image unit. See Division of Labor. You’ll see how to set up the CISampler objects in Creating a Detective Lens Image Unit.\n\nFigure 2-22  An image used to generate lens highlights\n\nThe lens kernel routine (see Listing 2-7) takes nine input parameters:\n\ndownsampled_src is the sampler associated with the downsampled version of the image.\n\nhires_src is the sampler associated with the high resolution image.\n\nhighlights is the sampler associated with the highlight image.\n\ncenter is a two-element vector that specifies the center of the magnifying lens.\n\nradius is the radius of the magnifying lens.\n\nroundness is a value that specifies how convex the lens is.\n\nopacity specifies how opaque the glass of the magnifying lens is. If the lens has a reflection-free coating, this value is 0.0. If it is as reflective as possible, the value is 1.0.\n\nhighlight size is a two-element vector that specifies the height and width of the highlight image.\n\nNow that you know a bit about the lens characteristics and the input parameters needed for the kernel routine, take a look at Listing 2-7. After the necessary declarations, the routine starts out by calculating normalized distance. The three lines of code that perform this calculation are typical of routines that operate within a portion of an image. Part of the routine is devoted to calculating and applying a transform that determines which pixel from the highlight image to fetch, and then modifying the magnified pixel by the highlight pixel. You’ll find more information in the detailed explanation for each numbered line of code that follows the listing.\n\nListing 2-7  A kernel routine for a lens\n\nkernel vec4 lens(sampler downsampled_src, sampler highres_src, sampler highlights,\n\n\n            vec2 center, float radius, float magnification,\n\n\n            float roundness, float opacity, vec2 highlightsSize) \n// 1\n\n\n{\n\n\n    float dist, mapdist; \n// 2\n\n\n    vec2 v0; \n// 3\n\n\n    vec4 pix, pix2, mappix; \n// 4\n\n\n \n\n\n    v0 = destCoord() - center; \n// 5\n\n\n    dist = length(v0); \n// 6\n\n\n    v0 = normalize(v0); \n// 7\n\n\n \n\n\n    pix = sample(downsampled_src, samplerCoord(downsampled_src)); \n// 8\n\n\n    mapdist = (dist / radius) * roundness; \n// 9\n\n\n    mappix = sample(highlights, samplerTransform(highlights,\n\n\n                highlightsSize * (v0 * mapdist + 1.0) * 0.5)); \n// 10\n\n\n    mappix *= opacity; \n// 11\n\n\n    pix2 = sample(highres_src, samplerCoord(highres_src)); \n// 12\n\n\n    pix2 = mappix + (1.0 - mappix.a) * pix2; \n// 13\n\n\n \n\n\n    return mix(pix, pix2, clamp(radius - dist, 0.0, 1.0)); \n// 14\n\n\n}\n\nHere’s what the code does:\n\nTakes three sampler objects, four float values, and two vec2 data types as parameters. Note that when you write the Objective-C portion of the filter, you must pass each float and vec2 values as NSNumber objects. Otherwise, the filter will not work. See Kernel Routine Rules.\n\nDeclares two variables: dist provides intermediate storage for calculating a mapdist distance. mapdist is used to determine which pixel to fetch from the highlight image.\n\nDeclares a two-element vector for storing normalized distance.\n\nDeclares three four-element vectors for storing pixel values associated with the three sampler sources.\n\nSubtracts the vector that represents the center point of the lens from the vector that represents the destination coordinate.\n\nCalculates the length of the difference vector.\n\nNote: The length function is defined in the OpenGL Shading Language specification:\n\nfloat length(genType x);\n\nIt returns the length of a vector, calculated by this formula:\n\nsqrt (x1*x2 + y1*y2)\n\nNormalizes the distance vector.\n\nNote: The normalize function is defined in the OpenGL Shading Language specification:\n\ngenType normalize(genType x);\n\nIt returns a vector with a length of 1 that lies in the same direction as x.\n\nFetches a pixel from the downsampled image. Recall that this image represents the pixels that appear outside the lens—the “unmagnified” pixels.\n\nCalculates the distance value that is needed to determine which pixel to fetch from the highlight image. This calculation is needed because the size of the highlight image is independent of the diameter of the lens. The calculation ensures that the highlight image stretches or shrinks to fit the area of the lens.\n\nFetches a pixel from the highlight image by applying a transform based on the distance function and the size of the highlight.\n\nModifies the pixel fetched from the highlight image to account for the opacity of the lens.\n\nFetches a pixel from the high resolution image. You’ll see later (Creating a Detective Lens Image Unit) that the magnification is applied in the Objective-C portion of the image unit.\n\nModifies the pixel from the high resolution image by the opacity-adjusted highlight pixel.\n\nSoftens the edge between the magnified (high resolution image) and unmagnified (downsampled image) pixels.\n\nThe mix and clamp functions provided by OpenGL Shading Language have hardware support and, as a result, are much more efficient for you to use than to implement your own.\n\nThe clamp function\n\ngenType clamp (genType x, float minValue, float maxValue)\n\nreturns:\n\nmin(max(x, minValue), maxValue)\n\nIf the destination coordinate falls within the area of the lens, the value of x is returned; otherwise clamp returns 0.0.\n\nThe mix function\n\ngenType mix (genType x, genType y,float a)\n\nreturns the linear blend between the first two arguments (x, y) passed to the function:\n\nx * (1 - a) + y * a\n\nIf the destination coordinate falls outside of the area of the lens (a = 0.0), mix returns an unmagnified pixel. If the destination coordinate falls on the edges of the lens (a = 1.0), mix returns a linear blend of the magnified and unmagnified pixels. If the destination coordinate inside the area of the lens, mix returns magnified pixel.\n\nThe Lens Holder Kernel Routine\n\nThe lens holder kernel routine is generator routine in that is does not operate on any pixels from the source image. The kernel routine generates an image from a material map, and that image sits on top of the source image. See Figure 2-23.\n\nFigure 2-23  A magnifying lens holder placed over a checkerboard pattern\n\nThe material map for the lens holder is shown in Figure 2-24. It is a digital photograph of a highly reflective ball. You can just as easily use another image of reflective material.\n\nFigure 2-24  A material map image\n\nThe kernel routine performs several calculations to determine which pixel to fetch from the material map for each location on the lens holder. The routine warps the material map to fit the inner part of the lens holder and warps it in reverse so that it fits the outer part of the lens holder. If the fillet radius is less than one-half the lens holder radius, the routine colors the flat portion of the lens holder by using pixels from the center portion of the material map. It may be a bit easier to see the results of the warping by looking at the lens holder in Figure 2-25, which was created from a checkerboard pattern. The figure also demonstrates that if you don’t use an image that has reflections in it, the lens holder won’t look realistic.\n\nFigure 2-25  A lens holder generated from a checkerboard image\n\nThe lens holder kernel routine (see Listing 2-8) takes six input parameters:\n\nmaterial is a sampler object that fetches samples from the material map shown in Figure 2-24.\n\ncenter is a two-element vector that specifies the center of the lens that the lens holder is designed to hold.\n\ninnerRadius is the distance from the center of the lens to the inner edge of the lens holder.\n\nouterRadius is the distance from the center of the lens to the outer edge of the lens holder.\n\nfilletRadius is the distance from the lens holder edge towards the lens holder center. This value should be less than half the lens holder radius.\n\nmaterialSize is a two-element vector that specifies the height and width of the material map.\n\nThe routine shown in Listing 2-8 starts with the necessary declarations. Similar to the lens kernel routine, the lens holder kernel routine calculates normalized distance. Its effect is limited to a ring shape, so the normalized distance is needed to determine where to apply the effect. The routine also calculated whether the destination coordinate is on the inner or outer portion of the ring (that is, lens holder). Part of the routine constructs a transform that is then used to fetch a pixel from the material map. The material map size is independent of the inner and outer lens holder diameter, so a transform is necessary to perform the warping required to map the material onto the lens holder. You’ll find more information in the detailed explanation for each numbered line of code that follows the listing.\n\nListing 2-8  A kernel routine that generates a magnifying lens holder\n\nkernel vec4 ring(sampler material, vec2 center, float innerRadius,\n\n\n                    float outerRadius, float filletRadius, vec2 materialSize) \n// 1\n\n\n{\n\n\n    float dist, f, d0, d1, alpha;\n\n\n    vec2 t0, v0;\n\n\n    vec4 pix;\n\n\n \n\n\n    t0 = destCoord() - center; \n// 2\n\n\n    dist = length(t0);\n// 3\n\n\n    v0 = normalize(t0);\n// 4\n\n\n \n\n\n    d0 = dist - (innerRadius + filletRadius); \n// 5\n\n\n    d1 = dist - (outerRadius - filletRadius); \n// 6\n\n\n    f = (d1 > 0.0) ? (d1 / filletRadius) : min(d0 / filletRadius, 0.0); \n// 7\n\n\n    v0 = v0 * f; \n// 8\n\n\n \n\n\n    alpha = clamp(dist - innerRadius, 0.0, 1.0) * clamp(outerRadius - dist, 0.0, 1.0); \n// 9\n\n\n \n\n\n    v0 = materialSize * (v0 + 1.0) * 0.5; \n// 10\n\n\n    pix = sample(material, samplerTransform(material, v0)); \n// 11\n\n\n \n\n\n  return pix * alpha; \n// 11\n\n\n}\n\nHere’s what the code does:\n\nTakes one sampler object, three float values, and two vec2 data types as parameters. Note that when you write the Objective-C portion of the filter, you must pass each float and vec2 value as NSNumber objects. Otherwise, the filter will not work. See Kernel Routine Rules.\n\nSubtracts the vector that represents the center point of the lens from the vector that represents the destination coordinate.\n\nCalculates the length of the difference vector.\n\nNormalizes the length.\n\nCalculates whether the destination coordinate is on the inner portion of the lens holder.\n\nCalculates whether the destination coordinate is on the outer portion of the lens holder.\n\nCalculates a shaping value that depends on the location of the destination coordinate: [-1...0] in the inner portion of the lens holder, [0...1] in the outer portion of the lens holder, and 0 otherwise.\n\nNote: The ternary operator?: is used as follows:\n\nexpression_1 ? expression_2 : expression_3\n\nIf expression 1 is true, then expression 2 is evaluated and expression 3 is ignored.\n\nIf expression 1 is false, then expression 3 is evaluated and expression 2 is ignored.\n\nModifies the normalized distance to account for the lens holder fillet. This value will shape the lens holder.\n\nCalculates an alpha value for the pixel at the destination coordinate. If the the location falls short of the inner radius, the alpha value is clamped to 0.0. Similarly, if the location overshoots the outer radius, the result is clamped to 0.0. Alpha values within the lens holder are clamped to 1.0. Pixels not on the lens holder are transparent.\n\nModifies the v0 vector by the width and height of the material map. Then scales the vector to account for the size of the material map.\n\nFetches a pixel from the material map by applying the v0 vector as a transform.\n\nApplies alpha to the pixel prior to returning it.\n\nSolution to the Kernel Challenge\n\nThe kernel routine for a monochrome filter should look similar to what’s shown in Listing 2-9.\n\nListing 2-9  A solution to the monochrome filter challenge\n\nkernel vec4 monochrome(sampler image, __color color)\n\n\n{\n\n\n    vec4 pixel;\n\n\n    pixel = sample(image, samplerCoord(image));\n\n\n    pixel.g = pixel.b = pixel.r;\n\n\n    return pixel * color;\n\n\n}\nNext Steps\n\nNow that you’ve seen how to write a variety of kernel routines, you are ready to move on to writing the Objective-C portion of an image unit. The next chapter shows how to create a project from the Xcode image unit template. You’ll see how to create an image unit for several of the kernel routines described in this chapter.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2011 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2011-06-06\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "PDF Document Creation, Viewing, and Transforming",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_pdf/dq_pdf.html#//apple_ref/doc/uid/TP30001066-CH214-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nPDF Document Creation, Viewing, and Transforming\n\nPDF documents store resolution-independent vector graphics, text, and images as a series of commands written in a compact programming language. A PDF document can contain multiple pages of images and text. PDF is useful for creating cross-platform, read-only documents and for drawing resolution-independent graphics.\n\nQuartz creates, for all applications, high-fidelity PDF documents that preserve the drawing operations of the application, as shown in Figure 13-1. The resulting PDF may be optimized for a specific use (such as a particular printer, or for the web) by other parts of the system, or by third-party products. PDF documents generated by Quartz view correctly in Preview and Acrobat.\n\nFigure 13-1  Quartz creates high-quality PDF documents\n\nQuartz not only uses PDF as its “digital paper” but also includes as part of its API a number of functions that you can use to display and generate PDF files and to accomplish a number of other PDF-related tasks.\n\nFor detailed information about PDF, including the PDF language and syntax, see PDF Reference, Fourth Edition, Version 1.5.\n\nOpening and Viewing a PDF\n\nQuartz provides the data type CGPDFDocumentRef to represent a PDF document. You create a CGPDFDocument object using either the function CGPDFDocumentCreateWithProvider or the function CGPDFDocumentCreateWithURL. After you create a CGPDFDocument object, you can draw it to a graphics context. Figure 13-2 shows a PDF document displayed inside a window.\n\nFigure 13-2  A PDF document\n\nListing 13-1 shows how to create a CGPDFDocument object and obtain the number of pages in the document. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 13-1  Creating a CGPDFDocument object from a PDF file\n\nCGPDFDocumentRef MyGetPDFDocumentRef (const char *filename)\n\n\n{\n\n\n    CFStringRef path;\n\n\n    CFURLRef url;\n\n\n    CGPDFDocumentRef document;\n\n\n    size_t count;\n\n\n \n\n\n    path = CFStringCreateWithCString (NULL, filename,\n\n\n                         kCFStringEncodingUTF8);\n\n\n    url = CFURLCreateWithFileSystemPath (NULL, path, \n// 1\n\n\n                        kCFURLPOSIXPathStyle, 0);\n\n\n    CFRelease (path);\n\n\n    document = CGPDFDocumentCreateWithURL (url);\n// 2\n\n\n    CFRelease(url);\n\n\n    count = CGPDFDocumentGetNumberOfPages (document);\n// 3\n\n\n    if (count == 0) {\n\n\n        printf(\"`%s' needs at least one page!\", filename);\n\n\n        return NULL;\n\n\n    }\n\n\n    return document;\n\n\n}\n\nHere’s what the code does:\n\nCalls the Core Foundation function to create a CFURL object from a CFString object that represents the filename of the PDF file to display.\n\nCreates a CGPDFDocument object from a CFURL object.\n\nGets the number of pages in the PDF so that the next statement in the code can ensure that the document has at least one page.\n\nYou can see how to draw a PDF page to a graphics context by looking at the code in Listing 13-2. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 13-2  Drawing a PDF page\n\nvoid MyDisplayPDFPage (CGContextRef myContext,\n\n\n                    size_t pageNumber,\n\n\n                    const char *filename)\n\n\n{\n\n\n    CGPDFDocumentRef document;\n\n\n    CGPDFPageRef page;\n\n\n \n\n\n    document = MyGetPDFDocumentRef (filename);\n// 1\n\n\n    page = CGPDFDocumentGetPage (document, pageNumber);\n// 2\n\n\n    CGContextDrawPDFPage (myContext, page);\n// 3\n\n\n    CGPDFDocumentRelease (document);\n// 4\n\n\n}\n\nHere’s what the code does:\n\nCalls your function (see Listing 13-1) to create a CGPDFDocument object from a filename you supply.\n\nGets the page for the specified page number from the PDF document.\n\nDraws the specified page from the PDF file by calling the function CGContextDrawPDFPage. You need to supply a graphics context and the page to draw.\n\nReleases the CGPDFDocument object.\n\nCreating a Transform for a PDF Page\n\nQuartz provides a function—CGPDFPageGetDrawingTransform—that creates an affine transform by mapping a box in a PDF page to a rectangle you specify. The prototype for this function is:\n\nCGAffineTransform CGPDFPageGetDrawingTransform (\n\n\n        CGPPageRef page,\n\n\n        CGPDFBox box,\n\n\n        CGRect rect,\n\n\n        int rotate,\n\n\n        bool preserveAspectRatio\n\n\n);\n\nThe function returns an affine transform using that following algorithm:\n\nIntersects the rectangle associated with the type of PDF box you specify in the box parameter (media, crop, bleed, trim, or art) and the /MediaBox entry of the specified PDF page. The intersection results in an effective rectangle.\n\nRotates the effective rectangle by the amount specified by the /Rotate entry for the PDF page.\n\nCenters the resulting rectangle on rectangle you supply in the rect parameter.\n\nIf the value of the rotate parameter you supply is nonzero and a multiple of 90, the function rotates the effective rectangle by the number of degrees you supply. Positive values rotate the rectangle to the right; negative values rotate the rectangle to the left. Note that you pass degrees, not radians. Keep in mind that the /Rotate entry for the PDF page contains a rotation as well, and the rotate parameter you supply is combined with the /Rotate entry.\n\nScales the effective rectangle, if necessary, so that it coincides with the edges of the rectangle you supply.\n\nIf you specify to preserve the aspect ratio by passing true in the preserveAspectRatio parameter, then the final rectangle coincides with the edges of the more restrictive dimension of the rectangle you supply in the rect parameter.\n\nYou can use this function, for example, if you are writing a PDF viewing application similar to that shown in Figure 13-3. If you were to provide a Rotate Left/Rotate Right feature, you could call CGPDFPageGetDrawingTransform to compute the appropriate transform for the current window size and rotation setting.\n\nFigure 13-3  A PDF page rotated 90 degrees to the right\n\nListing 13-3 shows a function that creates an affine transform for a PDF page using the parameters passed to the function, applies the transform, and then draws the PDF page. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 13-3  Creating an affine transform for a PDF page\n\nvoid MyDrawPDFPageInRect (CGContextRef context,\n\n\n                    CGPDFPageRef page,\n\n\n                    CGPDFBox box,\n\n\n                    CGRect rect,\n\n\n                    int rotation,\n\n\n                    bool preserveAspectRatio)\n\n\n{\n\n\n    CGAffineTransform m;\n\n\n \n\n\n    m = CGPDFPageGetDrawingTransform (page, box, rect, rotation,\n// 1\n\n\n                                    preserveAspectRato);\n\n\n    CGContextSaveGState (context);\n// 2\n\n\n    CGContextConcatCTM (context, m);\n// 3\n\n\n    CGContextClipToRect (context,CGPDFPageGetBoxRect (page, box));\n// 4\n\n\n    CGContextDrawPDFPage (context, page);\n// 5\n\n\n    CGContextRestoreGState (context);\n// 6\n\n\n}\n\nHere’s what the code does:\n\nCreates an affine transform from the parameters supplied to the function.\n\nSaves the graphics state.\n\nConcatenates the CTM with the affine transform.\n\nClips the graphics context to the rectangle specified by the box parameter. The function CGPDFPageGetBoxRect obtains the page bounding box (media, crop, bleed, trim, and art boxes) associated with the constant you supply—kCGPDFMediaBox, kCGPDFCropBox, kCGPDFBleedBox, kCGPDFTrimBox, or kCGPDFArtBox.\n\nDraws the PDF page to the transformed and clipped context.\n\nRestores the graphics state.\n\nCreating a PDF File\n\nIt’s as easy to create a PDF file using Quartz 2D as it is to draw to any graphics context. You specify a location for a PDF file, set up a PDF graphics context, and use the same drawing routine you’d use for any graphics context. The function MyCreatePDFFile, shown in Listing 13-4, shows all the tasks your code performs to create a PDF file. A detailed explanation for each numbered line of code appears following the listing.\n\nNote that the code delineates PDF pages by calling the functions CGPDFContextBeginPage and CGPDFContextEndPage. You can pass a CFDictionary object to specify page properties including the media, crop, bleed, trim, and art boxes. For a list of dictionary key constants and a more detailed description of each, see CGPDFContext Reference.\n\nListing 13-4  Creating a PDF file\n\nvoid MyCreatePDFFile (CGRect pageRect, const char *filename)\n// 1\n\n\n{\n\n\n    CGContextRef pdfContext;\n\n\n    CFStringRef path;\n\n\n    CFURLRef url;\n\n\n    CFDataRef boxData = NULL;\n\n\n    CFMutableDictionaryRef myDictionary = NULL;\n\n\n    CFMutableDictionaryRef pageDictionary = NULL;\n\n\n \n\n\n    path = CFStringCreateWithCString (NULL, filename, \n// 2\n\n\n                                kCFStringEncodingUTF8);\n\n\n    url = CFURLCreateWithFileSystemPath (NULL, path, \n// 3\n\n\n                     kCFURLPOSIXPathStyle, 0);\n\n\n    CFRelease (path);\n\n\n    myDictionary = CFDictionaryCreateMutable(NULL, 0,\n\n\n                        &kCFTypeDictionaryKeyCallBacks,\n\n\n                        &kCFTypeDictionaryValueCallBacks); \n// 4\n\n\n    CFDictionarySetValue(myDictionary, kCGPDFContextTitle, CFSTR(\"My PDF File\"));\n\n\n    CFDictionarySetValue(myDictionary, kCGPDFContextCreator, CFSTR(\"My Name\"));\n\n\n    pdfContext = CGPDFContextCreateWithURL (url, &pageRect, myDictionary); \n// 5\n\n\n    CFRelease(myDictionary);\n\n\n    CFRelease(url);\n\n\n    pageDictionary = CFDictionaryCreateMutable(NULL, 0,\n\n\n                        &kCFTypeDictionaryKeyCallBacks,\n\n\n                        &kCFTypeDictionaryValueCallBacks); \n// 6\n\n\n    boxData = CFDataCreate(NULL,(const UInt8 *)&pageRect, sizeof (CGRect));\n\n\n    CFDictionarySetValue(pageDictionary, kCGPDFContextMediaBox, boxData);\n\n\n    CGPDFContextBeginPage (pdfContext, pageDictionary); \n// 7\n\n\n    myDrawContent (pdfContext);\n// 8\n\n\n    CGPDFContextEndPage (pdfContext);\n// 9\n\n\n    CGContextRelease (pdfContext);\n// 10\n\n\n    CFRelease(pageDictionary); \n// 11\n\n\n    CFRelease(boxData);\n\n\n}\n\nHere’s what the code does:\n\nTakes as parameters a rectangle that specifies the size of the PDF page and a string that specifies the filename.\n\nCreates a CFString object from a filename passed to the function MyCreatePDFFile.\n\nCreates a CFURL object from the CFString object.\n\nCreates an empty CFDictionary object to hold metadata. The next two lines add a title and creator. You can add as many key-value pairs as you’d like using the function CFDictionarySetValue. For more information on creating dictionaries, see CFDictionary Reference.\n\nCreates a PDF graphics context, passing three parameters:\n\nA CFURL object that specifies a location for the PDF data.\n\nA pointer to a rectangle that defines the default size and location of the PDF page. The origin of the rectangle is typically (0, 0). Quartz uses this rectangle as the default bounds of the page media box. If you pass NULL, Quartz uses a default page size of 8.5 by 11 inches (612 by 792 points).\n\nA CFDictionary object that contains PDF metadata. Pass NULL if you don’t have metadata to add.\n\nYou can use the CFDictionary object to specify output intent options—intent subtype, condition, condition identifier, registry name, destination output profile, and a human-readable text string that contains additional information or comments about the intended target device or production condition. For more information about output intent options, see CGPDFContext Reference.\n\nCreates a CFDictionary object to hold the page boxes for the PDF page. This example sets the media box.\n\nSignals the start of a page. When you use a graphics context that supports multiple pages (such as PDF), you call the function CGPDFContextBeginPage together with CGPDFContextEndPage to delineate the page boundaries in the output. Each page must be bracketed by calls to CGPDFContextBeginPage and CGPDFContextEndPage. Quartz ignores all drawing operations performed outside a page boundary in a page-based context.\n\nCalls an application-defined function to draw content to the PDF context. You supply your drawing routine here.\n\nSignals the end of a page in a page-based graphics context.\n\nReleases the PDF context.\n\nReleases the page dictionary.\n\nAdding Links\n\nYou can add links and anchors to PDF context you create. Quartz provides three functions, each of which takes a PDF graphics context as a parameter, along with information about the links:\n\nCGPDFContextSetURLForRect lets you specify a URL to open when the user clicks a rectangle in the current PDF page.\n\nCGPDFContextSetDestinationForRect lets you set a destination to jump to when the user clicks a rectangle in the current PDF page. You must supply a destination name.\n\nCGPDFContextAddDestinationAtPoint lets you set a destination to jump to when the user clicks a point in the current PDF page. You must supply a destination name.\n\nProtecting PDF Content\n\nTo protect PDF content, there are a number of security options you can specify in the auxiliary dictionary you pass to the function CGPDFContextCreate. You can set the owner password, user password, and whether the PDF can be printed or copied by including the following keys in the auxiliary dictionary:\n\nkCGPDFContextOwnerPassword, to define the owner password of the PDF document. If this key is specified, the document is encrypted using the value as the owner password; otherwise, the document is not encrypted. The value of this key must be a CFString object that can be represented in ASCII encoding. Only the first 32 bytes are used for the password. There is no default value for this key. If the value of this key cannot be represented in ASCII, the document is not created and the creation function returns NULL. Quartz uses 40-bit encryption.\n\nkCGPDFContextUserPassword, to define the user password of the PDF document. If the document is encrypted, then the value of this key is the user password for the document. If not specified, the user password is the empty string. The value of this key must be a CFString object that can be represented in ASCII encoding; only the first 32 bytes are used for the password. If the value of this key cannot be represented in ASCII, the document is not created and the creation function returns NULL.\n\nkCGPDFContextAllowsPrinting specifies whether the document can be printed when it is unlocked with the user password. The value of this key must be a CFBoolean object. The default value of this key is kCFBooleanTrue.\n\nkCGPDFContextAllowsCopying specifies whether the document can be copied when it is unlocked with the user password. The value of this key must be a CFBoolean object. The default value of this key is kCFBooleanTrue.\n\nListing 14-4 (in the next chapter) shows code that checks PDF document to see if it’s locked and if it is, attempts to open the document with a password.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Document Revision History",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageIOGuide/ikpg_revhx/ikpg_revhx.html#//apple_ref/doc/uid/TP40005462-CH217-TP1",
    "html": "Documentation Archive\nDeveloper\nSearch\nImage I/O Programming Guide\nTable of Contents\nIntroduction\nBasics of Using Image I/O\nCreating and Using Image Sources\nWorking with Image Destinations\nRevision History\nPrevious\nDocument Revision History\n\nThis table describes the changes to Image I/O Programming Guide.\n\nDate\tNotes\n2016-09-13\t\n\nMinor edits.\n\n\n2010-07-14\t\n\nMinor edits.\n\n\n2010-06-25\t\n\nRevised content to reflect that Image I/O is available on iOS.\n\n\n2007-07-02\t\n\nNew document that explains how to read and write image data using the Image I/O framework.\n\n\n\nPrevious\n\n\n\n\n\nCopyright © 2001, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Working with Image Destinations",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageIOGuide/ikpg_dest/ikpg_dest.html#//apple_ref/doc/uid/TP40005462-CH219-SW3",
    "html": "Documentation Archive\nDeveloper\nSearch\nImage I/O Programming Guide\nTable of Contents\nIntroduction\nBasics of Using Image I/O\nCreating and Using Image Sources\nWorking with Image Destinations\nRevision History\nNext\nPrevious\nWorking with Image Destinations\n\nAn image destination abstracts the data-writing task and eliminates the need for you to manage data through a raw buffer. An image destination can represent a single image or multiple images. It can contain thumbnail images as well as properties for each image. After creating a CGImageDestination object for the appropriate destination (URL, CFData object, or Quartz data consumer), you can add image data and set image properties. When you are finished adding data, call the function CGImageDestinationFinalize.\n\nSetting the Properties of an Image Destination\n\nThe function CGImageDestinationSetProperties adds a dictionary (CFDictionaryRef) of properties (key-value pairs) to the images in an image destination. Although setting properties is optional, there are many situations for which you will want to set them. For example, if your application allows users to add keywords to images or change saturation, exposure, or other values, you’ll want to save that information in the options dictionary.\n\nImage I/O defines an extensive set of keys to specify such things as compression quality, background compositing color, Exif dictionary keys, color model values, GIF dictionary keys, Nikon and Canon camera keys, and many more. See CGImageProperties Reference.\n\nWhen setting up the dictionary, you have two choices. You can either create a CFDictionary object or you can create an NSDictionary object, then cast it as a CFDictionaryRef when you pass the options dictionary to the function CGImageDestinationSetProperties. (CFDictionary and NSDictionary are interchangeable, or toll-free bridged.) Listing 3-1 shows a code fragment that assigns key-value pairs for three properties, then creates a dictionary that contains those properties. Because this is a code fragment, the necessary calls to release the CFNumber and CFDictionary objects created by the code are not shown. When you write your code, you need to call CFRelease when you no longer need each of these objects.\n\nWhen you set up a key-value pair for a property, you need to consult the reference documentation (see CGImageDestination Reference and CGImageProperties Reference) for the expected data type of the value. As you can see in Listing 3-1, numerical values typically need to be wrapped in a CFNumber object. When you use Core Foundation types for dictionary values, you can also supply the callback constants when you create the dictionary—kCFTypeDictionaryKeyCallBacks and kCFTypeDictionaryValueCallBacks. (See CFDictionary Reference.)\n\nListing 3-1  Setting the properties of an image destination\n\nfloat compression = 1.0; // Lossless compression if available.\n\n\nint orientation = 4; // Origin is at bottom, left.\n\n\nCFStringRef myKeys[3];\n\n\nCFTypeRef   myValues[3];\n\n\nCFDictionaryRef myOptions = NULL;\n\n\nmyKeys[0] = kCGImagePropertyOrientation;\n\n\nmyValues[0] = CFNumberCreate(NULL, kCFNumberIntType, &orientation);\n\n\nmyKeys[1] = kCGImagePropertyHasAlpha;\n\n\nmyValues[1] = kCFBooleanTrue;\n\n\nmyKeys[2] = kCGImageDestinationLossyCompressionQuality;\n\n\nmyValues[2] = CFNumberCreate(NULL, kCFNumberFloatType, &compression);\n\n\nmyOptions = CFDictionaryCreate( NULL, (const void **)myKeys, (const void **)myValues, 3,\n\n\n                      &kCFTypeDictionaryKeyCallBacks, &kCFTypeDictionaryValueCallBacks);\n\n\n// Release the CFNumber and CFDictionary objects when you no longer need them.\nWriting an Image to an Image Destination\n\nTo write an image to a destination, you first need to create an image destination object by calling the CGImageDestinationCreateWithURL, CGImageDestinationCreateWithData, or CGImageDestinationCreateWithDataConsumer functions. You need to supply the UTI of the resulting image file. You can either supply a UTI or the equivalent constant, if one if available. See Table 1-1.\n\nAfter you create an image destination, you can add an image to it by calling the CGImageDestinationAddImage or CGImageDestinationAddImageFromSource functions. If the format of the image destination file supports multiple images, you can repeatedly add images. Calling the function CGImageDestinationFinalize signals Image I/O that you are finished adding images. Once finalized, you cannot add any more data to the image destination.\n\nListing 3-2 shows how you might implement a method to write an image file. Although this listing shows how to use an image destination from within an Objective-C method, you can just as easily create and use an image destination in a procedural C function. The options parameter includes any properties you want to specify for the image, such as camera or compression settings.\n\nListing 3-2   A method that writes an image to a URL\n\n- (void) writeCGImage: (CGImageRef) image toURL: (NSURL*) url withType: (CFStringRef) imageType andOptions: (CFDictionaryRef) options\n\n\n{\n\n\n   CGImageDestinationRef myImageDest = CGImageDestinationCreateWithURL((CFURLRef)url, imageType, 1, nil);\n\n\n   CGImageDestinationAddImage(myImageDest, image, options);\n\n\n   CGImageDestinationFinalize(myImageDest);\n\n\n   CFRelease(myImageDest);\n\n\n}\nCreating an Animated Image\n\nImage I/O can also be used to create animated images. When creating an animated image, you call CGImageDestinationAddImage for each frame you want to add to the image. You must also specify other properties that control how the animation is performed.\n\nListing 3-3 shows how to create an animated PNG image. First it creates a pair of dictionaries to hold the animation properties. The first dictionary specifies the number of time the animated PNG should repeat its animation before stopping on the final frame. The second dictionary specifies the frame delay used by every frame in the sequence. After creating the image destination, the code sets the file properties for the destination image and then adds the frames, one at a time. Finally, the CGImageDestinationFinalize method is called to complete the animated PNG.\n\nListing 3-3  Creating an animated PNG file\n\nlet loopCount = 1\n\n\nlet frameCount = 60\n\n\n \n\n\nvar fileProperties = NSMutableDictionary()\n\n\nfileProperties.setObject(kCGImagePropertyPNGDictionary, forKey: NSDictionary(dictionary: [kCGImagePropertyAPNGLoopCount: frameCount]))\n\n\n \n\n\nvar frameProperties = NSMutableDictionary()\n\n\nframeProperties.setObject(kCGImagePropertyPNGDictionary, forKey: NSDictionary(dictionary: [kCGImagePropertyAPNGDelayTime: 1.0 / Double(frameCount)]))\n\n\n \n\n\nguard let destination = CGImageDestinationCreateWithURL(fileURL, kUTTypePNG, frameCount, nil) else {\n\n\n    // Provide error handling here.\n\n\n}\n\n\n \n\n\nCGImageDestinationSetProperties(destination, fileProperties.copy() as? NSDictionary)\n\n\n \n\n\nfor i in 0..<frameCount {\n\n\n    autoreleasepool {\n\n\n        let radians = M_PI * 2.0 * Double(i) / Double(frameCount)\n\n\n        guard let image = imageForFrame(size: CGSize(width: 300, height: 300)) else {\n\n\n            return\n\n\n        }\n\n\n        \n\n\n        CGImageDestinationAddImage(destination, image, frameProperties)\n\n\n    }\n\n\n}\n\n\n \n\n\nif !CGImageDestinationFinalize(destination) {\n\n\n    // Provide error handling here.\n\n\n}\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Basics of Using Image I/O",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageIOGuide/imageio_basics/ikpg_basics.html#//apple_ref/doc/uid/TP40005462-CH216-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nImage I/O Programming Guide\nTable of Contents\nIntroduction\nBasics of Using Image I/O\nCreating and Using Image Sources\nWorking with Image Destinations\nRevision History\nNext\nPrevious\nBasics of Using Image I/O\n\nThe Image I/O framework provides opaque data types for reading image data from a source (CGImageSourceRef) and writing image data to a destination (CGImageDestinationRef). It supports a wide range of image formats, including the standard web formats, high dynamic range images, and raw camera data. Image I/O has many other features such as:\n\nThe fastest image decoders and encoders for the Mac platform\n\nThe ability to load images incrementally\n\nSupport for image metadata\n\nEffective caching\n\nYou can create image source and image destination objects from:\n\nURLs. Images whose location can be specified as a URL can act as a supplier or receiver of image data. In Image I/O, a URL is represented as the Core Foundation data type CFURLRef.\n\nThe Core Foundation objects CFDataRef and CFMutableDataRef.\n\nQuartz data consumer (CGDataConsumerRef) and data provider (CGDataProviderRef) objects.\n\nUsing the Image I/O Framework in Your Application\n\nImage I/O resides in the Application Services framework in OS X, and in the Image I/O framework in iOS. After adding the framework to your application, import the header file by including this statement:\n\n#import <ImageIO/ImageIO.h>\n\nSupported Image Formats\n\nThe Image I/O framework understands most of the common image file formats, such as JPEG, JPEG2000, RAW, TIFF, BMP, and PNG. Not all formats are supported on each platform. For the most up-to-date list of what Image I/O supports, you can call the these functions:\n\nCGImageSourceCopyTypeIdentifiers returns an array of the Uniform Type Identifiers (UTIs) that Image I/O supports as image sources.\n\nCGImageDestinationCopyTypeIdentifiers returns an array of the uniform type identifiers (UTIs) that Image I/O supports as image destinations.\n\nYou can then use the CFShow function to print the array to the debugger console in Xcode, as shown in Listing 1-1. The strings in the array returned by these functions take the form of com.apple.pict, public.jpeg, public.tiff, and so on. Table 1-1 lists the UTIs for many common image file formats. OS X and iOS define constants for most common image file formats; The full set of constants are declared in the UTCoreTypes.h header file. You can use these constants when you need to specify an image type, either as a hint for an image source (kCGImageSourceTypeIdentifierHint) or as an image type for an image destination.\n\nListing 1-1  Getting and printing supported UTIs\n\nCFArrayRef mySourceTypes = CGImageSourceCopyTypeIdentifiers();\n\n\nCFShow(mySourceTypes);\n\n\nCFArrayRef myDestinationTypes = CGImageDestinationCopyTypeIdentifiers();\n\n\nCFShow(myDestinationTypes);\nTable 1-1  Common uniform type identifiers (UTIs) and image content type constants\n\nUniform type identifier\n\n\t\n\nImage content type constant\n\n\n\n\npublic.image\n\n\t\n\nkUTTypeImage\n\n\n\n\npublic.png\n\n\t\n\nkUTTypePNG\n\n\n\n\npublic.jpeg\n\n\t\n\nkUTTypeJPEG\n\n\n\n\npublic.jpeg-2000 (OS X only)\n\n\t\n\nkUTTypeJPEG2000\n\n\n\n\npublic.tiff\n\n\t\n\nkUTTypeTIFF\n\n\n\n\ncom.apple.pict (OS X only)\n\n\t\n\nkUTTypePICT\n\n\n\n\ncom.compuserve.gif\n\n\t\n\nkUTTypeGIF\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Creating and Using Image Sources",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageIOGuide/imageio_source/ikpg_source.html#//apple_ref/doc/uid/TP40005462-CH218-SW3",
    "html": "Documentation Archive\nDeveloper\nSearch\nImage I/O Programming Guide\nTable of Contents\nIntroduction\nBasics of Using Image I/O\nCreating and Using Image Sources\nWorking with Image Destinations\nRevision History\nNext\nPrevious\nCreating and Using Image Sources\n\nAn image source abstracts the data-access task and eliminates the need for you to manage data through a raw memory buffer. An image source can contain more than one image, thumbnail images, properties for each image, and the image file. When you are working with image data and your application runs in OS X v10.4 or later, image sources are the preferred way to move image data into your application. After creating a CGImageSource object, you can obtain images, thumbnails, image properties, and other image information using the functions described in CGImageSource Reference.\n\nCreating an Image from an Image Source\n\nOne of the most common tasks you’ll perform with the Image I/O framework is to create an image from an image source, similar to what’s shown in Listing 2-1. This example shows how to create an image source from a path name and then extract the image. When you create an image source object, you can provide a hint as to the format of the image source file.\n\nWhen you create an image from an image source, you must specify an index and you can provide a dictionary of properties (key-value pairs) to specify such things as whether to create a thumbnail or allow caching. CGImageSource Reference and CGImageProperties Reference list keys and the expected data type of the value for each key.\n\nYou need to supply an index value because some image file formats allow multiple images to reside in the same source file. For an image source file that contains only one image, pass 0. You can find out the number of images in an image source file by calling the function CGImageSourceGetCount.\n\nListing 2-1  Creating an image from an image source\n\nCGImageRef MyCreateCGImageFromFile (NSString* path)\n\n\n{\n\n\n    // Get the URL for the pathname passed to the function.\n\n\n    NSURL *url = [NSURL fileURLWithPath:path];\n\n\n    CGImageRef        myImage = NULL;\n\n\n    CGImageSourceRef  myImageSource;\n\n\n    CFDictionaryRef   myOptions = NULL;\n\n\n    CFStringRef       myKeys[2];\n\n\n    CFTypeRef         myValues[2];\n\n\n \n\n\n    // Set up options if you want them. The options here are for\n\n\n    // caching the image in a decoded form and for using floating-point\n\n\n    // values if the image format supports them.\n\n\n    myKeys[0] = kCGImageSourceShouldCache;\n\n\n    myValues[0] = (CFTypeRef)kCFBooleanTrue;\n\n\n    myKeys[1] = kCGImageSourceShouldAllowFloat;\n\n\n    myValues[1] = (CFTypeRef)kCFBooleanTrue;\n\n\n    // Create the dictionary\n\n\n    myOptions = CFDictionaryCreate(NULL, (const void **) myKeys,\n\n\n                   (const void **) myValues, 2,\n\n\n                   &kCFTypeDictionaryKeyCallBacks,\n\n\n                   & kCFTypeDictionaryValueCallBacks);\n\n\n    // Create an image source from the URL.\n\n\n    myImageSource = CGImageSourceCreateWithURL((CFURLRef)url, myOptions);\n\n\n    CFRelease(myOptions);\n\n\n    // Make sure the image source exists before continuing\n\n\n    if (myImageSource == NULL){\n\n\n        fprintf(stderr, \"Image source is NULL.\");\n\n\n        return  NULL;\n\n\n    }\n\n\n    // Create an image from the first item in the image source.\n\n\n    myImage = CGImageSourceCreateImageAtIndex(myImageSource,\n\n\n                                           0,\n\n\n                                           NULL);\n\n\n \n\n\n    CFRelease(myImageSource);\n\n\n    // Make sure the image exists before continuing\n\n\n    if (myImage == NULL){\n\n\n         fprintf(stderr, \"Image not created from image source.\");\n\n\n         return NULL;\n\n\n    }\n\n\n \n\n\n    return myImage;\n\n\n}\nCreating a Thumbnail Image from an Image Source\n\nSome image source files contain thumbnail images that you can retrieve. If thumbnails aren’t already present, Image I/O gives you the option of creating them. You can also specify a maximum thumbnail size and whether to apply a transform to the thumbnail image.\n\nListing 2-2 shows how to create an image source from data, set up a dictionary that contains options related to the thumbnail, and then create a thumbnail image. You use the kCGImageSourceCreateThumbnailWithTransform key to specify whether the thumbnail image should be rotated and scaled to match the orientation and pixel aspect ratio of the full image.\n\nListing 2-2  Creating a thumbnail image\n\nCGImageRef MyCreateThumbnailImageFromData (NSData * data, int imageSize)\n\n\n{\n\n\n    CGImageRef        myThumbnailImage = NULL;\n\n\n    CGImageSourceRef  myImageSource;\n\n\n    CFDictionaryRef   myOptions = NULL;\n\n\n    CFStringRef       myKeys[3];\n\n\n    CFTypeRef         myValues[3];\n\n\n    CFNumberRef       thumbnailSize;\n\n\n \n\n\n   // Create an image source from NSData; no options.\n\n\n   myImageSource = CGImageSourceCreateWithData((CFDataRef)data,\n\n\n                                               NULL);\n\n\n   // Make sure the image source exists before continuing.\n\n\n   if (myImageSource == NULL){\n\n\n        fprintf(stderr, \"Image source is NULL.\");\n\n\n        return  NULL;\n\n\n   }\n\n\n \n\n\n   // Package the integer as a  CFNumber object. Using CFTypes allows you\n\n\n   // to more easily create the options dictionary later.\n\n\n   thumbnailSize = CFNumberCreate(NULL, kCFNumberIntType, &imageSize);\n\n\n \n\n\n   // Set up the thumbnail options.\n\n\n   myKeys[0] = kCGImageSourceCreateThumbnailWithTransform;\n\n\n   myValues[0] = (CFTypeRef)kCFBooleanTrue;\n\n\n   myKeys[1] = kCGImageSourceCreateThumbnailFromImageIfAbsent;\n\n\n   myValues[1] = (CFTypeRef)kCFBooleanTrue;\n\n\n   myKeys[2] = kCGImageSourceThumbnailMaxPixelSize;\n\n\n   myValues[2] = (CFTypeRef)thumbnailSize;\n\n\n \n\n\n   myOptions = CFDictionaryCreate(NULL, (const void **) myKeys,\n\n\n                   (const void **) myValues, 2,\n\n\n                   &kCFTypeDictionaryKeyCallBacks,\n\n\n                   & kCFTypeDictionaryValueCallBacks);\n\n\n \n\n\n  // Create the thumbnail image using the specified options.\n\n\n  myThumbnailImage = CGImageSourceCreateThumbnailAtIndex(myImageSource,\n\n\n                                          0,\n\n\n                                          myOptions);\n\n\n  // Release the options dictionary and the image source\n\n\n  // when you no longer need them.\n\n\n  CFRelease(thumbnailSize);\n\n\n  CFRelease(myOptions);\n\n\n  CFRelease(myImageSource);\n\n\n \n\n\n   // Make sure the thumbnail image exists before continuing.\n\n\n   if (myThumbnailImage == NULL){\n\n\n         fprintf(stderr, \"Thumbnail image not created from image source.\");\n\n\n         return NULL;\n\n\n   }\n\n\n \n\n\n   return myThumbnailImage;\n\n\n}\nIncrementally Loading an Image\n\nIf you have a very large image, or are loading image data over the web, you may want to create an incremental image source so that you can draw the image data as you accumulate it. You need to perform the following tasks to load an image incrementally from a CFData object:\n\nCreate the CFData object for accumulating the image data.\n\nCreate an incremental image source by calling the function CGImageSourceCreateIncremental.\n\nAdd image data to the CFData object.\n\nCall the function CGImageSourceUpdateData, passing the CFData object and a Boolean value (bool data type) that specifies whether the data parameter contains the entire image, or just partial image data. In any case, the data parameter must contain all the image file data accumulated up to that point.\n\nIf you have accumulated enough image data, create an image by calling CGImageSourceCreateImageAtIndex, draw the partial image, and then release it.\n\nCheck to see if you have all the data for an image by calling the function CGImageSourceGetStatusAtIndex. If the image is complete, this function returns kCGImageStatusComplete. If the image is not complete, repeat steps 3 and 4 until it is.\n\nRelease the incremental image source.\n\nDisplaying Image Properties\n\nDigital photos are tagged with a wealth of information about the image—image dimensions, resolution, orientation, color profile, aperture, metering mode, focal length, creation date, keywords, caption, and much more. This information is extremely useful for image handling and editing, but only if the data is exposed in the user interface. Although the CGImageSourceCopyPropertiesAtIndex function retrieves a dictionary of all the properties associated with an image in an image source, you’ll need to write code that traverses that dictionary to retrieve and then display that information.\n\nIn this section you’ll take a close look at a routine from the OS X ImageApp sample code, which is an image display application that you can download and experiment with. One of the features of the ImageApp sample code is an image Info window that displays a thumbnail image and image properties for the currently active image, as shown in Figure 2-1.\n\nFigure 2-1  An Info window that displays image properties\n\nYou can take a look at the ImageInfoPanel.h and ImageInfoPanel.m files for all the implementation details of this panel; you’ll also need to look at the nib file for the project to see how the window and bindings are set up. To get an idea of how you can use CGImageSource functions to support an image editing application, take a look at Listing 2-3. A detailed explanation for each numbered line of code appears following the listing. (Keep in mind that this routine is not a standalone routine—you can’t simply paste it into your own program. It is an excerpt from the ImageApp sample code.)\n\nListing 2-3  A routine that creates an image source and retrieves properties\n\n \n\n\n- (void) setURL:(NSURL*)url\n\n\n{\n\n\n    if ([url isEqual:mUrl])\n\n\n        return;\n\n\n \n\n\n    mUrl = url;\n\n\n \n\n\n    CGImageSourceRef source = CGImageSourceCreateWithURL((CFURLRef)url, NULL); \n// 1\n\n\n    if (source)\n\n\n    {\n\n\n        NSDictionary* props =\n\n\n           (NSDictionary*) CGImageSourceCopyPropertiesAtIndex(source, 0, NULL); \n// 2\n\n\n        [mTree setContent:[self propTree:props]]; \n// 3\n\n\n        NSDictionary* thumbOpts = [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            (id)kCFBooleanTrue, (id)kCGImageSourceCreateThumbnailWithTransform,\n\n\n            (id)kCFBooleanTrue, (id)kCGImageSourceCreateThumbnailFromImageIfAbsent,\n\n\n            [NSNumber numberWithInt:128], (id)kCGImageSourceThumbnailMaxPixelSize,\n\n\n            nil]; \n// 4\n\n\n        CGImageRef image = CGImageSourceCreateThumbnailAtIndex(source, 0,\n\n\n                                      (CFDictionaryRef)thumbOpts); \n// 5\n\n\n        [mThumbView setImage:image]; \n// 6\n\n\n        CGImageRelease(image); \n// 7\n\n\n        [mFilePath setStringValue:[mUrl path]]; \n// 8\n\n\n \n\n\n        NSString* uti = (NSString*)CGImageSourceGetType(source); \n// 9\n\n\n        [mFileType setStringValue:[NSString stringWithFormat:@\"%@\\n%@\",\n\n\n                        ImageIOLocalizedString(uti), uti]]; \n// 10\n\n\n \n\n\n        CFDictionaryRef fileProps = CGImageSourceCopyProperties(source, nil); \n// 11\n\n\n        [mFileSize setStringValue:[NSString stringWithFormat:@\"%@ bytes\",\n\n\n            (id)CFDictionaryGetValue(fileProps, kCGImagePropertyFileSize)]]; \n// 12\n\n\n    }\n\n\n    else  \n// 13\n\n\n    {\n\n\n        [mTree setContent:nil];\n\n\n        [mThumbView setImage:nil];\n\n\n        [mFilePath setStringValue:@\"\"];\n\n\n        [mFileType setStringValue:@\"\"];\n\n\n        [mFileSize setStringValue:@\"\"];\n\n\n    }\n\n\n}\n\nHere’s what the code does:\n\nCreates an image source object from the URL passed to the routine.\n\nCopies the properties for the image located at index location 0. Some image file formats can support more than one image, but this example assumes a single image (or that the image of interest is always the first one in the file). The CGImageSourceCopyPropertiesAtIndex function returns a CFDictionary object. Here, the code casts the CFDictionary as an NSDictionary object, as these data types are interchangeable (sometimes referred to as toll-free bridged).\n\nThe dictionary that’s returned contains properties that are key-value pairs. However, some of the values are themselves dictionaries that contain properties. Take a look at Figure 2-1 and you’ll see not only simple key-value pairs (such as Color Model-RGB) but you’ll also see Exif properties, IPTC Properties, JFIF Properties, and TIFF Properties, each of which is a dictionary. Clicking a disclosure triangle for one of these displays the properties in that dictionary. You’ll need to get these dictionaries and their properties so they can be displayed appropriately in the Info panel. That’s what the next step accomplishes.\n\nExtracts properties from the dictionary and sets them to a tree controller. If you look at the ImageInfoPanel.h file, you’ll see that the mTree variable is an NSTreeController object that is an outlet in Interface Builder. This controller manages a tree of objects. In this case, the objects are properties of the image.\n\nThe propTree: method is provided in the ImageInfoPanel.m file. It’s purpose is to traverse the property dictionary retrieved in the previous step, extract the image properties, and build the array that’s bound to the NSTreeController object.\n\nThe properties appears in a table of keys and values in Figure 2-1.\n\nSets up a dictionary of options to use when creating an image from the image source. Recall that options are passed in a dictionary. The Info panel shown in Figure 2-1 displays a thumbnail image. The code here sets up options that create a thumbnail that is rotated and scaled to the same orientation and aspect ratio of the full image. If a thumbnail does not already exist, one is created, and its maximum pixel size is 128 by 128 pixels.\n\nCreates a thumbnail image from the first image in the image source, using the options set up in the previous step.\n\nSets the thumbnail image to the view in the Info panel.\n\nReleases the image; it is no longer needed.\n\nExtracts the path from the URL passed to the method, and sets the string to the text field that’s bound to the Info panel. This is the Path text field in Figure 2-1.\n\nGets the uniform type identifier of the image source. (This can be different from the type of the images in the source.)\n\nCalls a function to retrieve the localized string for the UTI (ImageIOLocalizedString is declared in ImagePanel.m) and then sets the string to the text field that’s bound to the Info panel. This is the Type text field in Figure 2-1.\n\nRetrieves a dictionary of the properties associated with the image source. These properties apply to the container (such as the file size), not necessarily the individual images in the image source.\n\nRetrieves the file size value from the image source dictionary obtained in the previous step, then sets the associated string to the text field that’s bound to the Info panel. This is the Size text field shown in Figure 2-1.\n\nIf the source is not created, makes sure that all the fields in the user interface reflect that fact.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Color Spaces",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/csintro/csintro_colorspace/csintro_colorspace.html#//apple_ref/doc/uid/TP30001148-CH222-BBCBDGDD",
    "html": "Documentation Archive\nDeveloper\nSearch\nColor Management Overview\nTable of Contents\nIntroduction\nColor: A Brief Overview\nColor Spaces\nColor Management Systems\nRevision History\nNext\nPrevious\nColor Spaces\n\nA color space describes an environment in which colors are represented, ordered, compared, or computed. A color space defines a one-, two-, three-, or four-dimensional environment whose components (or color components) represent intensity values. A color component is also referred to as a color channel. For example, RGB space is a three-dimensional color space whose stimuli are the red, green, and blue intensities that make up a given color; and red, green, and blue are color channels. Visually, these spaces are often represented by various solid shapes, such as cubes, cones, or polyhedra.\n\nFor additional information on color components, see Color-Component Values, Color Values, and Color.\n\nApple’s ColorSync technology directly supports several different color spaces to give you the convenience of working in whatever kind of color data most suits your needs. The ColorSync color spaces fall into several groups, or base families. They are:\n\nGray spaces, used for grayscale display and printing; see Gray Spaces\n\nRGB-based color spaces, used mainly for displays and scanners; see RGB-Based Color Spaces\n\nCMYK-based color spaces, used mainly for color printing; see CMY-Based Color Spaces\n\nDevice-independent color spaces, such as L*a*b, used mainly for color comparisons, color differences, and color conversion; see Device-Independent Color Spaces\n\nNamed color spaces, used mainly for printing and graphic design; see Named Color Spaces\n\nHeterogeneous HiFi color spaces, also referred to as multichannel color spaces, primarily used in new printing processes involving the use of red-orange, green and blue, and also for spot coloring, such as gold and silver metallics; see Color-Component Values, Color Values, and Color\n\nAll color spaces within a base family are related to each other by very simple mathematical formulas or differ only in details of storage format.\n\nGray Spaces\n\nGray spaces typically have a single component, ranging from black to white, as shown in Figure 2-1. Gray spaces are used for black-and-white and grayscale display and printing. A properly plotted gray space should have a fifty percent value as its midpoint.\n\nFigure 2-1  Gray space\nRGB-Based Color Spaces\n\nThe RGB space is a three-dimensional color space whose components are the red, green, and blue intensities that make up a given color. For example, scanners read the amounts of red, green, and blue light that are reflected from or transmitted through an image and then convert those amounts into digital values. Information displayed on a color monitor begins with digital values that are converted to analog or digital signals for display on the monitor. The signals are transmitted to the elements on the face of the monitor, causing them to glow at various intensities of red, green, and blue (the combination of which makes up the required hue, saturation, and brightness of the desired colors).\n\nRGB-based color spaces are the most commonly used color spaces in computer graphics, primarily because they are directly supported by most color displays and scanners. RGB color spaces are device dependent and additive. The groups of color spaces within the RGB base family include\n\nRGB spaces\n\nHSV and HLS spaces\n\nRGB Spaces\n\nAny color expressed in RGB space is some mixture of three primary colors: red, green, and blue. Most RGB-based color spaces can be visualized as a cube, as in Figure 2-2, with corners of black, the three primaries (red, green, and blue), the three secondaries (cyan, magenta, and yellow), and white.\n\nFigure 2-2  RGB color space (Red corner is hidden from view)\nsRGB Color Space\n\nThe sRGB color space is based on the ITU-R BT.709 standard. It specifies a gamma of 2.2 and a white point of 6500 degrees K. You can read more about sRGB space at the International Color Consortium site at http://www.color.org/. This space gives a complementary solution to the current strategies of color management systems, by offering an alternate, device-independent color definition that is easier to handle for device manufacturers and the consumer market. sRGB color space can be used if no other RGB profile is specified or available. ColorSync provides full support for sRGB, including an sRGB profile.\n\nNote that as an open architecture, ColorSync is not tied to the use of the sRGB color space and can support any RGB space that the user might prefer. For example, high end users with good quality reproduction devices may find that the sRGB space, which limits colors to the sRGB gamut, is too restrictive for their required color quality.\n\nHSV and HLS Color Spaces\n\nHSV space and HLS space are transformations of RGB space that can describe colors in terms more natural to an artist. The name HSV stands for hue, saturation, and value. (HSB space, or hue, saturation, and brightness, is synonymous with HSV space.) HLS stands for hue, lightness, and saturation. The two spaces can be thought of as being single and double cones, as shown in Figure 2-3.\n\nThe components in HLS space are analogous, but not completely identical, to the components in HSV space:\n\nThe hue component in both color spaces is an angular measurement, analogous to position around a color wheel. A hue value of 0 indicates the color red; the color green is at a value corresponding to 120, and the color blue is at a value corresponding to 240. Horizontal planes through the cones in Figure 2-3 are hexagons; the primaries and secondaries (red, yellow, green, cyan, blue, and magenta) occur at the vertices of the hexagons.\n\nThe saturation component in both color spaces describes color intensity. A saturation value of 0 (in the middle of a hexagon) means that the color is “colorless” (gray); a saturation value at the maximum (at the outer edge of a hexagon) means that the color is at maximum “colorfulness” for that hue angle and brightness.\n\nFigure 2-3  HSV (or HSB) color space and HLS color space\n\nThe value component in HSV describes the brightness. In both color spaces, a value of 0 represents the absence of light, or black. In HSV space, a maximum value means that the color is at its brightest. In HLS space, a maximum value for lightness means that the color is white, regardless of the current values of the hue and saturation components.\n\nCMY-Based Color Spaces\n\nCMY-based color spaces are most commonly used in color printing systems. They are device dependent and subtractive in nature. The groups of color spaces within the CMY family include\n\nCMY, which is not very common except on low-end color printers\n\nCMYK, which models the way inks or dyes are applied to paper in printing\n\nThe name CMYK refers to cyan, magenta, yellow, and key (represented by black). Cyan, magenta, and yellow are the three primary colors in this color space, and red, green, and blue are the three secondaries. Theoretically black is not needed. However, when full-saturation cyan, magenta, and yellow inks are mixed equally on paper, the result is usually a dark brown, rather than black. Therefore, black ink is overprinted in darker areas to expand the dynamic range and give a better appearance. Printing with black ink makes it possible to use less cyan, magenta, and yellow ink. This may prevent saturation, especially on materials such as plain paper which cannot accept too much ink. Using black can also reduce the cost per page because cyan, magenta, and yellow inks are generally more expensive than black ink. It can also provide a sharper image, because a single dot of black ink is used in place of three dots of other inks.\n\nFigure 2-4 shows how additive and subtractive colors mix to form other colors.\n\nFigure 2-4  Additive and subtractive colors\n\nTheoretically, the relation between RGB values and CMY values in CMYK space is quite simple:\n\nCyan        = 1.0 – red\n\n\nMagenta     = 1.0 – green\n\n\nYellow      = 1.0 – blue\n\n(where red, green, and blue intensities are expressed as fractional values varying from 0 to 1). In reality, the process of deriving the cyan, magenta, yellow, and black values from a color expressed in RGB space is complex, involving device-specific, ink-specific, and even paper-specific calculations of the amount of black to add in dark areas (black generation) and the amount of other ink to remove (undercolor removal) where black is to be printed. Therefore, when ColorSync converts between CMYK and RGB color spaces, it uses an elaborate system of multi-dimensional lookup tables, which ColorSync knows how to interpret. This information is stored in profiles, see Color Management Systems.\n\nDevice-Independent Color Spaces\n\nSome color spaces can express color in a device-independent way. Whereas RGB colors vary with display and scanner characteristics, and CMYK colors vary with printer, ink, and paper characteristics, device-independent colors are not dependent on any particular device and are meant to be true representations of colors as perceived by the human eye. These color representations, called device-independent color spaces, result from work carried out by the Commission Internationale d’Eclairage (CIE) and for that reason are also called CIE-based color spaces.\n\nThe most common method of identifying color within a color space is a three-dimensional geometry. The three color attributes, hue, saturation, and brightness, are measured, assigned numeric values, and plotted within the color space.\n\nConversion from an RGB color space to a CMYK color space involves a number of variables. The type of printer or printing press, the paper stock, and the inks used all influence the balance between cyan, magenta, yellow, and black. In addition, different devices have different gamuts, or ranges of colors that they can produce. Because the colors produced by RGB and CMYK specifications are specific to a device, they’re called device-dependent color spaces. Device color spaces enable the specification of color values that are directly related to their representation on a particular device.\n\nDevice-independent color spaces can be used as interchange color spaces to convert color data from the native color space of one device to the native color space of another device.\n\nThe CIE created a set of color spaces that specify color in terms of human perception. It then developed algorithms to derive three imaginary primary constituents of color—X, Y, and Z—that can be combined at different levels to produce all the color the human eye can perceive. The resulting color model, CIEXYZ, and other CIE color models form the basis for all color management systems. Although the RGB and CMYK values differ from device to device, human perception of color remains consistent across devices. Colors can be specified in the CIE-based color spaces in a way that is independent of the characteristics of any particular display or reproduction device. The goal of this standard is for a given CIE-based color specification to produce consistent results on different devices, up to the limitations of each device.\n\nXYZ Space\n\nThere are several CIE-based color spaces, but all are derived from the fundamental XYZ space. The XYZ space allows colors to be expressed as a mixture of the three tristimulus values X, Y, and Z. The term tristimulus comes from the fact that color perception results from the retina of the eye responding to three types of stimuli. After experimentation, the CIE set up a hypothetical set of primaries, XYZ, that correspond to the way the eye’s retina behaves.\n\nThe CIE defined the primaries so that all visible light maps into a positive mixture of X, Y, and Z, and so that Y correlates approximately to the apparent lightness of a color. Generally, the mixtures of X, Y, and Z components used to describe a color are expressed as percentages ranging from 0 percent up to, in some cases, just over 100 percent.\n\nOther device-independent color spaces based on XYZ space are used primarily to relate some particular aspect of color or some perceptual color difference to XYZ values.\n\nYxy Space\n\nYxy space expresses the XYZ values in terms of x and y chromaticity coordinates, somewhat analogous to the hue and saturation coordinates of HSV space. The coordinates are shown in the following formulas, used to convert XYZ into Yxy:\n\nY = Y\n\n\nx = X / (X+Y+Z)\n\n\ny = Y / (X+Y+Z)\n\nNote that the Z tristimulus value is incorporated into the new coordinates and does not appear by itself. Since Y still correlates to the lightness of a color, the other aspects of the color are found in the chromaticity coordinates x and y. This allows color variation in Yxy space to be plotted on a two-dimensional diagram. Figure 2-5 shows the layout of colors in the x and y plane of Yxy space.\n\nFigure 2-5  Yxy chromaticities in the CIE color space\nL*u*v* Space and L*a*b* Space\n\nOne problem with representing colors using the XYZ and Yxy color spaces is that they are perceptually nonlinear: it is not possible to accurately evaluate the perceptual closeness of colors based on their relative positions in XYZ or Yxy space. Colors that are close together in Yxy space may seem very different to observers, and colors that seem very similar to observers may be widely separated in Yxy space.\n\nL*u*v* space and L*a*b* space are nonlinear transformations of the XYZ tristimulus space. These spaces are designed to have a more uniform correspondence between geometric distances and perceptual distances between colors that are seen under the same reference illuminant. A rendering of L*a*b space is shown in Figure 2-6.\n\nFigure 2-6  L*a*b* color space\n\nBoth L*u*v* space and L*a*b* space represent colors relative to a reference white point, which is a specific definition of what is considered white light, represented in terms of XYZ space, and usually based on the whitest light that can be generated by a given device.\n\nNote: Because L*u*v* space and L*a*b* space represent colors relative to a specific definition of white light, they are not completely device independent; two numerically equal colors are truly identical only if they were measured relative to the same white point.\n\nMeasuring colors in relation to a white point allows for color measurement under a variety of illuminations.\n\nA primary benefit of using L*u*v* space and L*a*b* space is that the perceived difference between any two colors is proportional to the geometric distance in the color space between their color values, if the color differences are small. Use of L*u*v* space or L*a*b* space is common in applications where closeness of color must be quantified, such as in colorimetry, gemstone evaluation, or dye matching.\n\nIndexed Color Spaces\n\nIn situations where you use only a limited number of colors, it can be impractical or impossible to specify colors directly. If you have a bitmap with only a few bits per pixel (1, 2, 4, or 8, for example), each pixel is too small to contain a complete color specification; its color must be specified as an index into a list or table of color values. If you are using spot colors in printing or pen colors in plotting, it can be simpler and more precise to specify each color as an index into a list of colors instead of an actual color value. Also, if you want to restrict the user to drawing with a specific set of colors, you can put the colors in a list and specify them by index.\n\nIndexed space is the color space you use when drawing with indirectly specified colors. An indexed color value (a color specification in indexed color space) consists of an index value that refers to a color in a color list. Color values are defined in Color-Component Values, Color Values, and Color.\n\nNamed Color Spaces\n\nIn a named color space, each color has a name; colors are generally ordered so that each has an equal perceived distance from its neighbors in the color space. A named color space provides a relatively small number of discrete colors.\n\nColor systems using named color spaces have existed for many years. Graphic artists and designers using named color systems can “see” the real color by looking at a color chip or swatch. Printing shops can reproduce a specified color accurately.\n\nNamed color systems are useful for spot colors, but they have several drawbacks:\n\nThey are not useful for images, which require a continuous range of colors.\n\nThey are highly device dependent and proprietary.\n\nColors are tied to medium-specific formulations.\n\nApplications that use these systems require a device-specific database for each supported printer, making it difficult to add additional devices.\n\nColor-Component Values, Color Values, and Color\n\nEach of the color spaces described here requires one or more numeric values in a particular format to specify a color.\n\nEach dimension, or component, in a color space has a color-component value. An unsigned 16-bit color-component value can vary from 0 to 65,535 (0xFFFF), although the numerical interpretation of that range is different for different color spaces. In most cases, color-component intensities are interpreted numerically as varying between 0 and 1.0. An exception occurs for the a* and b* channels of the Lab color space, where values ranging from 0 to 65,535 are interpreted numerically as varying from -128.0 to approximately 128.0.\n\nDepending on the color space, one, two, three, or four color-component values combine to make a color value. For HiFi colors, up to eight color-component values combine to make a color. A color value is a structure; it is the complete specification of a color in a given color space.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2005 Apple Computer, Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2005-07-07\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Color Management Systems",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/csintro/csintro_conversion/csintro_conversion.html#//apple_ref/doc/uid/TP30001148-CH223-BBCBDGDD",
    "html": "Documentation Archive\nDeveloper\nSearch\nColor Management Overview\nTable of Contents\nIntroduction\nColor: A Brief Overview\nColor Spaces\nColor Management Systems\nRevision History\nNext\nPrevious\nColor Management Systems\n\nColor management is the process of ensuring consistent color across peripheral devices and across operating-system platforms. Members of the computer and publishing industries developed color management systems (CMSs) to convert colors from the color space of one device to the color space of another device. A color management system gives the user the ability to perform color matching, to see in advance which colors cannot be accurately reproduced on a specific device, to simulate the range of colors of one device on another, and to calibrate peripheral devices using a device profile and a calibration application.\n\nA color management system includes:\n\nCollections of color characteristics. These collections are given various names, such as color tags, precision transforms, or profiles.\n\nA color management module (CMM). This is a mathematical engine that performs the color matching among, and transformation between, collections of color characteristics.\n\nA system component for invoking color matching. ColorSync is Apple’s implementation of the ICC specification, providing system-level color management of images, documents, and devices.\n\nThe Color Matching Problem\n\nWhen an image is output to a particular device, the device displays only those colors that are within its gamut. Likewise, when an image is created by scanning, all colors from the original image are reduced to the colors within the scanner’s gamut. Devices with different gamuts cannot reproduce each other’s colors exactly, but careful shifting of the colors used on one device can improve the visual match when the image is displayed on another.\n\nFigure 3-1 shows examples of two devices’ color gamuts, projected onto Yxy space. Both devices produce less than the total possible range of colors, and the printer gamut is restricted to a significantly smaller range than the RGB gamut. The problem illustrated by Figure 3-1 is to display the same image on both devices with a minimum of visual mismatch. The solution to the problem is to match the colors of the image using profiles for both devices and one or more color management modules. A profile is a structure that provides a means of defining the color characteristics of a given device in a particular state.\n\nFigure 3-1  Color gamuts for two devices expressed in Yxy space\n\nColor conversion is the process of converting colors from one color space to another. Color matching, which entails color conversion, is the process of selecting colors from the destination gamut that most closely approximate the colors from the source image. Color matching always involves color conversion, whereas color conversion may not entail color matching.\n\nProfiles\n\nColor matching or color conversion across different color spaces requires the use of a profile for each device involved. Profiles provide the information necessary to understand how a particular device reproduces color. A profile may contain such information as lightest and darkest possible tones (referred to as white point and black point), the difference between specific “targets” and what is actually captured, and maximum densities for red, green, blue, cyan, magenta, and yellow. Together these measurements represent the data which describe a particular color gamut.\n\nProfiles are documents containing data that describe how to transform colors from device color space to an intermediate color space. This file format allows for the description of a wide variety of devices and is regularly updated with improvements. If needed, you can extend the data format to suit your software’s purpose.\n\nProfiles can contain different kinds of information. For example, a scanner profile and a printer profile have different sets of minimum required tags and element data. They can also contain optional and private tagged elements which can contain custom information used by particular color management modules. However, all profiles have at least a header followed by a required element tag table. The required tags may represent lookup tables, for example. The required tags for various profile classes are described in the International Color Consortium Profile Format Specification. To obtain a copy of this specification, or to get other information about the ICC, visit the ICC Web site at http://www.color.org/.\n\nSource and Destination Profiles\n\nThe profile that is associated with the image and describes the characteristics of the device on which the image was created is called the source profile. Displaying the image requires using another profile, which is associated with the output device, such as a display. The profile for that device is called the destination profile. If the image is destined for a display, color matching requires using the display’s profile (the destination profile) along with the image source profile to match the image colors to the display gamut. If the image is printed, color matching must use the printer profile to match the image colors to the printer, including generating black and removing excessive color densities (known as undercolor removal, or UCR) where appropriate.\n\nProfile Connection Space\n\nA profile connection space (PCS) is a device-independent color space used as an intermediate when converting from one device-dependent color space to another. Profile connection spaces are typically based on spaces derived from the CIE color space. ColorSync supports two of these spaces, XYZ and L*a*b.\n\nProfile Classes\n\nThere are a variety of classes, or categories of profiles, each of which plays a role in the color matching process. They include:\n\nDevice profiles\n\nColor space profiles\n\nAbstract profiles\n\nDevice link profiles\n\nNamed color space profiles\n\nA device profile characterizes a particular device: that is, it describes the characteristics of a color space for a physical device in a particular state. A display, for example, might have a single profile, or it might have several, based on differences in gamma value and white point. A printer might have a different profile for each paper type or ink type it uses because each paper type and ink type constitutes a different printer state. Broadly speaking, there are three kinds of device profiles—input, display, and output. Input device profiles characterize scanners and digital cameras. Display devices profiles characterize monitors and LCD panels. Output device profile characterize printers, printing presses, and film recorders.\n\nA color space profile contains the data necessary to convert color values between a PCS and a non-device color space (such as L*a*b to or from L*u*v, or XYZ to or from Yxy), for color matching. Color space profiles provide a convenient means for CMMs to convert between different non-device profiles.\n\nAbstract profiles allow applications to perform special color effects independent of the devices on which the effects are rendered. For example, an application may choose to implement an abstract profile that increases yellow hue on all devices. Abstract profiles allow users of the application to make subjective color changes to images or graphics objects.\n\nA device link profile represents a one-way link or connection between devices. It can be created from a set of multiple profiles, such as various device profiles associated with the creation and editing of an image. It does not represent any device model, nor can it be embedded into images.\n\nA named color space profile contains data for a list of named colors. The profile specifies a device color value and the corresponding CIE value for each color in the list.\n\nEmbedded Profiles\n\nProfiles can be embedded within images. For example, profiles can be embedded in JPEG, EPS, TIFF, and PICT files and in the private file formats used by applications. Embedded profiles allow for the automatic interpretation of color information as the color image is transferred from one device to another.\n\nEmbedding a profile in an image guarantees that the image can be rendered correctly on a different system. Although profiles can be several hundred KB or even larger, the typical RGB profile is around 500 bytes.\n\nColor Management Modules\n\nA color management module (CMM) uses profiles to convert and match a color in a given color space on a given device to or from another color space or device, perhaps a device-independent color space. When colors consistent with one device’s gamut are displayed on a device with a different gamut, a CMM attempts to minimize the perceived differences in the displayed colors between the two devices. The CMM does this by mapping the out-of-gamut colors into the range of colors that can be produced by the destination device. The CMM uses lookup tables and algorithms for color matching, previewing, color reproduction capabilities of one device on another, and checking for colors that cannot be reproduced.\n\nRendering Intents\n\nRendering intent refers to the approach taken when a CMM maps or translates the colors of an image to the color gamut of a destination device—that is, a rendering intent specifies a gamut-matching strategy. The ICC specification defines a profile tag for each of four rendering intents: perceptual matching, relative colorimetric matching, saturation matching, and absolute colorimetric matching.\n\nPerceptual matching is typically used for photographic content. All the colors of one gamut are scaled to fit within another gamut. Colors maintain their relative positions. Perceptual matching usually produces better results than colorimetric matching for realistic images such as scanned photographs. The eye can compensate for gamuts differences and when printed on a CMYK device, the image may look similar to the original on an RGB device. A side effect is that most of the colors of the original space may be altered to fit in the new space.\n\nRelative colorimetric matching is typically used for spot colors. Colors that fall within the overlapping gamuts of both devices are left unchanged. For example, to match an image from the RGB gamut onto the CMYK printer gamut, only the colors in the RGB gamut that fall outside the printer gamut are altered. Allows some colors in both images to be exactly the same, which is useful when colors must match quantitatively. A disadvantage is that many colors may map to a single color, resulting in tone compression. All colors outside the printer gamut, for example, would be converted to colors at the edge of its gamut, reducing the number of colors in the image and possibly altering its appearance. Colors outside the gamut are usually converted to colors with the same lightness, but different saturation, at the edge of the gamut. The final image may be lighter or darker overall than the original image, but the blank areas will coincide.\n\nSaturation matching is typically used for business graphics. The relative saturation of colors is maintained as well as can be achieved from gamut to gamut. Colors outside the gamut of the destination space are usually converted to colors with the same saturation of the source space, but with different lightness, at the edge of the gamut. Can be useful for some graphic images, such as bar graphs and pie charts, when the actual color displayed is less important than its vividness.\n\nAbsolute colorimetric matching is most often used in proofing. This type of matching preserves the native device white point of the source image instead of mapping to D50 relative. Absolute colorimetric matching is most often used in simulation or proofing operations where a device is trying to simulate the behavior of another device and media. For example, simulating newsprint on a monitor with absolute colorimetric intent would allow white space to be displayed onscreen as yellowish background because of the differences in white points between the two devices.\n\nColorSync\n\nColorSync is Apple’s platform-independent color management system that provides essential services for fast, consistent, and accurate color calibration, proofing, and reproduction. In OS X, ColorSync is fully integrated into the operating system. In most cases, color matching occurs behind-the-scenes, without any effort required on the part of applications. As soon as a device is connected to the computer, OS X registers at least one profile for the device. ColorSync uses the registered profiles to ensure color matching throughout the digital workflow.\n\nThe ColorSync Manager is the application programming interface (API) to color matching services in OS X. Only those developers who require color matching support beyond what OS X provides automatically need to use this API. Some typical reasons for using the ColorSync Manager API include:\n\nYou are a hardware vendor and want to register a profile for your scanner, camera, or printer.\n\nYour application supports creative professionals for whom color fidelity is essential. Your application may need to support soft proofing, preparation of materials for printing press, or other custom color tasks.\n\nYou can find detailed information on how ColorSync works in OS X and guidance on whether your application needs to use the ColorSync Manager API by reading Technical Note TN2035 ColorSync on OS X.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2005 Apple Computer, Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2005-07-07\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Document Revision History",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/csintro/csintro_revhx/csintro_revhx.html#//apple_ref/doc/uid/TP30001148-CH212-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nColor Management Overview\nTable of Contents\nIntroduction\nColor: A Brief Overview\nColor Spaces\nColor Management Systems\nRevision History\nPrevious\nDocument Revision History\n\nThis table describes the changes to Color Management Overview.\n\nDate\tNotes\n2005-07-07\t\n\nAdded link to technical note on ColorSync in OS X.\n\n\n2004-10-05\t\n\nChanged the title from \"Color and Color Management Systems\" to make it more consistent with the titles of similar documentation. No other changes.\n\n\n \t\n\nChanged the title from Color and Color Management Systems to make it more consistent with the titles of similar documentation. No other changes.\n\n\n2004-05-27\t\n\nFirst version.\n\n\n \t\n\nThe contents of this document were previously published as a chapter in Managing Color With ColorSync, a book that targets color management in Mac OS 9 and earlier.\n\n\n\nPrevious\n\n\n\n\n\nCopyright © 2004, 2005 Apple Computer, Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2005-07-07\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Preparing an Image Unit for Distribution",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageUnitTutorial/PackagingtheFilter/PackagingtheFilter.html#//apple_ref/doc/uid/TP40004531-CH5-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nImage Unit Tutorial\nTable of Contents\nIntroduction\nAn Image Unit and Its Parts\nWriting Kernels\nWriting the Objective-C Portion\nPreparing an Image Unit for Distribution\nRevision History\nNext\nPrevious\nPreparing an Image Unit for Distribution\n\nPreparation involves three tasks, described in the following sections:\n\nValidating an Image Unit\n\nTesting an Image Unit\n\nCompleting the Necessary Licensing and Trademark Agreements .\n\nValidating an Image Unit\n\nYou can validate your image unit with the ImageUnitAnalyzer tool by following these steps:\n\nAfter you install the latest developer tools, you can find the ImageUnitAnalyzer tool in/Developer/usr/bin.\n\nYou can also download the tool from Software Licensing & Trademark Agreements.\n\nOpen Terminal and drag the ImageUnitAnalyzer into the Terminal window. Then drag the image unit you want to validate into the Terminal window and press Return.\n\nTip: Dragging icons into the Terminal window is a shortcut to typing the tool and image unit names and their full paths, which can get tedious.\n\nIf your image unit is correctly packaged and the Objective-C code and kernel routine are well formed, ImageUnitAnalyzer provides output similar to what’s shown in Listing 4-1. If your imagine unit has problems, you’ll see failure statements that, in most cases, describe what elements failed. See Figure 4-1.\n\nListing 4-1  Output produced by ImageUnitAnalyzer for a passing unit\n\n##############################################################\n\n\n########### Image Unit Validation Tool Version 1.0 ###########\n\n\n###########  Copyright 2005, Apple Computer, Inc.  ###########\n\n\n##############################################################\n\n\n \n\n\nVALIDATING IMAGE UNIT: /Users/polly/Development/InvertColorImageUnit.plugin\n\n\n \n\n\nVALIDATING IMAGE UNIT BUNDLE\n\n\n- PASS: Image Unit has correct extension plugin\n\n\n- PASS: Description.plist exists /Users/polly/Development/InvertColorImageUnit.plugin/Contents/Resources/Description.plist\n\n\nPASS VALIDATING IMAGE UNIT BUNDLE\n\n\n \n\n\nVALIDATING IMAGE UNIT PLUGIN STRUCTURE\n\n\n- PASS: Description.plist is a valid XML file\n\n\n- PASS: Image Unit version is valid\n\n\n- PASS: The Image Unit contains valid filter list. Now analyzing filter list…\n\n\n- PASS: The entry InvertColorFilter does have a valid filter attributes dictionary\n\n\n- PASS: The entry InvertColorFilter does have a valid filter categories description\n\n\n- PASS: The entry InvertColorFilter does have a valid filter display name: InvertColorFilter\n\n\n- PASS: The entry InvertColorFilter does have a valid filter name: InvertColorFilter\n\n\n- PASS: The entry InvertColorFilter does have a valid filter dictionary\n\n\n- PASS: The Image Unit contains valid filters\n\n\nPASS VALIDATING IMAGE UNIT PLUGIN STRUCTURE\n\n\n \n\n\nVALIDATING IMAGE UNIT CIFILTERS\n\n\n- PASS: The filter InvertColorFilter was created successfully\n\n\nTesting filter invertColor\n\n\nPASS VALIDATING IMAGE UNIT CIFILTERS\n\n\n \n\n\nVerification of /Users/polly/Development/InvertColorImageUnit.plugin succeeded\n\n\n** PASS\n\nIf your image unit fails with a –1 error but all the validation statements are marked “PASS”, make sure that you passed only objects from the Objective-C portion of the image unit. See Kernel Routine Rules.\n\nFigure 4-1  Output produced by ImageUnitAnalyzer for a failing unit\nTesting an Image Unit\n\nAfter your image unit passes validation, you’ll want to make sure that it works properly by following these steps:\n\nCopy the image unit to /Library/Graphics/Image Units.\n\nYou can also install an image unit in a User/Library/Graphics/Image Units/, but you may need to create the Graphic/Image Units folder.\n\nLaunch the Quartz Composer development tool.\n\nOpen the Patch Creator and type the image unit name into the Search field.\n\nWhen you locate the image unit, drag it to the workspace.\n\nCreate a simple Quartz Composer composition that uses the image, as described in Writing Simple Kernel Routines.\n\nTry the filter on a variety of images and with a variety of input value and make sure that it works.\n\nCompleting the Necessary Licensing and Trademark Agreements\n\nBefore you can distribute your image unit or use the Image Units logo provided by Apple, you’ll need to complete the licensing and trademark agreements and any other tasks described on the Software Licensing & Trademark Agreements website. See:\n\nhttp://developer.apple.com/softwarelicensing/agreements/imageunits.html\n\nThe Image Units logo (see Figure 4-2) is available through a licensing agreement.\n\nFigure 4-2  The Image Units logo\nFurther Reading\n\nIn addition to Core Image Kernel Language Reference, you might find these resources useful as you design and write your own image units:\n\nAlgorithms for Image Processing and Computer Vision, J. R. Parker, 1997. John Wiley & Sons.\n\nOpenGL Shading Language, Second Edition, Randi J. Rost, 2006. Addison-Wesley Professional.\n\nOpenGL Shading Language, available for download from http://www.opengl.org/documentation/glsl/.\n\nDigital Image Processing, Second Edition, Rafael C. Gonzalez and Richard E. Woods. Addison-Wesley Publishers.\n\nDigital Image Processing, Kenneth R. Castleman, Prentice Hall.\n\nThe Renderman Companion: A Programmer’s Guide to Realistic Computer Graphics, Steve Upstill, Addison-Wesley Professional.\n\nGPU Gems: Programming Techniques, Tips, and Tricks for Real-Time Graphics, Randima Fernando, Addison-Wesley Professional. There are a number of other books in the GPU Gems series that you might also find useful.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2011 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2011-06-06\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Color: A Brief Overview",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/csintro/csintro_ovrvw/csintro_ovrvw.html#//apple_ref/doc/uid/TP30001148-CH205-BBCBDGDD",
    "html": "Documentation Archive\nDeveloper\nSearch\nColor Management Overview\nTable of Contents\nIntroduction\nColor: A Brief Overview\nColor Spaces\nColor Management Systems\nRevision History\nNext\nPrevious\nColor: A Brief Overview\n\nColor is a sensation and, therefore, a subjective experience. The sensation of color is one component of the visual sensation, caused by the sensitivity of the human eye to light. Light can be perceived either directly from light sources (such as the sun, a fire, incandescent or fluorescent bulbs, television screens, and computer displays) or indirectly, when light from these sources is transmitted through or reflected by objects. Color sensation is also affected by how the brain processes information and is specific to each individual. Thus color perception is a very complex phenomenon.\n\nThe foundation of the color reproduction process is trichromatic color vision, which describes the capacity of the human eye to respond equally to two or more sets of stimuli having different visible spectra. This means that two or more visible spectra may exist that will be perceived as the same color, a phenomenon known as metamerism. Because of this property, spectral color reproduction, a very expensive and impractical process, can be replaced by trichromatic color reproduction, a process that is much cheaper and easier to control.\n\nTrichromatic color reproduction induces the illusion of a color using various amounts of only three primary colors: either red, green, and blue mixed additively or cyan, magenta, and yellow mixed subtractively. Additive and subtractive colors are described in Additive and Subtractive Color. Trichromatic color reproduction is the fundamental mechanism used in the majority of color reproduction devices, from television, computer display and movie screens, to magazines, newspapers, large posters, and small pages printed on your desktop printer.\n\nComputers enable you to control color digitally and many peripherals have been developed for acquiring, displaying, and reproducing color. As a result, there is a need for a mechanism to maintain color control in an environment that can include different computer operating systems and hardware, as well as a wide variety of devices and media connected to the computer.\n\nColor Perception\n\nThe eye contains two types of receptors, cones and rods. The rods measure illumination and are not sensitive to color. The cones contain a chemical known as Rhodopsin, which is variously sensitive to reds and blues and has a default sensitivity to yellow. The color the eyes see in an object depends on how much red, green, and blue light is reflected to a small region in the back of the eye called the fovea, which contains a great majority of the cones present in the eye. Black is perceived when no light is reflected to the eye.\n\nEven the conditions in which color is viewed greatly affect the perception of color. The light source and environment must be standardized for accurate viewing. When viewing colors, people in the graphic arts industry, for example, avoid fluorescent and tungsten lighting, use a particular illuminant that is similar to daylight, and proof against a neutral gray surface.\n\nColor images frequently contain hundreds of distinctly different colors. To reproduce such images on a color peripheral device is impractical. However, a very broad range of colors can be visually matched by a mixture of three “primary” lights. This allows colors to be reproduced on a display by a mixture of red, green, and blue lights (the primary colors of the additive color space shown in Figure 2-4) or on a printer by a mixture of cyan, magenta, and yellow inks or pigments (the primary colors of the subtractive color space shown in Figure 2-4). Black is printed to increase contrast and make up for the deficiency of the inks (making black the key, or K, in CMYK).\n\nHue, Saturation, and Value (or Brightness)\n\nColor is described as having three dimensions. These dimensions are hue, saturation, and value. Hue is the name of the color, which places the color in its correct position in the spectrum. For example, if a color is described as blue, it is distinguished from yellow, red, green, or other colors. Saturation refers to the degree of intensity in a color, or a color’s strength. A neutral gray is considered to have zero saturation. A saturated red would have a color similar to apple red. Pink is an example of an unsaturated red. Value (or brightness) describes differences in the intensity of light reflected from or transmitted by a color image. The hue of an object may be blue, but the terms dark and light distinguish the value, or brightness, of one object from another. The 3-dimensional color spaces based on hue, saturation and value are described in HSV and HLS Color Spaces.\n\nAdditive and Subtractive Color\n\nThe additive color theory refers to the process of mixing red, green, and blue lights, which are each approximately one-third of the visible spectrum. Additive color theory explains how red, green, and blue light can be added to make white light. Red and green projected together produce yellow, red and blue produce magenta, and blue and green produce cyan. With red, blue, and green transmitted light, all the colors of the rainbow can be matched.\n\nThe subtractive color theory refers to the process of combining subtractive colorants such as inks or dyes. In this theory, various levels of cyan, magenta, and yellow absorb or “subtract” a portion of the spectrum of white light that is illuminating an object. The color of an object is the result of the color lights that are not absorbed by the object. An apple appears red because the surface of the apple absorbs the blue and green light.\n\nMonitors use the additive color space, output printing devices use the subtractive color space.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2005 Apple Computer, Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2005-07-07\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Writing the Objective-C Portion",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageUnitTutorial/WritingtheObjective-CPortion/WritingtheObjective-CPortion.html#//apple_ref/doc/uid/TP40004531-CH4-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nImage Unit Tutorial\nTable of Contents\nIntroduction\nAn Image Unit and Its Parts\nWriting Kernels\nWriting the Objective-C Portion\nPreparing an Image Unit for Distribution\nRevision History\nNext\nPrevious\nWriting the Objective-C Portion\n\nThe kernel, although it is the heart of an image processing filter, becomes an image unit only after it is properly packaged as a plug-in. Your most important tasks in the packaging process are to copy your kernel routine (or routines) into the kernel skeletal file provided by Xcode and to modify the code in the in the implementation and interface files for the filter. There are a few other housekeeping tasks you’ll need to perform to correctly categorize the filter and to identify your image unit as your product. You’ll also need to describe its input parameters.\n\nThis chapter provides an overview to the image unit template in Xcode and describes the contents of the filter files. Then, it shows how to create these image units:\n\nA color inversion image unit that uses the kernel routine discussed in Color Inversion. It uses one sampler object and does not require a region-of-interest method.\n\nA pixellate image unit that uses the kernel routine discussed in Pixellate. It uses one sampler object and requires one region-of-interest method.\n\nA detective lens image unit, which uses the two kernel routines discussed in Writing Kernel Routines for a Detective Lens . It uses requires several sampler objects and two a region-of-interest methods.\n\nBefore you follow the instructions in this chapter for writing the Objective-C portion of an image unit, you should understand how the kernel routines discussed in Writing Kernels work.\n\nThe Image Unit Template in Xcode\n\nXcode provides a template that facilitates the packaging process. To help you package an image unit as a plug-in, Xcode provides the skeletal files and methods that are needed and names the files appropriately. Figure 3-1 shows the files automatically created by Xcode for an image unit project named DetectiveLens.\n\nFigure 3-1  The files in an image unit project\n\nXcode automatically names files using the project name that you supply. These are the default files provided by Xcode:\n\n<ProjectName>Filter.m is the implementation file for the filter. You will need to modify this file.\n\n<ProjectName>Filter.h is the interface file for the filter. You will need to modify this file.\n\n<ProjectName>FilterKernel.cikernel is the file that contains the kernel routine (or routines) and any subroutines needed by the kernel. The default code is a funHouse kernel routine and a smoothstep subroutine that’s used by funHouse. (This subroutine is not the same as the one provided by OpenGl Shading Language.) All you need to do is completely replace this code with the kernel routine (or routines) that you’ve written (as well as tested and debugged) and any required subroutines.\n\n<ProjectName>PlugInLoader.m is the file that implements the plug-in protocol needed to load an image unit. You do not need to make any modifications to this file unless your product requires custom tasks on loading. This chapter does not provide any information on customizing the loading process. It assumes that you’ll use the file as is.\n\n<ProjectName>PlugInLoader.h is the interface file for the plug-in loading protocol.\n\n<ProjectName>.plugin is the plug-in that you will distribute. When you create the project, this file name appears in red text to indicate that the file does not yet exist. After you build the project, the file name changes to black text to indicate that the plug-in exists.\n\nDescription.plist defines several properties of the filter: filter name, filter categories, localized display name, filter class, and information about the input parameters to the filter. Executable image units (which are the only image units you’ll see in this tutorial) may have input parameters of any class, but Core Image does not generate an automatic user interface for custom classes (see CIFilter Image Kit Additions). Input parameters for non-executable image units must be one of these classes: CIColor, CIVector, CIImage, or NSNumber. (For more information on executable and nonexecutable filters, see Core Image Programming Guide.)\n\nThe default template file is shown in Figure 3-2. Xcode fills in the filter name for you based on the project name that you provide. You need to make changes to the filter categories and localized display name. The filter categories should include all the categories defined by the Core Image API that apply to your filter. For a list of the possible categories, see CIFilter Class Reference.\n\nInfo.plist contains properties of the plug-in, such as development region, bundle identifier, principal class, and product name. You’ll want to modify the bundle identifier and make sure that you define the product name variable in Xcode.\n\nThe default code is for a filter that mimics the effect of a fun house mirror. You should recognize the kernel routine as the same one discussed in Listing 2-6. The project will build as a valid image unit without the need for you to change a single line of code.\n\nFigure 3-2  The default property list file\n\nThe filter interface and implementation files provided by Xcode are the ones that you need to customize for your kernel routine (or routines). The interface file declares a subclass of CIFilter. Xcode automatically names the subclass <ProjectName>Filter. For example, if you supply InvertColor as the project name, the interface file uses InvertColorFilter as the subclass name. The default interface file declares four instance variables for the filter: inputImage, inputVector, inputWidth, and inputAmount, which, from the perspective of a filter client, are the input parameters to the filter. You may need to delete one or more of these instance variables and add any that are needed by your image unit.\n\nThe implementation file contains four methods that you’ll need to modify for your purposes:\n\ninit gets the kernel file from the bundle, and loads the kernel routines. Unless you require customization at initialization time, you may not need to modify this method.\n\nregionOf:destRect:userInfo is callback function that defines the region of interest (ROI). If you are writing a filter that does not require an ROI, you can delete this method. If you are unsure of what an ROI is, see Core Image Programming Guide and Region-of-Interest Methods. In general, filters that map one source pixel to one destination pixel do not require an ROI. Just about all other types of filters will require that you provide an ROI method, except certain generator filters.\n\ncustomAttributes is a method that defines the attributes of each input parameter. This is required so that the filter host can query your filter for the input parameters, their data types, and their default, minimum, and maximum values. You can also provide such useful information as slider minimum and maximum values. When a filter host calls the attributes method, Core Image actually invokes your customAttributes method. If your filter does not require any input parameters other than an input image, you can delete this method.\n\noutputImage creates one or more CISampler objects, performs any necessary calculations, calls the apply: method of the CIFilter class and returns a CIImage object. The exact nature of this method depends on the complexity of the filter, as you’ll see by reading the rest of this chapter.\n\nNote: A multipass filter is one that either applies two or more kernel routines or applies the same kernel routine more than once before returning a CIImage object from the outputImage method.\n\nThe next sections show how to modify the filter files for three types of image units:\n\nCreating a Color Inversion Image Unit uses one kernel routine but does not need an ROI method.\n\nCreating a Pixellate Image Unit uses one kernel routine and an ROI method.\n\nCreating a Detective Lens Image Unit is a multipass filter that uses two kernel routines and an ROI method for each kernel routine.\n\nCreating a Color Inversion Image Unit\n\nA color inversion filter represents one of the simplest image units that you can write. It uses the kernel routine discussed in Color Inversion. If you take a look at the kernel routine shown in Listing 2-1 you’ll see that the routine does not use any input parameters other than an input image. So there is no need to supply a customAttributes method. The kernel routine maps one source pixel to a destination pixel that’s at the same coordinates as the source pixel. As a result, there is no need to supply a method that calculates an ROI. You will need to make modifications to the filter interface file and to the outputImage method in the filter implementation file. The default init method will work as is, provided that you do not rename any of the files that Xcode automatically names for you.\n\nTo package the color inversion kernel routine as image unit, follow these steps:\n\nLaunch Xcode and choose File > New Project.\n\nIn the Assistant window, scroll to to Standard Apple Plug-ins, select Image Unit Plug-in for Objective C and click Next.\n\nEnter InvertColor as the project name and click Finish.\n\nOpen the InvertColorFilterKernel.cikernel file and replace the code with the code from Listing 2-1.\n\nOpen the InvertColor.h file and delete all the instance variables except for inputImage, as this filter doesn’t have any input parameters other than an input image. Then save and close the file.\n\nOpen the InvertColor.m file and delete the regionOf:destRect:userInfo: and customAttributes methods.\n\nModify the outputImage method so that it looks as follows:\n\n- (CIImage *)outputImage\n\n\n{\n\n\n    CISampler *src;\n\n\n   // Create a sampler from the input image because a kernel routine takes\n\n\n   // a sampler, not an image, as input\n\n\n    src = [CISampler samplerWithImage:inputImage];\n\n\n    // Use the apply: method on the CIFilter object\n\n\n    return [self apply:_InvertColorFilterKernel, src, nil];\n\n\n}\n\nSave and close the InvertColor.m file.\n\nOpen the Description.plist file.\n\nChange CICategoryDistortionEffect to CICategoryColorEffect.\n\nSave and close the Description.plist file.\n\nOpen the Description.strings file.\n\nEnter the localized display name for the filter by adding the following:\n\n\"InvertColorFilter\" = \"Invert Color Filter\";\n\nYou’ll want to provide localized display names for all input parameters and for all languages that represent your image unit users.\n\nSave and close the Description.strings file.\n\nBuild the project.\n\nIt should succeed unless you’ve introduced some typographical errors.\n\nQuit Xcode.\n\nFind the InvertColor.plugin file in your project. If you want, move it to a convenient location for validation and testing purposes.\n\nMake sure the image unit is valid. (See Validating an Image Unit.)\n\nInstall the validated image unit in /Library/Graphics/Image Units/.\n\nTest the image unit. (See Testing an Image Unit.)\n\nThat’s all there is to building an image unit for a simple filter! Read the next sections to create image units for more complex filters.\n\nCreating a Pixellate Image Unit\n\nWriting an image unit to package the pixellate kernel routine (see Pixellate) is a bit more challenging than packaging the color inversion kernel routine because you need to supply a region-of-interest method. Recall that the pixellate kernel routine uses many pixels from the source image to compute the value of each destination pixel. The number of pixels is defined by the dot size. For example, if the dot size is 4, then the kernel routine uses 16 pixels from the source image: a 4-pixel-by-4-pixel grid. The calculation works well unless the destination pixel is at the edge, in which case there won’t be enough pixels to fetch. That’s where the region-of-interest method comes to play. If the grid of pixels is always inset by the radius of the dot size, you’ll avoid the problem of not having enough pixels to fetch.\n\nThe code in Listing 3-1 is a region-of-interest method that calls the Quartz function CGRectInset. This function returns a rectangle that is smaller or larger than the rectangle passed to it. In this case, the ROI method insets the rectangle passed to it by the dot radius.\n\nListing 3-1  A region-of-interest method for the pixellate image unit\n\n \n\n\n- (CGRect)regionOf: (int)sampler  destRect: (CGRect)rect  userInfo: (NSNumber *)radius\n\n\n{\n\n\n    return CGRectInset(rect, -[radius floatValue], -[radius floatValue]);\n\n\n}\n\nNow that you’ve seen the required region-of-interest method, you are ready to use Xcode to create the pixellate image unit.\n\nLaunch Xcode and choose File > New Project.\n\nIn the Assistant window, scroll to to Standard Apple Plug-ins, select Image Unit Plug-in for Objective C and click Next.\n\nEnter Pixellate as the project name and click Finish.\n\nOpen the PixellateFilterKernel.cikernel file and replace the code with the code from Listing 2-4.\n\nOpen the Pixellate.h file and modify the interface so that the filter has two instance variables: inputImage and inputScale. Then save and close the file.\n\nOpen the Pixellate.m file and modify the regionOf:destRect:userInfo: so that it is the same as the method in Listing 3-1.\n\nModify the customAttributes methods so that is has only one attribute: inputScale, and set appropriate values for the minimum, slider minimum, slider maximum, default, identity and type attributes. The method should look similar to the following:\n\n- (NSDictionary *)customAttributes\n\n\n{\n\n\n    return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n \n\n\n        [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeMin,\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeSliderMin,\n\n\n            [NSNumber numberWithDouble:  200.00], kCIAttributeSliderMax,\n\n\n            [NSNumber numberWithDouble:  4.00], kCIAttributeDefault,\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeIdentity,\n\n\n            kCIAttributeTypeScalar,           kCIAttributeType,\n\n\n            nil],                               @\"inputScale\",\n\n\n \n\n\n        nil];\n\n\n}\n\nModify the outputImage method so that it looks as follows:\n\n- (CIImage *)outputImage\n\n\n{\n\n\n    float radius;\n\n\n    CISampler *src;\n\n\n    // Set up the sampler to use the input image\n\n\n    src = [CISampler samplerWithImage:inputImage];\n\n\n    radius = [inputScale floatValue] * 0.5;\n\n\n    // Set the region-of-interest method for the kernel routine\n\n\n    [_PixellateFilterKernel setROISelector:@selector(regionOf:destRect:userInfo:)];\n\n\n    // Apply the filter to the kernel, passing the sampler and scale\n\n\n    return [self apply:_PixellateFilterKernel, src,\n\n\n        inputScale,\n\n\n    // This option specifies the domain of definition of the destination image\n\n\n        kCIApplyOptionDefinition, [[src definition] insetByX:-radius Y:-radius],\n\n\n    // This option sets up the ROI method to gets radius value\n\n\n        kCIApplyOptionUserInfo, [NSNumber numberWithFloat:radius], nil];\n\n\n}\n\nNote that you need to set up a domain of definition so that the destination image is inset by the radius of the dot. Otherwise, you’ll get a non-transparent edge around the destination image. (See Region-of-Interest Methods and Core Image Programming Guide.)\n\nSave and close the Pixellate.m file.\n\nOpen the Description.plist file.\n\nChange CICategoryDistortionEffect to CICategoryStylizeEffect.\n\nSave and close the Description.plist file.\n\nBuild the project.\n\nIt should succeed unless you’ve introduced some typographical errors.\n\nQuit Xcode.\n\nFind the Pixellate.plugin file in your project. If you want, move it to a convenient location for validation and testing purposes.\n\nMake sure the image unit is valid. (See Validating an Image Unit.)\n\nInstall the validated image unit in /Library/Graphics/Image Units/.\n\nYou can optionally install the image unit in \"User\"/Library/Graphics/Image Units/ although you may need to create the /Graphics/Image Units/ folder because it is not created by default. .\n\nTest the image unit. (See Testing an Image Unit.)\n\nCreating a Detective Lens Image Unit\n\nThe detective lens image unit is the most challenging image unit in this chapter because it brings together two kernel routines and uses them along with a Core Image built-in filter. This image unit requires four CISampler objects and two region-of-interest methods (one per kernel routine).\n\nThis section assumes that you’ve read Writing Kernel Routines for a Detective Lens . Make sure that you understand the detective lens definition (see Detective Lens Anatomy) and the kernel routines needed for this image unit (see Listing 2-7 and Listing 2-8).\n\nNote: The detective lens image unit discussed here is the same as the lens image unit provided in the CIAnnotation sample application. (See /Developer/Examples.) The CIAnnotation application passes a downsampled image to the lens image unit. Then, it passes a magnification factor to the lens image unit that allows the downsampled image to be magnified to its full resolution when underneath the lens.\n\nThe detective lens filter is a multipass filter. You’ll need to first apply the lens kernel routine to the input image. Then you need to apply the lens holder kernel routine. Finally, you’ll need to composite the output images from each kernel routine. For this you’ll use the Core Image filter CISourceOverCompositing.\n\nNote: A multipass filter is one that either applies more than one kernel routine or that repeatedly applies the same kernel routine.\n\nTo create the detective lens image unit, follow these steps:\n\nLaunch Xcode and choose File > New Project.\n\nIn the Assistant window, scroll to to Standard Apple Plug-ins, select Image Unit Plug-in for Objective C and click Next.\n\nEnter DetectiveLens as the project name and click Finish.\n\nOpen the DetectiveLensFilterKernel.cikernel file and replace the code with the code from Listing 2-7 and Listing 2-8.\n\nOpen the DetectiveLens.h file and modify the input parameters so they match what’s shown in Listing 3-2. Then save and close the file.\n\n\n\n\n\nListing 3-2  The input parameters to the detective lens filter\n\n@interface DetectiveLensFilter : CIFilter\n\n\n{\n\n\n    CIImage      *inputImage;\n\n\n    CIVector     *inputCenter; // center of the lens\n\n\n    NSNumber     *inputLensDiameter; // diameter of the lens\n\n\n    NSNumber     *inputRingWidth; // width of the lens holder\n\n\n    NSNumber     *inputMagnification; // lens magnification\n\n\n    NSNumber     *inputRingFilletRadius; // lens holder fillet radius\n\n\n    NSNumber     *inputRoundness;  // roundness of the lens, range is 0...1\n\n\n    NSNumber     *inputShineOpacity; // opacity of the lens, range is 0...1\n\n\n}\n\nOpen the DetectiveLens.m file. There are many modifications that you’ll need to make to this file.\n\nAdd the following static declarations just after the @implementation statement:\n\nstatic CIKernel *_lensKernel = nil; // for the lens kernel routine\n\n\nstatic CIKernel *_ringKernel = nil; // for the lens holder kernel routine\n\n\nstatic CIImage *_ringMaterialImage = nil; // for the material map\n\n\nstatic CIImage *_lensShineImage = nil; // for the highlight image\n\nYou need one CIKernel object for each kernel routine that the image unit uses.\n\nYou need one CIImage object for each image. Recall that the lens kernel routine uses a highlight image and the lens holder kernel routine uses a material map. The input image is part of the interface declaration for the filter because it’s provided by the filter client. In contrast, the highlight and material images need to be included as part of the image unit.\n\nModify the init method so that it fetches both kernel routines, using the static CIKernel objects that you just declared. Replace this statement:\n\n_DectiveLensFilterKernel = [[kernels objectAtIndex:0] retain];\n\nwith these two statements:\n\n// Fetch the lens kernel routine\n\n\n_lensKernel = [[kernels objectAtIndex:0] retain];\n\n\n// Fetch the lens holder kernel routine\n\n\n_ringKernel = [[kernels objectAtIndex:1] retain];\n\nModify the init method so that it opens the files that contain the highlight image needed by the lens kernel routine and the material map needed for the lens holder kernel routine. Add the following lines of code to the init method.\n\nYou need to modify the file names and extensions if they don’t match what’s shown (myMaterial.tiff and myHighlight.tiff).\n\nNSString    *path = nil;\n\n\nNSURL    *url = nil;\n\n\npath = [bundle pathForResource:@\"myMaterial\" ofType:@\"tiff\"];\n\n\nurl = [NSURL fileURLWithPath:path];\n\n\n_ringMaterialImage = [[CIImage imageWithContentsOfURL:url] retain];\n\n\n \n\n\npath = [bundle pathForResource:@\"myHighlight\" ofType:@\"tiff\"];\n\n\nurl = [NSURL fileURLWithPath:path];\n\n\n _lensShineImage = [[CIImage imageWithContentsOfURL:url] retain];\n\nFor each file, the code gets the string that defines the path to the file. Then it creates an NSURL object from that path name. Finally, the code supplies the NSURL object to the imageWithContentsOfURL: method of the CIImage class, and retains the image so that it can be used later.\n\nModify the customAttributes method so that the NSDictionary object that it returns contains the relevant information for each of the input parameters. Then, when a filter host calls the attributes method for the filter, Core Image invokes your customAttributes method and returns the default, minimum, maximum, and so on, values for each of the input parameters. After modifying the customAttributes method, it should appear as follows:\n\n- (NSDictionary *)customAttributes\n\n\n{\n\n\n    return [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n \n\n\n        [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            [CIVector vectorWithX:200.0 Y:200.0],  kCIAttributeDefault,\n\n\n            kCIAttributeTypePosition,              kCIAttributeType,\n\n\n            nil],                                  @\"inputCenter\",\n\n\n \n\n\n        [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeMin,\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeSliderMin,\n\n\n            [NSNumber numberWithDouble:500.00], kCIAttributeSliderMax,\n\n\n            [NSNumber numberWithDouble:250.00], kCIAttributeDefault,\n\n\n            [NSNumber numberWithDouble:250.00], kCIAttributeIdentity,\n\n\n            kCIAttributeTypeDistance,           kCIAttributeType,\n\n\n            nil],                               @\"inputLensDiameter\",\n\n\n \n\n\n        [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeMin,\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeSliderMin,\n\n\n            [NSNumber numberWithDouble:500.00], kCIAttributeSliderMax,\n\n\n            [NSNumber numberWithDouble: 22.00], kCIAttributeDefault,\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeIdentity,\n\n\n            kCIAttributeTypeDistance,           kCIAttributeType,\n\n\n            nil],                               @\"inputRingWidth\",\n\n\n \n\n\n        [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeMin,\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeSliderMin,\n\n\n            [NSNumber numberWithDouble: 30.00], kCIAttributeSliderMax,\n\n\n            [NSNumber numberWithDouble:  9.20], kCIAttributeDefault,\n\n\n            [NSNumber numberWithDouble:  7.00], kCIAttributeIdentity,\n\n\n            kCIAttributeTypeDistance,           kCIAttributeType,\n\n\n            nil],                               @\"inputRingFilletRadius\",\n\n\n \n\n\n        [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeMin,\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeSliderMin,\n\n\n            [NSNumber numberWithDouble: 10.00], kCIAttributeSliderMax,\n\n\n            [NSNumber numberWithDouble:  3.00], kCIAttributeDefault,\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeIdentity,\n\n\n            kCIAttributeTypeScalar,             kCIAttributeType,\n\n\n            nil],                               @\"inputMagnification\",\n\n\n \n\n\n        [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            [NSNumber numberWithDouble:  0.00], kCIAttributeMin,\n\n\n            [NSNumber numberWithDouble:  0.00], kCIAttributeSliderMin,\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeSliderMax,\n\n\n            [NSNumber numberWithDouble:  0.86], kCIAttributeDefault,\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeIdentity,\n\n\n            kCIAttributeTypeScalar,             kCIAttributeType,\n\n\n            nil],                               @\"inputRoundness\",\n\n\n \n\n\n        [NSDictionary dictionaryWithObjectsAndKeys:\n\n\n            [NSNumber numberWithDouble:  0.00], kCIAttributeMin,\n\n\n            [NSNumber numberWithDouble:  0.00], kCIAttributeSliderMin,\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeSliderMax,\n\n\n            [NSNumber numberWithDouble:  0.50], kCIAttributeDefault,\n\n\n            [NSNumber numberWithDouble:  1.00], kCIAttributeIdentity,\n\n\n            kCIAttributeTypeScalar,             kCIAttributeType,\n\n\n            nil],                               @\"inputShineOpacity\",\n\n\n \n\n\n        nil];\n\n\n}\n\nWrite a region-of-interest method for the lens holder kernel routine.\n\nRecall that the region-of-interest method returns the rectangle that specifies the region of the sampler to use for fetching image data for the kernel routine. The region of interest for the lens holder kernel routine is simply the rectangle that specifies the size of the material map.\n\nThe region-of-interest method must have signature compatible with the following:\n\n- (CGRect) regionOf:(int)samplerIndex destRect:(CGRect)r userInfo:obj;\n\nThis method is a callback that’s invoked by Core Image whenever your kernel routine needs a sample for processing. The lens holder kernel routine uses only one sampler, whose sampler index is 0. (Sampler indexes for an ROI method start at 0 and are sequential.) If the sampler index is 0, then the ROI method should return the size of the material map. Otherwise, it needs to return the destination rectangle that Core Image passed to the routine.\n\nThe userInfo parameter for any region-of-interest method is what you use to pass any necessary data to the method. This particular region-of-interest method needs to have the sampler for the material map passed to it so that the ROI method can determine the size of the map. The extent method of the CISampler class does just that.\n\n- (CGRect)ringROI:(int)sampler forRect:(CGRect)R userInfo:(CISampler *)material\n\n\n{\n\n\n    if (sampler == 0)\n\n\n        return [material extent];\n\n\n    return R;\n\n\n}\n\nWrite a region-of-interest method for the lens kernel routine. This method is a bit more complex than the one for the lens holder kernel routine.\n\nThis method needs to return the region of interest for three sampler objects:\n\nSampler 0 fetches samples from the downsampled image (that is, what appears as unmagnified—the pixels outside the lens). The region of interest is simply the rectangle passed to the ROI method.\n\nSampler 1 fetches samples from the high resolution image. The region of interest depends on the magnification and width of the lens. The number of pixels needed from the source image is defined by the width of the lens divided by the magnification. The origin of the rectangle that defines this area is the center of the lens minus the number of pixels needed.\n\nSampler 2 fetches samples from the highlight image. The region of interest is a rectangle (CGRect data type) that describes the size of the highlight image. You can obtain the size using the extent method of the CISampler object.\n\nThe userInfo needed for this particular region-of-interest method is an array that contains three of the filter input parameters (center of lens, width of lens, magnification factor) and the CISampler object for the highlight image.\n\nThe lens ROI method should look similar to the following:\n\n- (CGRect)lensROI:(int)sampler forRect:(CGRect)R userInfo:(NSArray *)array\n\n\n{\n\n\n    CIVector *oCenter;\n\n\n    NSNumber *oWidth, *oMagnification;\n\n\n    CISampler *shine;\n\n\n \n\n\n    // Fetch the necessary input parameters from the userInfo parameter\n\n\n    oCenter = [array objectAtIndex:0];\n\n\n    oWidth = [array objectAtIndex:1];\n\n\n    oMagnification = [array objectAtIndex:2];\n\n\n    shine = [array objectAtIndex:3];  // shine is a CISampler object\n\n\n    if (sampler == 2)\n\n\n        return [shine extent];\n\n\n \n\n\n    // Determine the area of the original image used with the lens where it is\n\n\n    // currently we only need R, because the lens is a magnifier\n\n\n    if (sampler == 1)\n\n\n    {\n\n\n        float cx, cy, width, mag;\n\n\n \n\n\n        cx = [oCenter X];\n\n\n        cy = [oCenter Y];\n\n\n        width = [oWidth floatValue];\n\n\n        mag = [oMagnification floatValue];\n\n\n        width /= mag; // calculates the actual pixels needed from the source\n\n\n        R = CGRectMake(cx - width, cy - width, width*2.0, width*2.0);\n\n\n    }\n\n\n    return R; // If the sampler is 0, ROI calculation is not needed.\n\n\n}\n\nWrite the outputImage method.\n\nFor each kernel routine, this method:\n\nCreates CISampler objects, performing any necessary set up work for them.\n\nCalculates any values needed by the ROI method or by the kernel routine. This includes calculating the rectangle that defines the shape of the destination image (otherwise known as the domain of definition).\n\nSets up the userInfo data needed by the ROI method so that is can be passed as an option (kCIApplyOptionUserInfo) to the apply: method of the CIFilter object.\n\nSets the ROI method to use for the kernel routine.\n\nCalls the apply: method of the CIFilter object.\n\nThen the method composites the resulting images into a final image by using the Core Image CISourceOverCompositing filter.\n\nThe method is rather long and performs many tasks, so you’ll want to read the detailed explanation for each lettered line (a, b, c, and so on) that appears following the code.\n\n- (CIImage *)outputImage\n\n\n{\n\n\n    float radius, cx, cy, ringwidth, mag;\n\n\n    CGRect R, extent;\n\n\n    CISampler *src, *shine, *material;\n\n\n    CIImage *lensedImage, *ringImage;\n\n\n    CIFilter *compositedImage;\n\n\n    NSArray *array;\n\n\n    CISampler *magsrc;\n\n\n    CGAffineTransform CT;\n\n\n    CIVector *shineSize, *materialSize;\n\n\n \n\n\n    // ********* Lens *********\n\n\n    src = [CISampler samplerWithImage:inputImage]; \n// 1\n\n\n    shine = [CISampler samplerWithImage:_lensShineImage]; \n// 2\n\n\n    // Set up work needed for the magnified image sampler\n\n\n    cx = [inputCenter X];\n\n\n    cy = [inputCenter Y];\n\n\n    mag = [inputMagnification floatValue];\n\n\n    CT = CGAffineTransformTranslate(CGAffineTransformScale(\n\n\n        CGAffineTransformMakeTranslation(cx, cy), mag, mag), -cx, -cy);\n\n\n    magsrc = [CISampler samplerWithImage:[inputImage imageByApplyingTransform:CT]]; \n// 3\n\n\n \n\n\n    radius = [inputLensDiameter floatValue] * 0.5; \n// 4\n\n\n    R.origin.x = cx - radius; \n// 5\n\n\n    R.origin.y = cy - radius;\n\n\n    R.size.width = 2.0 * radius;\n\n\n    R.size.height = 2.0 * radius;\n\n\n \n\n\n    extent = [shine extent]; \n// 6\n\n\n    shineSize = [CIVector vectorWithX:extent.size.width Y:extent.size.height]; \n// 7\n\n\n \n\n\n    array = [NSArray arrayWithObjects:inputCenter, inputLensDiameter,\n\n\n                inputMagnification, shine, nil]; \n// 8\n\n\n    [_lensKernel setROISelector:@selector(lensROI:forRect:userInfo:)]; \n// 9\n\n\n \n\n\n    lensedImage = [self apply:_lensKernel, src, magsrc, shine, inputCenter,\n\n\n            [NSNumber numberWithFloat:radius + 2.0],\n\n\n              inputMagnification, inputRoundness,\n\n\n              inputShineOpacity, shineSize,\n\n\n             kCIApplyOptionDefinition, [[src definition] unionWithRect:R],\n\n\n             kCIApplyOptionUserInfo, array, nil];  \n// 10\n\n\n \n\n\n    // ********* Lens Holder   *********\n\n\n    material = [CISampler samplerWithImage:_ringMaterialImage]; \n// 11\n\n\n    ringwidth = [inputRingWidth floatValue]; \n// 12\n\n\n \n\n\n    R.origin.x = cx - radius - ringwidth; \n// 13\n\n\n    R.origin.y = cy - radius - ringwidth;\n\n\n    R.size.width = 2.0 * (radius + ringwidth);\n\n\n    R.size.height = 2.0 * (radius + ringwidth);\n\n\n    extent = [material extent]; \n// 14\n\n\n \n\n\n    materialSize = [CIVector vectorWithX:extent.size.width Y:extent.size.height]; \n// 15\n\n\n \n\n\n    [_ringKernel setROISelector:@selector(ringROI:forRect:userInfo:)]; \n// 16\n\n\n \n\n\n    ringImage = [self apply:_ringKernel, material, inputCenter,\n\n\n                [NSNumber numberWithFloat:radius],\n\n\n                [NSNumber numberWithFloat:radius+ringwidth],\n\n\n                inputRingFilletRadius,\n\n\n                  materialSize,\n\n\n                  kCIApplyOptionDefinition,\n\n\n                  [CIFilterShape shapeWithRect:R],\n\n\n                  kCIApplyOptionUserInfo, material, nil]; \n// 17\n\n\n \n\n\n    // ********* Lens and Lens Holder Composited *********\n\n\n    compositedImage = [CIFilter filterWithName:@\"CISourceOverCompositing\"\n\n\n            keysAndValues:@\"inputImage\", ringImage,\n\n\n            @\"inputBackgroundImage\", lensedImage, nil]; \n// 18\n\n\n \n\n\n    return [compositedImage valueForKey:@\"outputImage\"]; \n// 19\n\n\n \n\n\n}\n\nHere is what the code does:\n\n1. Creates a CISampler object for the source image.\n\n2. Creates a CISampler object for the highlight image.\n\n3. Creates a CISampler object for the magnified source, using the transform calculated in the previous lines of code.\n\n4. Extracts the diameter of the lens as a float value, then calculates the radius.\n\n5. Computes, along with the next three lines of code, the rectangle that will be used later to compute the size of the destination image—the domain of definition.\n\n6. Retrieves the size of the highlight image.\n\n7. Creates a CIVector object that contains the size of the highlight image.\n\n8. Sets up the array that’s passed as the userInput parameter to the region-of-interest method. Recall that the ROI method takes three of the filter input parameters (inputCenter, inputLensDiameter, and inputMagnification) as well as the CISampler object for the highlight image.\n\n9. Sets the region-of-interest method for the lens CIKernel object. This is the method that Core Image invokes whenever your lens kernel routine requires a sample.\n\n10. Applies the lens kernel routine to the input image, supplying the necessary input variables, the domain of definition, and the array that’s needed by the ROI method. Note that the domain of definition is specified as a CIFilterShape object that is the union of the previously calculated rectangle (see e) and the domain of definition of the CISampler object for the source image.\n\n11. Creates a CISampler object for the material map needed by the lens holder kernel routine.\n\n12. Extracts the width of the lens holder as a float value.\n\n13. Calculates, along with the next three lines of code, the rectangle that is used for the domain of definition. Notice that this rectangle encloses the lens holder. Core Image will use this information to restrict calculations. The lens holder kernel routine won’t be called for any pixel that falls outside this rectangle.\n\n14. Retrieves the size of the material map.\n\n15. Creates a CIVector object that represents the width and height of the material map.\n\n16. Sets the region-of-interest method for the lens holder CIKernel object. This is the method that Core Image invokes whenever your lens holder kernel routine requires a sample.\n\n17. Applies the lens holder kernel routine to the material map, supplying the necessary input variables, the rectangle the defines the domain of definition, and the CISampler object for the material map (passed as the userInfo parameter).\n\n18. Creates a filter object for the Core Image CISourceOverCompositing filter, supplying the image produced by the len holder kernel routine as the foreground image and the image produced by the lens kernel routine as the background image.\n\n19. Returns the value associated with the outputImage key for the filter.\n\nSave and close the DetectiveLens.m file.\n\nSelect Project > Add to Project to add the highlight image file (myHighlight.tiff) to the project.\n\nThe name and file type must match what you provide in the init method.\n\nSelect Project > Add to Project to add the material map file (myMaterialMap.tiff) to the project.\n\nThe name and file type must match what you provide in the init method.\n\nOpen the Description.plist file.\n\nChange the display name (look for the key CIAttributeFilterDisplayName) from DetectiveLens to Detective Lens.\n\nSave and close the Description.plist file.\n\nBuild the project.\n\nIt should succeed unless you’ve introduced some typographical errors.\n\nQuit Xcode.\n\nFind the DetectiveLens.plugin file in your project. If you want, move it to a convenient location for validation and testing.\n\nMake sure the image unit is valid and works properly by following the instructions in Validating an Image Unit and Testing an Image Unit.\n\nNext Steps\n\nIf you’ve successfully created all the image units in this chapter, you might try modifying the detective lens image unit by adding a handle that’s typical of a detective lens!\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2011 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2011-06-06\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Document Revision History",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageUnitTutorial/RevisionHistory.html#//apple_ref/doc/uid/TP40004531-CH2-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nImage Unit Tutorial\nTable of Contents\nIntroduction\nAn Image Unit and Its Parts\nWriting Kernels\nWriting the Objective-C Portion\nPreparing an Image Unit for Distribution\nRevision History\nPrevious\nDocument Revision History\n\nThis table describes the changes to Image Unit Tutorial.\n\nDate\tNotes\n2011-06-06\t\n\nCorrected a broken link to the ImageUnitAnalyzer tool.\n\n\n2009-05-06\t\n\nUpdated line 7 of listing 2-7.\n\n\n2008-06-09\t\n\nUpdated the table of input parameters to kernel routines.\n\n\n \t\n\nCorrected typographical errors in the code.\n\n\n2008-04-08\t\n\nCorrected several technical errors in the code listings.\n\n\n \t\n\nMade corrections to the detective lens filter image unit methods outputImage and init.\n\n\n \t\n\nMade one change in the outputImage method of the pixellate filter.\n\n\n2007-12-11\t\n\nFixed minor technical error.\n\n\n \t\n\nAdded details on the valid classes for filter input parameters that you can declare in the description.plist file for an image unit. See The Image Unit Template in Xcode.\n\n\n2007-03-20\t\n\nNew document that shows how to write and package image processing filters.\n\n\n\nPrevious\n\n\n\n\n\nCopyright © 2011 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2011-06-06\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Introduction to Quartz Composer User Guide",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/QuartzComposerUserGuide/qc_intro/qc_intro.html#//apple_ref/doc/uid/TP40005381",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz Composer User Guide\nTable of Contents\nIntroduction\nQuartz Composer Basic Concepts\nThe Quartz Composer User Interface\nBasic and Advanced Tasks, Tips, and Tricks\nTutorial: Creating a Composition\nGlossary\nRevision History\nNext\nIntroduction to Quartz Composer User Guide\n\nImportant: This document is no longer being updated. For the latest information about Apple SDKs, visit the documentation website.\n\nQuartz Composer is a development tool for processing and rendering graphical data. Its visual programming environment lets you develop graphic processing modules, called compositions, without writing a single line of code. Quartz Composer is also a framework that lets you programmatically access, manage, and manipulate compositions created with the development tool. This document, however, is a guide to the Quartz Composer development tool supplied in OS X v10.5. By reading this guide, you’ll get an introduction to using the Quartz Composer editor and find out how to use it to create a composition. You’ll also see how to use compositions as screen savers and in QuickTime movies.\n\nYou should read this document if you are a developer or visual designer who wants to:\n\nGet an orientation to the Quartz Composer development tool supplied in OS X v10.5\n\nCreate compositions that process graphical content\n\nExperiment with the latest OS X graphics technologies\n\nQuartz Composer brings together a rich set of graphical and nongraphical technologies, including Quartz 2D, Core Image, Core Video, OpenGL, QuickTime, MIDI System Services, RSS (Really Simple Syndication), and XML. The development tool lets you explore the visual technologies available in OS X without needing to learn the programming interface for that technology.\n\nThe information in this document pertains to OS X v10.5.\n\nOrganization of This Document\n\nThis document is organized as follows:\n\nQuartz Composer Basic Concepts defines the terms used in Quartz Composer and describes the evaluation model and the coordinate system.\n\nThe Quartz Composer User Interface gives an overview of the editor, the viewer, the patch creator, and the other user interface elements in the tool supplied in OS X v10.5.\n\nBasic and Advanced Tasks, Tips, and Tricks describes the fundamental operations needed to create a composition, gives timesaving tips, and shows how to use some of the more advanced features.\n\nTutorial: Creating a Composition provides step-by-step instructions for creating a composition and using it as a screen saver and a QuickTime movie.\n\nGlossary defines the new terms used in the document.\n\nSee Also\n\nSample Quartz compositions are available in /Developer/Examples/Quartz Composer.\n\nThe Quartz Composer developer mailing list (quartzcomposer-dev) is an excellent place to discuss issues or topics with other Quartz Composer developers.\n\nQuartz Composer Programming Guide describes how to use the Quartz Composer framework to integrate compositions into an application.\n\nQuartz Composer Custom Patch Programming Guide provides instructions on creating your own patches that you can then use in the Quartz Composer development tool.\n\nNext\n\n\n\n\n\nCopyright © 2004, 2007 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2007-07-17\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "An Image Unit and Its Parts",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageUnitTutorial/Overview/Overview.html#//apple_ref/doc/uid/TP40004531-CH6-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nImage Unit Tutorial\nTable of Contents\nIntroduction\nAn Image Unit and Its Parts\nWriting Kernels\nWriting the Objective-C Portion\nPreparing an Image Unit for Distribution\nRevision History\nNext\nPrevious\nAn Image Unit and Its Parts\n\nThis tutorial provides detailed information on how to write the various parts of an image unit so that they work together as an image unit. It’s important that you have an idea not only of what the parts are, but how they fit together. This chapter provides such an overview. It describes the parts of an image unit, discusses what each one does, and provides guidelines for writing some of the components in an image unit.\n\nBefore you start this chapter, you should be familiar with the concepts described in Core Image Programming Guide, have already used some of the built-in image processing filters provided by Core Image, and understand the classes defined by the Core Image API (see Core Image Reference Collection).\n\nThe Major Parts of an Image Unit\n\nAn image processing filter, when packaged as an executable image unit, has three major parts:\n\nA kernel routine file. This file contains one or more kernel routines and any needed subroutines. The kernel routine must be written using the Core Image Kernel Language (CIKL). A C-like language, CIKL is a subset of the OpenGL Shading Language (glslang). CIKL restricts some of the glslang keywords that you can use, but introduces a number of keywords and data types that are not provided by glslang. (See Core Image Kernel Language Reference.)\n\nObjective-C filter files. Each filter has an interface and implementation file that performs all the set up work required prior to applying a kernel routine.\n\nPlug-in loading files. Each image unit has an interface and implementation file that implements the plug-in loading protocol.\n\nDivision of Labor\n\nImage processing tasks are divided into low-level (kernel) and high-level (Objective-C) tasks. When you first start writing image units, you might find it challenging to divide the tasks appropriately. If you strive to keep kernel routines lean, you will likely succeed in dividing the tasks appropriately.\n\nA kernel routine operates on individual pixels and uses the GPU (assuming one is available). For best performance, a kernel routine should be as focused as possible on pixel processing. Any set up work or calculations that can be done outside the kernel routine should be done outside the kernel routine, in the Objective-C filter files. As you’ll see, because Core Image expects certain tasks to be performed outside the kernel routine, the Xcode image unit plug-in template provides methods set up for just this purpose. In Writing the Objective-C Portion you see the specifics, but for now, a general understanding is all you’ll need.\n\nThese are the tasks typically performed in the Objective-C filter files:\n\nRetrieve the files needed for the filter. Image I/O is a high-level task that is typically done during the initialization phase of the image unit filter. Files can include the image (or images) to be processed and any other image data needed by the kernel routine (such as a texture or an environment map).\n\nSet up one or more CISampler objects. A sampler (lowercase “s”) is a data source for a kernel routine. (It is defined in Core Image Kernel Language Reference.) A CISampler object is a Core Image class that encapsulates a sampler, references a file to fetch samples from, defines a coordinate transform (if any) to use on the samples, and defines modes to use for interpolation and wrapping. The data source referenced by a CISampler object can be a texture, an environment map, an image to process, a lookup table—whatever is needed by the kernel routine.\n\nSet up one or more CIKernel objects. A kernel (lowercase “k”) refers to a kernel routine. (It is defined in Core Image Kernel Language Reference.) A CIKernel object is a Core Image class that encapsulates a kernel file, references each of the kernel routines in the file and defines a region-of-interest method for each of the kernel routines that requires such a method.\n\nSet a region-of-interest method and any input parameters required for that method. A region of interest (ROI) defines the area in the source image from which a sampler takes pixel information to provide to the kernel for processing. Simple filters—those for which there is a 1:1 mapping between a source and destination pixel—don’t need a method to calculate the region of interest because Core Image assumes a 1:1 mapping if you don’t supply an ROI method. See Region-of-Interest Methods for more details.\n\nSet up input parameters for the kernel routine. The Objective-C portion of an image unit is where you perform all calculations possible so that the values you pass to the kernel routine are ready to use. For example, you could calculate the radius of an effect in the Objective-C portion rather than pass a diameter to the kernel and perform the radius calculation in the kernel. This way, the calculation is performed only once, and not for every pixel that’s processed.\n\nApply the kernel routine. You can invoke a kernel routine more than once (as you might for effects that require iteration, such as a blur effect). You can use more than one kernel routine to process an image. You can also combine your kernel routine with an effect produced by one of the built-in Core Image filters.\n\nKernel Routine Rules\n\nA kernel routine is like a worker on an assembly line—it specializes in a focused task. Each time the routine is invoked, it produces a vec4 data type from the materials (input parameters) given to it. The routine must perform as little work as possible to be efficient. Assembly line work goes fastest when workers use preassembled subcomponents. It’s also true of kernel routines. Anything that can be calculated ahead of time and passed to the routine should be. As you become more experienced at writing kernel routines, you’ll devise clever ways to pare down the code in the routine. The examples in Writing Kernels should give you some ideas. Core Image also helps in this regard by restricting what sorts of operations can be done in a kernel routine.\n\nKeep the following in mind as you design and write kernel routines:\n\nFlow control statements (if, for, while, do while) are supported only when the loop condition can be inferred at the time the code compiles.\n\nThe input parameters to a kernel routine can be any of these data types: sampler, __table sampler, __color, float, vec2, vec3, or vec4. However, when you apply a kernel routine in the Objective-C portion of an image unit, you must supply objects. See Table 1-1.\n\nA kernel routine does not take images as input parameters. Instead, it takes a sampler object that’s associated with an image. It is the job of the sampler object to fetch image data and provide it to the kernel routine. All sampler objects are set up as CISampler objects in the Objective-C portion of an image unit. See Division of Labor.\n\nYou are restricted to using what’s described in Core Image Kernel Language Reference. The Core Image Kernel Language (CIKL) is a subset of OpenGL Shading Language (glslang), so not everything that’s defined by glslang is allowed by CIKL. However, you’ll find that most of the keywords in gslang are available to you. In addition, CIKL provides a number of data types, keywords, and functions that are not available in glslang.\n\nYou can’t use arrays.\n\nA kernel routine computes an output pixel by using an inverse mapping back to the corresponding pixels of the input images. Although you can express most pixel computations this way—some more naturally than others—there are some image processing operations for which this is difficult, if not impossible. For example, computing a histogram is difficult to describe as an inverse mapping to the source image. You also cannot perform seed fills or any image analysis operations that require complex conditional statements.\n\nA routine is faster if you unroll loops.\n\nTable 1-1 lists the valid input parameters for a kernel routine and the associated objects that must be passed to the kernel routine from the Objective-C portion of an image unit. Core Image extracts the appropriate base data type from the higher-level Objective-C object that you supply. If you don’t use an object, the filter may unexpectedly quit. For example, if, in the Objective-C portion of the image unit, you pass a floating-point value directly instead of packaging it as an NSNumber object, your filter will not work. In fact, when you use the Image Unit Validator tool on such an image unit, the image unit fails with a cryptic message. (See Validating an Image Unit.)\n\nTable 1-1  Kernel routine input parameters and their associated objects\n\nKernel routine input parameter\n\n\t\n\nObject\n\n\n\n\nsampler\n\n\t\n\nCISampler\n\n\n\n\n__table sampler\n\n\t\n\nCISampler\n\n\n\n\n__color\n\n\t\n\nCIColor\n\n\n\n\nfloat\n\n\t\n\nNSNumber\n\n\n\n\nvec2, vec3, or vec4\n\n\t\n\nCIVector\n\nRegion-of-Interest Methods\n\nThe region of interest (ROI) is the area of the sampler source data that your kernel uses for its per-pixel processing. A kernel routine always returns a vec4 data type—that is, one pixel. However, the routine can operate on any number of pixels to produce that vec4 data type. If the mapping between the source and the destination is not one-to-one, then you must define a region-of-interest method in the Objective-C filter file.\n\nYou do not need an ROI method when a kernel routine:\n\nProcesses a pixel from the working-space coordinate (r,s) of the sampler to produce a pixel at the working-space coordinate (r,s) in the destination image.\n\nYou must supply an ROI method when a kernel routine:\n\nUses many source pixels in the calculation of one destination pixel. For example, a distortion filter might use a pixel (r,s) and its neighbors from the source image to produce a single pixel (r,s) in the destination image.\n\nUses values provided by a sampler that are unrelated to the working-space coordinates in the source image and the destination. For example, a texture, a color ramp, or a table that approximates a function provides values that are unrelated to the notion of working coordinates.\n\nYou supply an ROI method for each kernel routine in an image unit that needs you. (An image unit can contain one or more kernel routines.) Each ROI method that you supply must use a method signature of the following form:\n\n- (CGRect) regionOf:(int)samplerIndex\n\n\n            destRect:(CGRect)r\n\n\n            userInfo:obj;\n\nYou can replace regionOf with an appropriate name. For example, an ROI method for a blur filter could be named blurROI:destRect:userInfo:.\n\nCore Image invokes your ROI method when appropriate, passing to it the sampler index (which you’ll learn more about later), the rectangle for the region being rendered, and any data that is needed by your routine. The method must define the ROI for each sampler data source used by the kernel routine. If a sampler data source used by the kernel routine doesn’t require an ROI method, then you can pass back the destRect rectangle for that sampler. You simply check the samplerIndex value passed to the method. If an ROI calculation is need for the sampler, perform the calculation and pass back the appropriate rectangle. If an ROI calculation is not needed for that particular sampler, then pass back the destRect passed to the method.\n\nFor example, if your kernel routine accesses pixels within a radius r around the current target, you need to offset the destination rectangle in the ROI method by the radius r. You’ll see how all this works in more detail later.\n\nNote: Whereas the ROI defines the area in a source image from which to fetch pixels, the domain of definition defines the area of the destination image that accepts processed pixels. The domain of definition is the area outside of which all pixels in the destination are transparent (that is, the alpha component is equal to 0). For more details, see Core Image Programming Guide.\n\nNext Steps\n\nNow that you have a general idea of what the major parts of an image unit are and what each does, you are ready to move on to writing kernel routines. Writing Kernels starts with a few simple kernel routines and progresses to more complex ones. Not only will you se how to write kernel routines, but you’ll see how you can test simple kernel routines without the need to provide Objective-C code.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2011 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2011-06-06\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Taking Snapshots and Setting Pictures",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageKitProgrammingGuide/IKImagePicker/IKImagePicker.html#//apple_ref/doc/uid/TP40004907-CH7-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nImageKit Programming Guide\nTable of Contents\nIntroduction\nBasics of Using the Image Kit\nViewing, Editing, and Saving Images in an Image View\nBrowsing Images\nShowing Slides\nTaking Snapshots and Setting Pictures\nBrowsing Filters and Setting Input Parameters\nGlossary\nRevision History\nNext\nPrevious\nTaking Snapshots and Setting Pictures\n\nMore and more applications provide the user with the opportunity to take a snapshot or provide an image to use as a personal identifier, such as the buddy picture in iChat. The IKPictureTaker class provides such support. Its simple user interface lets users choose, crop, or rotate an existing image, or take a snapshot using an iSight camera or other digital camera.\n\nThis chapter describes the user interface provided by the IKPictureTaker class and gives step-by-step instructions for creating an application that uses the picture taker.\n\nThe Picture Taker User Interface\n\nThe picture taker panel can appear as a modal window or a sheet. Figure 5-1 shows the picture taker sheet as it appears the first time the user opens it. The application can set the default image. In this example, it is the ladybug image that comes with OS X.\n\nThe user has a number of options when the presented with the picture taker:\n\nAccept the default image\n\nDrag an image directly to the picture taker\n\nClick Choose and select an image through the Open panel as shown in Figure 5-3.\n\nClick the camera button to take a snapshot\n\nFigure 5-1  The picture taker as a sheet\n\nThe user can modify the image by using the slider to size and crop it. If the picture taker has an effects button (located next to the camera button), the user can click it to apply an effect, as shown in Figure 5-2. The effects button is optional. You’ll see how to turn on the option in Using the Picture Taker in the Contact Information Application.\n\nFigure 5-2  Adding an effect\n\nThe user clicks the Set button to accept an image and close the picture taker, or the Cancel button to close the sheet without choosing an image.\n\nFigure 5-3 shows the panel that appears when the user clicks the Choose button.\n\nFigure 5-3  Choosing an image with the Open panel\n\nIf the user previously chose images for this application, the Recent Pictures menu is populated with those images, as shown in Figure 5-4. The user can choose any of the recent items.\n\nFigure 5-4  The Recent Pictures menu\n\nFigure 5-5 shows the user interface that appears after the user clicks the camera button on a computer that has an iSight or other digital camera connected to it. Whatever is positioned in front of the camera appears in the window. The numbers below the window, along with sound, provide a countdown prior to the camera capturing the image.\n\nFigure 5-5  Taking a snapshot\nUsing the Picture Taker in an Application\n\nThis section shows how to write an application that uses a picture taker. First you’ll get an overview of how the picture taker works in general, then you’ll take a look at how to implement the picture taker in an application that lets the user set up personal contact information.\n\nTo display the picture taker in any application, you first need to create a shared instance by calling the pictureTaker method of the IKPictureTaker class.\n\nIKPictureTaker *pictureTaker = [IKPictureTaker pictureTaker];\n\nYou can customize the picture taker appearance and behavior by using the setValue:forKey: method and providing a key constant (defined by the picture taker) along with the appropriate value. For example, you can control the size of the crop area using the key IKPictureTakerCropAreaSizeKey and providing a size (as an NSValue object). See IKPictureTaker Class Reference for a complete list of the options you can set.\n\nYou can launch the picture taker as a standalone window using this method:\n\n[pictureTaker beginPictureTakerWithDelegate:self didEndSelector:@selector(pictureTakerDidEnd:returnCode:contextInfo:) contextInfo:nil];\n\nor as a sheet, using this method:\n\n[myPictureTaker beginPictureTakerSheetForWindow:aWindow withDelegate:self didEndSelector:@selector(pictureTakerDidEnd:returnCode:contextInfo:) contextInfo:nil];\n\nYou need to provide a selector method that has a method signature that matches the following:\n\n- (void) pictureTakerDidEnd:(IKPictureTaker *) picker\n\nTypically you would retrieve the output image in this callback by using the outputImage method of the IKPictureTaker class. The output image represents the edited image; the inputImage method retrieves the unedited image.\n\n- (void) pictureTakerDidEnd:(IKPictureTaker *) picker\n\n\n    returnCode:(NSInteger) code\n\n\n    contextInfo:(void*) contextInfo\n\n\n{\n\n\n    NSImage *image = [picker outputImage];\n\n\n}\nHow the Contact Information Application Works\n\nThe rest of this chapter shows how to create a Contact Information application that uses the picture taker to add a photo to a contact entry. When it opens, the Contact Information application displays a window that lets the user enter a name, email address, phone number, and iChat address, as shown in Figure 5-6. The user can add an image by clicking the Add Photo button, which will open the picture taker.\n\nFigure 5-6  An application that uses the picture taker\nUsing the Picture Taker in the Contact Information Application\n\nThis section provides step-by-step instructions for implementing the picture taker in the Contact Information application. First you’ll set up the Xcode project, the project files, and the controller interface. Then you’ll add the necessary routines to the implementation file. Finally, you’ll connect the user interface to the picture taker action.\n\nSetting Up the Project, Project Files, and the Controller Interface\n\nFollow these steps to set up the project:\n\nIn Xcode, create a Cocoa application named Contact Information.\n\nAdd the Quartz framework to the project.\n\nFor details, see Using the Image Kit in Xcode.\n\nDouble-click the MainMenu.nib file.\n\nIn the nib document window, double-click the Window icon and create a layout that looks similar to Figure 5-6.\n\nIt really doesn’t matter what you create. The important elements for you to add from the Library are a push button object (labeled Add Photo) and an Image Well (NSImageView) object.\n\nSave the nib file and return to Xcode.\n\nIn Xcode, choose File > New File.\n\nChoose “Objective-C Class NSWindowController subclass\" and click Next.\n\nName the file MyController.m and keep the option to create the header file. Then click Finish.\n\nIn the MyController.h file, import the Quartz framework by adding this statement:\n\n#import <Quartz/Quartz.h>\n\nAdd an instance variable for the image that appears in the contact information window.\n\nAt this point, the interface should look as follows:\n\n@interface MyController: NSWindowController\n\n\n{\n\n\n    IBOutlet NSImageView *  mImageView;\n\n\n}\n\nSave MyController.h.\n\nAdding Routines to the Implementation File\n\nImplement the picture taker routines by following these steps:\n\nOpen MyController.m\n\nIn the implementation file, first add an awakeFromNib method.\n\nThis method needs to retrieve a shared instance of the picture taker and then set the default image to show when the picture taker first opens. You can set any image that you’d like, but the example below uses the ladybug image provided in the Desktop Pictures folder.\n\nYour awakeFromNib method should look similar to the following:\n\n- (void) awakeFromNib\n\n\n{\n\n\n    IKPictureTaker *picker = [IKPictureTaker pictureTaker];\n\n\n    [picker setInputImage:[[NSImage alloc]\n\n\n        initByReferencingFile:@\"/Library/Desktop Pictures/Nature/Ladybug.jpg\"]];\n\n\n \n\n\n}\n\nNext, you need to add a method that launches the picture taker.\n\nThe method first retrieves the shared instance of the picture taker. Then it sets an option for showing the effects button. You can set any options you like and that are defined in IKPictureTaker Class Reference. Finally, the method launches the picture taker as a sheet. You must provide the window that contains the image view, the delegate (which in this case is self), and a selector for a callback method. You’ll write the callback next.\n\n- (void) launchPictureTaker:(id)sender\n\n\n{\n\n\n    IKPictureTaker *picker = [IKPictureTaker pictureTaker];\n\n\n \n\n\n    [picker setValue:[NSNumber numberWithBool:YES]\n\n\n              forKey:IKPictureTakerShowEffectsKey];\n\n\n \n\n\n    [picker beginPictureTakerSheetForWindow:[mImageView window]\n\n\n               withDelegate:self\n\n\n               didEndSelector:@selector(pictureTakerDidEnd:code:contextInfo:)\n\n\n               contextInfo:nil];\n\n\n}\n\nIf you prefer to launch the picture taker as a separate window, you can substitute the following for the last call in the launchPictureTaker: method.\n\n[picker beginPictureTakerWithDelegate:self\n\n\n           didEndSelector:@selector(pictureTakerDidEnd:code:contextInfo:)\n\n\n           contextInfo:nil];\n\nWrite a didEndSelector method to provide as a callback.\n\nThis method is invoked when the user dismisses the picture taker either by choosing an image or by clicking the Cancel button. If the user chooses an image, the method retrieves the output image and then sets it to the view so that the image that’s displayed changes to the newly selected image. Otherwise, the method does nothing.\n\n- (void) pictureTakerDidEnd:(IKPictureTaker*) pictureTaker code:(int) returnCode contextInfo:(void*) ctxInf\n\n\n{\n\n\n    if(returnCode == NSOKButton){\n\n\n        NSImage *outputImage = [pictureTaker outputImage];\n\n\n        [mImageView setImage:outputImage];\n\n\n    }\n\n\n    else{\n\n\n        // The user canceled, so there is nothing to do.\n\n\n    }\n\n\n}\n\nSave the MyController.m file.\n\nOpen the MyController.h file and add the launch method prototype just before the @end statement. When inserted, the interface for MyController should look like this:\n\n@interface MyController : NSWindowController {\n\n\n   IBOutlet NSImageView * mImageView;\n\n\n}\n\n\n \n\n\n- (IBAction) launchPictureTaker:(id)sender;\n\n\n@end\n\nClose the MyController.h file.\n\nConnecting the User Interface to the Picture Taker Action\n\nTo make the connections necessary for the picture taker to launch when the user clicks the Add Photo button, follow these steps:\n\nIn Interface Builder, drag an Object (NSObject) from the Library to the nib document window.\n\nIn the Identity inspector, type MyController in the Class field.\n\nIf the Contact Information window that you created is not open, open it.\n\nControl-drag from the Add Photo button to the instance of MyController.\n\nIn the connections panel, choose launchPictureTaker:.\n\nControl-drag from MyController to the NSImageView instance in the window. Then, in the connections panel, choose mImageView.\n\nSave the nib file.\n\nIn Xcode, click Build and Go. Then test your application to make sure the picture taker works.\n\nCheck to make sure the effects button is displayed along with any other options you set up.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2008 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2008-06-09\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Browsing Filters and Setting Input Parameters",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageKitProgrammingGuide/IKFilterBrowser/IKFilterBrowser.html#//apple_ref/doc/uid/TP40004907-CH8-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nImageKit Programming Guide\nTable of Contents\nIntroduction\nBasics of Using the Image Kit\nViewing, Editing, and Saving Images in an Image View\nBrowsing Images\nShowing Slides\nTaking Snapshots and Setting Pictures\nBrowsing Filters and Setting Input Parameters\nGlossary\nRevision History\nNext\nPrevious\nBrowsing Filters and Setting Input Parameters\n\nWhen Core Image was introduced in OS X v10.4, it provided developers with access to one hundred built-in image processing filters and the ability to create custom filters. Although image filters are easy to use, the Core Image programming interface does not provide a user interface for choosing filters and for setting filter input parameters. You either used Core Image to set up and apply filters programmatically or created and managed your own user interface. With the introduction of the Image Kit framework in OS X v10.5, all that has changed.\n\nThe IKFilterBrowserPanel and IKFilterBrowserView classes provide a filter browser that lets users:\n\nBrowse filters by category\n\nApply a filter and preview its effects\n\nChoose a filter\n\nCreate filter collections\n\nSee a description of the filter\n\nAs a developer, you can choose which filter categories the browser displays and whether or not to show a preview. After a user chooses a filter, you can use the IKFilterUIView class to obtain a view that contains controls for the input parameters of the filter.\n\nThis chapter:\n\nDescribes the filter browser and filter user interface view\n\nDiscusses the user interface options you can set\n\nShows how to set up and use the filter browser in an application\n\nProvides instructions for obtaining a filter user interface view and adding it into an existing view\n\nThe Filter Browser\n\nThe IKFilterBrowserPanel and IKFilterBrowserView classes provides a panel that displays a list, by category, of Core Image filters available on the system, as shown in Figure 6-1. The search field at the top allows users to quickly find filters.\n\nFigure 6-1  The filter browser\n\nThe user can click the Preview button to toggle a preview the preview off and on. The preview area lets the user see the effect of applying a filter using the default settings. Your application can set the preview image by registering for the notification IKFilterBrowserWillPreviewFilterNotification. A short description of the filter appears below the preview.\n\nBy clicking the plus (+) button, the user can add a custom group—referred to as a collection—as shown in Figure 6-2. The user can drag filters to the collection as a convenience for accessing sets of commonly used filters. Collections always appear below the built-in categories. To remove a collection, the user clicks the minus (–) button.\n\nFigure 6-2  A Photo Corrections collection\n\nYou can control the visual appearance of the controls, including their size, in the filter browser by specifying the appropriate setting.\n\nThe Filter User Interface View\n\nThe IKFilterUIView class provides a view that contains controls for the input parameters of a filter. You can specify the size of the controls as miniature (IKUISizeMini), small (IKUISizeSmall), or regular (IKUISizeRegular).\n\nA filter can optionally define basic, intermediate, advanced, and development sets of input parameters. These sets define which input parameters can be exposed in the user interface. For example, a filter that has many input parameters could define a basic set of input parameters for the typical consumer to control, and then programmatically set the other input parameters to default values. The same filter, however, can define an advanced set of input parameters that allow professional customers to control all the input parameters. Your application can request a specific set of input parameters. If the filter defines that set, then the filter user interface view reflects your request.\n\nFigure 6-3 shows a window that displays a filter user interface view for the gloom filter provided by Core Image (CIGloom). The controls for this filter are labeled Intensity and Radius. In this example, the image that is processed by the filter is labeled Background image and appears in the image well shown in the figure.\n\nFigure 6-3  Controls for the Gloom filter appear below the background image\n\nThe output of one Core Image filter can be directed to the input of another Core Image filter. Typically you’ll want to write an application that allows users to apply more than one Core Image filter to an image. Figure 6-4 shows an application that inserts user interface views for two Core Image filters into a window—the gloom filter and the pixelate filter (CIPixellate). The controls for the pixelate filter are labeled Scale and Center.\n\nFigure 6-4  Two filter views in a window\n\nSo far you’ve seen the user interface views for the built-in filters provided by Core Image. Developers who write image units can provide a custom view for each of the filters defined in the image unit by implementing the IKFilterCustomUIProvider protocol. For information on image units, see Image Unit Tutorial.\n\nOpening the Filter Browser in an Application\n\nBefore you display the filter browser in an application, you need to load all the filters on the system by calling the loadAllPlugIns method of the CIPlugIn class. Then you create a shared instance of the IKFilterBrowserPanel class by calling the filterBrowserPanelWithStyleMask: method. You can choose between the Aqua style (default) and the brushed metal look (NSTexturedBackgroundWindowMask). You show the filter browser by calling one of several methods, depending on whether you want the browser to appear as a sheet, a separate window, or in a custom view. In this section, you’ll see how to create a filter browser that opens in a separate window.\n\nThis section shows hot to create an application that opens a filter browser in a separate window. First you’ll set up the Xcode project, the project files, and the controller interface. Next you’ll add the necessary routines to the implementation file. Then, you’ll create the user interface in Interface Builder. Finally, you'll make a few refinements by adding an options dictionary and using a different background window.\n\nSetting Up the Project, Project Files, and the Controller Interface\n\nFollow these steps to set up the project:\n\nIn Xcode, create a Cocoa application and name it My Filter Browser.\n\nAdd the Quartz and Quartz Core frameworks to the project.\n\nFor details, see Using the Image Kit in Xcode.\n\nChoose File > New File.\n\nChoose Objective-C Class and click Next.\n\nName the file FilterBrowserController.m and keep the option to create the header file. Then click Finish.\n\nIn FilterBrowserController.h import the Quartz Core and Quartz frameworks by adding these statements:\n\n#import <Quartz/Quartz.h>\n\n\n#import <QuartzCore/QuartzCore.h>\n\nCreate an instance variable for the filter browser. Your interface should look as follows:\n\n@interface FilterBrowserController : NSObject {\n\n\n     IKFilterBrowserPanel *filterBrowserPanel;\n\n\n}\n\n\n@end\n\nSave the FilterBrowserController.h file.\n\nAdding Routines to the Implementation File\n\nTo implement the filter browser routines, follow these steps:\n\nOpen the FilterBrowserController.m file.\n\nIn the implementation file, add an awakeFromNib method.\n\nThis method needs to load all Core Image plug-ins, as shown:\n\n- (void)awakeFromNib\n\n\n{\n\n\n    [CIPlugIn loadAllPlugIns];\n\n\n}\n\nNext you’ll write a showFilterBrowserPanel: method that is invoked when the user chooses the Open Filter Browser menu item. You’ll create the menu item later.\n\nThe method first defines a constant to set the mask so that the filter browser uses the brushed metal look. This is optional. The default is to use the Aqua look. Note that you must provide a selector for the method that is invoked when the user closes the browser. You’ll write the selector method next.\n\n- (IBAction)showFilterBrowserPanel:(id)sender\n\n\n{\n\n\n    int myStyleMask = NSTexturedBackgroundWindowMask;\n\n\n    if(!filterBrowserPanel)\n\n\n        filterBrowserPanel = [IKFilterBrowserPanel\n\n\n               filterBrowserPanelWithStyleMask:myStyleMask];\n\n\n \n\n\n    [filterBrowserPanel beginWithOptions:NULL modelessDelegate:self didEndSelector:@selector(browserPanelDidEndSelector:returnCode:contextInfo:) contextInfo:nil];\n\n\n \n\n\n}\n\nAdd the browserPanelDidEndSelector:returnCode:contextInfo: selector method.\n\nThis method simply takes the filter browser out of the screen list so that is it not visible.\n\n- (void)browserPanelDidEndSelector:(NSWindow *)sheet returnCode:(int)returnCode contextInfo:(void *)contextInfo\n\n\n{\n\n\n    [sheet orderOut:self];\n\n\n}\n\nSave the FilterBrowserController.m file.\n\nOpen the FilterBrowserController.h file and add this method declaration, making sure that you insert it before the @end statement.\n\n- (IBAction)showFilterBrowserPanel:(id)sender;\n\nSave the FilterBrowserController.h file.\n\nCreating the User Interface\n\nSet up the user interface in Interface Builder by following these steps:\n\nDouble-click the MainMenu.nib file to open Interface Builder.\n\nDelete the window icon in the nib document window.\n\nChoose File > Synchronize With Xcode.\n\nDrag an NSObject from the library to the nib document window.\n\nIn the Identity inspector for the NSObject, choose FilterBrowserController from the Class pop-up menu.\n\nDouble-click the MainMenu icon in the nib document window.\n\nDrag a submenu item from the Library to the MainMenu and place it between the Format and View menu items. Name the menu Filter.\n\nName the item in the Filter menu Show Filter Browser.\n\nControl-drag from the Show Filter Browser menu item to the FilterBrowserController and in the connections panel choose showFilterBrowserPanel:.\n\nSave the nib file.\n\nIn Xcode, click Build and Go.\n\nWhen the application launches, choose Show Filter Browser from the Filter menu.\n\nTry the Search feature. Click the Preview button to toggle the preview off and back on. Then click the plus (+) button and add a collection. Drag a few filters into your collection.\n\nQuit the application.\n\nNow that the application runs, you’ll make a few refinements by adding an options dictionary and using a different window background.\n\nRefining the User Interface\n\nThere are a number of options you can set to modify the filter browser user interface. Follow these steps to change the options:\n\nIn Xcode, open the FilterBrowserController.m file and add a method that creates a dictionary of options. Make sure that you place this method before the showFilterBrowserPanel: method .\n\nThe Image Kit provides constants that let you specify the size of controls to use in the browser. The default is to use regular size controls. You’ll change the size to miniature by using the key constant IKUISizeFlavor and the value IKUISizeMini.\n\n-(NSDictionary *)createFilterBrowserUIOptions\n\n\n{\n\n\n NSMutableDictionary *options = [[NSMutableDictionary alloc] init];\n\n\n [options setObject:IKUISizeMini forKey:IKUISizeFlavor];\n\n\n return options;\n\n\n}\n\nWhen you write a more complex application, you could let the user set the control size as a preference. Then your options dictionary method could set the options based on user preferences.\n\nModify the showFilterBrowserPanel: method so that it calls the createFilterBrowserUIOptions method.\n\nIn addition, set the style mask to 0 to get the default look.\n\nAfter modification, the method to show the filter browser should look as follows:\n\n- (IBAction)showFilterBrowserPanel:(id)sender\n\n\n{\n\n\n  int myStyleMask = 0;\n\n\n  NSDictionary * options = [self createFilterBrowserUIOptions];\n\n\n \n\n\n  if(!filterBrowserPanel)\n\n\n    filterBrowserPanel = [IKFilterBrowserPanel\n\n\n        filterBrowserPanelWithStyleMask:myStyleMask];\n\n\n \n\n\n  [filterBrowserPanel beginWithOptions:options\n\n\n     modelessDelegate:self\n\n\n     didEndSelector:@selector(browserPanelDidEndSelector:returnCode:contextInfo:)\n\n\n     contextInfo:nil];\n\n\n \n\n\n}\n\nIn Xcode, click Build and Go.\n\nAfter these modifications, the filter browser should look similar to the following:\n\nNote that if you created a collection previously, the collection appears in the filter browser. Collections are persistent across launches of an application, on a per user basis.\n\nGetting a Filter User Interface View\n\nThis section shows how to get a view for the input parameters of a filter and insert that filter user interface view into an existing view. You’ll build upon the code in the last section by first adding code to respond to a user event—a double click—in the filter browser. When the user double-clicks the name of a filter in the filter browser, the application requests a filter user interface view for the selected filter and displays that view, as shown in Figure 6-3 and Figure 6-4.\n\nYou need to register for a user event notification—either IKFilterBrowserFilterSelectedNotification or IKFilterBrowserFilterDoubleClickNotification of the IKFilterBrowserPanel class. After the user chooses a filter, you call the viewForUIConfiguration:excludedKeys: method of the CIFilter class (an Image Kit addition) to obtain a view for the filter. You can provide a configuration dictionary to specify the size of controls to use for the filter input parameters, just as you did for the filter browser itself. You exclude keys for input parameters that you don’t want to show (such as the input image which, in most cases, will already be displayed onscreen).\n\nThe application you’ll build here doesn’t perform any image processing. It simply shows how to set up the user interface. However, it does show how to stack several filter user interface views together. Applying Filters to an Image discusses using the Image Kit user interface together with Core Image.\n\nCreating a Filter View Controller\n\nFollow these steps to set up the filter view controller:\n\nIn Xcode, open the project that you created in the last section.\n\nChoose File > New File.\n\nChoose Objective-C Class and click Next.\n\nName the file FilterViewController.m and keep the option to create the header file. Then click Finish.\n\nThis is the controller for the filter user interface view. It obtains the filter, gets a view that contains the controls for the input parameters of the filter, and adds the view to the user interface.\n\nIn the FilterViewController.h file, add statements to import the Quartz and Quartz Core frameworks. You’ll also need to add a directive for the FilterBrowserController class to allow the two controllers to communicate.\n\n#import <Quartz/Quartz.h>\n\n\n#import <QuartzCore/QuartzCore.h>\n\n\n \n\n\n@class FilterBrowserController;\n\nAdd two IBOutlet objects to the interface—one for a view that contains the filter UI view and another for the filter browser controller. You’ll set these up in Interface Builder later.\n\nThe interface should look as follows:\n\n@interface FilterViewController : NSObject {\n\n\n    IBOutlet id    filterUIContainerBox;\n\n\n    IBOutlet FilterBrowserController *browserController;\n\n\n}\n\n\n@end\n\nAdd a method signature for an addFilter: method.\n\nThis is the method that adds the filter user interface view to the panel. You’ll write the method later. When the user double-clicks a filter name, the method is invoked and passed a notification object that contains the name of the selected filter.\n\n- (void)addFilter:(NSNotification*)notification;\n\nSave the FilterViewController.h file.\n\nImplementing the Filter View Controller\n\nTo implement the routines necessary for the filter view controller, follow these steps:\n\nIn Xcode, open the FilterViewController.m file.\n\nAdd an awakeFromNib method. This method sets the background color of the filter view and makes sure that the panel is in front.\n\n- (void)awakeFromNib\n\n\n{\n\n\n    [[filterUIContainerBox window] setBackgroundColor:[NSColor whiteColor]];\n\n\n    [[filterUIContainerBox window] orderFront:self];\n\n\n}\n\nWrite a method that creates and returns a dictionary of user interface options for the view. You’ll set the size of the filter input parameter controls. (You can set additional options if you’d like.)\n\n-(NSDictionary *)createFilterBrowserUIOptions\n\n\n{\n\n\n  NSMutableDictionary *options = [[NSMutableDictionary alloc] init];\n\n\n [options setObject:IKUISizeMini forKey:IKUISizeFlavor];\n\n\n \n\n\n return options;\n\n\n}\n\nWrite an addFilter: method.\n\nThis method gets the view for the selected filter and inserts it into the panel you created previously in Xcode. The embedded comments explain what the code does.\n\n- (void)addFilter:(NSNotification*)notification\n\n\n{\n\n\n    // Get the filter name from the notification.\n\n\n    NSString    *filterName = [notification object];\n\n\n    // Create a CIFilter object.\n\n\n    CIFilter    *newFilter = [CIFilter filterWithName:filterName];\n\n\n    // Add a filter user interface view only if a filter object exists.\n\n\n    if(newFilter)\n\n\n    {\n\n\n        // Get  the frame rectangle that contains the filter UI container.\n\n\n        NSRect  windowFrame = [[filterUIContainerBox window] frame];\n\n\n        // Set the default values for the filter.\n\n\n        [newFilter setDefaults];\n\n\n        // Get an options dictionary, which specifies the size of the controls.\n\n\n        NSDictionary    *options = [self createFilterBrowserUIOptions];\n\n\n        // Create  a view for the filter, exclude the input image key.\n\n\n        IKFilterUIView    *filterContentView =\n\n\n             [newFilter viewForUIConfiguration:options\n\n\n                        excludedKeys:[NSArray arrayWithObject:@\"inputImage\"]];\n\n\n        // Retrieve  the bounding rectangle for the view.\n\n\n        NSRect  contentBounds = [filterContentView bounds];\n\n\n       // Make the view width match the width of the filter UI container.\n\n\n        contentBounds.size.width = [filterUIContainerBox bounds].size.width;\n\n\n       // Set up automatic resizing to accommodate additional views.\n\n\n       [filterContentView setAutoresizingMask:NSViewMinYMargin];\n\n\n        // Increase the window frame size to accommodate the filter UI view.\n\n\n        windowFrame.size.height += [filterContentView bounds].size.height;\n\n\n        windowFrame.origin.y -= [filterContentView bounds].size.height;\n\n\n        // Set the frame of the filter UI container  to be the window frame.\n\n\n        // Display the changes and animate for a nice effect.\n\n\n        [[filterUIContainerBox  window] setFrame:windowFrame\n\n\n                                        display:YES animate:YES];\n\n\n        // Add the filter UI view to the filter container.\n\n\n        [filterUIContainerBox  addSubview:filterContentView];\n\n\n \n\n\n}\n\nSave the FilterViewController.m file.\n\nModifying the Filter Browser Controller\n\nNext you’ll need to make a few modifications to the filter browser controller to allow the view and browser controllers to communicate. You’ll also need to register for a notification whenever the user double-clicks a filter name. Follow these steps:\n\nOpen the FilterBrowserController.h file and add this import statement:\n\n#import \"FilterViewController.h\"\n\nAdd an IBOutlet for the FilterViewController. The modified interface should now look like this:\n\n@interface FilterBrowserController : NSObject {\n\n\n    IKFilterBrowserPanel    *filterBrowserPanel;\n\n\n    IBOutlet FilterViewController *viewController;\n\n\n \n\n\n}\n\n\n \n\n\n- (IBAction)showFilterBrowserPanel:(id)sender;\n\n\n \n\n\n@end\n\nOpen the FilterBrowserController.m file and modify the awakeFromNib method to register for the double-click notification setting the observer as FilterViewController and the method as addFilter:.\n\nThe awakeFromNib method should now looks as follows:\n\n \n\n\n- (void)awakeFromNib\n\n\n{\n\n\n    [CIPlugIn loadAllPlugIns];\n\n\n    [[NSNotificationCenter defaultCenter] addObserver:viewController\n\n\n                         selector:@selector(addFilter:)\n\n\n                         name:IKFilterBrowserFilterDoubleClickNotification\n\n\n                         object:nil];\n\n\n \n\n\n}\n\nSave and close the FilterBrowserController.h and FilterBrowserController.m files.\n\nCreating the User Interface\n\nSet up the user interface in Interface Builder by following these steps:\n\nDouble-click the MainMenu.nib file to open Interface Builder.\n\nDrag a panel from the Library to the nib document window.\n\nChange the name of the Panel icon to FilterInputs.\n\nIn the Attributes inspector, change the title of the panel to Filter Input Parameters.\n\nChoose File > Synchronize With Xcode.\n\nDrag an Object (NSObject) from the Library to the nib document window.\n\nIn the Identity inspector, select FilterViewController from the Class pop-up menu.\n\nIn the nib document window, control-drag from the FilterBrowserController icon to the FilterViewController icon. Then in the connections panel, choose the viewController outlet.\n\nControl-drag from the FilterViewController icon to the FilterBrowserController icon. Then connect to the browserController outlet.\n\nControl-drag from the FilterInputs icon to the FilterViewController icon. Then in the connections panel choose delegate.\n\nDrag a Custom View to the panel. Make sure to hold it over the panel until the panel opens, then drop it in the opened panel.\n\nControl-drag from the FilterViewController icon to the custom view. Then in the connections panel choose filterUIContainerBox.\n\nClick the custom view in the panel.\n\nSet the size of the view to 320 by 2 and make sure that the view (which will be a thin line) is positioned at the top of the window.\n\nThis is the view to which you’ll add the filter user interface view. Unless you want to start by showing something in this view, its height can be negligible.\n\nSet the springs and struts for the Custom View like this:\n\nSet the springs and struts for the content view of the Custom View so that only the struts—which are the outer Autosizing elements —are set.\n\nNote: It’s easiest to see the content view by setting the view style to column view and then clicking the Filter Inputs icon in the nib document window.\n\nSet the size of the panel to 320 by 8.\n\nUnless you want to show something in the panel other than filter input parameters, the panel height can be as small as possible.\n\nSave the MainMenu.nib file.\n\nIn Xcode, click Build and Go.\n\nChoose Show Filter Browser from the Filter menu. Double-click one or more filters to test your code. Each time you double-click a filter name, the Filter Input Parameters panel should show the input parameters for that filter.\n\nApplying Filters to an Image\n\nYou’ve seen in the previous sections how to write code that presents a simple user interface for the filter browser and filter input parameters. A full-featured image processing application requires a more sophisticated user interface. For example, it would:\n\nShow the filter input parameter panel only after an image is open and a filter is selected.\n\nAdd code to delineate visually the input parameters of each filter, by adding a bounding box, a filter name, varying the background color for each filter UI view, or by some other means.\n\nAllow the user to delete a filter from the input parameters panel and possible rearrange the order.\n\nA fully functioning image processing application also requires code that opens and saves images and applies selected filters to an image. The IUUIDemoApplication example provided with the developer tools for OS X v10.5 provides most of the functionality needed in a full-featured image processing application. Now that you know how the user interface portion of the filter browser and filters works, you may want to take a look at IUUIDemoApplication to see how to combine the user interface code with code that applies Core Image filters.\n\nYou can find the application in:\n\n/Developer/Examples/Quartz/Core Image/\n\nIUUIDemoApplication has a controller for the filter browser and another controller for the filter user interface view. It also creates an object that tracks filters and another that tracks filter user interface views. It uses the Core Image programming interface to process images.\n\nIf you are not familiar with Core Image, see:\n\nCore Image Programming Guide\n\nCore Image Reference Collection\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2008 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2008-06-09\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Viewing, Editing, and Saving Images in an Image View",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageKitProgrammingGuide/ImageViews/ImageViews.html#//apple_ref/doc/uid/TP40004907-CH4-SW9",
    "html": "Documentation Archive\nDeveloper\nSearch\nImageKit Programming Guide\nTable of Contents\nIntroduction\nBasics of Using the Image Kit\nViewing, Editing, and Saving Images in an Image View\nBrowsing Images\nShowing Slides\nTaking Snapshots and Setting Pictures\nBrowsing Filters and Setting Input Parameters\nGlossary\nRevision History\nNext\nPrevious\nViewing, Editing, and Saving Images in an Image View\n\nThe IKImageView class displays a single image in a frame and optionally can allow a user to drag an image to it. It is similar to the NSImageView class, except that IKImageView supports any image file format that Quartz 2D and the Image I/O framework support, including Quartz images (CGImageRef), images that have metadata, and images whose location is specified as an NSURL object. In addition, the IKImageView class supports zoom, rotation, selection, cropping, and other image editing operations.\n\nThis chapter shows the user interface for an image view, the image edit panel (IKImageEditPanel class), and the save options accessory view (pane) (IKSaveOptions class). It provides instructions for creating an image editing application incrementally. You’ll see how to:\n\nView images in an image view\n\nSet up an image view to use the Image Edit panel\n\nSet up a pane for saving images in various formats\n\nAdd zoom controls\n\nAdd image editing tools\n\nAdd a Save panel that uses image saving options\n\nThe Image View User Interface\n\nThe image view (IKImageView) in the user interface looks similar to any view that contains an image. The image shown in Figure 2-1 could just as easily be in an NSView object, an NSImageView object, or a Carbon window.\n\nFigure 2-1  An image view\n\nAn IKImageView object has an important characteristic that no other image container has. When the user double-clicks an image in an image view, an Image Edit panel appears, as shown in Figure 2-2. The panel allows the user to make adjustments to the image, apply a number of effects, and view image metadata and properties. The Adjust pane provides the most commonly used image adjustments.\n\nFigure 2-2  The Adjust pane of the Image Edit panel\n\nThe Effects pane (shown in Figure 2-3) allows the user, with a single click, to preview and apply sharpening, blurring, or color filters.\n\nFigure 2-3  The Effects pane of the Image Edit panel\n\nThe Details pane displays all the metadata for an image, as well as a comprehensive list of image properties, as shown in Figure 2-4. If the image information is more than can fit in the pane, the Image Kit supplies scroll bars automatically.\n\nFigure 2-4  The Details pane of the Image Edit panel\n\nThere are very few tasks you need to perform for the Image Edit panel to appear. You need to set the appropriate image view state, set up a shared instance of the Image Edit panel (IKImageEditPanel), and set the data source. The Image Kit framework takes care of the rest—showing the panel, switching panes, responding to changes made by the user, fetching and displaying image information, and closing the Image Edit panel.\n\nThe image view supports the use of tools for moving zooming, cropping, rotating, and applying annotations to an image. You need to provide a user interface for zoom operation and tool selection. At a minimum, you need to set up a menu similar to what’s shown in Figure 2-5.\n\nFigure 2-5  A menu that supports zooming and tools\n\nIdeally, you would also provide controls in the image view window, as shown in Figure 2-6. The figure shows two segmented controls (created in Interface Builder using NSSegmentedControl controls), each of which uses custom icons that you provide.\n\nFigure 2-6  Controls that support zooming and tool selection\n\nThe image view keeps track of the zoom factor, the rotation angle, and whether the image can be edited. You need to respond to the user’s selection by setting the tool mode. The Image Kit framework takes care of animating move, zoom, and rotation operations; showing a rotation circle (see Figure 2-7); displaying crop and selection rectangles; and displaying an annotation area. You need to implement copying data to the pasteboard, cropping the image, and handling the text for the annotation area.\n\nFigure 2-7  The rotation user interface\n\nNo image viewing and editing application would be complete without the ability to save an edited image. The IKSaveOptions class provides an accessory view for an NSSavePanel object that allows the user to choose an image file format and to set options appropriate for that format. The user can choose from among a number of formats. The options appropriate for that format appear below the format. For example, for a TIFF format, the user can choose compression options (as shown in Figure 2-8).\n\nFigure 2-8  A Save As dialog with an accessory view (pane) for file format options\nViewing an Image in an Image View\n\nThis section shows how to open and display an image in an image view. You’ll set the image view options so that the Image Edit panel opens when the user double-clicks the image. First you’ll set up the Xcode project, the project files, and the controller interface. Then you’ll create the user interface in Interface Builder. Finally, you’ll add the necessary routines to the implementation file.\n\nSetting Up the Project, Project Files, and the Controller Interface\n\nFollow these steps to set up the project:\n\nOpen Xcode, choose File > New Project.\n\nChoose Cocoa Application and click Next.\n\nName the project My Image Viewer, and click Finish.\n\nChoose Project > Add to Project and add the Quartz and Quartz Core frameworks.\n\nFor details, see Using the Image Kit in Xcode.\n\nChoose Project > Add to Project, navigate to an image to use as a default image, and click Add.\n\nIn the sheet that appears, click Add.\n\nThis image appears in the view whenever the application launches.\n\nChoose File > New File.\n\nChoose Objective-C Class and click Next.\n\nName the file Controller.m and keep the option to create the header file. Then click Finish.\n\nIn the Controller.h file, import the Quartz framework by adding this statement just below the statement to import Cocoa:\n\n#import <Quartz/Quartz.h>\n\nAdd a directive for the IKImageView class:\n\n@class IKImageView;\n\nAdd instance variables to the Controller interface.\n\nYou need an image view and a window to contain the view. You’ll set these up later in Interface Builder.\n\nIBOutlet IKImageView *  mImageView;\n\n\nIBOutlet NSWindow *     mWindow;\n\nYou need to keep track of image properties and the uniform type identifier of the image in the view.\n\nNSDictionary * mImageProperties;\n\n\nNSString *     mImageUTType;\n\nSave and close the Controller.h file.\n\nIn the Controller.m file, import the Application Kit classes by adding this statement.\n\n#import <AppKit/AppKit.h>\n\nClose the Controller.m file.\n\nCreating the User Interface\n\nSet up the user interface in Interface Builder by following these steps:\n\nDouble-click the MainMenu.nib file (located in the Resources group) to open Interface Builder.\n\nChoose File > Synchronize With Xcode.\n\nDouble-click the Window icon in the nib document window.\n\nIn the Size inspector, set the width of the window to 800 and the height to 600.\n\nDrag a Image View from the Library to the window and resize the view to fit the window.\n\nTip: Type Image View in the search field to locate it easily.\n\nIn the Size inspector, the Autosizing springs should look as follows. If they don’t, set them so they do.\n\nDrag an Object (NSObject) from the Library to the nib document window.\n\nType Controller in the Name field of the Identity inspector and press Return.\n\nChoose Controller from the Class pop-up menu.\n\nControl-drag from the controller icon to the title bar of the window. Then click the mWindow outlet that appears in the connections panel.\n\nControl-drag from the controller icon to the IKImageView view. Then click the mImageView outlet that appears in the connections panel.\n\nControl-drag from the window icon to the controller icon. Then click the delegate outlet that appears in the connections panel.\n\nSave the nib file.\n\nAdding Routines to the Implementation File\n\nNow you’ll go back to Xcode to add code to implement the image viewer.\n\nOpen the Controller.m file.\n\nAdd an openImageURL: method to take care of opening images.\n\nThe easiest way of opening an image is to use the setImageWithURL: method. This method is best for RAW images.\n\n- (void)openImageURL: (NSURL*)url\n\n\n{\n\n\n     [mImageView setImageWithURL: url];\n\n\n     [mWindow setTitleWithRepresentedFilename: [url path]];\n\n\n}\n\nAn alternate implementation is to use the Quartz opaque data types CGImageRef and CGImageSourceRef and their associated functions to create an image source, extract an image, get the image properties, and set the image to the image view. If you are using a TIFF file that contains multiple images, you need this implementation to display any image other than the first one. That’s because setImageWithURL: displays only the first image of a multiple-image file.\n\n- (void)openImageURL: (NSURL*)url\n\n\n{\n\n\n    CGImageRef          image = NULL;\n\n\n    CGImageSourceRef    isr = CGImageSourceCreateWithURL( (CFURLRef)url, NULL);\n\n\n \n\n\n    if (isr)\n\n\n    {\n\n\n        image = CGImageSourceCreateImageAtIndex(isr, 0, NULL);\n\n\n        if (image)\n\n\n        {\n\n\n         mImageProperties = (NSDictionary*)CGImageSourceCopyPropertiesAtIndex(\n\n\n                isr, 0, (CFDictionaryRef)mImageProperties);\n\n\n        }\n\n\n        CFRelease(isr);\n\n\n    }\n\n\n \n\n\n    if (image)\n\n\n    {\n\n\n      [mImageView setImage: image\n\n\n          imageProperties: mImageProperties];\n\n\n \n\n\n     [mWindow setTitleWithRepresentedFilename: [url path]];\n\n\n      CGImageRelease(image);\n\n\n    }\n\n\n}\n\nAdd an awakeFromNib method.\n\nThis method first creates a URL for the default image file by getting the path to the resource in the bundle and then converting the path to a URL. After opening the URL, the method sets up the image view so that the Image Edit panel can open, and the image zooms to fit the view.\n\nMake sure that you substitute the appropriate string for “earring”, which should be the name of the default image file without its filename extension. Also, use the appropriate extension.\n\n- (void)awakeFromNib\n\n\n{\n\n\n    NSString *   path = [[NSBundle mainBundle] pathForResource: @\"earring\"\n\n\n                            ofType: @\"jpg\"];\n\n\n    NSURL *      url = [NSURL fileURLWithPath: path];\n\n\n \n\n\n    [self openImageURL: url];\n\n\n \n\n\n    // customize the IKImageView...\n\n\n    [mImageView setDoubleClickOpensImageEditPanel: YES];\n\n\n    [mImageView setCurrentToolMode: IKToolModeMove];\n\n\n    [mImageView zoomImageToFit: self];\n\n\n}\n\nOpen the Controller.h files and add the following method signature.\n\n- (void)openImageURL: (NSURL*)url;\n\nClick Build and Go.\n\nWhen the application launches, double-click the image to make sure that the Image Edit panel opens.\n\nIf the window opens, but an image does not appear, make sure that you’ve included the correct filename in your code.\n\nResize the image to make sure that it zooms to fit the size of the window.\n\nIf the image does not zoom to fit, check the connections between the window and the controller in Interface Builder.\n\nThe next section shows how to use the IKSaveOptions class to add an accessory view to the Save panel.\n\nSaving Images\n\nThe IKSaveOptions class handles image and PDF saving options. After creating an NSSavePanel object, you allocate and initialize a save options accessory view and then add the view to the Save dialog. Keep in mind that IKSaveOptions handles the options, but it does not actually save the image or PDF. After you set up the save options accessory panel, you also need to implement a save method.\n\nFollow these steps to add the image options accessory view to the My Image Viewer application:\n\nOpen the Controller.h file and add a save options instance variable.\n\nIKSaveOptions * mSaveOptions;\n\nAdd the following method signature:\n\n- (IBAction)saveImage: (id)sender;\n\nIn the Controller.m file, add a method that presents a Save panel with the save options accessory view (pane).\n\nThe method first creates an instance of the NSSavePanel class. Then it allocates a save options object and initializes it with the image properties and UT type of the image that will be saved. Next the code adds the save options view to the Save panel. It shows the Save panel as a sheet, providing a selector that is invoked when the Save panel terminates. (You’ll write that method next.)\n\n- (IBAction)saveImage: (id)sender\n\n\n{\n\n\n    NSSavePanel * savePanel = [NSSavePanel savePanel];\n\n\n    mSaveOptions = [[IKSaveOptions alloc]\n\n\n                        initWithImageProperties: mImageProperties\n\n\n                                    imageUTType: mImageUTType];\n\n\n    [mSaveOptions addSaveOptionsAccessoryViewToSavePanel: savePanel];\n\n\n \n\n\n    NSString * fileName = [[mWindow representedFilename] lastPathComponent];\n\n\n    [savePanel beginSheetForDirectory: NULL\n\n\n                    file: fileName\n\n\n          modalForWindow: mWindow\n\n\n           modalDelegate: self\n\n\n          didEndSelector: @selector(savePanelDidEnd:returnCode:contextInfo:)\n\n\n             contextInfo: NULL];\n\n\n \n\n\n \n\n\n}\n\nAdd a savePanelDidEnd method that performs the actual saving.\n\nIf the user clicks the Save button, the method obtains the filename and type and retrieves the image from the view. If the image exists, the method uses the CGImageDestinationRef opaque type (from the Image I/O framework) to create an image destination based on a URL representation of the file path. It saves the image with its properties, finalizes the destination, and releases the CGImageDestination object because it is no longer needed.\n\n- (void)savePanelDidEnd: (NSSavePanel *)sheet\n\n\n             returnCode: (int)returnCode\n\n\n            contextInfo: (void *)contextInfo\n\n\n{\n\n\n    if (returnCode == NSOKButton)\n\n\n    {\n\n\n        NSString * path = [sheet filename];\n\n\n        NSString * newUTType = [mSaveOptions imageUTType];\n\n\n        CGImageRef image;\n\n\n \n\n\n        image = [mImageView image];\n\n\n        if (image)\n\n\n        {\n\n\n            NSURL * url = [NSURL fileURLWithPath: path];\n\n\n            CGImageDestinationRef dest = CGImageDestinationCreateWithURL((CFURLRef)url,\n\n\n                         (CFStringRef)newUTType, 1, NULL);\n\n\n            if (dest)\n\n\n            {\n\n\n                CGImageDestinationAddImage(dest, image,\n\n\n                            (CFDictionaryRef)[mSaveOptions imageProperties]);\n\n\n                CGImageDestinationFinalize(dest);\n\n\n                CFRelease(dest);\n\n\n            }\n\n\n        } else\n\n\n        {\n\n\n            NSLog(@\"*** saveImageToPath - no image\");\n\n\n        }\n\n\n    }\n\n\n}\n\nSave the Controller.m file.\n\nOpen the MainMenu.nib file.\n\nDouble-click the MainMenu icon in the nib document window.\n\nOpen the File menu so you can see the menu items New, Open, and so on.\n\nControl-drag from the Save menu item to the controller.\n\nIn the connections panel for the controller, choose saveImage:.\n\nSave the nib file.\n\nIn Xcode, click Build and Go.\n\nAfter the application launches, choose File > Save. You should see a Save panel appear with an accessory view that looks similar to the following.\n\nChoose other items from the Format menu to see the options that are offered for each image format.\n\nSupporting Zooming\n\nTo support zooming, you need to add menu commands or controls (or both) for zooming, and a method to respond to the commands.\n\nTo support zooming, follow these steps:\n\nAdd the following method signature to the Controller.h file, then save the file:\n\n- (IBAction)doZoom: (id)sender;\n\nOpen the Controller.m file and add constants for the zoom factor.\n\n#define ZOOM_IN_FACTOR  1.414214 // doubles the area\n\n\n#define ZOOM_OUT_FACTOR 0.7071068 // halves the area\n\nAdd a method to handle commands to zoom out, zoom in, zoom to fit, and zoom to the actual size of the image.\n\n- (IBAction)doZoom: (id)sender\n\n\n{\n\n\n \n\n\n    NSInteger zoom;\n\n\n    CGFloat   zoomFactor;\n\n\n \n\n\n    if ([sender isKindOfClass: [NSSegmentedControl class]])\n\n\n        zoom = [sender selectedSegment];\n\n\n    else\n\n\n        zoom = [sender tag];\n\n\n \n\n\n    switch (zoom)\n\n\n    {\n\n\n        case 0:\n\n\n            zoomFactor = [mImageView zoomFactor];\n\n\n            [mImageView setZoomFactor: zoomFactor * ZOOM_OUT_FACTOR];\n\n\n            break;\n\n\n        case 1:\n\n\n            zoomFactor = [mImageView zoomFactor];\n\n\n            [mImageView setZoomFactor: zoomFactor * ZOOM_IN_FACTOR];\n\n\n            break;\n\n\n        case 2:\n\n\n            [mImageView zoomImageToActualSize: self];\n\n\n            break;\n\n\n        case 3:\n\n\n            [mImageView zoomImageToFit: self];\n\n\n            break;\n\n\n    }\n\n\n}\n\nOpen the MainMenu.nib file.\n\nDouble-click the MainMenu icon in the nib document window.\n\nCreate a new menu by dragging a Submenu Menu item from the Library and dropping it between the Edit and Format menu items.\n\nName the new menu Views.\n\nAdd menu items from the library to the Views menu so that you have enough for these zoom commands. Use the inspector to add the tags and key equivalents shown in the table.\n\nCommand\n\n\t\n\nTag\n\n\t\n\nKey equivalent\n\n\n\n\nZoom Out\n\n\t\n\n0\n\n\t\n\nCommand –\n\n\n\n\nZoom In\n\n\t\n\n1\n\n\t\n\nCommand =\n\n\n\n\nZoom to Actual Size\n\n\t\n\n2\n\n\t\n\n\n\n\nZoom to Fit\n\n\t\n\n3\n\n\t\n\nControl-drag from each item to the Controller icon. Then, in the connections panel for the controller, click doZoom:.\n\nSave the MainMenu.nib file.\n\nIn Xcode, click Build and Go. Then make sure that the zoom commands operate as expected.\n\nQuit the application and go back to the MainMenu.nib file in Interface Builder.\n\nThis time you’ll revise the window user interface by adding zoom controls.\n\nChange the image view size so that the top of the view is 50 pixels from the top of the window.\n\nYou’ll need this space to put zoom controls.\n\nDrag a segmented control from the Library to the top-left side of the window.\n\nIn the Attributes inspector for the control set the number of segments to 4.\n\nDouble-click each segment to change its label and to use the inspector to add a tag. Use the commands and tags shown previously for the Views menu.\n\nIf possible, you should add artwork, similar to the following, to the Xcode project so that you can use icons instead of text labels.\n\nIn the Size inspector, set the Control Size to small.\n\nIn the Attributes inspector, choose Select Any in the Mode pop-up menu.\n\nControl-drag from the segmented zoom control to the Controller icon. Then, in the connections panel for the controller, click doZoom:.\n\nClick the Info button in the nib document window. When the MainMenu.nib Info window opens, set the Deployment Target pop-up menu to OS X v10.5.\n\nSave the nib file.\n\nIn Xcode, click Build and Go. Then make sure that the zoom commands operate as expected.\n\nAdding Image Editing Tools\n\nThe IKImageView class has built-in support for move, select, crop, rotate, and annotate tools. To set up the image editing application to include the tools for these actions, follow these steps:\n\nIn Xcode, add image files (Project > Add to Project), similar to the following, for each of the tool modes—move, select, crop, rotate, and annotate.\n\nYou’ll need to use an application such as Adobe Illustrator to create icons. See OS X Human Interface Guidelines for information on using icons in the interface.\n\nAdd the following method signature to the Controller.h file:\n\n- (IBAction)switchToolMode: (id)sender;\n\nAdd a method to the Controller.m file to handle setting the tool mode.\n\n- (IBAction)switchToolMode: (id)sender\n\n\n{\n\n\n \n\n\n    NSInteger newTool;\n\n\n    if ([sender isKindOfClass: [NSSegmentedControl class]])\n\n\n        newTool = [sender selectedSegment];\n\n\n    else\n\n\n       newTool = [sender tag];\n\n\n \n\n\n    switch (newTool)\n\n\n    {\n\n\n        case 0:\n\n\n            [mImageView setCurrentToolMode: IKToolModeMove];\n\n\n            break;\n\n\n        case 1:\n\n\n            [mImageView setCurrentToolMode: IKToolModeSelect];\n\n\n            break;\n\n\n        case 2:\n\n\n            [mImageView setCurrentToolMode: IKToolModeCrop];\n\n\n            break;\n\n\n        case 3:\n\n\n            [mImageView setCurrentToolMode: IKToolModeRotate];\n\n\n            break;\n\n\n        case 4:\n\n\n            [mImageView setCurrentToolMode: IKToolModeAnnotate];\n\n\n            break;\n\n\n    }\n\n\n}\n\nSave the Controller.m file.\n\nOpen the MainMenu.nib file.\n\nAdd a submenu to the Views menu and name it Tools.\n\nAdd the following items to the Tools submenu, using the inspector to add the tag values.\n\nItem\n\n\t\n\nTag\n\n\n\n\nMove\n\n\t\n\n0\n\n\n\n\nSelect\n\n\t\n\n1\n\n\n\n\nCrop\n\n\t\n\n2\n\n\n\n\nRotate\n\n\t\n\n3\n\n\n\n\nAnnotate\n\n\t\n\n4\n\nControl-drag from the Move menu item to the Controller icon. Choose switchToolMode: in the connections panel for the controller.\n\nRepeat the previous step for each of the other items in the Tools menu.\n\nAdd a segmented control to the top-right side of the window so the control is aligned with the zoom controls.\n\nIn the Attributes inspector, set the number of segments to 5.\n\nIn the Size inspector, set the Size to small and the Width to 60.\n\nSwitch to the Attributes inspector.\n\nFor each segment, drag the appropriate icon from the Media pane of the Library, and set the segment to autosize.\n\nFrom left to right, the icons should represent Move, Select, Crop, Annotate, and Rotate.\n\nChoose Select One from the Mode pop-up menu.\n\nMake sure that Selected is checked for Segment 1.\n\nIn the Size inspector set the Autosizing struts and springs so they look as follows:\n\nControl-drag from the segmented control to the controller and connect to the switchToolMode: action.\n\nSave the nib file.\n\nIn Xcode, click Build and Go. Then test the Tools menu and the toolbar.\n\nThe move and rotate tools operate without any additional code on your part. The selection and crop tools copy the selected areas to the pasteboard. You need to provide code that implements pasting from the pasteboard. The annotate tool simply shows a colored circle. You need to write code that supports text entry and editing, and saves the annotation.\n\nSupporting Opening Image Files\n\nAs it is now, your application opens a default image file. It would be greatly improved if it allowed the user to choose an image other than the default. Next you’ll write an open image method that is invoked when the user chooses File > Open. You need to provide a selector that is invoked when the Open panel closes. If the user chooses an image, your openImageURL: method (which you already added to the application) is invoked.\n\nFollow these steps to support letting the user open image files:\n\nAdd the following method signature to the Controller.h file:\n\n- (IBAction)openImage: (id)sender;\n\nWrite an open image method.\n\nThe method creates an instance of the NSOpenPanel class and defines the allowable filename extensions. It then calls a method that shows the Open panel.\n\n- (IBAction)openImage: (id)sender\n\n\n{\n\n\n    NSOpenPanel * openPanel = [NSOpenPanel openPanel];\n\n\n    NSString *    extensions = @\"tiff/tif/TIFF/TIF/jpg/jpeg/JPG/JPEG/pdf/PDF\";\n\n\n    NSArray *     types = [extensions pathComponents];\n\n\n \n\n\n    [openPanel beginSheetForDirectory: NULL\n\n\n                                 file: NULL\n\n\n                                types: types\n\n\n                       modalForWindow: mWindow\n\n\n                        modalDelegate: self\n\n\n                       didEndSelector: @selector(openPanelDidEnd:returnCode:contextInfo:)\n\n\n                          contextInfo: NULL];\n\n\n \n\n\n}\n\nWrite a selector method that’s invoked when the Open panel terminates.\n\nIf the user chooses an image, this method calls the openImageURL: method, passing to it the first item in the selected path.\n\n- (void)openPanelDidEnd: (NSOpenPanel *)panel\n\n\n             returnCode: (int)returnCode\n\n\n            contextInfo: (void  *)contextInfo\n\n\n{\n\n\n    if (returnCode == NSOKButton)\n\n\n    {\n\n\n        [self openImageURL: [[panel URLs] objectAtIndex: 0]];\n\n\n    }\n\n\n}\n\nSave the Controller.m file.\n\nDouble-click the MainMenu icon in the nib document window.\n\nClick the File menu so you can see the items in it—New, Open, and so on.\n\nControl-drag from the Open menu item to the controller and connect to the openImage: action in the connections panel.\n\nSave the nib file.\n\nIn Xcode, click Build and Go.\n\nChoose File > Open and make sure that you can open an image.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2008 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2008-06-09\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Glossary",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageKitProgrammingGuide/Glossary/Glossary.html#//apple_ref/doc/uid/TP40004907-CH9-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nImageKit Programming Guide\nTable of Contents\nIntroduction\nBasics of Using the Image Kit\nViewing, Editing, and Saving Images in an Image View\nBrowsing Images\nShowing Slides\nTaking Snapshots and Setting Pictures\nBrowsing Filters and Setting Input Parameters\nGlossary\nRevision History\nNext\nPrevious\nGlossary\nCore Image  \n\nAn image processing programming interface, introduced in OS X v10.4, that is part of the Quartz Core framework.\n\n\n\nfilter  \n\nCode that uses Core Image to process digital images.\n\n\n\nfilter browser  \n\nThe user term for the Image Kit filter browser panel class (IKFilterBrowserPanel) which allows users to browse Core Image filters.\n\n\n\nimage browser view  \n\nThe Image Kit view class (IKImageBrowserView) that is optimized for browsing images. The user term is image browser.\n\n\n\nImage Edit panel  \n\nThe Image Kit edit panel class (IKImageEditPanel) that is optimized for editing images. The user term is Image Edit window.\n\n\n\nImage I/O  \n\nA framework (ImageIO.framework) that provides opaque data types for reading data from an image source and writing data to an image destination.\n\n\n\nImage Kit  \n\nA programming interface that supports browsing, viewing, and editing images and browsing and controlling Core Image filters.\n\n\n\nimage unit  \n\nA Core Image filter that is packaged for distribution as an NSBundle object\n\n\n\npicture taker panel  \n\nThe Image Kit picture taker class (IKPictureTaker) that allows users to choose images by browsing the file system or by taking a snapshot with an iSight or other digital camera. The user term is picture taker.\n\n\n\nQuick Look framework  \n\nA framework, introduced in OS X v10.5, that provides a plug-in architecture for custom document types. A Quick Look document type can be displayed as a preview in the Finder and as an item in such applications as iPhoto or any application that supports slideshows created with the IKSlideshow class.\n\n\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2008 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2008-06-09\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Browsing Images",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageKitProgrammingGuide/ImageBrowser/ImageBrowser.html#//apple_ref/doc/uid/TP40004907-CH5-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nImageKit Programming Guide\nTable of Contents\nIntroduction\nBasics of Using the Image Kit\nViewing, Editing, and Saving Images in an Image View\nBrowsing Images\nShowing Slides\nTaking Snapshots and Setting Pictures\nBrowsing Filters and Setting Input Parameters\nGlossary\nRevision History\nNext\nPrevious\nBrowsing Images\n\nApplications such as iPhoto provide a rich user experience for viewing digital image collections. By using the IKImageBrowserView class and its related protocols—IKImageBrowserDataSource, IKImageBrowserDelegate, and IKImageBrowserItem—any application can support browsing large numbers of images efficiently.\n\nThis chapter describes the user interface provided by the IKImageBrowserView class, discusses the related protocols, lists the image representation types that the browser can display, and includes step-by-step instructions for creating an application that uses the image browser.\n\nThe Image Browser User Interface\n\nThe IKImageBrowserView class provides a view that displays an array of images. The size of each image depends on the zoom value that you set for the image browser view. Typically, an application provides a control, such as the slider shown in Figure 3-1, that allows the user to control the zoom value.\n\nFigure 3-1  The image browser view\n\nWhen fully zoomed in, the image browser displays a single image, as shown in Figure 3-2.\n\nFigure 3-2  The image browser zoomed\n\nYou can set up the image browser view to support dragging images into the window to add them as well as dragging images within the window to rearrange them. In addition to drag and drop, your application should support adding photos through the NSOpenPanel class, as shown in Figure 3-3.\n\nFigure 3-3  Adding images\n\nThe IKImageBrowserView class also supports scrolling (see Figure 3-2), which is necessary for large image sets or when the user has the zoom value set such that the image set doesn’t fit in the view. Your application needs to take one of the following approaches:\n\nEmbed the image browser in an NSScrollView object, which is the typical approach.\n\nConnect the image browser to an NSScroller object. This approach is a bit more efficient but doesn’t allow auto-hiding of the scrollers.\n\nThe Requirements of an Image Browser Application\n\nThis section gives an overview of the Image Kit classes and protocols needed to write a full-featured image browser application and describes how such an application works. After reading this section you’ll be ready to read the rest of this chapter, which provides step-by-step instructions for these programming tasks:\n\nDisplaying Images in an Image Browser\n\nSupporting Zoom\n\nSupporting Removing and Reordering Items\n\nSupporting Drag and Drop\n\nSetting Browser and Cell Appearance\n\nThe Image Kit Classes and Protocols for an Image Browser Application\n\nAn image browser application has a number of requirements. In addition to creating an instance of the IKImageBrowserView class, you’ll need to provide a data source that provides the items to show in the image browser. You’ll also need to implement the required methods of the IKImageBrowserDataSource and IKImageBrowserItem protocols.\n\nThe IKImageBrowserDataSource protocol defines methods for accessing the data source. You use this protocol to add and remove items, to move items, and to obtain information about individual items or groups. Your application must implement two methods:\n\nnumberOfItemsInImageBrowser: returns the number of items in the image browser.\n\nimageBrowser:itemAtIndex:returns the object located at a specified index. The returned object must implement the required methods of the IKImageBrowserItem protocol.\n\nMethods that support editing or that return group information are optional. See IKImageBrowserDataSource Protocol Reference for details.\n\nThe IKImageBrowserItem protocol defines required and optional methods that the browser view uses to access a particular item from the data source. You must implement three methods:\n\nimageRepresentationType returns the image representation type. An image browser view can handle a variety of image representations. You must specify which representation type the image browser view uses by returning the appropriate constant. Table 3-1 lists the constants.\n\nimageUID returns a string that uniquely identifies the data source item, such as the string that represents the path to the item.\n\nimageRepresentation returns the representation (such as the path to an image) for an item in the image browser view. The representation must match the image representation type.\n\nYou can optionally implement methods that return the image version, title, subtitle, and whether or not the item can be selected. See IKImageBrowserItem Protocol Reference for details.\n\nTable 3-1  Image representations and types\n\nImage representation constants\n\n\t\n\nImage representation data types\n\n\n\n\nIKImageBrowserPathRepresentationType\n\n\t\n\nA path (NSString object)\n\n\n\n\nIKImageBrowserNSURLRepresentationType\n\n\t\n\nNSURL object\n\n\n\n\nIKImageBrowserNSImageRepresentationType\n\n\t\n\nNSImage object\n\n\n\n\nIKImageBrowserCGImageRepresentationType\n\n\t\n\nCGImage object\n\n\n\n\nIKImageBrowserCGImageSourceRepresentationType\n\n\t\n\nCGImageSource object\n\n\n\n\nIKImageBrowserNSDataRepresentationType\n\n\t\n\nNSData object\n\n\n\n\nIKImageBrowserNSBitmapImageRepresentationType\n\n\t\n\nNSBitmapImageRep object\n\n\n\n\nIKImageBrowserQTMovieRepresentationType\n\n\t\n\nQTMovie object\n\n\n\n\nIKImageBrowserQTMoviePathRepresentationType\n\n\t\n\nA path (NSString object) to a QTMovie object\n\n\n\n\nIKImageBrowserQCCompositionRepresentationType\n\n\t\n\nQCComposition object\n\n\n\n\nIKImageBrowserQCCompositionPathRepresentationType\n\n\t\n\nA path (NSString object) to a QCComposition object\n\n\n\n\nIKImageBrowserQuickLookPathRepresentationType\n\n\t\n\nA path (NSString object) to load an object using the Quick Look framework\n\n\n\n\nIKImageBrowserIconRefRepresentationType\n\n\t\n\nIconRef object\n\n\n\n\nIKImageBrowserIconRefPathRepresentationType\n\n\t\n\nA path (NSString object) to an IconRef object\n\nThe IKImageBrowserDelegate informal protocol defines methods that respond to user events, such as a selection. See IKImageBrowserDelegate Protocol Reference for a description of the delegate methods.\n\nHow the Sample Image Browser Application Works\n\nWhen the sample application that you’ll build in the rest of this chapter launches, it allocates two mutable arrays, one that represents images that are displayed in the image browser view (image array) and another that represents images that need to be imported into the image browser view (import images array). Initially, both arrays are empty. The application then performs any setup work for the image browser view, such as setting the style of the cells and whether the movement of items is animated. After the setup is complete, the window with the image browser view opens without any images. The image browser view has a vertical scroll bar. The window has a button for importing images.\n\nClicking the Import Images button invokes an action to add images to the image browser view. The application uses an instance of the NSOpenPanel class to solicit a selection from the user. If the user makes a selection, the Open panel returns the selection as a path. This application allows users to choose either a file or a folder. A folder can contain files, other folders, or both files and folders. However, files are the only items that are added to the image browser view, which means that the application must traverse all paths until each resolves to a single file.\n\nThe application creates a data source object—an image object—for each file. The object has one instance variable, the path to the file (represented as an NSString object). Each image object is appended to the import images array. After all the objects are added to that array, the application appends that array to the images array. The application then removes all objects from the import images array so that the array can be ready to import more images should the user choose to do so. Finally the application calls the reloadData method of the IKImageBrowserView class to populate the image browser view with the images represented by the import images array.\n\nThe image browser application has five major parts to its implementation:\n\nThe nib file. You need to add an Image Browser view (IKImageBrowserView class) to a window. You’ll add a scroll view for the vertical scroller, and a button for importing images.\n\nThe window controller. This is the main class. You create it in Xcode and then instantiate it in Interface Builder. The window controller is the data source of the image browser view. The controller manages the mutable array for the image objects that represent the items in the image browser view and the mutable array for imported image objects. In Interface Builder, you’ll need to make the appropriate connections between the image browser view and the window controller.\n\nThe IKImageBrowserDataSource protocol. You need to implement the required methods for this protocol: numberOfItemsInImageBrowser: and imageBrowser:itemAtIndex:.\n\nThe IKImageBrowserItem protocol. You need to declare an interface for an image object that represents a single item in the image browser view. In the implementation for the image object, you need to provide the required methods for this protocol: imageUID, imageRepresentationType, and imageRepresentation.\n\nImage importing. You need to set up an Open panel and write code that traverses all folder paths until all paths are resolved to individual items.\n\nThe sections that follow provide step-by-step instructions for writing the image browser application.\n\nDisplaying Images in an Image Browser\n\nThis section shows how to create an application that displays images in an image browser. First, you’ll set up the Xcode project, the project files, and the interface for the image browser controller. Then you’ll add routines to the implementation. Finally, you’ll set up the user interface in Interface Builder.\n\nSetting Up the Project, Project Files, and the Controller Interface\n\nFollow these steps to set up the project:\n\nOpen Xcode and create a Cocoa application named Browse Images.\n\nAdd the Quartz framework.\n\nFor more details, seeUsing the Image Kit in Xcode.\n\nChoose File > New File.\n\nChoose “Objective-C Class NSWindowController subclass” and click Next.\n\nName the file ImageBrowserController.m and keep the option to create the header file. Then click Finish.\n\nIn the ImageBrowserController.h file, import the Quartz framework by adding this statement:\n\n#import <Quartz/Quartz.h>\n\nAdd IBOutlet outlet for the image browser and two mutable arrays .\n\nOne array will hold paths to images that are currently displayed in the browser. The other array will hold paths to images that are about to be imported into the browser.\n\n@interface ImageBrowserController : NSWindowController {\n\n\n \n\n\n    IBOutlet id mImageBrowser;\n\n\n    NSMutableArray * mImages;\n\n\n    NSMutableArray * mImportedImages;\n\n\n}\n\n\n@end\n\nAdd the following method signature:\n\n- (IBAction) addImageButtonClicked:(id) sender;\n\nSave the ImageBrowserController.h file.\n\nAdding Routines to the Implementation File\n\nFollow these steps to implement the main tasks of the image browser application:\n\nOpen the ImageBrowserController.m file.\n\nIn the implementation file, add an awakeFromNib method.\n\nThis method allocates and initializes the images and imported images arrays. For visual interest, the method also sets the image browser to animate as it updates.\n\n- (void) awakeFromNib\n\n\n{\n\n\n    mImages = [[NSMutableArray alloc] init];\n\n\n    mImportedImages = [[NSMutableArray alloc] init];\n\n\n \n\n\n    [mImageBrowser setAnimates:YES];\n\n\n}\n\nAdd a method for updating the data source for the image browser controller.\n\nThis method needs to add the recently imported items to the image objects array, then empty the imported images array. Finally it reloads the image browser, which causes the image browser to update the display with the recently added images.\n\n- (void) updateDatasource\n\n\n{\n\n\n    [mImages addObjectsFromArray:mImportedImages];\n\n\n    [mImportedImages removeAllObjects];\n\n\n    [mImageBrowser reloadData];\n\n\n}\n\nImplement the two required methods of the image browser data source protocol.\n\nThe number of items in the image browser is simply the number of items in the images array. An item’s index is simply its index in the images array.\n\n- (int) numberOfItemsInImageBrowser:(IKImageBrowserView *) view\n\n\n{\n\n\n    return [mImages count];\n\n\n}\n\n\n \n\n\n- (id) imageBrowser:(IKImageBrowserView *) view itemAtIndex:(int) index\n\n\n{\n\n\n    return [mImages objectAtIndex:index];\n\n\n}\n\nIn the IKImageBrowserController.m file, immediately after the import statement, create an interface for a data source object.\n\nThe data source object defines one instance variable, a string that is a path to an item to display.\n\n@interface MyImageObject : NSObject{\n\n\n    NSString * mPath;\n\n\n}\n\n\n@end\n\nIn the same file, create the implementation for the data source object MyImageObject. In the next three steps, you’ll be entering methods between the the following two statements, as you would normally for a Cocoa class.\n\n@implementation MyImageObject\n\n\n // Methods here\n\n\n@end\n\nWrite a method that sets the path object.\n\n- (void) setPath:(NSString *) path\n\n\n{\n\n\n    if(mPath != path){\n\n\n        mPath = path;\n\n\n    }\n\n\n}\n\nImplement the three required methods of the IKImageBrowserItem protocol.\n\nThe data source is a file path representation and its unique identifier is the path itself.\n\n- (NSString *)  imageRepresentationType\n\n\n{\n\n\n    return IKImageBrowserPathRepresentationType;\n\n\n}\n\n\n \n\n\n- (id)  imageRepresentation\n\n\n{\n\n\n    return mPath;\n\n\n}\n\n\n \n\n\n- (NSString *) imageUID\n\n\n{\n\n\n    return mPath;\n\n\n}\n\nNow that you are done with the MyImageObject implementation, you need to add a method to the ImageBrowserController implementation. This method is invoked when the user clicks an Import Images button.\n\nThis method calls a routine that displays the Open panel and returns the path chosen by the user. If there is a path, the method sets up an independent thread, calling the addImagesWithPaths: method, which you’ll write next. You’ll write an open files routine later. Check to make sure that you add the following method in the ImageBrowserController implementation,\n\n- (IBAction) addImageButtonClicked:(id) sender\n\n\n{\n\n\n    NSArray *path = openFiles();\n\n\n \n\n\n    if(!path){\n\n\n        NSLog(@\"No path selected, return...\");\n\n\n        return;\n\n\n    }\n\n\n   [NSThread detachNewThreadSelector:@selector(addImagesWithPaths:) toTarget:self withObject:path];\n\n\n}\n\nWrite a method that adds an array of paths to the data source.\n\nThe method parses all paths in the paths array and adds them to a temporary array. (You’ll write the addImagesWithPath: method in the next step. Note this is path, singular.) It then updates the data source in the main thread.\n\nAdd this method after the addImageButtonClicked: method.\n\n- (void) addImagesWithPaths:(NSArray *) paths\n\n\n{\n\n\n    int i, n;\n\n\n \n\n\n \n\n\n    n = [paths count];\n\n\n    for(i=0; i<n; i++){\n\n\n        NSString *path = [paths objectAtIndex:i];\n\n\n        [self addImagesWithPath:path recursive:NO];\n\n\n    }\n\n\n \n\n\n    [self performSelectorOnMainThread:@selector(updateDatasource)\n\n\n                           withObject:nil\n\n\n                        waitUntilDone:YES];\n\n\n \n\n\n}\n\nWrite the method that adds images at a path.\n\nThis method checks to see if the path identifies a directory or a file. If the path is a directory, the code parses the directory content and calls the addImagesAtPath: method. If the path is to a file, the code calls a method that adds a single image to the imported images array. You’ll write the addAnImageWithPath: method next.\n\nNote this method has an option to enable or disable recursion. Enabling recursion allows you to traverse nested folders to retrieve the individual items in each folder.\n\nAdd this method before the addImagesWithPaths: method.\n\n- (void) addImagesWithPath:(NSString *) path recursive:(BOOL) recursive\n\n\n{\n\n\n    int i, n;\n\n\n    BOOL dir;\n\n\n \n\n\n    [[NSFileManager defaultManager] fileExistsAtPath:path isDirectory:&dir];\n\n\n    if(dir){\n\n\n        NSArray *content = [[NSFileManager defaultManager]\n\n\n                              directoryContentsAtPath:path];\n\n\n        n = [content count];\n\n\n       for(i=0; i<n; i++){\n\n\n            if(recursive)\n\n\n               [self addImagesWithPath:\n\n\n                     [path stringByAppendingPathComponent:\n\n\n                            [content objectAtIndex:i]]\n\n\n                            recursive:NO];\n\n\n            else\n\n\n              [self addAnImageWithPath:\n\n\n                     [path stringByAppendingPathComponent:\n\n\n                           [content objectAtIndex:i]]];\n\n\n        }\n\n\n    }\n\n\n    else\n\n\n        [self addAnImageWithPath:path];\n\n\n}\n\n\n \n\nWrite a method that adds a single image to the imported images array.\n\nThe method creates an image object and adds the path to the object. The code then adds the image object to the imported images array.\n\nAdd this method before the addImagesWithPath:recursive: method.\n\n- (void) addAnImageWithPath:(NSString *) path\n\n\n{\n\n\n    MyImageObject *p;\n\n\n \n\n\n    p = [[MyImageObject alloc] init];\n\n\n    [p setPath:path];\n\n\n    [mImportedImages addObject:p];\n\n\n}\n\nWrite an open files routine that displays an NSOpenPanel object and retrieves the path chosen by the user.\n\nPlace this routine at the top of the ImageBrowserController.m file, immediately after the import statement.\n\nstatic NSArray *openFiles()\n\n\n{\n\n\n    NSOpenPanel *panel;\n\n\n \n\n\n    panel = [NSOpenPanel openPanel];\n\n\n    [panel setFloatingPanel:YES];\n\n\n    [panel setCanChooseDirectories:YES];\n\n\n    [panel setCanChooseFiles:YES];\n\n\n    int i = [panel runModal];\n\n\n    if(i == NSOKButton){\n\n\n        return [panel URLs];\n\n\n    }\n\n\n \n\n\n    return nil;\n\n\n}\n\nBuild the Project.\n\nThis ensures that Interface Builder detects the action that you added.\n\nSave the IKImageBrowserController.m file.\n\nCreating the User Interface\n\nSet up the user interface in Interface Builder by following these steps:\n\nDouble-click the MainMenu.nib file to open Interface Builder.\n\nChoose File > Synchronize With Xcode.\n\nIf it’s not already open, double-click the Window icon in the nib document window.\n\nDrag a Image Browser View from the Library to the window and resize the view so that you leave space at the bottom of the window for a button.\n\nIn the Size inspector for the view, make sure that the Autosizing springs and struts look as follows:\n\nChoose Layout > Embed Objects In > Scroll View.\n\nIn the Scroll View Attributes inspector, leave Show Vertical Scroller selected but deselect Show Horizontal Scroller.\n\nThis will allow users to scroll easily through large numbers of images.\n\nIn the Size pane, set the Autosizing springs and struts so they look the same as those shown in Step 5.\n\nDrag an Object (NSObject) from the Library to the nib document window.\n\nIn the Identity inspector, choose ImageBrowserController from the Class pop-up menu.\n\nControl-drag from the ImageBrowserController icon to the title bar of the window. Then connect to window in the connections panel.\n\nControl-drag from the ImageBrowserController icon to the IKImageBrowserView view. Then connect to mImageBrowser in the connections panel.\n\nControl-drag from the IKImageBrowserView view to the ImageBrowserController icon. Then connect to _dataSource in the connections panel.\n\nControl-drag from the window icon to the controller icon. Then connect to delegate in the connections panel.\n\nDrag a Push Button from the library to the lower right portion of the window and label it Import Images.\n\nIn the Size inspector, set Autosizing to have outer struts on the left and bottom.\n\nControl-drag from the Import Images button to the the ImageBrowserController icon and connect to the addImageButtonClicked: action in the connections panel.\n\nSave the nib file.\n\nIn Xcode, click Build and Go.\n\nClick the Import Images button and make sure that the image browser works.\n\nSupporting Zoom\n\nNext you’ll add the ability for the user to zoom images. You’ll define zoom factors, add controls to the interface, and then add a zoom method that’s invoked by the controls.\n\nTo add the ability for users to zoom images in an image browser:\n\nAdd a zoom method.\n\nThis method responds to the zoom controls in the user interface. You’ll add the controls later. The method needs to set the zoom value and then signal the need to update the browser display.\n\n- (IBAction) zoomSliderDidChange:(id)sender\n\n\n{\n\n\n    [mImageBrowser setZoomValue:[sender floatValue]];\n\n\n    [mImageBrowser setNeedsDisplay:YES];\n\n\n}\n\nAdd the method signature to the ImageBrowserController.h file.\n\n- (IBAction) zoomSliderDidChange:(id)sender;\n\nSave the ImageBrowserController.h and ImageBrowserController.m files.\n\nDouble-click the MainMenu.nib file to open Interface Builder.\n\nDrag a Horizontal Slider from the Library to the browser window and position it in the lower left of the window.\n\nIn the Size inspector for the slider and set the Autosizing struts appropriately.\n\nSet the size to Mini.\n\nIn the Attributes inspector set the State to Continuous to cause the action method to send its state continuously while the mouse is down.\n\nSet the slider minimum and maximum values.\n\nEnter 0 for the minimum value and 1.0 for the maximum value.\n\nControl-drag from the slider to the ImageBrowserController icon and in the connections panel choose zoomSliderDidChange:.\n\nSave the MainMenu.nib file.\n\nIn Xcode, click Build and Go.\n\nTry the zoom controls and make sure they work.\n\nSupporting Removing and Reordering Items\n\nThe image browser will be far more useful if users can remove items and reorder them. You need to set the option to allow reordering. Then you need to implement the methods defined by the IKImageBrowserDataSource protocol that support editing items.\n\nFollow these steps to support removing and reordering:\n\nOpen the ImageBrowserController.m file.\n\nImplement the IKImageBrowserDataSource protocol method for removing items.\n\n- (void) imageBrowser:(IKImageBrowserView *) view removeItemsAtIndexes: (NSIndexSet *) indexes\n\n\n{\n\n\n    [mImages removeObjectsAtIndexes:indexes];\n\n\n}\n\nImplement the IKImageBrowserDataSource protocol method to move items from one location to another.\n\nThis method first removes items from the data source and stores them temporarily in an array. Then it inserts the removed items into the images array at the new location.\n\n- (BOOL) imageBrowser:(IKImageBrowserView *) view  moveItemsAtIndexes: (NSIndexSet *)indexes toIndex:(unsigned int)destinationIndex\n\n\n{\n\n\n      int index;\n\n\n      NSMutableArray *temporaryArray;\n\n\n \n\n\n      temporaryArray = [[NSMutableArray alloc] init];\n\n\n      for(index=[indexes lastIndex]; index != NSNotFound;\n\n\n                         index = [indexes indexLessThanIndex:index])\n\n\n      {\n\n\n          if (index < destinationIndex)\n\n\n              destinationIndex --;\n\n\n \n\n\n          id obj = [mImages objectAtIndex:index];\n\n\n          [temporaryArray addObject:obj];\n\n\n          [mImages removeObjectAtIndex:index];\n\n\n      }\n\n\n \n\n\n      // Insert at the new destination\n\n\n      int n = [temporaryArray count];\n\n\n      for(index=0; index < n; index++){\n\n\n          [mImages insertObject:[temporaryArray objectAtIndex:index]\n\n\n                        atIndex:destinationIndex];\n\n\n      }\n\n\n \n\n\n      return YES;\n\n\n}\n\nIn the awakeFromNib method, set the image browser to allow reordering.\n\nAfter modification, the method should look as follows:\n\n- (void) awakeFromNib\n\n\n{\n\n\n    mImages = [[NSMutableArray alloc] init];\n\n\n    mImportedImages = [[NSMutableArray alloc] init];\n\n\n \n\n\n    [mImageBrowser setAllowsReordering:YES];\n\n\n    [mImageBrowser setAnimates:YES];\n\n\n \n\n\n}\n\nIn Xcode, click Build and Go.\n\nAdd images to the browser. Then try deleting a few items. Select several items and move them to a new location.\n\nNote: If you want to support dragging items, you should also implement imageBrowser:writeItemsAtIndexes:toPasteboard:.\n\nSupporting Drag and Drop\n\nIt is convenient for users to be able to drag items directly to the browser. Drag and drop requires that you set a dragging destination delegate and implement three methods of the NSDraggingDestination protocol. (See Drag and Drop Programming Topics.)\n\nTo support the ability for users to drag and drop images, follow these steps:\n\nIn the awakeFromNib method, set the dragging destination delegate for the image browser.\n\nAfter modification, the method should look as follows:\n\n- (void) awakeFromNib\n\n\n{\n\n\n    mImages = [[NSMutableArray alloc] init];\n\n\n    mImportedImages = [[NSMutableArray alloc] init];\n\n\n    [mImageBrowser setAllowsReordering:YES];\n\n\n    [mImageBrowser setAnimates:YES];\n\n\n    [mImageBrowser setDraggingDestinationDelegate:self];\n\n\n}\n\nImplement the performDragOperation: method of the NSDraggingDestination protocol.\n\nThis method looks for paths in the pasteboard. If there are paths, the method retrieves them, adds them to the data source, then reloads the image browser.\n\n- (BOOL) performDragOperation:(id <NSDraggingInfo>)sender\n\n\n{\n\n\n    NSData *data = nil;\n\n\n    NSString *errorDescription;\n\n\n    NSPasteboard *pasteboard = [sender draggingPasteboard];\n\n\n \n\n\n    if ([[pasteboard types] containsObject:NSFilenamesPboardType])\n\n\n        data = [pasteboard dataForType:NSFilenamesPboardType];\n\n\n    if(data){\n\n\n        NSArray *filenames = [NSPropertyListSerialization\n\n\n            propertyListFromData:data\n\n\n                mutabilityOption:kCFPropertyListImmutable\n\n\n                          format:nil\n\n\n                errorDescription:&errorDescription];\n\n\n        int i;\n\n\n        int n = [filenames count];\n\n\n        for(i=0; i<n; i++){\n\n\n            [self addImagesWithPath:[filenames objectAtIndex:i] recursive:NO];\n\n\n        }\n\n\n        [self updateDatasource];\n\n\n    }\n\n\n \n\n\n    return YES;\n\n\n}\n\nImplement the draggingEntered: method of the NSDraggingDestination protocol.\n\n- (NSDragOperation)draggingEntered:(id <NSDraggingInfo>)sender\n\n\n{\n\n\n    return NSDragOperationCopy;\n\n\n}\n\nImplement the draggingUpdated: method of the NSDraggingDestination protocol.\n\n- (NSDragOperation)draggingUpdated:(id <NSDraggingInfo>)sender\n\n\n{\n\n\n    return NSDragOperationCopy;\n\n\n}\n\nIn Xcode, click Build and Go.\n\nDrag some images to the image browser. Then add some more images. Note that images dragged to the browser are added at the end.\n\nSetting Browser and Cell Appearance\n\nThe IKImageBrowserView class provides several methods that control the image browser and browser item appearance. You can set the display style for cells so that the cells are shadowed, outlined, or appear with a title or subtitle. You can specify whether to constrain the size of items to their original size. You can also set the background color of the image browser and a number of other options, all of which are detailed in IKImageBrowserView Class Reference.\n\nYou set cell appearance by including the appropriate statements in the awakeFromNib: method, such as:\n\n [mImageBrowser setCellsStyleMask:IKCellsStyleOutlined | IKCellsStyleShadowed];\n\n\n [mImageBrowser setConstrainsToOriginalSize:YES];\n\nIf you support groups, you can set bezel and disclosure styles on a per-group basis—the group style is not a property of the image browser.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2008 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2008-06-09\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Showing Slides",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageKitProgrammingGuide/Slideshows/Sildeshows.html#//apple_ref/doc/uid/TP40004907-CH6-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nImageKit Programming Guide\nTable of Contents\nIntroduction\nBasics of Using the Image Kit\nViewing, Editing, and Saving Images in an Image View\nBrowsing Images\nShowing Slides\nTaking Snapshots and Setting Pictures\nBrowsing Filters and Setting Input Parameters\nGlossary\nRevision History\nNext\nPrevious\nShowing Slides\n\nSeveral applications provided with OS X have a slideshow feature built in, including Mail and Preview. Starting with OS X v10.5, any application can provide slideshow support by using the IKSlideshow class. In addition to digital images, a slideshow can display pages from PDF documents, QuickTime movies, Quartz Composer compositions, and custom document formats supported through the Quick Look framework.\n\nThis chapter describes the user interface provided by the IKSlideshow class and provides step-by-step instructions for creating a slideshow application.\n\nThe Slideshow User Interface\n\nWhen a slideshow runs, by default it opens to the first image provided by the slideshow data source. The slideshow uses the entire screen, as shown in Figure 4-1. The controls at the bottom of the screen allow the user to move forward and backward, pause and play, view an index layout of all items in the show, fit the images to the screen, and close the slideshow.\n\nFigure 4-1  A slideshow with controls\n\nThe user can override the automatic advance by clicking the pause control or by manually moving through the show using the onscreen controls or the arrow keys on the keyboard.\n\nThe index sheet (see Figure 4-2) lets the user see many slides at the same time and to move through them using the arrow keys or by clicking an image. If the user double-clicks an image or presses Return, the slideshow resumes, starting with the selected image.\n\nFigure 4-2  The index sheet\nWriting a Slideshow Application\n\nThis section describes how to write an application that supports slideshows. First you’ll see what needs to be done to support slideshows in any application. Then you’ll take a look at a specific implementation of a simple slideshow application and the steps required to write the application.\n\nA slideshow application requires a shared instance of the IKSlideshow class and a data source. You obtain the shared instance using the method sharedSlideshow. You run the slideshow using the method runSlideshowWithDataSource:options:.\n\nYou must implement two methods of the IKSlideshowDataSource protocol:\n\nnumberOfSlideshowItems returns the number of items in the slideshow.\n\nslideshowItemAtIndex: returns the slideshow item at the given index. The item can be specified as an image object (CGImage, NSImage), a string that represents a path to a file, or as a URL (NSURL).\n\nThe IKSlideshow class defines a number of other methods that are useful depending on the nature of your application. For example, you can export slideshow items to an application, reload data, run or stop a slideshow, set a time interval that determines when the slideshow starts to play automatically, and get the index of the item that’s currently playing.\n\nThe IKSlideshowDataSource protocol defines several optional methods that you might want to implement, such as methods that allow your application to take action at certain points in the slideshow: slideshowDidStop, slideshowWillStart, and slideshowDidChangeCurrentIndex:. See IKSlideshowDataSource Protocol Reference for a complete descriptions of the methods in this protocol.\n\nHow the Simple Slideshow Application Works\n\nThe slideshow application that you’ll create in this section is quite simple. It implements the required methods of the the IKSlideshowDataSource protocol and allows the user to stop the slideshow and reload another set of images to view. It provides pathnames for each item in the slideshow. Although the paths could lead to any image file format supported by Quartz 2D and the Image I/O framework, including paths to PDF documents, for this application you’ll restrict the datasource mode to images. In this mode, Image Kit displays only the first page of any PDF documents that are mixed in with the images in a folder.\n\nThe most complicated part of this application is the code that fetches the pathnames for each item in the slideshow. The path-fetching code must be able to accept a folder path and resolve any folder path to the individual items in the path. The user should be able to provide a path to a folder that contains both individual items and other folders, but the resulting slideshow should show only individual files. In other words, the slideshow should not display folder icons, but rather, the items in the folder.\n\nWhen launched, the simple slideshow application presents the user with an Open panel (dialog) for choosing a folder. The user closes the slideshow by clicking the Close icon provided by the slideshow controls shown in Figure 4-1. The user can start another slideshow by choosing File > Choose Images, which presents the Open panel again. The slideshow application terminates when the user chooses Quit from the menu or presses Command-Q.\n\nThe user interface for this application is very simple—a Choose Images menu item, which you’ll create using Interface Builder. All other user interface elements are provided by the IKSlideshow class or other parts of the system.\n\nTo control the application, you’ll use the NSWindowController class. Your controller will have:\n\nA mutable array for holding the paths to each of the items in the slideshow\n\nAn instance of the IKSlideshow class\n\nWriting the Simple Slideshow Application\n\nNow that you have an overview of the slideshow application, it’s time to write the code. First you’ll set up the Xcode project, the project files, and the controller interface. Then you’ll add the necessary routines to the implementation file. Finally, you’ll create the user interface in Interface Builder.\n\nSetting Up the Project, Project Files, and the Controller Interface\n\nFollow these steps to set up the project:\n\nCreate a Cocoa application and name it Simple Slideshow.\n\nAdd the Quartz framework to the project.\n\nFor details, see Using the Image Kit in Xcode.\n\nChoose File > New File.\n\nSelect “Objective-C NSWindowController subclass” and click Next.\n\nName the file SlideshowController.m and keep the option to create the header file. Then click Finish.\n\nIn the SlideshowController.h file, import the Quartz framework by adding this statement:\n\n#import <Quartz/Quartz.h>\n\nAdd an instance variable for the slideshow and a mutable array for image paths.\n\nThe interface should look as follows:\n\n@interface SlideshowController : NSWindowController {\n\n\n    IKSlideshow        *mSlideshow;\n\n\n    NSMutableArray *mImagePaths;\n\n\n}\n\n\n@end\n\nSave the SlideshowController.h file.\n\nAdding Routines to the Implementation File\n\nImplement the slideshow routines by following these steps:\n\nOpen the SlideshowController.m file.\n\nIn the implementation file, add an awakeFromNib method.\n\nThe method first obtains a shared instance of the slideshow. Next it loads images. You’ll write the loadImages method and its supporting methods later on. The loadImages method, if successful, will add image paths to the mImagePaths array. If paths are added, then the awakeFromNib method runs the slideshow, using the paths as the data source.\n\nThe following awakeFromNib method sets the mode to images only. If there are PDF documents in any folders that you add, Image Kit renders only the first page.\n\nNote that the method does not set any options for the slideshow. However, you can set any of the options specified by the slideshow option key constants defined in IKSlideshow Class Reference.\n\n- (void)awakeFromNib\n\n\n{\n\n\n     mSlideshow = [IKSlideshow sharedSlideshow];\n\n\n     [self loadImages];\n\n\n     if ([mImagePaths count] > 0)\n\n\n        [mSlideshow runSlideshowWithDataSource:(id<IKSlideshowDataSource>)self\n\n\n                      inMode: IKSlideshowModeImages\n\n\n                      options: NULL];\n\n\n}\n\nNext you need to implement the two required methods of the IKSlideshowDataSource Protocol protocol.\n\nThe numberOfSlideshowItems method simply returns a count the number of paths in the mImagePaths array. The slideshowItemAtIndex: method returns the index of the mImagePaths array for the item in question.\n\n- (NSUInteger)numberOfSlideshowItems\n\n\n{\n\n\n \n\n\n    return [mImagePaths count];\n\n\n}\n\n\n \n\n\n \n\n\n- (id)slideshowItemAtIndex: (NSUInteger)index\n\n\n{\n\n\n   int i;\n\n\n   i = index % [mImagePaths count];\n\n\n    return [mImagePaths objectAtIndex: i];\n\n\n}\n\nImplement a loadImages method.\n\nThis and the next few steps are where most of the work is done in the slideshow application. The loadImages method allocates and initializes the mImagePaths array, if necessary. Otherwise, the method removes all objects in preparation for adding new slideshow items.\n\nNext the method calls a function to open files. You’ll write this routine in the next step. The openFiles routine invokes the Open panel (NSOpenPanel class) and returns an array of paths or NULL if the user cancels the Open panel. If there is an array of paths, the loadImages method iterates through the paths and calls the method addImagesFromPath:, which you’ll write in a moment.\n\n- (void)loadImages\n\n\n{\n\n\n    if (NULL == mImagePaths)\n\n\n    {\n\n\n        mImagePaths = [[NSMutableArray alloc] init];\n\n\n    } else {\n\n\n      [mImagePaths removeAllObjects];\n\n\n    }\n\n\n    NSArray * array = openFiles();\n\n\n    if (array != NULL)\n\n\n    {\n\n\n        NSEnumerator *  enumerator;\n\n\n        NSString *      path;\n\n\n \n\n\n        enumerator = [array objectEnumerator];\n\n\n        while (path = [enumerator nextObject])\n\n\n        {\n\n\n                [self addImagesFromPath: path];\n\n\n        }\n\n\n    }\n\n\n}\n\nNext you need to include an openFiles routine, which you’ll add in the SlideshowController.m file, but prior to the implementation statement.\n\nThe openFiles routine creates an instance of the NSOpenPanel class, sets up options to allow the user to choose directories as well as files, runs the panel, and returns either an array of the file names chosen by the user, or nil if the user clicks the Cancel button.\n\nstatic NSArray *openFiles()\n\n\n{\n\n\n    NSOpenPanel *panel;\n\n\n \n\n\n    panel = [NSOpenPanel openPanel];\n\n\n    [panel setFloatingPanel:YES];\n\n\n    [panel setCanChooseDirectories:YES];\n\n\n    [panel setCanChooseFiles:NO];\n\n\n    int i = [panel runModal];\n\n\n    if(i == NSOKButton){\n\n\n        return [panel URLs];\n\n\n    }\n\n\n        return nil;\n\n\n}\n\nWrite the addImagesFromPath: method called by the loadImages method.\n\nThe first task for this method is to obtain an array of all the string objects contained in the given path. Then the method iterates through the string objects. If the string does not have a filename extension, then it represents a path. In this case the method calls itself to recursively resolve the path into individual items. If the string has an extension, then it represents an individual item that can be added to the image path array.\n\n- (void)addImagesFromPath: (NSString *)path\n\n\n{\n\n\n   NSArray * array  = [[NSFileManager defaultManager]\n\n\n                   directoryContentsAtPath: path];\n\n\n \n\n\n \n\n\n    NSEnumerator *  enumerator;\n\n\n    NSString *      imagePath;\n\n\n \n\n\n    enumerator = [array objectEnumerator];\n\n\n    while (imagePath = [enumerator nextObject])\n\n\n    {\n\n\n        if ([[[imagePath pathExtension] lowercaseString] isEqualToString: @\"\"])\n\n\n        {\n\n\n             [self addImagesFromPath: [NSString stringWithFormat: @\"%@/%@/\",\n\n\n                                       path, imagePath]];\n\n\n \n\n\n      } else\n\n\n        {\n\n\n            [mImagePaths addObject: [NSString stringWithFormat: @\"%@/%@\",\n\n\n                                     path, imagePath]];\n\n\n        }\n\n\n    }\n\n\n}\n\nYou need to write a choose images method that is invoked when the user chooses that command.\n\nRecall that the user can stop a slideshow and start another by choosing File > Choose Images. You’ll add this menu item and connect to this action later on.\n\nThis method first calls the loadImages method to fetch and add new slideshow items. If there are items to add (that is, the user did not click Cancel in the Open panel), the method reloads the slideshow data and then runs the slideshow.\n\n- (IBAction) chooseImages:(id) sender\n\n\n{\n\n\n \n\n\n    [self loadImages];\n\n\n    if ([mImagePaths count] > 0) {\n\n\n        [mSlideshow reloadData];\n\n\n        [mSlideshow  runSlideshowWithDataSource: (id<IKSlideshowDataSource>)self\n\n\n            inMode: IKSlideshowModeImages\n\n\n            options: NULL];\n\n\n    }\n\n\n}\n\nSave the SlideshowController.m file.\n\nOpen the SlideshowController.h file and add these method declarations:\n\n- (IBAction)chooseImages:(id)sender;\n\n\n \n\n\n- (NSUInteger)numberOfSlideshowItems;\n\n\n- (id)slideshowItemAtIndex: (NSUInteger)index;\n\n\n- (void)loadImages;\n\nSave the SlideshowController.h file.\n\nCreating the User Interface\n\nSet up the user interface in Interface Builder by following these steps:\n\nDouble-click the MainMenu.nib file to open Interface Builder.\n\nChoose File > Synchronize With Xcode.\n\nDelete the window icon in the nib document window.\n\nDouble-click the MainMenu icon in the nib document window.\n\nDrag a Menu Item (NSMenuItem) from the Library to the File menu and name the item Choose Images.\n\nDrag an Object (NSObject) from the Library to the nib document window.\n\nIn the Identity inspector, type SlideshowController in the Name field. THen select SlideshowController from the Class pop-up menu.\n\nControl-drag from the Choose Images menu item to the SlideshowController and in the connections panel choose chooseImages:.\n\nSave the nib file.\n\nIn Xcode, click Build and Go. Then test your application to make sure it works.\n\nYou might get warnings if you placed the methods into your file in the wrong order.\n\nCheck to make sure that you can run a slideshow. Exit the slideshow, and then start another by choosing File > Choose Images.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2008 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2008-06-09\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Graphics Contexts",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_context/dq_context.html#//apple_ref/doc/uid/TP30001066-CH203-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nGraphics Contexts\n\nA graphics context represents a drawing destination. It contains drawing parameters and all device-specific information that the drawing system needs to perform any subsequent drawing commands. A graphics context defines basic drawing attributes such as the colors to use when drawing, the clipping area, line width and style information, font information, compositing options, and several others.\n\nYou can obtain a graphics context by using Quartz context creation functions or by using higher-level functions provided by one of the Mac OS X frameworks or the UIKit framework in iOS. Quartz provides functions for various flavors of Quartz graphics contexts including bitmap and PDF, which you can use to create custom content.\n\nThis chapter shows you how to create a graphics context for a variety of drawing destinations. A graphics context is represented in your code by the data type CGContextRef, which is an opaque data type. After you obtain a graphics context, you can use Quartz 2D functions to draw to the context, perform operations (such as translations) on the context, and change graphics state parameters, such as line width and fill color.\n\nDrawing to a View Graphics Context in iOS\n\nTo draw to the screen in an iOS application, you set up a UIView object and implement its drawRect: method to perform drawing. The view’s drawRect: method is called when the view is visible onscreen and its contents need updating. Before calling your custom drawRect: method, the view object automatically configures its drawing environment so that your code can start drawing immediately. As part of this configuration, the UIView object creates a graphics context (a CGContextRef opaque type) for the current drawing environment. You obtain this graphics context in your drawRect: method by calling the UIKit function UIGraphicsGetCurrentContext.\n\nThe default coordinate system used throughout UIKit is different from the coordinate system used by Quartz. In UIKit, the origin is in the upper-left corner, with the positive-y value pointing downward. The UIView object modifies the CTM of the Quartz graphics context to match the UIKit conventions by translating the origin to the upper left corner of the view and inverting the y-axis by multiplying it by -1. For more information on modified-coordinate systems and the implications in your own drawing code, see Quartz 2D Coordinate Systems.\n\nUIView objects are described in detail in View Programming Guide for iOS.\n\nCreating a Window Graphics Context in Mac OS X\n\nWhen drawing in Mac OS X, you need to create a window graphics context that’s appropriate for the framework you are using. The Quartz 2D API itself provides no functions to obtain a windows graphics context. Instead, you use the Cocoa framework to obtain a context for a window created in Cocoa.\n\nYou obtain a Quartz graphics context from within the drawRect: routine of a Cocoa application using the following line of code:\n\nCGContextRef myContext = [[NSGraphicsContext currentContext] graphicsPort];\n\nThe method currentContext returns the NSGraphicsContext instance of the current thread. The method graphicsPort returns the low-level, platform-specific graphics context represented by the receiver, which is a Quartz graphics context. (Don’t get confused by the method names; they are historical.) For more information see NSGraphicsContext Class Reference.\n\nAfter you obtain the graphics context, you can call any of the Quartz 2D drawing functions in your Cocoa application. You can also mix Quartz 2D calls with Cocoa drawing calls. You can see an example of Quartz 2D drawing to a Cocoa view by looking at Figure 2-1. The drawing consists of two overlapping rectangles, an opaque red one and a partially transparent blue one. You’ll learn more about transparency in Color and Color Spaces. The ability to control how much you can “see through” colors is one of the hallmark features of Quartz 2D.\n\nFigure 2-1  A view in the Cocoa framework that contains Quartz drawing\n\nTo create the drawing in Figure 2-1, you first create a Cocoa application Xcode project. In Interface Builder, drag a Custom View to the window and subclass it. Then write an implementation for the subclassed view, similar to what Listing 2-1 shows. For this example, the subclassed view is named MyQuartzView. The drawRect: method for the view contains all the Quartz drawing code. A detailed explanation for each numbered line of code appears following the listing.\n\nNote: The drawRect: method of the NSView class is invoked automatically each time the view needs to be drawn. To find out more about overriding the drawRect: method, see NSView Class Reference.\n\nListing 2-1  Drawing to a window graphics context\n\n@implementation MyQuartzView\n\n\n \n\n\n- (id)initWithFrame:(NSRect)frameRect\n\n\n{\n\n\n    self = [super initWithFrame:frameRect];\n\n\n    return self;\n\n\n}\n\n\n \n\n\n- (void)drawRect:(NSRect)rect\n\n\n{\n\n\n    CGContextRef myContext = [[NSGraphicsContext \n// 1\n\n\n                                currentContext] graphicsPort];\n\n\n   // ********** Your drawing code here ********** \n// 2\n\n\n    CGContextSetRGBFillColor (myContext, 1, 0, 0, 1);\n// 3\n\n\n    CGContextFillRect (myContext, CGRectMake (0, 0, 200, 100 ));\n// 4\n\n\n    CGContextSetRGBFillColor (myContext, 0, 0, 1, .5);\n// 5\n\n\n    CGContextFillRect (myContext, CGRectMake (0, 0, 100, 200));\n// 6\n\n\n  }\n\n\n \n\n\n@end\n\nHere’s what the code does:\n\nObtains a graphics context for the view.\n\nThis is where you insert your drawing code. The four lines of code that follow are examples of using Quartz 2D functions.\n\nSets a red fill color that’s fully opaque. For information on colors and alpha (which sets opacity), see Color and Color Spaces.\n\nFills a rectangle whose origin is (0,0) and whose width is 200 and height is 100. For information on drawing rectangles, see Paths.\n\nSets a blue fill color that’s partially transparent.\n\nFills a rectangle whose origin is (0,0) and whose width is 100 and height is 200.\n\nCreating a PDF Graphics Context\n\nWhen you create a PDF graphics context and draw to that context, Quartz records your drawing as a series of PDF drawing commands written to a file. You supply a location for the PDF output and a default media box—a rectangle that specifies bounds of the page. Figure 2-2 shows the result of drawing to a PDF graphics context and then opening the resulting PDF in Preview.\n\nFigure 2-2  A PDF created by using CGPDFContextCreateWithURL\n\nThe Quartz 2D API provides two functions that create a PDF graphics context:\n\nCGPDFContextCreateWithURL, which you use when you want to specify the location for the PDF output as a Core Foundation URL. Listing 2-2 shows how to use this function to create a PDF graphics context.\n\nCGPDFContextCreate, which you use when you want the PDF output sent to a data consumer. (For more information see Data Management in Quartz 2D.) Listing 2-3 shows how to use this function to create a PDF graphics context.\n\nA detailed explanation for each numbered line of code follows each listing.\n\niOS Note: A PDF graphics context in iOS uses the default coordinate system provided by Quartz, without applying a transform to match the UIKit coordinate system. If your application plans on sharing drawing code between your PDF graphics context and the graphics context provided by UIView object, your application should modify the CTM of the PDF graphics context to modify the coordinate system. See Quartz 2D Coordinate Systems.\n\nListing 2-2  Calling CGPDFContextCreateWithURL to create a PDF graphics context\n\nCGContextRef MyPDFContextCreate (const CGRect *inMediaBox,\n\n\n                                    CFStringRef path)\n\n\n{\n\n\n    CGContextRef myOutContext = NULL;\n\n\n    CFURLRef url;\n\n\n \n\n\n    url = CFURLCreateWithFileSystemPath (NULL, \n// 1\n\n\n                                path,\n\n\n                                kCFURLPOSIXPathStyle,\n\n\n                                false);\n\n\n    if (url != NULL) {\n\n\n        myOutContext = CGPDFContextCreateWithURL (url,\n// 2\n\n\n                                        inMediaBox,\n\n\n                                        NULL);\n\n\n        CFRelease(url);\n// 3\n\n\n    }\n\n\n    return myOutContext;\n// 4\n\n\n}\n\nHere’s what the code does:\n\nCalls the Core Foundation function to create a CFURL object from the CFString object supplied to the MyPDFContextCreate function. You pass NULL as the first parameter to use the default allocator. You also need to specify a path style, which for this example is a POSIX-style pathname.\n\nCalls the Quartz 2D function to create a PDF graphics context using the PDF location just created (as a CFURL object) and a rectangle that specifies the bounds of the PDF. The rectangle (CGRect) was passed to the MyPDFContextCreate function and is the default page media bounding box for the PDF.\n\nReleases the CFURL object.\n\nReturns the PDF graphics context. The caller must release the graphics context when it is no longer needed.\n\nListing 2-3  Calling CGPDFContextCreate to create a PDF graphics context\n\nCGContextRef MyPDFContextCreate (const CGRect *inMediaBox,\n\n\n                                    CFStringRef path)\n\n\n{\n\n\n    CGContextRef        myOutContext = NULL;\n\n\n    CFURLRef            url;\n\n\n    CGDataConsumerRef   dataConsumer;\n\n\n \n\n\n    url = CFURLCreateWithFileSystemPath (NULL, \n// 1\n\n\n                                        path,\n\n\n                                        kCFURLPOSIXPathStyle,\n\n\n                                        false);\n\n\n \n\n\n    if (url != NULL)\n\n\n    {\n\n\n        dataConsumer = CGDataConsumerCreateWithURL (url);\n// 2\n\n\n        if (dataConsumer != NULL)\n\n\n        {\n\n\n            myOutContext = CGPDFContextCreate (dataConsumer, \n// 3\n\n\n                                        inMediaBox,\n\n\n                                        NULL);\n\n\n            CGDataConsumerRelease (dataConsumer);\n// 4\n\n\n        }\n\n\n        CFRelease(url);\n// 5\n\n\n    }\n\n\n    return myOutContext;\n// 6\n\n\n}\n\nHere’s what the code does:\n\nCalls the Core Foundation function to create a CFURL object from the CFString object supplied to the MyPDFContextCreate function. You pass NULL as the first parameter to use the default allocator. You also need to specify a path style, which for this example is a POSIX-style pathname.\n\nCreates a Quartz data consumer object using the CFURL object. If you don’t want to use a CFURL object (for example, you want to place the PDF data in a location that can’t be specified by a CFURL object), you can instead create a data consumer from a set of callback functions that you implement in your application. For more information, see Data Management in Quartz 2D.\n\nCalls the Quartz 2D function to create a PDF graphics context passing as parameters the data consumer and the rectangle (of type CGRect) that was passed to the MyPDFContextCreate function. This rectangle is the default page media bounding box for the PDF.\n\nReleases the data consumer.\n\nReleases the CFURL object.\n\nReturns the PDF graphics context. The caller must release the graphics context when it is no longer needed.\n\nListing 2-4 shows how to call the MyPDFContextCreate routine and draw to it. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 2-4  Drawing to a PDF graphics context\n\n    CGRect mediaBox;\n// 1\n\n\n \n\n\n    mediaBox = CGRectMake (0, 0, myPageWidth, myPageHeight);\n// 2\n\n\n    myPDFContext = MyPDFContextCreate (&mediaBox, CFSTR(\"test.pdf\"));\n// 3\n\n\n \n\n\n    CFStringRef myKeys[1];\n// 4\n\n\n    CFTypeRef myValues[1];\n\n\n    myKeys[0] = kCGPDFContextMediaBox;\n\n\n    myValues[0] = (CFTypeRef) CFDataCreate(NULL,(const UInt8 *)&mediaBox, sizeof (CGRect));\n\n\n    CFDictionaryRef pageDictionary = CFDictionaryCreate(NULL, (const void **) myKeys,\n                                                        (const void **) myValues, 1,\n                                                        &kCFTypeDictionaryKeyCallBacks,\n                                                        & kCFTypeDictionaryValueCallBacks);\n\n\n    CGPDFContextBeginPage(myPDFContext, &pageDictionary);\n// 5\n\n\n        // ********** Your drawing code here **********\n// 6\n\n\n        CGContextSetRGBFillColor (myPDFContext, 1, 0, 0, 1);\n\n\n        CGContextFillRect (myPDFContext, CGRectMake (0, 0, 200, 100 ));\n\n\n        CGContextSetRGBFillColor (myPDFContext, 0, 0, 1, .5);\n\n\n        CGContextFillRect (myPDFContext, CGRectMake (0, 0, 100, 200 ));\n\n\n    CGPDFContextEndPage(myPDFContext);\n// 7\n\n\n    CFRelease(pageDictionary);\n// 8\n\n\n    CFRelease(myValues[0]);\n\n\n    CGContextRelease(myPDFContext);\n\nHere’s what the code does:\n\nDeclares a variable for the rectangle that you use to define the PDF media box.\n\nSets the origin of the media box to (0,0) and the width and height to variables supplied by the application.\n\nCalls the function MyPDFContextCreate (See Listing 2-3) to obtain a PDF graphics context, supplying a media box and a pathname. The macro CFSTR converts a string to a CFStringRef data type.\n\nSets up a dictionary with the page options. In this example, only the media box is specified. You don’t have to pass the same rectangle you used to set up the PDF graphics context. The media box you add here supersedes the rectangle you pass to set up the PDF graphics context.\n\nSignals the start of a page. This function is used for page-oriented graphics, which is what PDF drawing is.\n\nCalls Quartz 2D drawing functions. You replace this and the following four lines of code with the drawing code appropriate for your application.\n\nSignals the end of the PDF page.\n\nReleases the dictionary and the PDF graphics context when they are no longer needed.\n\nYou can write any content to a PDF that’s appropriate for your application—images, text, path drawing—and you can add links and encryption. For more information see PDF Document Creation, Viewing, and Transforming.\n\nCreating a Bitmap Graphics Context\n\nA bitmap graphics context accepts a pointer to a memory buffer that contains storage space for the bitmap. When you paint into the bitmap graphics context, the buffer is updated. After you release the graphics context, you have a fully updated bitmap in the pixel format you specify.\n\nNote: Bitmap graphics contexts are sometimes used for drawing offscreen. Before you decide to use a bitmap graphics context for this purpose, see Core Graphics Layer Drawing. CGLayer objects (CGLayerRef) are optimized for offscreen drawing because, whenever possible, Quartz caches layers on the video card.\n\niOS Note: iOS applications should use the function UIGraphicsBeginImageContextWithOptions instead of using the low-level Quartz functions described here. If your application creates an offscreen bitmap using Quartz, the coordinate system used by bitmap graphics context is the default Quartz coordinate system. In contrast, if your application creates an image context by calling the function UIGraphicsBeginImageContextWithOptions, UIKit applies the same transformation to the context’s coordinate system as it does to a UIView object’s graphics context. This allows your application to use the same drawing code for either without having to worry about different coordinate systems. Although your application can manually adjust the coordinate transformation matrix to achieve the correct results, in practice, there is no performance benefit to doing so.\n\nYou use the function CGBitmapContextCreate to create a bitmap graphics context. This function takes the following parameters:\n\ndata. Supply a pointer to the destination in memory where you want the drawing rendered. The size of this memory block should be at least (bytesPerRow*height) bytes.\n\nwidth. Specify the width, in pixels, of the bitmap.\n\nheight. Specify the height, in pixels, of the bitmap.\n\nbitsPerComponent. Specify the number of bits to use for each component of a pixel in memory. For example, for a 32-bit pixel format and an RGB color space, you would specify a value of 8 bits per component. See Supported Pixel Formats.\n\nbytesPerRow. Specify the number of bytes of memory to use per row of the bitmap.\n\nTip:  When you create a bitmap graphics context, you’ll get the best performance if you make sure the data and bytesPerRow are 16-byte aligned.\n\ncolorspace. The color space to use for the bitmap context. You can provide a Gray, RGB, CMYK, or NULL color space when you create a bitmap graphics context. For detailed information on color spaces and color management principles, see Color Management Overview. For information on creating and using color spaces in Quartz, see Color and Color Spaces. For information about supported color spaces, see Color Spaces and Bitmap Layout in the Bitmap Images and Image Masks chapter.\n\nbitmapInfo. Bitmap layout information, expressed as a CGBitmapInfo constant, that specifies whether the bitmap should contain an alpha component, the relative location of the alpha component (if there is one) in a pixel, whether the alpha component is premultiplied, and whether the color components are integer or floating-point values. For detailed information on what these constants are, when each is used, and Quartz-supported pixel formats for bitmap graphics contexts and images, see Color Spaces and Bitmap Layout in the Bitmap Images and Image Masks chapter.\n\nListing 2-5 shows how to create a bitmap graphics context. When you draw into the resulting bitmap graphics context, Quartz records your drawing as bitmap data in the specified block of memory. A detailed explanation for each numbered line of code follows the listing.\n\nListing 2-5  Creating a bitmap graphics context\n\nCGContextRef MyCreateBitmapContext (int pixelsWide,\n\n\n                            int pixelsHigh)\n\n\n{\n\n\n    CGContextRef    context = NULL;\n\n\n    CGColorSpaceRef colorSpace;\n\n\n    void *          bitmapData;\n\n\n    int             bitmapByteCount;\n\n\n    int             bitmapBytesPerRow;\n\n\n \n\n\n    bitmapBytesPerRow   = (pixelsWide * 4);\n// 1\n\n\n    bitmapByteCount     = (bitmapBytesPerRow * pixelsHigh);\n\n\n \n\n\n    colorSpace = CGColorSpaceCreateWithName(kCGColorSpaceGenericRGB);\n// 2\n\n\n    bitmapData = calloc( bitmapByteCount, sizeof(uint8_t) );\n// 3\n\n\n    if (bitmapData == NULL)\n\n\n    {\n\n\n        fprintf (stderr, \"Memory not allocated!\");\n\n\n        return NULL;\n\n\n    }\n\n\n    context = CGBitmapContextCreate (bitmapData,\n// 4\n\n\n                                    pixelsWide,\n\n\n                                    pixelsHigh,\n\n\n                                    8,      // bits per component\n\n\n                                    bitmapBytesPerRow,\n\n\n                                    colorSpace,\n\n\n                                    kCGImageAlphaPremultipliedLast);\n\n\n    if (context== NULL)\n\n\n    {\n\n\n        free (bitmapData);\n// 5\n\n\n        fprintf (stderr, \"Context not created!\");\n\n\n        return NULL;\n\n\n    }\n\n\n    CGColorSpaceRelease( colorSpace );\n// 6\n\n\n \n\n\n    return context;\n// 7\n\n\n}\n\nHere’s what the code does:\n\nDeclares a variable to represent the number of bytes per row. Each pixel in the bitmap in this example is represented by 4 bytes; 8 bits each of red, green, blue, and alpha.\n\nCreates a generic RGB color space. You can also create a CMYK color space. See Color and Color Spaces for more information and for a discussion of generic color spaces versus device dependent ones.\n\nCalls the calloc function to create and clear a block of memory in which to store the bitmap data. This example creates a 32-bit RGBA bitmap (that is, an array with 32 bits per pixel, each pixel containing 8 bits each of red, green, blue, and alpha information). Each pixel in the bitmap occupies 4 bytes of memory. In Mac OS X 10.6 and iOS 4, this step can be omitted—if you pass NULL as bitmap data, Quartz automatically allocates space for the bitmap.\n\nCreates a bitmap graphics context, supplying the bitmap data, the width and height of the bitmap, the number of bits per component, the bytes per row, the color space, and a constant that specifies whether the bitmap should contain an alpha channel and its relative location in a pixel. The constant kCGImageAlphaPremultipliedLast indicates that the alpha component is stored in the last byte of each pixel and that the color components have already been multiplied by this alpha value. See The Alpha Value for more information on premultiplied alpha.\n\nIf the context isn’t created for some reason, frees the memory allocated for the bitmap data.\n\nReleases the color space.\n\nReturns the bitmap graphics context. The caller must release the graphics context when it is no longer needed.\n\nListing 2-6 shows code that calls MyCreateBitmapContext to create a bitmap graphics context, uses the bitmap graphics context to create a CGImage object, then draws the resulting image to a window graphics context. Figure 2-3 shows the image drawn to the window. A detailed explanation for each numbered line of code follows the listing.\n\nListing 2-6  Drawing to a bitmap graphics context\n\n    CGRect myBoundingBox;\n// 1\n\n\n \n\n\n    myBoundingBox = CGRectMake (0, 0, myWidth, myHeight);\n// 2\n\n\n    myBitmapContext = MyCreateBitmapContext (400, 300);\n// 3\n\n\n    // ********** Your drawing code here ********** \n// 4\n\n\n    CGContextSetRGBFillColor (myBitmapContext, 1, 0, 0, 1);\n\n\n    CGContextFillRect (myBitmapContext, CGRectMake (0, 0, 200, 100 ));\n\n\n    CGContextSetRGBFillColor (myBitmapContext, 0, 0, 1, .5);\n\n\n    CGContextFillRect (myBitmapContext, CGRectMake (0, 0, 100, 200 ));\n\n\n    myImage = CGBitmapContextCreateImage (myBitmapContext);\n// 5\n\n\n    CGContextDrawImage(myContext, myBoundingBox, myImage);\n// 6\n\n\n    char *bitmapData = CGBitmapContextGetData(myBitmapContext); \n// 7\n\n\n    CGContextRelease (myBitmapContext);\n// 8\n\n\n    if (bitmapData) free(bitmapData); \n// 9\n\n\n    CGImageRelease(myImage);\n// 10\n\nHere’s what the code does:\n\nDeclares a variable to store the origin and dimensions of the bounding box into which Quartz will draw an image created from the bitmap graphics context.\n\nSets the origin of the bounding box to (0,0) and the width and height to variables previously declared, but whose declaration are not shown in this code.\n\nCalls the application-supplied function MyCreateBitmapContext (see Listing 2-5) to create a bitmap context that is 400 pixels wide and 300 pixels high. You can create a bitmap graphics context using any dimensions that are appropriate for your application.\n\nCalls Quartz 2D functions to draw into the bitmap graphics context. You would replace this and the next four lines of code with drawing code appropriate for your application.\n\nCreates a Quartz 2D image (CGImageRef) from the bitmap graphics context.\n\nDraws the image into the location in the window graphics context that is specified by the bounding box. The bounding box specifies the location and dimensions in user space in which to draw the image.\n\nThis example does not show the creation of the window graphics context. See Creating a Window Graphics Context in Mac OS X for information on how to create one.\n\nGets the bitmap data associated with the bitmap graphics context.\n\nReleases the bitmap graphics context when it is no longer needed.\n\nFree the bitmap data if it exists.\n\nReleases the image when it is no longer needed.\n\nFigure 2-3  An image created from a bitmap graphics context and drawn to a window graphics context\nSupported Pixel Formats\n\nTable 2-1 summarizes the pixel formats that are supported for bitmap graphics context, the associated color space (cs), and the version of Mac OS X in which the format was first available. The pixel format is specified as bits per pixel (bpp) and bits per component (bpc). The table also includes the bitmap information constant associated with that pixel format. See CGImage Reference for details on what each of the bitmap information format constants represent.\n\nTable 2-1  Pixel formats supported for bitmap graphics contexts\n\nCS\n\n\t\n\nPixel format and bitmap information constant\n\n\t\n\nAvailability\n\n\n\n\nNull\n\n\t\n\n8 bpp, 8 bpc, kCGImageAlphaOnly\n\n\t\n\nMac OS X, iOS\n\n\n\n\nGray\n\n\t\n\n8 bpp, 8 bpc,kCGImageAlphaNone\n\n\t\n\nMac OS X, iOS\n\n\n\n\nGray\n\n\t\n\n8 bpp, 8 bpc,kCGImageAlphaOnly\n\n\t\n\nMac OS X, iOS\n\n\n\n\nGray\n\n\t\n\n16 bpp, 16 bpc, kCGImageAlphaNone\n\n\t\n\nMac OS X\n\n\n\n\nGray\n\n\t\n\n32 bpp, 32 bpc, kCGImageAlphaNone|kCGBitmapFloatComponents\n\n\t\n\nMac OS X\n\n\n\n\nRGB\n\n\t\n\n16 bpp, 5 bpc, kCGImageAlphaNoneSkipFirst\n\n\t\n\nMac OS X, iOS\n\n\n\n\nRGB\n\n\t\n\n32 bpp, 8 bpc, kCGImageAlphaNoneSkipFirst\n\n\t\n\nMac OS X, iOS\n\n\n\n\nRGB\n\n\t\n\n32 bpp, 8 bpc, kCGImageAlphaNoneSkipLast\n\n\t\n\nMac OS X, iOS\n\n\n\n\nRGB\n\n\t\n\n32 bpp, 8 bpc, kCGImageAlphaPremultipliedFirst\n\n\t\n\nMac OS X, iOS\n\n\n\n\nRGB\n\n\t\n\n32 bpp, 8 bpc, kCGImageAlphaPremultipliedLast\n\n\t\n\nMac OS X, iOS\n\n\n\n\nRGB\n\n\t\n\n64 bpp, 16 bpc, kCGImageAlphaPremultipliedLast\n\n\t\n\nMac OS X\n\n\n\n\nRGB\n\n\t\n\n64 bpp, 16 bpc, kCGImageAlphaNoneSkipLast\n\n\t\n\nMac OS X\n\n\n\n\nRGB\n\n\t\n\n128 bpp, 32 bpc, kCGImageAlphaNoneSkipLast |kCGBitmapFloatComponents\n\n\t\n\nMac OS X\n\n\n\n\nRGB\n\n\t\n\n128 bpp, 32 bpc, kCGImageAlphaPremultipliedLast |kCGBitmapFloatComponents\n\n\t\n\nMac OS X\n\n\n\n\nCMYK\n\n\t\n\n32 bpp, 8 bpc, kCGImageAlphaNone\n\n\t\n\nMac OS X\n\n\n\n\nCMYK\n\n\t\n\n64 bpp, 16 bpc, kCGImageAlphaNone\n\n\t\n\nMac OS X\n\n\n\n\nCMYK\n\n\t\n\n128 bpp, 32 bpc, kCGImageAlphaNone |kCGBitmapFloatComponents\n\n\t\n\nMac OS X\n\nAnti-Aliasing\n\nBitmap graphics contexts support anti-aliasing, which is the process of artificially correcting the jagged (or aliased) edges you sometimes see in bitmap images when text or shapes are drawn. These jagged edges occur when the resolution of the bitmap is significantly lower than the resolution of your eyes. To make objects appear smooth in the bitmap, Quartz uses different colors for the pixels that surround the outline of the shape. By blending the colors in this way, the shape appears smooth. You can see the effect of using anti-aliasing in Figure 2-4. You can turn anti-aliasing off for a particular bitmap graphics context by calling the function CGContextSetShouldAntialias. The anti-aliasing setting is part of the graphics state.\n\nYou can control whether to allow anti-aliasing for a particular graphics context by using the function CGContextSetAllowsAntialiasing. Pass true to this function to allow anti-aliasing; false not to allow it. This setting is not part of the graphics state. Quartz performs anti-aliasing when the context and the graphic state settings are set to true.\n\nFigure 2-4  A comparison of aliased and anti-aliasing drawing\nObtaining a Graphics Context for Printing\n\nCocoa applications in Mac OS X implement printing through custom NSView subclasses. A view is told to print by invoking its print: method. The view then creates a graphics context that targets a printer and calls its drawRect: method. Your application uses the same drawing code to draw to the printer that it uses to draw to the screen. It can also customize the drawRect: call to an image to the printer that is different from the one sent to the screen.\n\nFor a detailed discussion of printing in Cocoa, see Printing Programming Guide for Mac.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Introduction",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageIOGuide/imageio_intro/ikpg_intro.html#//apple_ref/doc/uid/TP40005462",
    "html": "Documentation Archive\nDeveloper\nSearch\nImage I/O Programming Guide\nTable of Contents\nIntroduction\nBasics of Using Image I/O\nCreating and Using Image Sources\nWorking with Image Destinations\nRevision History\nNext\nIntroduction\n\nThe Image I/O programming interface allows applications to read and write most image file formats. Originally part of the Core Graphics framework, Image I/O resides in its own framework to allow developers to use it independently of Core Graphics (Quartz 2D). Image I/O provides the definitive way to access image data because it is highly efficient, allows easy access to metadata, and provides color management.\n\nThe Image I/O interface is available in OS X v10.4 and later and in iOS 4 and later.\n\nWho Should Read This Document?\n\nThis document is intended for developers who read or write image data in an application. Any developer currently using image importers or other image handling libraries should read this document to see how to use the Image I/O framework instead.\n\nOrganization of This Document\n\nThis document is organized into the following chapters:\n\nBasics of Using Image I/O discusses supported image formats and shows how to include the framework in an Xcode project.\n\nCreating and Using Image Sources shows how to create an image source, create an image from it, and extract properties for display in the user interface.\n\nWorking with Image Destinations provides information on creating an image destination, setting up its properties, and adding an image to it.\n\nSee Also\n\nThe Image I/O Reference Collection provides detailed descriptions of the functions, data types, and constants in the Image I/O framework.\n\nNext\n\n\n\n\n\nCopyright © 2001, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "PDF Document Parsing",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_pdf_scan/dq_pdf_scan.html#//apple_ref/doc/uid/TP30001066-CH220-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nPDF Document Parsing\n\nQuartz provides functions that let you inspect the PDF document structure and the content stream. Inspecting the document structure lets you read the entries in the document catalog and the contents associated with each entry. By recursively traversing the catalog, you can inspect the entire document.\n\nA PDF content stream is just what its name suggests—a sequential stream of data such as 'BT 12 /F71 Tf (draw this text) Tj . . . ' where PDF operators and their descriptors are mixed with the actual PDF content. Inspecting the content stream requires that you access it sequentially.\n\nThis chapter shows how to examine the structure of a PDF document and parse the contents of a PDF document.\n\nInspecting PDF Document Structure\n\nPDF files may contain multiple pages of images and text. You can use Quartz to access the metadata at the document and page levels as well as objects on a PDF page. This section provides a very brief introduction to the metadata you can access.\n\nA PDF document object (CGPDFDocument) contains all the information that relates to a PDF document, including its catalog and contents. The entries in the catalog recursively describe the contents of the PDF document. You can access the contents of a PDF document catalog by calling the function CGPDFDocumentGetCatalog.\n\nA PDF page object (CGPDFPage) represents a page in a PDF document and contains information that relates to a specific page, including the page dictionary and page contents. You can obtain a page dictionary by calling the function CGPDFPageGetDictionary.\n\nFigure 14-1 shows some of the metadata describing the two images—the text and the image of the rooster—that make up the PDF file displayed in Figure 13-2.\n\nFigure 14-1  Metadata for two images in a PDF file\n\nYou can obtain much more useful information by accessing PDF metadata. The items in Figure 14-1 are just a sample. For example, you can check to see if a PDF has thumbnail images (shown in Figure 14-2) using the code shown in Listing 14-1.\n\nListing 14-1  Getting a thumbnail view of a PDF\n\nCGPDFDictionaryRef d;\n\n\nCGPDFStreamRef stream; // represents a sequence of bytes\n\n\nd = CGPDFPageGetDictionary(page);\n\n\n// check for thumbnail data\n\n\nif (CGPDFDictionaryGetStream (d, “Thumb”, &stream)){\n\n\n    // get the data if it exists\n\n\n    data = CGPDFStreamCopyData (stream, &format);\n\nQuartz performs all the decryption and decoding of the data stream for you.\n\nFigure 14-2  Thumbnail images\n\nQuartz provides a number of functions that you can use to obtain individual values for items in the PDF metadata. You use the function CGPDFObjectGetValue, passing a CGPDFObjectRef, a PDF object type (kCGPDFObjectTypeBoolean, kCGPDFObjectTypeInteger, and so forth), and storage for the value. On return, the storage is filled with the value.\n\nThere are numerous other functions you can use to traverse the hierarchy of a PDF file to access the various nodes and their children. For example, the CGPDFArray functions (CGPDFArrayGetBoolean, CGPDFArrayGetDictionary, CGPDFArrayGetInteger, and so forth) let you access arrays of values to retrieve values of specific types. You can find out more about how to use these functions by reading the PDF specification.\n\nParsing PDF Content\n\nThe PDF content stream contains operators that signify parts of a PDF content stream that may be of interest to your application. An operator either marks a single point or a sequence. An operator is specified as a tag that has a property list or an object associated with it. A tag specifies what the point or content sequence represents. A property list is a dictionary that contains key-value pairs specified by the PDF content creator. When you parse a PDF content stream, your application looks for any markers of interest, inspects the tag, property list, or object associated with the marker, and then performs any further processing that’s appropriate. Consult the PDF Reference for a complete list of PDF operators.\n\nYou use a CGPDFScanner object (CGPDFScannerRef data type) to parse a PDF content stream. The CGPDFScanner object invokes callbacks for any operator in the stream for which you have registered a callback.\n\nYou perform the tasks described in the following sections to parse a content stream:\n\nWrite Callbacks for Operators. You need to write callbacks only for the operators you want to handle.\n\nCreate and Set Up the Operator Table.\n\nOpen the PDF Document.\n\nScan the Content Stream for Each Page.\n\nWhen it’s appropriate to do so, you need to make sure the you release the scanner, content stream, and operator table.\n\nThe following sections show how to parse a content stream to find marked-content operators (see Table 14-1). Marked content operators represent only some of the PDF operators used in PDF content. When you write your own code, you’d look for the PDF operators appropriate for your application.\n\nTable 14-1  Marked content operators represent some of the PDF operators that you can parse\n\nOperator\n\n\t\n\nDescription\n\n\n\n\nMP\n\n\t\n\nA marked point that has a tag associated with it.\n\n\n\n\nDP\n\n\t\n\nA marked point that has a tag and a property list or object associated with it.\n\n\n\n\nBMC\n\n\t\n\nSignals the start of a marked-content sequence (begin marked content) and is paired with the EMC marker that signals the end of the sequence. Has a tag associated with it.\n\n\n\n\nBDC\n\n\t\n\nSignals the start of a marked-content sequence and is paired with the EMC marker that signals the end of the sequence. Has a tag and a property list or object associated with it.\n\n\n\n\nEMC\n\n\t\n\nSignals the end of a marked-content sequence (end marked content) that begins with a BMC or a BDC marker. This operator does not have a tag associated with it.\n\nWrite Callbacks for Operators\n\nWhen Quartz invokes your callback for a PDF operators, it passes a CGPDFScanner object and a pointer to any information needed by your callback. Typically, your callback retrieves any items associated with the operator. For example, the callback for the MP operator that’s shown in Listing 14-2 calls the function CGPDFScannerPopName to retrieve the character string associated with the operator from the stack. If the code in the listing successfully retrieves the name from the scanner stack, it prints the name.\n\nQuartz has an assortment of CGPDFScannerPop functions for retrieving objects, Boolean values, names, numbers, strings, arrays, dictionaries, and streams. Each function returns a Boolean value to indicate whether the item was retrieved successfully.\n\nListing 14-2  A callback for the MP operator\n\nstatic void\n\n\nop_MP (CGPDFScannerRef s, void *info)\n\n\n{\n\n\n    const char *name;\n\n\n \n\n\n    if (!CGPDFScannerPopName(s, &name))\n\n\n        return;\n\n\n \n\n\n    printf(\"MP /%s\\n\", name);\n\n\n}\nCreate and Set Up the Operator Table\n\nA CGPDFOperatorTable object stores PDF operator callback functions that you write. The function CGPDFOperatorTableCreate creates an operator table, as shown in Listing 14-3. After you create an operator table, you call the function CGPDFOperatorTableSetCallback for each callback you want to add to the table. You pass the table, the string that specifies the PDF operator, and a pointer to a callback function you write to handle that operator. You can name the callbacks whatever you’d like. Just make sure that the callback name you pass to the function CGPDFOperatorTableSetCallback isn’t misspelled.\n\nThe code in Listing 14-3 sets a callback for each of the marked-content operators listed in Table 14-1. Your application would set callbacks only for those operators of interest. PDF operator strings are defined in the PDF Reference from Adobe.\n\nListing 14-3  Setting callbacks for an operator table\n\nCGPDFOperatorTableRef myTable;\n\n\n \n\n\nmyTable = CGPDFOperatorTableCreate();\n\n\n \n\n\nCGPDFOperatorTableSetCallback (myTable, \"MP\", &op_MP);\n\n\nCGPDFOperatorTableSetCallback (myTable, \"DP\", &op_DP);\n\n\nCGPDFOperatorTableSetCallback (myTable, \"BMC\", &op_BMC);\n\n\nCGPDFOperatorTableSetCallback (myTable, \"BDC\", &op_BDC);\n\n\nCGPDFOperatorTableSetCallback (myTable, \"EMC\", &op_EMC);\nOpen the PDF Document\n\nBefore you can scan the content of a PDF document, you need to open it. Listing 14-4 shows a code fragment that creates a CGPDFDocument object from a URL supplied to the code. Note that the listing is a code fragment, so that not all variables are declared. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 14-4  Opening a PDF document from a URL\n\nCGPDFDocumentRef myDocument;\n\n\nmyDocument = CGPDFDocumentCreateWithURL(url);\n// 1\n\n\nif (myDocument == NULL) {\n// 2\n\n\n        error (\"can't open `%s'.\", filename);\n\n\n        CFRelease (url);\n\n\n        return EXIT_FAILURE;\n\n\n}\n\n\nCFRelease (url);\n\n\nif (CGPDFDocumentIsEncrypted (myDocument)) {\n// 3\n\n\n    if (!CGPDFDocumentUnlockWithPassword (myDocument, \"\")) {\n\n\n        printf (\"Enter password: \");\n\n\n        fflush (stdout);\n\n\n        password = fgets(buffer, sizeof(buffer), stdin);\n\n\n        if (password != NULL) {\n\n\n            buffer[strlen(buffer) - 1] = '\\0';\n\n\n            if (!CGPDFDocumentUnlockWithPassword (myDocument, password))\n\n\n                error(\"invalid password.\");\n\n\n        }\n\n\n    }\n\n\n}\n\n\nif (!CGPDFDocumentIsUnlocked (myDocument)) {\n// 4\n\n\n        error(\"can't unlock `%s'.\", filename);\n\n\n        CGPDFDocumentRelease(myDocument);\n\n\n        return EXIT_FAILURE;\n\n\n    }\n\n\n}\n\n\n if (CGPDFDocumentGetNumberOfPages(myDocument) == 0) {\n// 5\n\n\n        CGPDFDocumentRelease(myDocument);\n\n\n        return EXIT_FAILURE;\n\n\n}\n\nHere’s what the code does:\n\nCreates a CGPDFDocument object from a URL supplied to the code.\n\nChecks to make sure that a CGPDFDocument object is created. If not, the code exits because it makes no sense to continue without a document.\n\nChecks whether the document is encrypted. If the document is encrypted, the code attempts to open is using a blank password. If that fails, the code asks the user for a password and attempts to unlock the document with the password.\n\nChecks whether the document is unlocked. If it’s not, the code exits.\n\nChecks to make sure the document has at least one page. Otherwise, the code exits.\n\nScan the Content Stream for Each Page\n\nThe code fragment in Listing 14-5 scans each page in a document. When the scanner encounters one of the PDF operators for which you registered a callback, Quartz invokes your callback. A detailed explanation for each numbered line of code follows the listing.\n\nListing 14-5  Scanning each page of a document\n\nint k;\n\n\nCGPDFPageRef myPage;\n\n\nCGPDFScannerRef myScanner;\n\n\nCGPDFContentStreamRef myContentStream;\n\n\n \n\n\nnumOfPages = CGPDFDocumentGetNumberOfPages (myDocument);\n// 1\n\n\nfor (k = 0; k < numOfPages; k++) {\n\n\n    myPage = CGPDFDocumentGetPage (myDocument, k + 1 );\n// 2\n\n\n    myContentStream = CGPDFContentStreamCreateWithPage (myPage);\n// 3\n\n\n    myScanner = CGPDFScannerCreate (myContentStream, myTable, NULL);\n// 4\n\n\n    CGPDFScannerScan (myScanner);\n// 5\n\n\n    CGPDFPageRelease (myPage);\n// 6\n\n\n    CGPDFScannerRelease (myScanner);\n// 7\n\n\n    CGPDFContentStreamRelease (myContentStream);\n// 8\n\n\n }\n\n\n CGPDFOperatorTableRelease(myTable);\n// 9\n\nHere’s what the code does:\n\nGets the number of pages in the document that you previously opened. See Open the PDF Document.\n\nRetrieves a page to scan. The page numbers start at 1.\n\nCreates a content stream for the page.\n\nCreates a scanner for the content stream. You must pass the content stream and the operator table that you previously created and set with callbacks. See Create and Set Up the Operator Table. You can also pass any data that your callbacks need.\n\nParses the content stream associated with the scanner. Quartz invokes your callback each time it encounters one of the operators for which you provided a callback.\n\nReleases the page.\n\nReleases the scanner.\n\nReleases the content stream.\n\nReleases the operator table after scanning all the pages in the PDF.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Glossary",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/glossary/glossary.html#//apple_ref/doc/uid/TP30001066-CH221-SW7",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nGlossary\nalpha value  \n\nThe graphics state parameter that Quartz uses to determine how to composite newly painted objects to the existing page. At full intensity (alpha = 1.0), newly painted objects are opaque. At zero intensity, newly painted objects are invisible (alpha = 0.0).\n\n\n\naxial gradient  \n\nA fill that varies along an axis between two defined end points. All points that lie on a line perpendicular to the axis have the same color value. Also called a linear gradient.\n\n\n\nbitmap  \n\nA rectangular array (or raster) of pixels, each pixel representing a point in an image. Bitmap images are also called sampled images.\n\n\n\nblend mode  \n\nSpecifies how Quartz combines the foreground painting with the background painting.\n\n\n\nclipping area  \n\nA path used to constrain the drawing of other objects within its bounds.\n\n\n\ncolor space  \n\n A one-, two-, three-, or four-dimensional environment whose components (or channels) represent intensity values. For example, RGB space is a three-dimensional color space whose stimuli are the red, green, and blue intensities that make up a given color; and red, green, and blue are color channels.\n\n\n\nconcatenation  \n\nAn operation that combines two matrices by multiplying them together.\n\n\n\ncurrent graphics state  \n\nThe parameters values that determine how Quartz renders results as it paints.\n\n\n\ncurrent point  \n\nThe last location Quartz used when painting a path.\n\n\n\ncurrent transformation matrix  \n\nAn affine transform that Quartz uses to map points from one coordinate space to another.\n\n\n\ndevice color space  \n\nA color space that is tied to the system of color representation for a particular device. This type of color space is not suitable for interchanges of color data between different devices.\n\n\n\ndevice-independent color space  \n\nA color representation that is portable between devices and that is used for the interchanges of color data from the native color space of one device to the native color space of another device. Colors in a device-independent color space appear the same when displayed on different devices, to the extent that the capabilities of the device allow.\n\n\n\neven-odd rule  \n\nA fill rule that determines when to paint a pixel. The outcome does not depend on the direction that path segments are drawn. Compare with nonzero winding number rule.\n\n\n\nfill  \n\nAn operation that paints the area within a path.\n\n\n\ngeneric color space  \n\nA device-independent color space chosen automatically by Mac OS X to produce the best color for the drawing destination.\n\n\n\ngradient  \n\nA fill that varies from one color to another. See also axial gradient and radial gradient.\n\n\n\ngraphics context  \n\nAn opaque data type (CGContextRef) that encapsulates the information Quartz uses to draw images to an output device, such as a PDF file, a bitmap, or a window on a display. The information inside a graphics context includes graphics drawing parameters and a device-specific representation of the paint on the page.\n\n\n\nidentity transform  \n\nAn affine transform that, when applied to input coordinates, always returns the input coordinates.\n\n\n\nimage mask  \n\nA bitmap that specifies an area to paint, but not the color. An image mask acts like a stencil to specify where to place color on the page.\n\n\n\ninversion  \n\nAn operation that produces original coordinates from transformed ones.\n\n\n\nlayer context  \n\nAn offscreen drawing destination (CGLayerRef) designed for optimal performance. A a layer context is a much better choice for offscreen drawing than a bitmap graphics context.\n\n\n\nline cap  \n\nThe style that Quartz uses to draw the endpoint of a line—butt, round, or projecting square.\n\n\n\nline dash pattern  \n\nThe repeating series of line segments and spaces used to paint a dashed line.\n\n\n\nline join  \n\n The style that Quartz uses to draw the junction between connected line segments—miter, round, or bevel.\n\n\n\nline width  \n\nThe total width of a line, expressed in user space units.\n\n\n\nlinear gradient  \n\nSee axial gradient.\n\n\n\nnonzero winding number rule  \n\nA fill rule that determines when to paint a pixel. The outcome depends on the direction that path segments are drawn. Compare with even-odd rule.\n\n\n\npage  \n\nThe virtual canvas that Quartz paints to.\n\n\n\npainter’s model  \n\nA drawing model in which each successive drawing operation applies a layer of paint to a page.\n\n\n\npath  \n\nOne or more shapes (known as subpaths) that Quartz paints as a unit. A subpath can consist of straight lines, curves, or both. It can be open or closed.\n\n\n\npattern  \n\nA sequence of drawing operations that Quartz can repeatedly paint to a graphics context.\n\n\n\npattern space  \n\nAn abstract space that maps to the default user space by the transformation matrix (the pattern matrix) you specify when you create the pattern. Pattern space is separate from user space. The untransformed pattern space maps to the base (untransformed) user space, regardless of the state of the current transformation matrix.\n\n\n\npremultiplied alpha  \n\nA source color whose components are already multiplied by an alpha value. Premultiplying speeds up the rendering of an image by eliminating an extra multiplication operation per color component. See also alpha value.\n\n\n\nradial gradient  \n\nA fill that varies radially along an axis between two defined ends, which typically are both circles. Points share the same color value if they lie on the circumference of a circle whose center point falls on the axis. The radius of the circular sections of the gradient are defined by the radii of the end circles; the radius of each intermediate circle varies linearly from one end to the other.\n\n\n\nrendering intent  \n\nSpecifies how Quartz maps colors from the source color space to those that are within the gamut of the destination color space of a graphics context.\n\n\n\nrotation  \n\nAn operation that moves the coordinate space the specified angle.\n\n\n\nscaling  \n\nAn operation that changes the scale of the coordinate space by the specified x and y factors, effectively stretching or shrinking coordinates. The magnitude of the x and y factors governs whether the new coordinates are larger or smaller than the original. A negative factor flips the corresponding axis.\n\n\n\nshadow  \n\nAn image painted underneath, and offset from, a graphics object such that the shadow mimics the effect of a light source cast on the graphics object.\n\n\n\nstroke  \n\nAn operation that paints a line that straddles a path.\n\n\n\ntiling  \n\nThe process of rendering pattern cells to a portion of a page. Quartz has three tiling options—no distortion, constant spacing with minimal distortion, and constant spacing.\n\n\n\ntranslation  \n\nAn operation that moves the origin of the coordinate space by the number of units specified for the x and y axes.\n\n\n\ntransparency layer  \n\nA composite of two or more objects that Quartz treats as a single object when applying effects, such as shadows.\n\n\n\nuser space  \n\nThe device-independent coordinate system that you use when drawing using Quartz 2D.\n\n\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Document Revision History",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/RevisionHistory.html#//apple_ref/doc/uid/TP30001066-CH2-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nPrevious\nDocument Revision History\n\nThis table describes the changes to Quartz 2D Programming Guide.\n\nDate\tNotes\n2018-06-07\t\n\nFixed errors in code listings.\n\n\n2014-09-17\t\n\nAdded info about using vImage to work with raw pixel data.\n\n\n2013-12-16\t\n\nRemoved text chapter; use Core Text instead.\n\n\n2012-09-19\t\n\nCorrected typos and minor technical issues.\n\n\n2010-11-19\t\n\nUpdated for OS X v10.6 and iOS 4.2.\n\n\n2010-06-25\t\n\nMinor clarifications and editing.\n\n\n2009-05-18\t\n\nUpdated the font names in text examples to reflect fonts available on both iOS and OS X.\n\n\n2008-06-04\t\n\nUpdated for iOS SDK.\n\n\n \t\n\nAdded information about image formats to Bitmap Image Information.\n\n\n \t\n\nCorrected typos.\n\n\n2007-12-11\t\n\nRevised text chapter and added a glossary.\n\n\n \t\n\nAdded code that creates a dictionary and adds metadata to it. See Listing 13-4.\n\n\n2007-07-02\t\n\nUpdated for OS X v10.5.\n\n\n \t\n\nRenamed the Shadings chapter to Gradients and revised it to include information on the use of the CGGradientRef opaque data type.\n\n\n \t\n\nIn Data Management in Quartz 2D and a link and information about Image I/O Programming Guide.\n\n\n \t\n\nUpdated the introduction with recent, relevant related documentation and added a description of the revised Gradients chapter.\n\n\n \t\n\nRevised Quartz 2D Opaque Data Types to include CGGradientRef and provided links to information on CGImageSourceRef and CGImageDestinationRef opaque data types which are part of the Image I/O framework.\n\n\n \t\n\nUpdated Table 2-1 with additional pixel formats.\n\n\n2007-01-08\t\n\nFixed a number of minor technical issues.\n\n\n \t\n\nImproved the wording in the first paragraph of Gradients.\n\n\n \t\n\nMade a correction to the floating-point gray information in Table 2-1.\n\n\n \t\n\nCorrected the declarations in Listing 14-5\n\n\n2006-10-03\t\n\nMade minor technical improvements.\n\n\n \t\n\nAdded cross references to the reference documentation for the constants listed in Table 2-1.\n\n\n \t\n\nRemoved information on using a CGGLContextRef object because the use of a graphics context for OpenGL rendering is not reliable and is not recommended.\n\n\n \t\n\nAdded thread safety information to Creating a PostScript Converter Object.\n\n\n2006-07-24\t\n\nMade minor technical improvements.\n\n\n \t\n\nChanged Listing 2-6 so that is correctly frees the bitmap data.\n\n\n \t\n\nAdded cross references to Creating an Image From Part of a Larger Image and Creating an Image from a Bitmap Graphics Context that link to examples of creating graphics contexts.\n\n\n2006-06-28\t\n\nMade minor changes to clarify a few concepts.\n\n\n \t\n\nRevised Figure 12-2 and the text that describes it.\n\n\n \t\n\nRevised Figure 1-2 and the text that describes it.\n\n\n \t\n\nRevised information in “Python Bindings for Quartz 2D”.\n\n\n \t\n\nProvided hyperlinks to the functions and methods discussed in Data Management in Quartz 2D.\n\n\n \t\n\nCorrected a typographical error in Listing 2-2.\n\n\n2006-02-07\t\n\nCorrected typographical error.\n\n\n2006-01-10\t\n\nMade minor typographical and technical corrections.\n\n\n2005-11-09\t\n\nCorrected several technical, typographical, and formatting errors.\n\n\n \t\n\nMade changes to code in Listing 12-1.\n\n\n \t\n\nRevised introductory paragraphs in Transforms.\n\n\n \t\n\nRevised several sentences in How Quartz 2D Draws Text.\n\n\n2005-07-07\t\n\nCorrected typos and added clarification about Quartz OpenGL graphics context.\n\n\n2005-06-04\t\n\nFixed typos and added a Python script name.\n\n\n2005-04-29\t\n\nUpdated for OS X v10.4.\n\n\n \t\n\nChanged the title from Drawing With Quartz 2D to make it more consistent with the titles of similar documentation.\n\n\n \t\n\nRevised the Introduction to reflect the new content.\n\n\n \t\n\nSimplified the code in Figure 3-1.\n\n\n \t\n\nRevised the introductions for Color and Color Spaces, Transforms, Bitmap Images and Image Masks, and PDF Document Parsing.\n\n\n \t\n\nMade changes to code in Code that uses layers to draw a flag so that more appropriately-sized layers are used; substituted the function CGContextDrawLayerAtPoint for CGContextDrawLayerInRect.\n\n\n \t\n\nRevised the section Setting Blend Modes; added figures that show actual output produced using blend modes.\n\n\n \t\n\nRevised the section Using Blend Modes with Images and replaced the figures with better examples of drawing an image using different blend modes.\n\n\n \t\n\nAdded information about Core Image and Core Video in the opening paragraphs of Overview of Quartz 2D.\n\n\n \t\n\nIntroduced the notion of CGLayer objects in the section Drawing Destinations: The Graphics Context.\n\n\n \t\n\nAdded the new Tiger opaque objects to Quartz 2D Opaque Data Types.\n\n\n \t\n\nAdded blend mode to Graphics States. Added information about using blend modes to Paths and Bitmap Images and Image Masks.\n\n\n \t\n\nRevised Graphics Contexts to show how to use HIView. Also added new figures to many sections and provided information on HIView coordinates as compared to Quartz coordinates.\n\n\n \t\n\nAdded Table 2-1 to show the supported color spaces and pixel formats.\n\n\n \t\n\nReplaced Figure 2-4 to show an enlargement of aliased and antialiasing drawing and text.\n\n\n \t\n\nAdded Ellipses and revised discussions on Painting a Path and Clipping to a Path to reflect new Tiger content.\n\n\n \t\n\nChanged “clipping region” to “clipping area” throughout the entire book.\n\n\n \t\n\nRevised information on Creating Color Spaces to reflect Tiger content.\n\n\n \t\n\nAdded Evaluating Affine Transforms and Getting the User to Device Space Transform.\n\n\n \t\n\nRevised the chapter formerly titled Data Providers and Data Consumers to contain information on image sources and image destinations, and how to move data between Quartz 2D and Core Image. Retitled the chapter Data Management in Quartz 2D to reflect the revised content.\n\n\n \t\n\nRenamed the Bitmap Image chapter to Bitmap Images and Image Masks and substantially revised the content to reflect information about image sources, the new image creation functions, image masking function, and using blend modes to composite images.\n\n\n \t\n\nAdded the chapter Core Graphics Layer Drawing.\n\n\n \t\n\nAdded the chapter PDF Document Parsing, which contains some material from the old PDF Document chapter along with new material on scanners and content streams.\n\n\n \t\n\nAdded Copying Font Variations and PostScript Fonts to the Text chapter.\n\n\n2004-06-28\t\n\nRevised for Mac OS X v10.3.\n\n\n2001-07-01\t\n\nFirst version.\n\n\n\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Introduction to Color Management Overview",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/csintro/csintro_intro/csintro_intro.html#//apple_ref/doc/uid/TP30001148",
    "html": "Documentation Archive\nDeveloper\nSearch\nColor Management Overview\nTable of Contents\nIntroduction\nColor: A Brief Overview\nColor Spaces\nColor Management Systems\nRevision History\nNext\nIntroduction to Color Management Overview\n\nImportant: This document is no longer being updated. For the latest information about Apple SDKs, visit the documentation website.\n\nNote: This document was previously titled Color and Color Management Systems.\n\nThis document provides a general introduction to the basics of color and color management. Read this document to learn about:\n\nColor perception\n\nColor spaces\n\nThe values used to describe color\n\nColor conversion and color matching\n\nThe role of color management systems\n\nColor management is the process of maintaining consistent color among devices. Different imaging devices such as scanners, displays, and printers work in different color spaces, and each can have a different gamut (the range of colors a device can display). Color displays from different manufacturers all use RGB colors but may have different RGB gamuts as a result of the device type and its age. Printers and presses work in CMYK space and can vary drastically in their gamuts, especially if they use different printing technologies. Even a single printer’s gamut can vary significantly depending on the ink or type of paper in use. It’s easy to see that conversion from RGB colors on an individual display to CMYK colors on an individual printer using a specific ink and paper type can lead to unpredictable results.\n\nWho Should Read This Document\n\nThis document provides the conceptual foundation developers need to understand ColorSync developer documentation. You should be familiar with the concepts in this document before you read ColorSync Manager Reference.\n\nThe information in this document is also useful for anyone who works with color and wants to understand the challenges of maintaining consistent color from input device (scanner, digital camera) to output device (monitor, printer, printing press).\n\nOrganization of This Document\n\nThis document is divided into these chapters:\n\nColor: A Brief Overview discusses color perception and additive and subtractive color systems.\n\nColor Spaces describes how different peripheral devices represent color and the values used to represent color.\n\nColor Management Systems discusses the color matching problem and how color management systems maintain consistent color among devices.\n\nSee Also\n\nFor information on ColorSync, Apple’s standards-based solution for color management, see:\n\nColorSync Manager Reference, provides a complete reference for the ColorSync Manager application programming interface.\n\nFor more information on color theory and color spaces, see:\n\nBruce Fraser, Fred Bunting, and Chris Murphy. Real World Color Management, first edition. Peachpit Press, 2003.\n\nRoy S. Berns. Billmeyer and Saltzman’s Principles of Color Technology, third edition. Wiley-Interscience, 2000.\n\nJames D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. Computer Graphics: Principles and Practice in C, second edition. Addison-Wesley, 1995.\n\nRoy Hall. Illumination and Color in Computer Generated Imagery. New York: Springer-Verlag, 1988.\n\nR.W.G. Hunt. Measuring Colour, third edition. Fountain Pr LTD, 2001.\n\nGünther Wyszecki and W.S. Stiles. Color Science: Concepts and Methods, Quantitative Data and Formulae, second edition. John Wiley & Sons, 2000.\n\nNext\n\n\n\n\n\nCopyright © 2004, 2005 Apple Computer, Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2005-07-07\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Introduction",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageUnitTutorial/Introduction/Introduction.html#//apple_ref/doc/uid/TP40004531",
    "html": "Documentation Archive\nDeveloper\nSearch\nImage Unit Tutorial\nTable of Contents\nIntroduction\nAn Image Unit and Its Parts\nWriting Kernels\nWriting the Objective-C Portion\nPreparing an Image Unit for Distribution\nRevision History\nNext\nIntroduction\n\nAn image unit is a Core Image filter that is packaged as an NSBundle object. Any image processing filter that uses Core Image should be packaged as an image unit. Doing so makes it easy to distribute your filter. An image unit is not only a Core Image filter packaged as a bundle, it is a distribution mechanism for an image processing filter. This means that when you create an image unit, you also get the benefit of consistent packaging and an Apple-provided logo that you can license to use.\n\nThis tutorial provides the steps you need to write an image processing filter for OS X. More specifically, it shows you how to create image units that contain an executable filter. An executable filter has one portion that uses the central processing unit (CPU) to execute and another portion that uses the graphics processing unit (GPU). This document does not discuss nonexecutable filters, because they consist only of code that runs on the GPU and therefore have limitations.\n\nOrganization of This Document\n\nThe document is organized into these chapter:\n\nAn Image Unit and Its Parts describes the major parts of an image processing unit, what each does, and how they work together.\n\nWriting Kernels provides examples, from simple to complex, of kernel routines.\n\nWriting the Objective-C Portion describes the image unit template provided by Xcode, discusses each of the files provided in the template, and shows how to package some of the kernel routines from the previous chapter as image units.\n\nPreparing an Image Unit for Distribution discusses installing, validating, and testing image units and tells where to get more information on licensing the image unit logo.\n\nPrerequisite Reading\n\nBefore reading this document you should:\n\nRead Core Image Programming Guide.\n\nBe familiar with the Core Image API. See Core Image Reference Collection.\n\nWrite code that uses one of the built-in Core Image filters. See Using Core Image Filters in Core Image Programming Guide.\n\nTake a look at Core Image Kernel Language Reference, which describes the procedural programming language used to write the kernel portion of an image unit.\n\nIf you are not a Cocoa programmer, don’t panic! The kernel routine portion of an image processing filter uses a procedural language. If you know C, you’ll catch on fast. The higher-level portion of an image processing filter uses the Core Image API, which is an Objective-C API, but is not part of the Cocoa framework. Because Xcode provides a template for writing an image unit, you’ll see that it’s relatively straightforward to use Objective-C to write an image unit.\n\nIf you are not an OpenGL programmer, don’t worry. The Core Image API was designed to hide all the messy details of dealing with the GPU from you. Although the kernel routine portion of an image processing filter uses a subset of the OpenGL Shading Language (glslang), you’ll see by looking at the examples that you don’t need prior knowledge to write kernel routines. You do, however, need to have an understanding of the mathematics behind the processing that you want to implement.\n\nSee Also\n\nThe resources in this section are valuable to any developer who is writing an image unit. You’ll find them most helpful as you work your way through this document and later on, when you are writing your own image units.\n\nImageUnitAnalyzer is a tool that you use to ensure that any image unit you write is valid. After you install the developer tools, you can find the analyzer in /Developer/usr/bin.\n\nCIFilterBrowser is a widget that lets you inspect all installed image units as well as Core Image built-in filters. You can view the input parameters and attributes of a filter and see a preview of an output image produced by the filter.\n\nCore Image Fun House is an application that lets you explore all installed image units and Core Image built-in filters. You can choose any image to process and then apply one or more filters to the image by stacking the filters together. You can also turn off or on any filter in the stack to more closely examine filter effects. After you install the developer tools, you can find Core Image Fun House in /Developer/Applications/Graphics Tools/.\n\nCIAnnotation is a sample application that contains two image unit projects and uses them for compositing images and painting over them.\n\nQuartz Composer is an application that you can use to explore motion graphics. Core Image developers can use this application to test image units and to try out kernel routines. After you install the developer tools, you can find Quartz Composer in /Developer/Applications/.\n\nQuartz Composer User Guide describes how to use the Quartz Composer development tool.\n\nThe Quartz-dev mailing list is a technical discussion forum for developers using Quartz technologies on OS X, including Core Image. To sign up, see http://lists.apple.com/mailman/listinfo/quartz-dev.\n\nNext\n\n\n\n\n\nCopyright © 2011 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2011-06-06\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Introduction to Image Kit Programming Guide",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/ImageKitProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40004907",
    "html": "Documentation Archive\nDeveloper\nSearch\nImageKit Programming Guide\nTable of Contents\nIntroduction\nBasics of Using the Image Kit\nViewing, Editing, and Saving Images in an Image View\nBrowsing Images\nShowing Slides\nTaking Snapshots and Setting Pictures\nBrowsing Filters and Setting Input Parameters\nGlossary\nRevision History\nNext\nIntroduction to Image Kit Programming Guide\n\nImportant: This document is no longer being updated. For the latest information about Apple SDKs, visit the documentation website.\n\nThe Image Kit is an Objective-C framework, introduced in OS X v10.5, for browsing, viewing, editing, and processing images in an efficient manner. It also supports browsing Core Image filters, previewing the effects, and providing controls for individual filters. This document describes Image Kit user interface elements and shows, through code examples, how to use Image Kit classes.\n\nAnyone developing a digital media application that supports images will want to read this document to find out how to use the Image Kit to provide the best user experience possible. You don’t need to be a seasoned Cocoa programmer to use the Image Kit or to read this guide, although prior experience with Objective-C is helpful and will make it easier to understand the code examples.\n\nThe examples in this document use Xcode version 3.0 and Interface Builder version 3.0.\n\nAt a Glance\n\nImage Kit has a variety of image capabilities that your app can adopt. You should first read Basics of Using the Image Kit to can an understanding of the tasks that each Image Kit class supports. You’ll also get an overview of how to use the framework with Apple’s developer tools.\n\nImage Views Support Viewing, Editing, and Saving Images\n\nImage Kit provides a user interface that displays a single image and supports editing and saving images.\n\nRelevant Chapter: Viewing, Editing, and Saving Images in an Image View\n\nUse and Image Browser to Support Looking Through Large Image Sets\n\nImage Kit’s browser class (IKImageBrowserView) provides a user interface that allows users to view large sets of images.\n\nRelevant Chapter: Browsing Images\n\nSlideshows are Easy to Support\n\nThe IKSlideshow class provides controls for viewing and advancing through a slideshow. You can also allow users to see index sheets and view slides in fullscreen mode.\n\nRelevant Chapter: Showing Slides\n\nUse the Picture Taker to Support Setting Buddy Photos\n\nThe IKPictureTaker class is a lightweight panel that you can use to allow users to choose pictures and take snapshots for use as a “buddy” picture or anywhere the user needs an icon-sized image.\n\nRelevant Chapter: Taking Snapshots and Setting Pictures\n\nBrowse and Set Parameters for Core Image Filters\n\nThe IKFilterBrowserPanel and IKFilterBrowserView classes support browsing built-in Core Image filters by category, previewing a filter’s effect, and applying the filter to an image.\n\nRelevant Chapter: Browsing Filters and Setting Input Parameters\n\nSee Also\n\nThe following documents are helpful guides and references for many of the tasks described in the book:\n\nImage Kit Reference Collection contains the class and protocol reference documentation.\n\nCore Image Programming Guide shows how to create Core Image filters (CIFilter objects) and apply them to images.\n\nImage I/O Programming Guide provides information on how to create an image source and extract an image from an image source.\n\nNext\n\n\n\n\n\nCopyright © 2008 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2008-06-09\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "PostScript Conversion",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_ps_convert/dq_ps_convert.html#//apple_ref/doc/uid/TP30001066-CH215-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nPostScript Conversion\n\nThe Preview application automatically converts PostScript files to PDF. The Quartz 2D API provides functions you can use to perform PostScript conversion in your application. The Quartz 2D PostScript conversions functions are not available in iOS.\n\nFollow these steps to convert a PostScript document to a PDF document:\n\nWrite callbacks. Quartz communicates the status of per page processes through callbacks.\n\nFill a callbacks structure.\n\nCreate a PostScript converter object.\n\nCreate a data provider object for the PostScript file you want to convert.\n\nCreate a data consumer object for the PDF that results from the conversion.\n\nPerform the conversion.\n\nEach of these steps is discussed in the sections that follow.\n\nWriting Callbacks\n\nCallbacks provide a way for Quartz to inform your application of the status of the conversion. If your application has a user interface, you can use the status information to provide feedback to the user, as shown in Figure 15-1.\n\nFigure 15-1  A status message for a PostScript conversion application\n\nYou can provide callbacks to inform your application that Quartz 2D is:\n\nStarting the conversion (CGPSConverterBeginDocumentCallback). Quartz 2D passes your callback a generic pointer to data you supply.\n\nEnding the conversion (CGPSConverterEndDocumentCallback). Quartz 2D passes your callback a generic pointer to data you supply and a Boolean value that indicates success (true) or failure (false).\n\nStarting a page (CGPSConverterBeginPageCallback). Quartz 2D passes your callback a generic pointer to data you supply, the page number, and a CFDictionary object, which is currently not used.\n\nEnding a page (CGPSConverterEndPageCallback). Quartz 2D passes your callback a generic pointer to data you supply and a Boolean value that indicates success (true) or failure (false)\n\nProgressing with the conversion (CGPSConverterProgressCallback). This callback is invoked periodically throughout the conversion. Quartz 2D passes your callback a generic pointer to data you supply.\n\nSending a message about the process (CGPSConverterMessageCallback). There are several kinds of messages that can be sent during a conversion process. The most likely are font substitution messages, and any messages that the PostScript code itself generates. Any PostScript messages written to stdout are routed through this callback—typically these are debugging or status messages. In addition, there can be error messages if the document is malformed.\n\nQuartz 2D passes your callback a generic pointer to data you supply and a CFString object that contains a message about the conversion.\n\nDeallocating the PostScript converter object (CGPSConverterReleaseInfoCallback). You can use this callback to deallocate the generic pointer if you’ve provided data and to perform any additional postprocessing tasks. Quartz 2D passes your callback a generic pointer to data you supply.\n\nSee the CGPSConverter Reference for the prototype each callback follows.\n\nFilling In a Callbacks Structure\n\nYou need to assign a version number and the callbacks you created to the appropriate fields of the CGPSConverterCallbacks data structure (shown in Listing 15-1). The version is 0. Assign NULL to those fields for which you do not supply a callback.\n\nListing 15-1  The PostScript converter callbacks data structure\n\nstruct CGPSConverterCallbacks {\n\n\n   unsigned int version;\n\n\n   CGPSConverterBeginDocumentCallback beginDocument;\n\n\n   CGPSConverterEndDocumentCallback endDocument;\n\n\n   CGPSConverterBeginPageCallback beginPage;\n\n\n   CGPSConverterEndPageCallback endPage;\n\n\n   CGPSConverterProgressCallback noteProgress;\n\n\n   CGPSConverterMessageCallback noteMessage;\n\n\n   CGPSConverterReleaseInfoCallback releaseInfo;\n\n\n};\nCreating a PostScript Converter Object\n\nYou call the function CGPSConverterCreate to create a PostScript converter object. This function takes three parameters:\n\nA pointer to generic data that you want passed to your callbacks. You can supply NULL if you don’t need to provide any data.\n\nA pointer to a filled-out CGPSConverterCallbacks data structure.\n\nNULL. This field is reserved for future use.\n\nImportant: Although the CGPSConverterConvert function is thread safe (it uses locks to prevent more than one conversion at a time in the same process), it is not thread safe with respect to the Resource Manager. If your application uses the Resource Manager on a separate thread, you should either use locks to prevent CGPSConverterConvert from executing during your usage of the Resource Manager or you should perform your conversions using the PostScript converter in a separate process.\n\nCreating Data Provider and Data Consumer Objects\n\nYou create a data provider object by calling the function CGDataProviderCreateWithURL, supplying a CFURL object that specifies the address of the PostScript file you want to convert.\n\nSimilarly, you create a data consumer object by calling the function CGDataConsumerCreateWithURL, supplying a CFURL object that specifies the address of the PDF document that results from the conversion.\n\nPerforming the Conversion\n\nYou call the function CGPSConverterConvert to perform the actual conversion from PostScript to PDF. This function takes as parameters:\n\nA PostScript converter object.\n\nA data provider object that supplies PostScript data.\n\nA data consumer object for the converted data.\n\nNULL. This parameter is reserved for future use.\n\nThe function returns true if the conversion is successful.\n\nYou can call the function CGPSConverterIsConverting to check whether the conversion is still progressing.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Gradients",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_shadings/dq_shadings.html#//apple_ref/doc/uid/TP30001066-CH207-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nGradients\n\nQuartz provides two opaque data types for creating gradients—CGShadingRef and CGGradientRef. You can use either of these to create axial or radial gradients. A gradient is a fill that varies from one color to another.\n\nAn axial gradient (also called a linear gradient) varies along an axis between two defined end points. All points that lie on a line perpendicular to the axis have the same color value.\n\nA radial gradient is a fill that varies radially along an axis between two defined ends, which typically are both circles. Points share the same color value if they lie on the circumference of a circle whose center point falls on the axis. The radius of the circular sections of the gradient are defined by the radii of the end circles; the radius of each intermediate circle varies linearly from one end to the other.\n\nThis chapter provides examples of the sorts of linear and radial gradients you can create with Quartz, compares the two approaches you can take to painting gradients, and then shows how to use each opaque data type to create a gradient.\n\nAxial and Radial Gradient Examples\n\nQuartz functions provide a rich vocabulary for creating gradient effects. This section shows some of the results you can achieve. The axial gradient in Figure 8-1 varies between one endpoint that is a shade of orange and another that is a shade of yellow. In this case, the axis is at a 45 degree angle with respect to the origin.\n\nFigure 8-1  An axial gradient along a 45 degree axis\n\nQuartz also lets you specify colors and locations along an axis to create more complex axial gradients, as shown in Figure 8-2. The color at the starting point is a shade of red and the color at the ending point is a shade of violet. However, there are also five locations on the axis whose color is set to orange, yellow, green, blue, and indigo, respectively. You can think of the result as six sequential linear gradients along the same axis. Although the axis used here is the same as that used in Figure 8-1 (45 degree angle), it doesn’t have to be. The angle of the axis is defined by the starting and ending point that you provide.\n\nFigure 8-2  An axial gradient created with seven locations and colors\n\nFigure 8-3 shows a radial gradient that varies between a small, bright red circle and a larger black one.\n\nFigure 8-3  A radial gradient that varies between two circles\n\nWith Quartz, you are not restricted to creating gradients based on color changes; you can vary only the alpha, or you can vary the alpha along with the other color components. Figure 8-4 shows a gradient whose red, green, and blue components remain constant as the alpha value varies from 1.0 to 0.1.\n\nNote: If you vary a gradient using alpha, you will not be able to capture that gradient when drawing to a PDF content. Because of this, such a gradient can’t be printed. If you need to draw a gradient to a PDF, use an alpha of 1.0.\n\nFigure 8-4  A radial gradient created by varying only the alpha component\n\nYou can position the circles in a radial gradient to create a variety of shapes. If one circle is partially or fully outside the other, Quartz creates a conical surface for circles that have unequal circumferences, and a cylindrical surface for circles that have equal circumferences. A common use of a radial gradient is to create a shaded sphere, as shown in Figure 8-5. In this case, a single point (a circle with a radius of 0) lies within a larger circle.\n\nFigure 8-5  A radial gradient that varies between a point and a circle\n\nYou can create more complex effects by nesting several radial gradients similar to the shape shown in Figure 8-6. The toroidal portion of the shape is created using concentric circles.\n\nFigure 8-6  Nested radial gradients\nA Comparison of CGShading and CGGradient Objects\n\nWith two type of objects available for creating gradients, you might be wondering which one is best to use. This section helps answer that question.\n\nThe CGShadingRef opaque data type gives you control over how the color at each point in the gradient is computed. Before you can create a CGShading object, you must create a CGFunction object (CGFunctionRef) that defines a function for computing colors in the gradient. Writing a custom function gives you the freedom to create smooth gradients, such as those shown in Figure 8-1, Figure 8-3, and Figure 8-5 or more unconventional effects, such as that shown in Figure 8-12.\n\nWhen you create a CGShading object, you specify whether it is axial (linear) or radial. Along with the gradient calculation function (encapsulated as a CGFunction object) you also supply a color space, and starting and ending points or radii, depending on whether you draw an axial or radial gradient. At drawing time, you simply pass the CGShading object along with the drawing context to the function CGContextDrawShading. Quartz invokes your gradient calculation function for each point in the gradient.\n\nA CGGradient object is a subset of a CGShading object that’s designed with ease-of-use in mind. The CGGradientRef opaque data type is straightforward to use because Quartz calculates the color at each point in the gradient for you—you don’t supply a gradient calculation function. When you create a gradient object, you provide an array of locations and colors. Quartz calculates a gradient for each set of contiguous locations, using the color you assign to each location as the end points for the gradient. You can set a gradient object to use a single starting and ending location, as shown in Figure 8-1, or you can provide a number of points to create an effect similar to what’s shown in Figure 8-2. The ability to provide more than two locations is an advantage over using a CGShading object, which is limited to two locations.\n\nWhen you create a CGGradient object, you simply set up a color space, locations, and a color for each location. When you draw to a context using a gradient object, you specify whether Quartz should draw an axial or radial gradient. At drawing time, you specify starting and ending points or radii, depending on whether you draw an axial or radial gradient, in contrast to CGShading objects, whose geometry is defined at creation time, not at drawing time.\n\nTable 8-1 summarizes the differences between the two opaque data types.\n\nTable 8-1  Differences between CGShading and CGGradient objects\n\nCGGradient\n\n\t\n\nCGShading\n\n\n\n\nCan use the same object to draw axial and radial gradients.\n\n\t\n\nNeed to create separate objects for axial and radial gradients.\n\n\n\n\nSet the geometry of the gradient at drawing time.\n\n\t\n\nSet the geometry of the gradient at object creation time.\n\n\n\n\nQuartz calculates the colors for each point in the gradient.\n\n\t\n\nYou must supply a callback function that calculates the colors for each point in the gradient.\n\n\n\n\nEasy to define more than two locations and colors.\n\n\t\n\nNeed to design your callback to use more than two locations and colors, so it takes a bit more work on your part.\n\nExtending Color Beyond the End of a Gradient\n\nWhen you create a gradient, you have the option of filling the space beyond the ends of the gradient with a solid color. Quartz uses the color defined at the boundary of the gradient as the fill color. You can extend beyond the start of a gradient, the end of a gradient, or both. You can apply the option to an axial or a radial gradient created using either a CGShading object or a CGGradient object. Each type of object supplies constants you can use to set the extension option, as you’ll see in Using a CGGradient Object and Using a CGShading Object.\n\nFigure 8-7 shows an axial gradient that extends at both the starting and ending locations. The line in the figure shows the axis of the gradient. As you can see, the fill colors correspond to the colors at the starting and ending points.\n\nFigure 8-7  Extending an axial gradient\n\nFigure 8-8 compares a radial gradient that does not use the extension options with one that uses extension options for both the starting and ending locations. Quartz takes the starting and ending color values and uses those solid colors to extend the surface as shown. The figure shows the starting and ending circles, and the axis of the gradient.\n\nFigure 8-8  Extending a radial gradient\nUsing a CGGradient Object\n\nThe CGGradient object is an abstract definition of a gradient—it simply specifies colors and locations, but not the geometry. You can use this same object for both axial and radial geometries. As an abstract definition, CGGradient objects are perhaps more readily reusable than their counterparts, CGShading objects. Not having the geometry locked in the CGGradient object allows for the possibility of iteratively painting gradients based on the same color scheme without the need for also tying up memory resources in multiple CGGradient objects.\n\nBecause Quartz calculates the gradient for you, using a CGGradient object to create and draw a gradient is fairly straightforward, requiring these steps:\n\nCreate a CGGradient object, supplying a color space, an array of two or more color components, an array of two or more locations, and the number of items in each of the two arrays.\n\nPaint the gradient by calling either CGContextDrawLinearGradient or CGContextDrawRadialGradient and supplying a context, a CGGradient object, drawing options, and the stating and ending geometry (points for axial gradients or circle centers and radii for radial gradients).\n\nRelease the CGGradient object when you no longer need it.\n\nA location is a CGFloat value in the range of 0.0 to 1.0, inclusive, that specifies the normalized distance along the axis of the gradient. A value of 0.0 specifies the starting point of the axis, while 1.0 specifies the ending point of the axis. Other values specify a proportion of the distance, such as 0.25 for one-fourth of the distance from the starting point and 0.5 for the halfway point on the axis. At a minimum, Quartz uses two locations. If you pass NULL for the locations array, Quartz uses 0 for the first location and 1 for the second.\n\nThe number of color components per color depends on the color space. For onscreen drawing, you’ll use an RGB color space. Because Quartz draws with alpha, each onscreen color has four components—red, green, blue, and alpha. So, for onscreen drawing, the number of elements in the color component array that you provide must contain four times the number of locations. Quartz RGBA color components can vary in value from 0.0 to 1.0, inclusive.\n\nListing 8-1 is a code fragment that creates a CGGradient object. After declaring the necessary variables, the code sets the locations and the requisite number of color components (for this example, 2 X 4 = 8). It creates a generic RGB color space. (In iOS, where generic RGB color spaces are not available, your code should call CGColorSpaceCreateDeviceRGB instead.) Then, it passes the necessary parameters to the function CGGradientCreateWithColorComponents. You can also use the function CGGradientCreateWithColors which is convenient if your application sets up CGColor objects.\n\nListing 8-1  Creating a CGGradient object\n\nCGGradientRef myGradient;\n\n\nCGColorSpaceRef myColorspace;\n\n\nsize_t num_locations = 2;\n\n\nCGFloat locations[2] = { 0.0, 1.0 };\n\n\nCGFloat components[8] = { 1.0, 0.5, 0.4, 1.0,  // Start color\n\n\n                          0.8, 0.8, 0.3, 1.0 }; // End color\n\n\n \n\n\nmyColorspace = CGColorSpaceCreateWithName(kCGColorSpaceGenericRGB);\n\n\nmyGradient = CGGradientCreateWithColorComponents (myColorspace, components,\n\n\n                          locations, num_locations);\n\nAfter you create a CGGradient object, you can use it to paint an axial or linear gradient. Listing 8-2 is a code fragment that declares and sets the starting and ending points for a linear gradient and then paints the gradient. Figure 8-1 shows the result. The code does not show how to obtain the CGContext object (myContext).\n\nListing 8-2  Painting an axial gradient using a CGGradient object\n\nCGPoint myStartPoint, myEndPoint;\n\n\nmyStartPoint.x = 0.0;\n\n\nmyStartPoint.y = 0.0;\n\n\nmyEndPoint.x = 1.0;\n\n\nmyEndPoint.y = 1.0;\n\n\nCGContextDrawLinearGradient (myContext, myGradient, myStartPoint, myEndPoint, 0);\n\nListing 8-3 is a code fragment that uses the CGGradient object created in Listing 8-1 to paint the radial gradient shown in Figure 8-9. This example illustrates the result of extending the area of the gradient by filling it with a solid color.\n\nListing 8-3  Painting a radial gradient using a CGGradient object\n\nCGPoint myStartPoint, myEndPoint;\n\n\nCGFloat myStartRadius, myEndRadius;\n\n\nmyStartPoint.x = 0.15;\n\n\nmyStartPoint.y = 0.15;\n\n\nmyEndPoint.x = 0.5;\n\n\nmyEndPoint.y = 0.5;\n\n\nmyStartRadius = 0.1;\n\n\nmyEndRadius = 0.25;\n\n\nCGContextDrawRadialGradient (myContext, myGradient, myStartPoint,\n\n\n                         myStartRadius, myEndPoint, myEndRadius,\n\n\n                         kCGGradientDrawsAfterEndLocation);\nFigure 8-9  A radial gradient painted using a CGGradient object\n\nThe radial gradient shown in Figure 8-4 was created using the variables shown in Listing 8-4.\n\nListing 8-4  The variables used to create a radial gradient by varying alpha\n\nCGPoint myStartPoint, myEndPoint;\n\n\nCGFloat myStartRadius, myEndRadius;\n\n\nmyStartPoint.x = 0.2;\n\n\nmyStartPoint.y = 0.5;\n\n\nmyEndPoint.x = 0.65;\n\n\nmyEndPoint.y = 0.5;\n\n\nmyStartRadius = 0.1;\n\n\nmyEndRadius = 0.25;\n\n\nsize_t num_locations = 2;\n\n\nCGFloat locations[2] = { 0, 1.0 };\n\n\nCGFloat components[8] = { 0.95, 0.3, 0.4, 1.0,\n\n\n                          0.95, 0.3, 0.4, 0.1 };\n\nListing 8-5 shows the variables used to create the gray gradient shown in Figure 8-10, which has three locations.\n\nListing 8-5  The variables used to create a gray gradient\n\nsize_t num_locations = 3;\n\n\nCGFloat locations[3] = { 0.0, 0.5, 1.0};\n\n\nCGFloat components[12] = {  1.0, 1.0, 1.0, 1.0,\n\n\n                            0.5, 0.5, 0.5, 1.0,\n\n\n                            1.0, 1.0, 1.0, 1.0 };\nFigure 8-10  An axial gradient with three locations\nUsing a CGShading Object\n\nYou set up a gradient by creating a CGShading object calling the function CGShadingCreateAxial or CGShadingCreateRadial, supplying the following parameters:\n\nA CGColorSpace object that describe the color space for Quartz to use when it interprets the color component values your callback supplies.\n\nStarting and ending points. For axial gradients, these are the starting and ending coordinates (in user space) of the axis. For radial gradients, these are the coordinates of the center of the starting and ending circles.\n\nStarting and ending radii (only for a radial gradient) for the circles used to define the gradient area.\n\nA CGFunction object, which you obtain by calling the function CGFunctionCreate, discussed later in this section. This callback routine must return a color to draw at a particular point.\n\nBoolean values that specify whether to fill the area beyond the starting or ending points with a solid color.\n\nThe CGFunction object you supply to the CGShading creation functions contains a callbacks structure and all the information Quartz needs to implement your callback. Perhaps the trickiest part of setting up a CGShading object is creating the CGFunction object. When you call the function CGFunctionCreate, you supply the following:\n\nA pointer to any data your callback needs.\n\nThe number of input values to your callback. Quartz requires that your callback takes one input value.\n\nAn array of floating-point values. Quartz supplies your callback with only one element in this array. An input value can range from 0, for the color at the start of the gradient, to 1, for the color at the end of the gradient.\n\nThe number of output values provided by your callback. For each input value, your callback must supply a value for each color component and an alpha value to designate opacity. The color component values are interpreted by Quartz in the color space you create and supply to the CGShading creation function. For example, if you are using an RGB color space, you supply the value 4 as the number of output values (R, G, B, and A).\n\nAn array of floating-point values that specify each of the color components and an alpha value.\n\nA callbacks data structure that contains the version of the structure (set this field to 0), your callback for generating color component values, and an optional callback to release the data supplied to your callback in the info parameter. If you were to name your callback myCalculateShadingValues, it would look like this:\n\nvoid myCalculateShadingValues (void *info, const CGFloat *in, CGFloat *out)\n\nAfter you create the CGShading object, you can set up additional clipping if you need to do so. Then, call the function CGContextDrawShading to paint the clipping area of the context with the gradient. When you call this function, Quartz invokes your callback to obtain color values that span the range from the starting point to the ending point.\n\nWhen you no longer need the CGShading object, you release it by calling the function CGShadingRelease.\n\nPainting an Axial Gradient Using a CGShading Object and Painting a Radial Gradient Using a CGShading Object provide step-by-step instructions on writing code that uses a CGShading object to draw a gradient.\n\nPainting an Axial Gradient Using a CGShading Object\n\nAxial and radial gradients require you to perform similar steps. This example shows how to draw an axial gradient using a CGShading object, create a semicircular clipping path in a graphics context, then paint the gradient to the clipped context to achieve the output shown in Figure 8-11.\n\nFigure 8-11  An axial gradient that is clipped and painted\n\nTo paint the axial gradient shown in the figure, follow the steps explained in these sections:\n\nSet Up a CGFunction Object to Compute Color Values\n\nCreate a CGShading Object for an Axial Gradient\n\nClip the Context\n\nPaint the Axial Gradient Using a CGShading Object\n\nRelease Objects\n\nSet Up a CGFunction Object to Compute Color Values\n\nYou can compute color values any way you like, as long as your color computation function takes three parameters:\n\nvoid *info. This is NULL or a pointer to data you pass to the CGShading creation function.\n\nconst CGFloat *in. Quartz passes the in array to your callback. The values in the array must be in the input value range defined for your CGFunction object. For this example, the input range is 0 to 1; see Listing 8-7.\n\nCGFloat *out. Your callback passes the out array to Quartz. It contains one element for each color component in the color space, and an alpha value. Output values should be in the output value range defined for your CGFunction object. For this example, the output range is 0 to 1; see Listing 8-7.\n\nFor more information on these parameters, see CGFunctionEvaluateCallback.\n\nListing 8-6 shows a function that computes color component values by multiplying the values defined in a constant array by the input value. Because the input value ranges from 0 through 1, the output values range from black (for RGB, the values 0, 0, 0), through (1, 0, .5) which is a purple hue. Note that the last component is always set to 1, so that the colors are always fully opaque.\n\nListing 8-6  Computing color component values\n\n \n\n\nstatic void myCalculateShadingValues (void *info,\n\n\n                            const CGFloat *in,\n\n\n                            CGFloat *out)\n\n\n{\n\n\n    CGFloat v;\n\n\n    size_t k, components;\n\n\n    static const CGFloat c[] = {1, 0, .5, 0 };\n\n\n \n\n\n    components = (size_t)info;\n\n\n \n\n\n    v = *in;\n\n\n    for (k = 0; k < components -1; k++)\n\n\n        *out++ = c[k] * v;\n\n\n     *out++ = 1;\n\n\n}\n\n\n \n\nAfter you write your callback to compute color values, you package it as part of a CGFunction object. It’s the CGFunction object you supply to Quartz when you create a CGShading object. Listing 8-7 shows a function that creates a CGFunction object that contains the callback from Listing 8-6. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 8-7  Creating a CGFunction object\n\n \n\n\nstatic CGFunctionRef myGetFunction (CGColorSpaceRef colorspace)\n// 1\n\n\n{\n\n\n    size_t numComponents;\n\n\n    static const CGFloat input_value_range [2] = { 0, 1 };\n\n\n    static const CGFloat output_value_ranges [8] = { 0, 1, 0, 1, 0, 1, 0, 1 };\n\n\n    static const CGFunctionCallbacks callbacks = { 0,\n// 2\n\n\n                                &myCalculateShadingValues,\n\n\n                                NULL };\n\n\n \n\n\n    numComponents = 1 + CGColorSpaceGetNumberOfComponents (colorspace);\n// 3\n\n\n    return CGFunctionCreate ((void *) numComponents, \n// 4\n\n\n                                1, \n// 5\n\n\n                                input_value_range, \n// 6\n\n\n                                numComponents, \n// 7\n\n\n                                output_value_ranges, \n// 8\n\n\n                                &callbacks);\n// 9\n\n\n}\n\nHere’s what the code does:\n\nTakes a color space as a parameter.\n\nDeclares a callbacks structure and fills it with the version of the structure (0), a pointer to your color component calculation callback, and NULL for the optional release function.\n\nCalculates the number of color components in the color space and increments the value by 1 to account for the alpha value.\n\nPasses a pointer to the numComponents value. This value is used by the callback myCalculateShadingValues to determine the number of components to compute.\n\nSpecifies that 1 is the number of input values to the callback.\n\nProvides an array that specifies the valid intervals for the input. This array contains 0 and 1.\n\nPasses the number of output values, which is the number of color components plus alpha.\n\nProvides an array that specifies the valid intervals for each output value. This array specifies, for each component, the intervals 0 and 1. Because there are four components, there are eight elements in this array.\n\nPasses a pointer to the callback structure declared and filled previously.\n\nCreate a CGShading Object for an Axial Gradient\n\nTo create a CGShading object, you call the function CGShadingCreateAxial, as shown in Listing 8-8, passing a color space, starting and ending points, a CGFunction object, and a Boolean value that specifies whether to fill the area beyond the starting and ending points of the gradient.\n\nListing 8-8  Creating a CGShading object for an axial gradient\n\nCGPoint     startPoint,\n\n\n            endPoint;\n\n\nCGFunctionRef myFunctionObject;\n\n\nCGShadingRef myShading;\n\n\n \n\n\nstartPoint = CGPointMake(0,0.5);\n\n\nendPoint = CGPointMake(1,0.5);\n\n\ncolorspace = CGColorSpaceCreateDeviceRGB();\n\n\nmyFunctionObject = myGetFunction (colorspace);\n\n\n \n\n\nmyShading = CGShadingCreateAxial (colorspace,\n\n\n                        startPoint, endPoint,\n\n\n                        myFunctionObject,\n\n\n                        false, false);\nClip the Context\n\nWhen you paint a gradient, Quartz fills the current context. Painting a gradient is different from working with colors and patterns, which are used to stroke and fill path objects. As a result, if you want your gradient to appear in a particular shape, you need to clip the context accordingly. The code in Listing 8-9 adds a semicircle to the current context so that the gradient is painted into that clip area, as shown in Figure 8-11.\n\nIf you look carefully, you’ll notice that the code should result in a half circle, whereas the figure shows a half ellipse. Why? You’ll see, when you look at the entire routine in A Complete Routine for an Axial Gradient Using a CGShading Object, that the context is also scaled. More about that later. Although you might not need to apply scaling or a clip in your application, these and many other options exist in Quartz 2D to help you achieve interesting effects.\n\nListing 8-9  Adding a semicircle clip to the graphics context\n\n    CGContextBeginPath (myContext);\n\n\n    CGContextAddArc (myContext, .5, .5, .3, 0,\n\n\n                    my_convert_to_radians (180), 0);\n\n\n    CGContextClosePath (myContext);\n\n\n    CGContextClip (myContext);\nPaint the Axial Gradient Using a CGShading Object\n\nCall the function CGContextDrawShading to fill the current context using the color gradient specified in the CGShading object:\n\nCGContextDrawShading (myContext, myShading);\nRelease Objects\n\nYou call the function CGShadingRelease when you no longer need the CGShading object. You also need to release the CGColorSpace object and the CGFunction object as shown in Listing 8-10.\n\nListing 8-10  Releasing objects\n\nCGShadingRelease (myShading);\n\n\nCGColorSpaceRelease (colorspace);\n\n\nCGFunctionRelease (myFunctionObject);\nA Complete Routine for an Axial Gradient Using a CGShading Object\n\nThe code in Listing 8-11 shows a complete routine that paints an axial gradient, using the CGFunction object set up in Listing 8-7 and the callback shown in Listing 8-6. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 8-11  Painting an axial gradient using a CGShading object\n\nvoid myPaintAxialShading (CGContextRef myContext,\n// 1\n\n\n                            CGRect bounds)\n\n\n{\n\n\n    CGPoint     startPoint,\n\n\n                endPoint;\n\n\n    CGAffineTransform myTransform;\n\n\n    CGFloat width = bounds.size.width;\n\n\n    CGFloat height = bounds.size.height;\n\n\n \n\n\n \n\n\n    startPoint = CGPointMake(0,0.5); \n// 2\n\n\n    endPoint = CGPointMake(1,0.5);\n// 3\n\n\n \n\n\n    colorspace = CGColorSpaceCreateDeviceRGB();\n// 4\n\n\n    myShadingFunction = myGetFunction(colorspace);\n// 5\n\n\n \n\n\n    shading = CGShadingCreateAxial (colorspace, \n// 6\n\n\n                                 startPoint, endPoint,\n\n\n                                 myShadingFunction,\n\n\n                                 false, false);\n\n\n \n\n\n    myTransform = CGAffineTransformMakeScale (width, height);\n// 7\n\n\n    CGContextConcatCTM (myContext, myTransform);\n// 8\n\n\n    CGContextSaveGState (myContext);\n// 9\n\n\n \n\n\n    CGContextClipToRect (myContext, CGRectMake(0, 0, 1, 1));\n// 10\n\n\n    CGContextSetRGBFillColor (myContext, 1, 1, 1, 1);\n\n\n    CGContextFillRect (myContext, CGRectMake(0, 0, 1, 1));\n\n\n \n\n\n    CGContextBeginPath (myContext);\n// 11\n\n\n    CGContextAddArc (myContext, .5, .5, .3, 0,\n\n\n                        my_convert_to_radians (180), 0);\n\n\n    CGContextClosePath (myContext);\n\n\n    CGContextClip (myContext);\n\n\n \n\n\n    CGContextDrawShading (myContext, shading);\n// 12\n\n\n    CGColorSpaceRelease (colorspace);\n// 13\n\n\n    CGShadingRelease (shading);\n\n\n    CGFunctionRelease (myShadingFunction);\n\n\n \n\n\n    CGContextRestoreGState (myContext); \n// 14\n\n\n}\n\nHere’s what the code does:\n\nTakes as parameters a graphics context and a rectangle to draw into.\n\nAssigns a value to the starting point. The routine calculates values based on a user space that varies from 0 to 1. You’ll scale the space later for the window that Quartz draws into. You can think of this coordinate location as x at the far left side and y at 50% from the bottom.\n\nAssigns a value to the ending point. You can think of this coordinate location as x at the far right side and y at 50% from the bottom. As you can see, the axis for the gradient is a horizontal line.\n\nCreates a color space for device RGB because this routine draws to the display.\n\nCreates a CGFunction object by calling the routine shown in Listing 8-7 and passing the color space you just created.\n\nCreates a CGShading object for an axial gradient. The last two parameters are false, to signal that Quartz should not fill the area beyond the starting and ending points.\n\nSets up an affine transform that is scaled to the height and width of the window used for drawing. Note that the height is not necessarily equal to the width. In this example, because the two aren’t equal, the end result is elliptical rather than circular.\n\nConcatenates the transform you just set up with the graphics context passed to the routine.\n\nSaves the graphics state to enable you to restore this state later.\n\nSets up a clipping area. This line and the next two lines clip the context to a rectangle that is filled with white. The effect is that the gradient is drawn to a window with a white background.\n\nCreates a path. This line and the next three lines set up an arc that is half a circle and adds it to the graphics context as a clipping area. The effect is that the gradient is drawn to an area that is half a circle. However, the circle will be transformed by the height and width of the window (see step 8), resulting in a final effect of a gradient drawn to a half ellipse. As the window is resized by the user, the clipping area is resized.\n\nPaints the gradient to the graphics context, transforming and clipping the gradient as described previously.\n\nReleases objects. This line and the next two lines release all the objects you created.\n\nRestores the graphics state to the state that existed before you set up the filled background and clipped to half a circle. The restored state is still transformed by the width and height of the window.\n\nPainting a Radial Gradient Using a CGShading Object\n\nThis example shows how to use a CGShading object to produce the output shown in Figure 8-12.\n\nFigure 8-12  A radial gradient created using a CGShading object\n\nTo paint a radial gradient, follow the steps explained in the following sections:\n\nSet Up a CGFunction Object to Compute Color Values.\n\nCreate a CGShading Object for a Radial Gradient\n\nPaint a Radial Gradient Using a CGShading Object\n\nRelease Objects\n\nSet Up a CGFunction Object to Compute Color Values\n\nThere is no difference between writing functions to compute color values for radial and axial gradients. In fact, you can follow the instruction outlined for axial gradients in Set Up a CGFunction Object to Compute Color Values. Listing 8-12 calculates color so that the color components vary sinusoidally, with a period based on frequency values declared in the function. The result seen in Figure 8-12 is quite different from the colors shown in Figure 8-11. Despite the differences in color output, the code in Listing 8-12 is similar to Listing 8-6 in that each function follows the same prototype. Each function takes one input value and calculates N values, one for each color component of the color space plus an alpha value.\n\nListing 8-12  Computing color component values\n\nstatic void  myCalculateShadingValues (void *info,\n\n\n                                const CGFloat *in,\n\n\n                                CGFloat *out)\n\n\n{\n\n\n    size_t k, components;\n\n\n    double frequency[4] = { 55, 220, 110, 0 };\n\n\n    components = (size_t)info;\n\n\n    for (k = 0; k < components - 1; k++)\n\n\n        *out++ = (1 + sin(*in * frequency[k]))/2;\n\n\n     *out++ = 1; // alpha\n\n\n}\n\nRecall that after you write a color computation function, you need to create a CGFunction object, as described for axial values in Set Up a CGFunction Object to Compute Color Values.\n\nCreate a CGShading Object for a Radial Gradient\n\nTo create a CGShading object or a radial gradient, you call the function CGShadingCreateRadial, as shown in Listing 8-13, passing a color space, starting and ending points, starting and ending radii, a CGFunction object, and Boolean values to specify whether to fill the area beyond the starting and ending points of the gradient.\n\nListing 8-13  Creating a CGShading object for a radial gradient\n\n    CGPoint startPoint, endPoint;\n\n\n    CGFloat startRadius, endRadius;\n\n\n \n\n\n    startPoint = CGPointMake(0.25,0.3);\n\n\n    startRadius = .1;\n\n\n    endPoint = CGPointMake(.7,0.7);\n\n\n    endRadius = .25;\n\n\n    colorspace = CGColorSpaceCreateDeviceRGB();\n\n\n    myShadingFunction = myGetFunction (colorspace);\n\n\n    CGShadingCreateRadial (colorspace,\n\n\n                    startPoint,\n\n\n                    startRadius,\n\n\n                    endPoint,\n\n\n                    endRadius,\n\n\n                    myShadingFunction,\n\n\n                    false,\n\n\n                    false);\nPaint a Radial Gradient Using a CGShading Object\n\nCalling the function CGContextDrawShading fills the current context using the specified color gradient specified in the CGShading object.\n\nCGContextDrawShading (myContext, shading);\n\nNotice that you use the same function to paint a gradient regardless of whether the gradient is axial or radial.\n\nRelease Objects\n\nYou call the function CGShadingRelease when you no longer need the CGShading object. You also need to release the CGColorSpace object and the CGFunction object as shown in Listing 8-14.\n\nListing 8-14  Code that releases objects\n\nCGShadingRelease (myShading);\n\n\nCGColorSpaceRelease (colorspace);\n\n\nCGFunctionRelease (myFunctionObject);\nA Complete Routine for Painting a Radial Gradient Using a CGShading Object\n\nThe code in Listing 8-15 shows a complete routine that paints a radial gradient using the CGFunction object set up in Listing 8-7 and the callback shown in Listing 8-12. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 8-15  A routine that paints a radial gradient using a CGShading object\n\nvoid myPaintRadialShading (CGContextRef myContext,\n// 1\n\n\n                            CGRect bounds);\n\n\n{\n\n\n    CGPoint startPoint,\n\n\n            endPoint;\n\n\n    CGFloat startRadius,\n\n\n            endRadius;\n\n\n    CGAffineTransform myTransform;\n\n\n    CGFloat width = bounds.size.width;\n\n\n    CGFloat height = bounds.size.height;\n\n\n \n\n\n    startPoint = CGPointMake(0.25,0.3); \n// 2\n\n\n    startRadius = .1;  \n// 3\n\n\n    endPoint = CGPointMake(.7,0.7); \n// 4\n\n\n    endRadius = .25; \n// 5\n\n\n \n\n\n    colorspace = CGColorSpaceCreateDeviceRGB(); \n// 6\n\n\n    myShadingFunction = myGetFunction (colorspace); \n// 7\n\n\n \n\n\n    shading = CGShadingCreateRadial (colorspace, \n// 8\n\n\n                            startPoint, startRadius,\n\n\n                            endPoint, endRadius,\n\n\n                            myShadingFunction,\n\n\n                            false, false);\n\n\n \n\n\n    myTransform = CGAffineTransformMakeScale (width, height); \n// 9\n\n\n    CGContextConcatCTM (myContext, myTransform); \n// 10\n\n\n    CGContextSaveGState (myContext); \n// 11\n\n\n \n\n\n    CGContextClipToRect (myContext, CGRectMake(0, 0, 1, 1)); \n// 12\n\n\n    CGContextSetRGBFillColor (myContext, 1, 1, 1, 1);\n\n\n    CGContextFillRect (myContext, CGRectMake(0, 0, 1, 1));\n\n\n \n\n\n    CGContextDrawShading (myContext, shading); \n// 13\n\n\n    CGColorSpaceRelease (colorspace); \n// 14\n\n\n    CGShadingRelease (shading);\n\n\n    CGFunctionRelease (myShadingFunction);\n\n\n \n\n\n    CGContextRestoreGState (myContext); \n// 15\n\n\n}\n\nHere’s what the code does:\n\nTakes as parameters a graphics context and a rectangle to draw into.\n\nAssigns a value to the center of the starting circle. The routine calculates values based on a user space that varies from 0 to 1. You’ll scale the space later for the window Quartz draws into. You can think of this coordinate location as x at 25% from the left and y at 30% from the bottom.\n\nAssigns the radius of the starting circle. You can think of this as 10% of the width of user space.\n\nAssigns a value to the center of the ending circle. You can think of this coordinate location as x at 70% from the left and y at 70% from the bottom.\n\nAssigns the radius of the ending circle. You can think of this as 25% of the width of user space. The ending circle will be larger than the starting circle. The conical shape will be oriented from left to right, tipped upwards.\n\nCreates a color space for device RGB because this routine draws to the display.\n\nCreates a CGFunctionObject by calling the routine shown in Listing 8-7 and passing the color space you just created. However, recall that you’ll use the color calculation function shown in Listing 8-12.\n\nCreates a CGShading object for a radial gradient. The last two parameters are false, to signal that Quartz should not fill the area beyond the starting and ending points of the gradient.\n\nSets up an affine transform that is scaled to the height and width of the window used for drawing. Note that the height is not necessarily equal to the width. In fact, the transformation will change whenever the user resizes the window.\n\nConcatenates the transform you just set up with the graphics context passed to the routine.\n\nSaves the graphics state to enable you to restore this state later.\n\nSets up a clipping area. This line and the next two lines clip the context to a rectangle that is filled with white. The effect is that the gradient is drawn to a window with a white background.\n\nPaints the gradient to the graphics context transforming the gradient as described previously.\n\nReleases object. This line and the next two lines release all the objects you created.\n\nRestores the graphics state to the state that existed before you set up the filled background. The restored state is still transformed by the width and height of the window.\n\nSee Also\n\nCGGradient Reference describes the functions that create CGGradient objects.\n\nCGShading Reference describes the functions that create CGShading objects.\n\nCGFunction Reference describes the functions needed to calculate gradient colors for a CGShading object.\n\nCGContext Reference describes the functions that draw to a context with CGGradient and CGShading objects.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Data Management in Quartz 2D",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_data_mgr/dq_data_mgr.html#//apple_ref/doc/uid/TP30001066-CH216-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nData Management in Quartz 2D\n\nManaging data is a task every graphics application needs to perform. For Quartz, data management refers to supplying data to or receiving data from Quartz 2D routines. Some Quartz 2D routines move data into Quartz, such as those that get image or PDF data from a file or another part of your application. Other routines accept Quartz data, such as those that write image or PDF data to a file or provide the data to another part of your application.\n\nQuartz provides a variety of functions for managing data. By reading this chapter, you should be able to determine which functions are best for your application.\n\nNote: The preferred way to read and write image data is to use the Image I/O framework, which is available in iOS 4 and Mac OS X 10.4 and later. See Image I/O Programming Guide for more information on the CGImageSourceRef and CGImageDestinationRef opaque data types. Image sources and destinations not only offer access to image data, but also provide better support for accessing image metadata.\n\nQuartz recognizes three broad categories of data sources and destinations:\n\nURL. Data whose location can be specified as a URL can act as a supplier or receiver of data. You pass a URL to a Quartz function using the Core Foundation data type CFURLRef.\n\nCFData. The Core Foundation data types CFDataRef and CFMutableDataRef are data objects that let simple allocated buffers take on the behavior of Core Foundation objects. CFData is “toll-free bridged” with its Cocoa Foundation counterpart, the NSData class; if you are using Quartz 2D with the Cocoa framework, you can pass an NSData object to any Quartz function that takes a CFData object.\n\nRaw data. You can provide a pointer to data of any type along with a set of callbacks that take care of basic memory management for the data.\n\nThe data itself, whether represented by a URL, a CFData object, or a data buffer, can be image data or PDF data. Image data can use any type of file format. Quartz understands most of the common image file formats. Some of the Quartz data management functions work specifically with image data, a few work only with PDF data, while others are more generic and can be used either for PDF or image data.\n\nURL, CFData, and raw data sources and destinations refer to data outside the realm of Mac OS X or iOS graphics technologies, as shown in Figure 10-1. Other graphics technologies in Mac OS X or iOS often provide their own routines to communicate with Quartz. For example, a Mac OS X application can send Quartz images to Core Image and use it to alter the image with sophisticated effects.\n\nFigure 10-1  Moving data to and from Quartz 2D in Mac OS X\nMoving Data into Quartz 2D\n\nThe functions for getting data from a data source are listed in Table 10-1. All these functions, except for CGPDFDocumentCreateWithURL, either return an image source (CGImageSourceRef) or a data provider (CGDataProviderRef). Image sources and data providers abstract the data-access task and eliminate the need for applications to manage data through a raw memory buffer.\n\nImage sources are the preferred way to move image data into Quartz. An image source represents a wide variety of image data. An image source can contain more than one image, thumbnail images, and properties for each image and the image file. After you have a CGImageSourceRef, you can accomplish these tasks:\n\nCreate images (CGImageRef) using the functions CGImageSourceCreateImageAtIndex, CGImageSourceCreateThumbnailAtIndex, or CGImageSourceCreateIncremental. A CGImageRef data type represents a single Quartz image.\n\nAdd content to an image source using the functions CGImageSourceUpdateData or CGImageSourceUpdateDataProvider.\n\nObtain information from an image source using the functions CGImageSourceGetCount , CGImageSourceCopyProperties, and CGImageSourceCopyTypeIdentifiers.\n\nThe function CGPDFDocumentCreateWithURL is a convenience function that creates a PDF document from the file located at the specified URL.\n\nData providers are an older mechanism with more limited functionality. They can be used to obtain image or PDF data.\n\nYou can supply a data provider to:\n\nAn image creation function, such as CGImageCreate, CGImageCreateWithPNGDataProvider, or CGImageCreateWithJPEGDataProvider.\n\nThe PDF document creation function CGPDFDocumentCreateWithProvider.\n\nThe function CGImageSourceUpdateDataProvider to update an existing image source with new data.\n\nFor more information on images, see Bitmap Images and Image Masks.\n\nTable 10-1  Functions that move data into Quartz 2D\n\nFunction\n\n\t\n\nUse this function\n\n\n\n\nCGImageSourceCreateWithDataProvider\n\n\t\n\nTo create an image source from a data provider.\n\n\n\n\nCGImageSourceCreateWithData\n\n\t\n\nTo create an image source from a CFData object.\n\n\n\n\nCGImageSourceCreateWithURL\n\n\t\n\nTo create an image source from a URL that specifies the location of image data.\n\n\n\n\nCGPDFDocumentCreateWithURL\n\n\t\n\nTo create a PDF document from data that resides at the specified URL.\n\n\n\n\nCGDataProviderCreateSequential\n\n\t\n\nTo read image or PDF data in a stream. You supply callbacks to handle the data.\n\n\n\n\nCGDataProviderCreateDirectAccess\n\n\t\n\nTo read image or PDF data in a block. You supply callbacks to handle the data.\n\n\n\n\nCGDataProviderCreateWithData\n\n\t\n\nTo read a buffer of image or PDF data supplied by your application. You provide a callback to release the memory you allocated for the data.\n\n\n\n\nCGDataProviderCreateWithURL\n\n\t\n\nWhenever you can supply a URL that specifies the target for data access to image or PDF data.\n\n\n\n\nCGDataProviderCreateWithCFData\n\n\t\n\nTo read image or PDF data from a CFData object.\n\nMoving Data out of Quartz 2D\n\nThe functions listed in Table 10-2 move data out of Quartz 2D. All these functions, except for CGPDFContextCreateWithURL, either return an image destination (CGImageDestinationRef) or a data consumer (CGDataConsumerRef). Image destination and data consumers abstract the data-writing task, letting Quartz take care of the details for you.\n\nAn image destination is the preferred way to move image data out of Quartz. Similar to image sources, an image destination can represent a variety of image data, from a single image to a destination that contains multiple images, thumbnail images, and properties for each image or for the image file. After you have a CGImageDestinationRef, you can accomplish these tasks:\n\nAdd images (CGImageRef) to a destination using the functions CGImageDestinationAddImage or CGImageDestinationAddImageFromSource. A CGImageRef data type represents a single Quartz image.\n\nSet properties using the function CGImageDestinationSetProperties.\n\nObtain information from an image destination using the functions CGImageDestinationCopyTypeIdentifiers or CGImageDestinationGetTypeID.\n\nThe function CGPDFContextCreateWithURL is a convenience function that writes PDF data to the location specified by a URL.\n\nData consumers are an older mechanism with more limited functionality. They are used to write image or PDF data. You can supply a data consumer to:\n\nThe PDF context creation function CGPDFContextCreate. This function returns a graphics context that records your drawing as a sequence of PDF drawing commands that are passed to the data consumer object.\n\nThe function CGImageDestinationCreateWithDataConsumer to create an image destination from a data consumer.\n\nNote: For the best performance when working with raw image data, use the vImage framework. You can import image data to vImage from a CGImageRef reference with the vImageBuffer_InitWithCGImage function. For details, see Accelerate Release Notes.\n\nFor more information on images, see Bitmap Images and Image Masks.\n\nTable 10-2  Functions that move data out of Quartz 2D\n\nFunction\n\n\t\n\nUse this function\n\n\n\n\nCGImageDestinationCreateWithDataConsumer\n\n\t\n\nTo write image data to a data consumer.\n\n\n\n\nCGImageDestinationCreateWithData\n\n\t\n\nTo write image data to a CFData object.\n\n\n\n\nCGImageDestinationCreateWithURL\n\n\t\n\nWhenever you can supply a URL that specifies where to write the image data.\n\n\n\n\nCGPDFContextCreateWithURL\n\n\t\n\nWhenever you can supply a URL that specifies where to write PDF data.\n\n\n\n\nCGDataConsumerCreateWithURL\n\n\t\n\nWhenever you can supply a URL that specifies where to write the image or PDF data.\n\n\n\n\nCGDataConsumerCreateWithCFData\n\n\t\n\nTo write image or PDF data to a CFData object.\n\n\n\n\nCGDataConsumerCreate\n\n\t\n\nTo write image or PDF data using callbacks you supply.\n\nMoving Data Between Quartz 2D and Core Image in Mac OS X\n\nThe Core Image framework is an Objective-C API provided in Mac OS X that supports image processing. Core Image lets you access built-in image filters for both video and still images and provides support for custom filters and near real-time processing. You can apply Core Image filters to Quartz 2D images. For example, you can use Core Image to correct color, distort the geometry of images, blur or sharpen images, and create a transition between images. Core Image also allows you to apply an iterative process to an image—one that feeds back the output of a filter operation to the input. To understand the capabilities of Core Image more fully, see Core Image Programming Guide.\n\nCore Image methods operate on images that are packaged as Core Image images, or CIImage objects. Core Image does not operate directly on Quartz images (CGImageRef data types). Quartz images must be converted to Core Image images before you apply a Core Image filter to the image.\n\nThe Quartz 2D API does not provide any functions that package Quartz images as Core Image images, but Core Image does. The following Core Image methods create a Core Image image from either a Quartz image or a Quartz layer (CGLayerRef). You can use them to move Quartz 2D data to Core Image.\n\nimageWithCGImage:\n\nimageWithCGImage:options:\n\nimageWithCGLayer:\n\nimageWithCGLayer:options:\n\nThe following Core Image methods return a Quartz image from a Core Image image. You can use them to move a processed image back into Quartz 2D:\n\ncreateCGImage:fromRect:\n\ncreateCGLayerWithSize:info:\n\nFor a complete description of Core Image methods, see Core Image Reference Collection.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Transparency Layers",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_trans_layers/dq_trans_layers.html#//apple_ref/doc/uid/TP30001066-CH210-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nTransparency Layers\n\nA transparency layer consists of two or more objects that are combined to yield a composite graphic. The resulting composite is treated as a single object. Transparency layers are useful when you want to apply an effect to a group of objects, such as the shadow applied to the three circles in Figure 9-1.\n\nFigure 9-1  Three circles as a composite in a transparency layer\n\nIf you apply a shadow to the three circles in Figure 9-1 without first rendering them to a transparency layer, you get the result shown in Figure 9-2.\n\nFigure 9-2  Three circles painted as separate entities\nHow Transparency Layers Work\n\nQuartz transparency layers are similar to layers available in many popular graphics applications. Layers are independent entities. Quartz maintains a transparency layer stack for each context and transparency layers can be nested. But because layers are always part of a stack, you can’t manipulate them independently.\n\nYou signal the start of a transparency layer by calling the function CGContextBeginTransparencyLayer, which takes as parameters a graphics context and a CFDictionary object. The dictionary lets you provide options to specify additional information about the layer, but because the dictionary is not yet used by the Quartz 2D API, you pass NULL. After this call, graphics state parameters remain unchanged except for alpha (which is set to 1), shadow (which is turned off), blend mode (which is set to normal), and other parameters that affect the final composite.\n\nAfter you begin a transparency layer, you perform whatever drawing you want to appear in that layer. Drawing operations in the specified context are drawn as a composite into a fully transparent backdrop. This backdrop is treated as a separate destination buffer from the context.\n\nWhen you are finished drawing, you call the function CGContextEndTransparencyLayer. Quartz composites the result into the context using the global alpha value and shadow state of the context and respecting the clipping area of the context.\n\nPainting to a Transparency Layer\n\nPainting to a transparency layer requires three steps:\n\nCall the function CGContextBeginTransparencyLayer.\n\nDraw the items you want to composite in the transparency layer.\n\nCall the function CGContextEndTransparencyLayer.\n\nThe three rectangles in Figure 9-3 are painted to a transparency layer. Quartz renders the shadow as if the rectangles are a single unit.\n\nFigure 9-3  Three rectangles painted to a transparency layer\n\nThe function in Listing 9-1 shows how to use a transparency layer to generate the rectangles in Figure 9-3. A detailed explanation for each numbered line of code follows the listing.\n\nListing 9-1  Painting to a transparency layer\n\nvoid MyDrawTransparencyLayer (CGContext myContext, \n// 1\n\n\n                                CGFloat wd,\n\n\n                                CGFloat ht)\n\n\n{\n\n\n    CGSize          myShadowOffset = CGSizeMake (10, -20);\n// 2\n\n\n \n\n\n    CGContextSetShadow (myContext, myShadowOffset, 10);   \n// 3\n\n\n    CGContextBeginTransparencyLayer (myContext, NULL);\n// 4\n\n\n    // Your drawing code here\n// 5\n\n\n    CGContextSetRGBFillColor (myContext, 0, 1, 0, 1);\n\n\n    CGContextFillRect (myContext, CGRectMake (wd/3+ 50,ht/2 ,wd/4,ht/4));\n\n\n    CGContextSetRGBFillColor (myContext, 0, 0, 1, 1);\n\n\n    CGContextFillRect (myContext, CGRectMake (wd/3-50,ht/2-100,wd/4,ht/4));\n\n\n    CGContextSetRGBFillColor (myContext, 1, 0, 0, 1);\n\n\n    CGContextFillRect (myContext, CGRectMake (wd/3,ht/2-50,wd/4,ht/4));\n\n\n    CGContextEndTransparencyLayer (myContext);\n// 6\n\n\n}\n\nHere’s what the code does:\n\nTakes three parameters—a graphics context and a width and height to use when constructing the rectangles.\n\nSets up a CGSize data structure that contains the x and y offset values for the shadow. This shadow is offset by 10 units in the horizontal direction and –20 units in the vertical direction.\n\nSets the shadow, specifying a value of 10 as the blur value. (A value of 0 specifies a hard edge shadow with no blur.)\n\nSignals the start of the transparency layer. From this point on, drawing occurs to this layer.\n\nThe next six lines of code set fill colors and fill the three rectangles shown in Figure 9-3. You replace these lines with your own drawing code.\n\nSignals the end of the transparency layer and signals that Quartz should composite the result into the context.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Shadows",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_shadows/dq_shadows.html#//apple_ref/doc/uid/TP30001066-CH208-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nShadows\n\nA shadow is an image painted underneath, and offset from, a graphics object such that the shadow mimics the effect of a light source cast on the graphics object, as shown in Figure 7-1. Text can also be shadowed. Shadows can make an image appear three dimensional or as if it’s floating.\n\nFigure 7-1  A shadow\n\nShadows have three characteristics:\n\nAn x-offset, which specifies how far in the horizontal direction the shadow is offset from the image.\n\nA y-offset, which specifies how far in the vertical direction the shadow is offset from the image.\n\nA blur value, which specifies whether the image has a hard edge, as seen in the left side of Figure 7-2, or a diffuse edge, as seen in the right side of the figure.\n\nThis chapter describes how shadows work and shows how to use the Quartz 2D API to create them.\n\nFigure 7-2  A shadow with no blur and another with a soft edge\nHow Shadows Work\n\nShadows in Quartz are part of the graphics state. You call the function CGContextSetShadow, passing a graphics context, offset values, and a blur value. After shadowing is set, any object you draw has a shadow drawn with a black color that has a 1/3 alpha value in the device RGB color space. In other words, the shadow is drawn using RGBA values set to {0, 0, 0, 1.0/3.0}.\n\nYou can draw colored shadows by calling the function CGContextSetShadowWithColor, passing a graphics context, offset values, a blur value, and a CGColor object. The values to supply for the color depend on the color space you want to draw in.\n\nIf you save the graphics state before you call CGContextSetShadow or CGContextSetShadowWithColor, you can turn off shadowing by restoring the graphics state. You also disable shadows by setting the shadow color to NULL.\n\nShadow Drawing Conventions Vary Based on the Context\n\nThe offsets described earlier specify where the shadow is located related to the image that cast the shadow. Those offsets are interpreted by the context and used to calculate the shadow’s location:\n\nA positive x offset indicates the shadows is to the right of the graphics object.\n\nIn Mac OS X, a positive y offset indicates upward displacement. This matches the default coordinate system for Quartz 2D.\n\nIn iOS, if your application uses Quartz 2D APIs to create a PDF or bitmap context, a positive y offset indicates upwards displacement.\n\nIn iOS, if the graphics context was created by UIKit, such as a graphics context created by a UIView object or a context created by calling the UIGraphicsBeginImageContextWithOptions function, then a positive y offset indicates a downward displacement. This matches the drawing conventions of the UIKit coordinate system.\n\nThe shadow-drawing convention is not affected by the current transformation matrix.\n\nPainting with Shadows\n\nFollow these steps to paint with shadows:\n\nSave the graphics state.\n\nCall the function CGContextSetShadow, passing the appropriate values.\n\nPerform all the drawing to which you want to apply shadows.\n\nRestore the graphics state.\n\nFollow these steps to paint with colored shadows:\n\nSave the graphics state.\n\nCreate a CGColorSpace object to ensure that Quartz interprets the shadow color values correctly.\n\nCreate a CGColor object that specifies the shadow color you want to use.\n\nCall the function CGContextSetShadowWithColor, passing the appropriate values.\n\nPerform all the drawing to which you want to apply shadows.\n\nRestore the graphics state.\n\nThe two rectangles in Figure 7-3 are drawn with shadows—one with a colored shadow.\n\nFigure 7-3  A colored shadow and a gray shadow\n\nThe function in Listing 7-1 shows how to set up shadows to draw the rectangles shown in Figure 7-3. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 7-1  A function that sets up shadows\n\nvoid MyDrawWithShadows (CGContextRef myContext, \n// 1\n\n\n                         CGFloat wd, CGFloat ht);\n\n\n{\n\n\n    CGSize          myShadowOffset = CGSizeMake (-15,  20);\n// 2\n\n\n    CGFloat           myColorValues[] = {1, 0, 0, .6};\n// 3\n\n\n    CGColorRef      myColor;\n// 4\n\n\n    CGColorSpaceRef myColorSpace;\n// 5\n\n\n \n\n\n    CGContextSaveGState(myContext);\n// 6\n\n\n \n\n\n    CGContextSetShadow (myContext, myShadowOffset, 5); \n// 7\n\n\n    // Your drawing code here\n// 8\n\n\n    CGContextSetRGBFillColor (myContext, 0, 1, 0, 1);\n\n\n    CGContextFillRect (myContext, CGRectMake (wd/3 + 75, ht/2 , wd/4, ht/4));\n\n\n \n\n\n    myColorSpace = CGColorSpaceCreateDeviceRGB ();\n// 9\n\n\n    myColor = CGColorCreate (myColorSpace, myColorValues);\n// 10\n\n\n    CGContextSetShadowWithColor (myContext, myShadowOffset, 5, myColor);\n// 11\n\n\n    // Your drawing code here\n// 12\n\n\n    CGContextSetRGBFillColor (myContext, 0, 0, 1, 1);\n\n\n    CGContextFillRect (myContext, CGRectMake (wd/3-75,ht/2-100,wd/4,ht/4));\n\n\n \n\n\n    CGColorRelease (myColor);\n// 13\n\n\n    CGColorSpaceRelease (myColorSpace); \n// 14\n\n\n \n\n\n    CGContextRestoreGState(myContext);\n// 15\n\n\n}\n\nHere’s what the code does:\n\nTakes three parameters—a graphics context and a width and height to use when constructing the rectangles.\n\nDeclares and creates a CGSize object that contains offset values for the shadow. These values specify a shadow offset 15 units to the left of the object and 20 units above the object.\n\nDeclares an array of color values. This example uses RGBA, but these values won’t take on any meaning until they are passed to Quartz along with a color space, which is necessary for Quartz to interpret the values correctly.\n\nDeclares storage for a color reference.\n\nDeclares storage for a color space reference.\n\nSaves the current graphics state so that you can restore it later.\n\nSets a shadow to have the previously declared offset values and a blur value of 5, which indicates a soft shadow edge. The shadow will appear gray, having an RGBA value of {0, 0, 0, 1/3}.\n\nThe next two lines of code draw the rectangle on the right side of Figure 7-3. You replace these lines with your own drawing code.\n\nCreates a device RGB color space. You need to supply a color space when you create a CGColor object.\n\nCreates a CGColor object, supplying the device RGB color space and the RGBA values declared previously. This object specifies the shadow color, which in this case is red with an alpha value of 0.6.\n\nSets up a color shadow, supplying the red color you just created. The shadow uses the offset created previously and a blur value of 5, which indicates a soft shadow edge.\n\nThe next two lines of code draw the rectangle on the left side of Figure 7-3. You replace these lines with your own drawing code.\n\nReleases the color object because it is no longer needed.\n\nReleases the color space object because it is no longer needed.\n\nRestores the graphics state to what it was prior to setting up the shadows.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Paths",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_paths/dq_paths.html#//apple_ref/doc/uid/TP30001066-CH211-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nPaths\n\nA path defines one or more shapes, or subpaths. A subpath can consist of straight lines, curves, or both. It can be open or closed. A subpath can be a simple shape, such as a line, circle, rectangle, or star, or a more complex shape such as the silhouette of a mountain range or an abstract doodle. Figure 3-1 shows some of the paths you can create. The straight line (at the upper left of the figure) is dashed; lines can also be solid. The squiggly path (in the middle top) is made up of several curves and is an open path. The concentric circles are filled, but not stroked. The State of California is a closed path, made up of many curves and lines, and the path is both stroked and filled. The stars illustrate two options for filling paths, which you’ll read about later in this chapter.\n\nFigure 3-1  Quartz supports path-based drawing\n\nIn this chapter, you’ll learn about the building blocks that make up paths, how to stroke and paint paths, and the parameters that affect the appearance of paths.\n\nPath Creation and Path Painting\n\nPath creation and path painting are separate tasks. First you create a path. When you want to render a path, you request Quartz to paint it. As you can see in Figure 3-1, you can choose to stroke the path, fill the path, or both stroke and fill the path. You can also use a path to constrain the drawing of other objects within the bounds of the path creating, in effect, a clipping area.\n\nFigure 3-2 shows a path that has been painted and that contains two subpaths. The subpath on the left is a rectangle, and the subpath on the right is an abstract shape made up of straight lines and curves. Each subpath is filled and its outline stroked.\n\nFigure 3-2  A path that contains two shapes, or subpaths\n\nFigure 3-3 shows multiple paths drawn independently. Each path contains a randomly generated curve, some of which are filled and others stroked. Drawing is constrained to a circular area by a clipping area.\n\nFigure 3-3  A clipping area constrains drawing\nThe Building Blocks\n\nSubpaths are built from lines, arcs, and curves. Quartz also provides convenience functions to add rectangles and ellipses with a single function call. Points are also essential building blocks of paths because points define starting and ending locations of shapes.\n\nPoints\n\nPoints are x and y coordinates that specify a location in user space. You can call the function CGContextMoveToPoint to specify a starting position for a new subpath. Quartz keeps track of the current point, which is the last location used for path construction. For example, if you call the function CGContextMoveToPoint to set a location at (10,10), that moves the current point to (10,10). If you then draw a horizontal line 50 units long, the last point on the line, that is, (60,10), becomes the current point. Lines, arcs, and curves are always drawn starting from the current point.\n\nMost of the time you specify a point by passing to Quartz functions two floating-point values to specify x and y coordinates. Some functions require that you pass a CGPoint data structure, which holds two floating-point values.\n\nLines\n\nA line is defined by its endpoints. Its starting point is always assumed to be the current point, so when you create a line, you specify only its endpoint. You use the function CGContextAddLineToPoint to append a single line to a subpath.\n\nYou can add a series of connected lines to a path by calling the function CGContextAddLines. You pass this function an array of points. The first point must be the starting point of the first line; the remaining points are endpoints. Quartz begins a new subpath at the first point and connects a straight line segment to each endpoint.\n\nArcs\n\nArcs are circle segments. Quartz provides two functions that create arcs. The function CGContextAddArc creates a curved segment from a circle. You specify the center of the circle, the radius, and the radial angle (in radians). You can create a full circle by specifying a radial angle of 2 pi. Figure 3-4 shows multiple paths drawn independently. Each path contains a randomly generated circle; some are filled and others are stroked.\n\nFigure 3-4  Multiple paths; each path contains a randomly generated circle\n\nThe function CGContextAddArcToPoint is ideal to use when you want to round the corners of a rectangle. Quartz uses the endpoints you supply to create two tangent lines. You also supply the radius of the circle from which Quartz slices the arc. The center point of the arc is the intersection of two radii, each of which is perpendicular to one of the two tangent lines. Each endpoint of the arc is a tangent point on one of the tangent lines, as shown in Figure 3-5. The red portion of the circle is what’s actually drawn.\n\nFigure 3-5  Defining an arc with two tangent lines and a radius\n\nIf the current path already contains a subpath, Quartz appends a straight line segment from the current point to the starting point of the arc. If the current path is empty, Quartz creates a new subpath at the starting point for the arc and does not add the initial straight line segment.\n\nCurves\n\nQuadratic and cubic Bézier curves are algebraic curves that can specify any number of interesting curvilinear shapes. Points on these curves are calculated by applying a polynomial formula to starting and ending points, and one or more control points. Shapes defined in this way are the basis for vector graphics. A formula is much more compact to store than an array of bits and has the advantage that the curve can be re-created at any resolution.\n\nFigure 3-6 shows a variety of curves created by drawing multiple paths independently. Each path contains a randomly generated curve; some are filled and others are stroked.\n\nFigure 3-6  Multiple paths; each path contains a randomly generated curve\n\nThe polynomial formulas that give to rise to quadratic and cubic Bézier curves, and the details on how to generate the curves from the formulas, are discussed in many mathematics texts and online sources that describe computer graphics. These details are not discussed here.\n\nYou use the function CGContextAddCurveToPoint to append a cubic Bézier curve from the current point, using control points and an endpoint you specify. Figure 3-7 shows the cubic Bézier curve that results from the current point, control points, and endpoint shown in the figure. The placement of the two control points determines the geometry of the curve. If the control points are both above the starting and ending points, the curve arches upward. If the control points are both below the starting and ending points, the curve arches downward.\n\nFigure 3-7  A cubic Bézier curve uses two control points\n\nYou can append a quadratic Bézier curve from the current point by calling the function CGContextAddQuadCurveToPoint, and specifying a control point and an endpoint. Figure 3-8 shows two curves that result from using the same endpoints but different control points. The control point determines the direction that the curve arches. It’s not possible to create as many interesting shapes with a quadratic Bézier curve as you can with a cubic one because quadratic curves use only one control point. For example, it’s not possible to create a crossover using a single control point.\n\nFigure 3-8  A quadratic Bézier curve uses one control point\nClosing a Subpath\n\nTo close the current subpath, your application should call CGContextClosePath. This function adds a line segment from the current point to the starting point of the subpath and closes the subpath. Lines, arcs, and curves that end at the starting point of a subpath do not actually close the subpath. You must explicitly call CGContextClosePath to close a subpath.\n\nSome Quartz functions treat a path’s subpaths as if they were closed by your application. Those commands treat each subpath as if your application had called CGContextClosePath to close it, implicitly adding a line segment to the starting point of the subpath.\n\nAfter closing a subpath, if your application makes additional calls to add lines, arcs, or curves to the path, Quartz begins a new subpath starting at the starting point of the subpath you just closed.\n\nEllipses\n\nAn ellipse is essentially a squashed circle. You create one by defining two focus points and then plotting all the points that lie at a distance such that adding the distance from any point on the ellipse to one focus to the distance from that same point to the other focus point is always the same value. Figure 3-9 shows multiple paths drawn independently. Each path contains a randomly generated ellipse; some are filled and others are stroked.\n\nFigure 3-9  Multiple paths; each path contains a randomly generated ellipse\n\nYou can add an ellipse to the current path by calling the function CGContextAddEllipseInRect. You supply a rectangle that defines the bounds of the ellipse. Quartz approximates the ellipse using a sequence of Bézier curves. The center of the ellipse is the center of the rectangle. If the width and height of the rectangle are equal (that is, a square), the ellipse is circular, with a radius equal to one-half the width (or height) of the rectangle. If the width and height of the rectangle are unequal, they define the major and minor axes of the ellipse.\n\nThe ellipse that is added to the path starts with a move-to operation and ends with a close-subpath operation, with all moves oriented in the clockwise direction.\n\nRectangles\n\nYou can add a rectangle to the current path by calling the function CGContextAddRect. You supply a CGRect structure that contains the origin of the rectangle and its width and height.\n\nThe rectangle that is added to the path starts with a move-to operation and ends with a close-subpath operation, with all moves oriented in the counter-clockwise direction.\n\nYou can add many rectangles to the current path by calling the function CGContextAddRects and supplying an array of CGRect structures. Figure 3-10 shows multiple paths drawn independently. Each path contains a randomly generated rectangle; some are filled and others are stroked.\n\nFigure 3-10  Multiple paths; each path contains a randomly generated rectangle\nCreating a Path\n\nWhen you want to construct a path in a graphics context, you signal Quartz by calling the function CGContextBeginPath . Next, you set the starting point for the first shape, or subpath, in the path by calling the function CGContextMoveToPoint. After you establish the first point, you can add lines, arcs, and curves to the path, keeping in mind the following:\n\nBefore you begin a new path, call the function CGContextBeginPath.\n\nLines, arcs, and curves are drawn starting at the current point. An empty path has no current point; you must call CGContextMoveToPoint to set the starting point for the first subpath or call a convenience function that implicitly does this for you.\n\nWhen you want to close the current subpath within a path, call the function CGContextClosePath to connect a segment to the starting point of the subpath. Subsequent path calls begin a new subpath, even if you do not explicitly set a new starting point.\n\nWhen you draw arcs, Quartz draws a line between the current point and the starting point of the arc.\n\nQuartz routines that add ellipses and rectangles add a new closed subpath to the path.\n\nYou must call a painting function to fill or stroke the path because creating a path does not draw the path. See Painting a Path for detailed information.\n\nAfter you paint a path, it is flushed from the graphics context. You might not want to lose your path so easily, especially if it depicts a complex scene you want to use over and over again. For that reason, Quartz provides two data types for creating reusable paths—CGPathRef and CGMutablePathRef. You can call the function CGPathCreateMutable to create a mutable CGPath object to which you can add lines, arcs, curves, and rectangles. Quartz provides a set of CGPath functions that parallel the functions discussed in The Building Blocks. The path functions operate on a CGPath object instead of a graphics context. These functions are:\n\nCGPathCreateMutable, which replacesCGContextBeginPath\n\nCGPathMoveToPoint, which replaces CGContextMoveToPoint\n\nCGPathAddLineToPoint, which replaces CGContextAddLineToPoint\n\nCGPathAddCurveToPoint, which replaces CGContextAddCurveToPoint\n\nCGPathAddEllipseInRect, which replaces CGContextAddEllipseInRect\n\nCGPathAddArc, which replaces CGContextAddArc\n\nCGPathAddRect, which replaces CGContextAddRect\n\nCGPathCloseSubpath, which replaces CGContextClosePath\n\nSee Quartz 2D Reference Collection for a complete list of the path functions.\n\nWhen you want to append the path to a graphics context, you call the function CGContextAddPath. The path stays in the graphics context until Quartz paints it. You can add the path again by calling CGContextAddPath.\n\nNote:  You can replace the path in a graphics context with the stroked version of the path by calling the function CGContextReplacePathWithStrokedPath.\n\nPainting a Path\n\nYou can paint the current path by stroking or filling or both. Stroking paints a line that straddles the path. Filling paints the area contained within the path. Quartz has functions that let you stroke a path, fill a path, or both stroke and fill a path. The characteristics of the stroked line (width, color, and so forth), the fill color, and the method Quartz uses to calculate the fill area are all part of the graphics state (see Graphics States).\n\nParameters That Affect Stroking\n\nYou can affect how a path is stroked by modifying the parameters listed in Table 3-1. These parameters are part of the graphics state, which means that the value you set for a parameter affects all subsequent stroking until you set the parameter to another value.\n\nTable 3-1  Parameters that affect how Quartz strokes the current path\n\nParameter\n\n\t\n\nFunction to set parameter value\n\n\n\n\nLine width\n\n\t\n\nCGContextSetLineWidth\n\n\n\n\nLine join\n\n\t\n\nCGContextSetLineJoin\n\n\n\n\nLine cap\n\n\t\n\nCGContextSetLineCap\n\n\n\n\nMiter limit\n\n\t\n\nCGContextSetMiterLimit\n\n\n\n\nLine dash pattern\n\n\t\n\nCGContextSetLineDash\n\n\n\n\nStroke color space\n\n\t\n\nCGContextSetStrokeColorSpace\n\n\n\n\nStroke color\n\n\t\n\nCGContextSetStrokeColorCGContextSetStrokeColorWithColor\n\n\n\n\nStroke pattern\n\n\t\n\nCGContextSetStrokePattern\n\nThe line width is the total width of the line, expressed in units of the user space. The line straddles the path, with half of the total width on either side.\n\nThe line join specifies how Quartz draws the junction between connected line segments. Quartz supports the line join styles described in Table 3-2. The default style is miter join.\n\nTable 3-2  Line join styles\n\nStyle\n\n\t\n\nAppearance\n\n\t\n\nDescription\n\n\n\n\nMiter join\n\n\t\n\n\t\n\nQuartz extends the outer edges of the strokes for the two segments until they meet at an angle, as in a picture frame. If the segments meet at too sharp an angle, a bevel join is used instead. A segment is too sharp if the length of the miter divided by the line width is greater than the miter limit.\n\n\n\n\nRound join\n\n\t\n\n\t\n\nQuartz draws a semicircular arc with a diameter equal to the line width around the endpoint. The enclosed area is filled in.\n\n\n\n\nBevel join\n\n\t\n\n\t\n\nQuartz finishes the two segments with butt caps. The resulting notch beyond the ends of the segments is filled with a triangle.\n\nThe line cap specifies the method used by CGContextStrokePath to draw the endpoint of the line. Quartz supports the line cap styles described in Table 3-3. The default style is butt cap.\n\nTable 3-3  Line cap styles\n\nStyle\n\n\t\n\nAppearance\n\n\t\n\nDescription\n\n\n\n\nButt cap\n\n\t\n\n\t\n\nQuartz squares off the stroke at the endpoint of the path. There is no projection beyond the end of the path.\n\n\n\n\nRound cap\n\n\t\n\n\t\n\nQuartz draws a circle with a diameter equal to the line width around the point where the two segments meet, producing a rounded corner. The enclosed area is filled in.\n\n\n\n\nProjecting square cap\n\n\t\n\n\t\n\nQuartz extends the stroke beyond the endpoint of the path for a distance equal to half the line width. The extension is squared off.\n\nA closed subpath treats the starting point as a junction between connected line segments; the starting point is rendered using the selected line-join method. In contrast, if you close the path by adding a line segment that connects to the starting point, both ends of the path are drawn using the selected line-cap method.\n\nA line dash pattern allows you to draw a segmented line along the stroked path. You control the size and placement of dash segments along the line by specifying the dash array and the dash phase as parameters to CGContextSetLineDash:\n\nvoid CGContextSetLineDash (\n\n\n    CGContextRef ctx,\n\n\n    CGFloat phase,\n\n\n    const CGFloat lengths[],\n\n\n    size_t count\n\n\n);\n\nThe elements of the lengths parameter specify the widths of the dashes, alternating between the painted and unpainted segments of the line. The phase parameter specifies the starting point of the dash pattern. Figure 3-11 shows some line dash patterns.\n\nFigure 3-11  Examples of line dash patterns\n\nThe stroke color space determines how the stroke color values are interpreted by Quartz. You can also specify a Quartz color (CGColorRef data type) that encapsulates both color and color space. For more information on setting color space and color, see Color and Color Spaces.\n\nFunctions for Stroking a Path\n\nQuartz provides the functions shown in Table 3-4 for stroking the current path. Some are convenience functions for stroking rectangles or ellipses.\n\nTable 3-4  Functions that stroke paths\n\nFunction\n\n\t\n\nDescription\n\n\n\n\nCGContextStrokePath\n\n\t\n\nStrokes the current path.\n\n\n\n\nCGContextStrokeRect\n\n\t\n\nStrokes the specified rectangle.\n\n\n\n\nCGContextStrokeRectWithWidth\n\n\t\n\nStrokes the specified rectangle, using the specified line width.\n\n\n\n\nCGContextStrokeEllipseInRect\n\n\t\n\nStrokes an ellipse that fits inside the specified rectangle.\n\n\n\n\nCGContextStrokeLineSegments\n\n\t\n\nStrokes a sequence of lines.\n\n\n\n\nCGContextDrawPath\n\n\t\n\nIf you pass the constant kCGPathStroke, strokes the current path. See Filling a Path if you want to both fill and stroke a path.\n\nThe function CGContextStrokeLineSegments is equivalent to the following code:\n\nCGContextBeginPath (context);\n\n\nfor (k = 0; k < count; k += 2) {\n\n\n    CGContextMoveToPoint(context, s[k].x, s[k].y);\n\n\n    CGContextAddLineToPoint(context, s[k+1].x, s[k+1].y);\n\n\n}\n\n\nCGContextStrokePath(context);\n\nWhen you call CGContextStrokeLineSegments, you specify the line segments as an array of points, organized as pairs. Each pair consists of the starting point of a line segment followed by the ending point of a line segment. For example, the first point in the array specifies the starting position of the first line, the second point specifies the ending position of the first line, the third point specifies the starting position of the second line, and so forth.\n\nFilling a Path\n\nWhen you fill the current path, Quartz acts as if each subpath contained in the path were closed. It then uses these closed subpaths and calculates the pixels to fill. There are two ways Quartz can calculate the fill area. Simple paths such as ovals and rectangles have a well-defined area. But if your path is composed of overlapping segments or if the path includes multiple subpaths, such as the concentric circles shown in Figure 3-12, there are two rules you can use to determine the fill area.\n\nThe default fill rule is called the nonzero winding number rule. To determine whether a specific point should be painted, start at the point and draw a line beyond the bounds of the drawing. Starting with a count of 0, add 1 to the count every time a path segment crosses the line from left to right, and subtract 1 every time a path segment crosses the line from right to left. If the result is 0, the point is not painted. Otherwise, the point is painted. The direction that the path segments are drawn affects the outcome. Figure 3-12 shows two sets of inner and outer circles that are filled using the nonzero winding number rule. When each circle is drawn in the same direction, both circles are filled. When the circles are drawn in opposite directions, the inner circle is not filled.\n\nYou can opt to use the even-odd rule. To determine whether a specific point should be painted, start at the point and draw a line beyond the bounds of the drawing. Count the number of path segments that the line crosses. If the result is odd, the point is painted. If the result is even, the point is not painted. The direction that the path segments are drawn doesn’t affect the outcome. As you can see in Figure 3-12, it doesn’t matter which direction each circle is drawn, the fill will always be as shown.\n\nFigure 3-12  Concentric circles filled using different fill rules\n\nQuartz provides the functions shown in Table 3-5 for filling the current path. Some are convenience functions for stroking rectangles or ellipses.\n\nTable 3-5  Functions that fill paths\n\nFunction\n\n\t\n\nDescription\n\n\n\n\nCGContextEOFillPath\n\n\t\n\nFills the current path using the even-odd rule.\n\n\n\n\nCGContextFillPath\n\n\t\n\nFills the current path using the nonzero winding number rule.\n\n\n\n\nCGContextFillRect\n\n\t\n\nFills the area that fits inside the specified rectangle.\n\n\n\n\nCGContextFillRects\n\n\t\n\nFills the areas that fits inside the specified rectangles.\n\n\n\n\nCGContextFillEllipseInRect\n\n\t\n\nFills an ellipse that fits inside the specified rectangle.\n\n\n\n\nCGContextDrawPath\n\n\t\n\nFills the current path if you pass kCGPathFill (nonzero winding number rule) or kCGPathEOFill (even-odd rule). Fills and strokes the current path if you pass kCGPathFillStroke or kCGPathEOFillStroke.\n\nSetting Blend Modes\n\nBlend modes specify how Quartz applies paint over a background. Quartz uses normal blend mode by default, which combines the foreground painting with the background painting using the following formula:\n\nresult = (alpha * foreground) + (1 - alpha) * background\n\nColor and Color Spaces provides a detailed discussion of the alpha component of a color, which specifies the opacity of a color. For the examples in this section, you can assume a color is completely opaque (alpha value = 1.0). For opaque colors, when you paint using normal blend mode, anything you paint over the background completely obscures the background.\n\nYou can set the blend mode to achieve a variety of effects by calling the function CGContextSetBlendMode, passing the appropriate blend mode constant. Keep in mind that the blend mode is part of the graphics state. If you use the function CGContextSaveGState prior to changing the blend mode, then calling the function CGContextRestoreGState resets the blend mode to normal.\n\nThe rest of this section show the results of painting the rectangles shown in Figure 3-13 over the rectangles shown in Figure 3-14. In each case (Figure 3-15 through Figure 3-30), the background rectangles are painted using normal blend mode. Then the blend mode is changed by calling the function CGContextSetBlendMode with the appropriate constant. Finally, the foreground rectangles are painted.\n\nFigure 3-13  The rectangles painted in the foreground\nFigure 3-14  The rectangles painted in the background\n\nNote:  You can also use blend modes to composite two images or to composite an image over any content that’s already drawn to the graphics context. Using Blend Modes with Images provides information on how to use blend modes to composite images and shows the results of applying blend modes to two images.\n\nNormal Blend Mode\n\nBecause normal blend mode is the default blend mode, you call the function CGContextSetBlendMode with the constant kCGBlendModeNormal only to reset the blend mode back to the default after you’ve used one of the other blend mode constants. Figure 3-15 shows the result of painting Figure 3-13 over Figure 3-14 using normal blend mode.\n\nFigure 3-15  Rectangles painted using normal blend mode\nMultiply Blend Mode\n\nMultiply blend mode specifies to multiply the foreground image samples with the background image samples. The resulting colors are at least as dark as either of the two contributing sample colors. Figure 3-16 shows the result of painting Figure 3-13 over Figure 3-14 using multiply blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeMultiply.\n\nFigure 3-16  Rectangles painted using multiply blend mode\nScreen Blend Mode\n\nScreen blend mode specifies to multiply the inverse of the foreground image samples with the inverse of the background image samples. The resulting colors are at least as light as either of the two contributing sample colors. Figure 3-17 shows the result of painting Figure 3-13 over Figure 3-14 using screen blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeScreen.\n\nFigure 3-17  Rectangles painted using screen blend mode\nOverlay Blend Mode\n\nOverlay blend mode specifies to either multiply or screen the foreground image samples with the background image samples, depending on the background color. The background color mixes with the foreground color to reflect the lightness or darkness of the background. Figure 3-18 shows the result of painting Figure 3-13 over Figure 3-14 using overlay blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeOverlay.\n\nFigure 3-18  Rectangles painted using overlay blend mode\nDarken Blend Mode\n\nSpecifies to create the composite image samples by choosing the darker samples (either from the foreground image or the background). The background image samples are replaced by any foreground image samples that are darker. Otherwise, the background image samples are left unchanged. Figure 3-19 shows the result of painting Figure 3-13 over Figure 3-14 using darken blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeDarken.\n\nFigure 3-19  Rectangles painted using darken blend mode\nLighten Blend Mode\n\nSpecifies to create the composite image samples by choosing the lighter samples (either from the foreground or the background). The result is that the background image samples are replaced by any foreground image samples that are lighter. Otherwise, the background image samples are left unchanged. Figure 3-20 shows the result of painting Figure 3-13 over Figure 3-14 using lighten blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeLighten.\n\nFigure 3-20  Rectangles painted using lighten blend mode\nColor Dodge Blend Mode\n\nSpecifies to brighten the background image samples to reflect the foreground image samples. Foreground image sample values that specify black do not produce a change. Figure 3-21 shows the result of painting Figure 3-13 over Figure 3-14 using color dodge blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeColorDodge.\n\nFigure 3-21  Rectangles painted using color dodge blend mode\nColor Burn Blend Mode\n\nSpecifies to darken the background image samples to reflect the foreground image samples. Foreground image sample values that specify white do not produce a change. Figure 3-22 shows the result of painting Figure 3-13 over Figure 3-14 using color burn blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeColorBurn.\n\nFigure 3-22  Rectangles painted using color burn blend mode\nSoft Light Blend Mode\n\nSpecifies to either darken or lighten colors, depending on the foreground image sample color. If the foreground image sample color is lighter than 50% gray, the background is lightened, similar to dodging. If the foreground image sample color is darker than 50% gray, the background is darkened, similar to burning. If the foreground image sample color is equal to 50% gray, the background is not changed. Image samples that are equal to pure black or pure white produce darker or lighter areas, but do not result in pure black or white. The overall effect is similar to what you’d achieve by shining a diffuse spotlight on the foreground image. Use this to add highlights to a scene. Figure 3-23 shows the result of painting Figure 3-13 over Figure 3-14 using soft light blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeSoftLight.\n\nFigure 3-23  Rectangles painted using soft light blend mode\nHard Light Blend Mode\n\nSpecifies to either multiply or screen colors, depending on the foreground image sample color. If the foreground image sample color is lighter than 50% gray, the background is lightened, similar to screening. If the foreground image sample color is darker than 50% gray, the background is darkened, similar to multiplying. If the foreground image sample color is equal to 50% gray, the foreground image is not changed. Image samples that are equal to pure black or pure white result in pure black or white. The overall effect is similar to what you’d achieve by shining a harsh spotlight on the foreground image. Use this to add highlights to a scene. Figure 3-24 shows the result of painting Figure 3-13 over Figure 3-14 using hard light blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeHardLight.\n\nFigure 3-24  Rectangles painted using hard light blend mode\nDifference Blend Mode\n\nSpecifies to subtract either the foreground image sample color from the background image sample color, or the reverse, depending on which sample has the greater brightness value. Foreground image sample values that are black produce no change; white inverts the background color values. Figure 3-25 shows the result of painting Figure 3-13 over Figure 3-14 using difference blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeDifference.\n\nFigure 3-25  Rectangles painted using difference blend mode\nExclusion Blend Mode\n\nSpecifies an effect similar to that produced by kCGBlendModeDifference, but with lower contrast. Foreground image sample values that are black don’t produce a change; white inverts the background color values. Figure 3-26 shows the result of painting Figure 3-13 over Figure 3-14 using exclusion blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeExclusion.\n\nFigure 3-26  Rectangles painted using exclusion blend mode\nHue Blend Mode\n\nSpecifies to use the luminance and saturation values of the background with the hue of the foreground image. Figure 3-27 shows the result of painting Figure 3-13 over Figure 3-14 using hue blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeHue.\n\nFigure 3-27  Rectangles painted using hue blend mode\nSaturation Blend Mode\n\nSpecifies to use the luminance and hue values of the background with the saturation of the foreground image. Areas of the background that have no saturation (that is, pure gray areas) don’t produce a change. Figure 3-28 shows the result of painting Figure 3-13 over Figure 3-14 using saturation blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeSaturation.\n\nFigure 3-28  Rectangles painted using saturation blend mode\nColor Blend Mode\n\nSpecifies to use the luminance values of the background with the hue and saturation values of the foreground image. This mode preserves the gray levels in the image. You can use this mode to color monochrome images or to tint color images. Figure 3-29 shows the result of painting Figure 3-13 over Figure 3-14 using color blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeColor.\n\nFigure 3-29  Rectangles painted using color blend mode\nLuminosity Blend Mode\n\nSpecifies to use the hue and saturation of the background with the luminance of the foreground image. This mode creates an effect that is inverse to the effect created by kCGBlendModeColor. Figure 3-30 shows the result of painting Figure 3-13 over Figure 3-14 using luminosity blend mode. To use this blend mode, call the function CGContextSetBlendMode with the constant kCGBlendModeLuminosity.\n\nFigure 3-30  Rectangles painted using luminosity blend mode\nClipping to a Path\n\nThe current clipping area is created from a path that serves as a mask, allowing you to block out the part of the page that you don’t want to paint. For example, if you have a very large bitmap image and want to show only a small portion of it, you could set the clipping area to display only the portion you want to show.\n\nWhen you paint, Quartz renders paint only within the clipping area. Drawing that occurs inside the closed subpaths of the clipping area is visible; drawing that occurs outside the closed subpaths of the clipping area is not.\n\nWhen the graphics context is initially created, the clipping area includes all of the paintable area of the context (for example, the media box of a PDF context). You alter the clipping area by setting the current path and then using a clipping function instead of a drawing function. The clipping function intersects the filled area of the current path with the existing clipping area. Thus, you can intersect the clipping area, shrinking the visible area of the picture, but you cannot increase the area of the clipping area.\n\nThe clipping area is part of the graphics state. To restore the clipping area to a previous state, you can save the graphics state before you clip, and restore the graphics state after you’re done with clipped drawing.\n\nListing 3-1 shows a code fragment that sets up a clipping area in the shape of a circle. This code causes drawing to be clipped, similar to what’s shown in Figure 3-3. (For another example, see Clip the Context in the chapter Gradients.)\n\nListing 3-1  Setting up a circular clip area\n\nCGContextBeginPath (context);\n\n\nCGContextAddArc (context, w/2, h/2, ((w>h) ? h : w)/2, 0, 2*PI, 0);\n\n\nCGContextClosePath (context);\n\n\nCGContextClip (context);\nTable 3-6  Functions that clip the graphics context\n\nFunction\n\n\t\n\nDescription\n\n\n\n\nCGContextClip\n\n\t\n\nUses the nonzero winding number rule to calculate the intersection of the current path with the current clipping path.\n\n\n\n\nCGContextEOClip\n\n\t\n\nUses the even-odd rule to calculate the intersection of the current path with the current clipping path.\n\n\n\n\nCGContextClipToRect\n\n\t\n\nSets the clipping area to the area that intersects both the current clipping path and the specified rectangle.\n\n\n\n\nCGContextClipToRects\n\n\t\n\nSets the clipping area to the area that intersects both the current clipping path and region within the specified rectangles.\n\n\n\n\nCGContextClipToMask\n\n\t\n\nMaps a mask into the specified rectangle and intersects it with the current clipping area of the graphics context. Any subsequent path drawing you perform to the graphics context is clipped. (See Masking an Image by Clipping the Context.)\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Transforms",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_affine/dq_affine.html#//apple_ref/doc/uid/TP30001066-CH204-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nTransforms\n\nThe Quartz 2D drawing model defines two completely separate coordinate spaces: user space, which represents the document page, and device space, which represents the native resolution of a device. User space coordinates are floating-point numbers that are unrelated to the resolution of pixels in device space. When you want to print or display your document, Quartz maps user space coordinates to device space coordinates. Therefore, you never have to rewrite your application or write additional code to adjust the output from your application for optimum display on different devices.\n\nYou can modify the default user space by operating on the current transformation matrix, or CTM. After you create a graphics context, the CTM is the identity matrix. You can use Quartz transformation functions to modify the CTM and, as a result, modify drawing in user space.\n\nThis chapter:\n\nProvides an overview of the functions you can use to perform transformations\n\nShows how to modify the CTM\n\nDescribes how to create an affine transform\n\nShows how to determine if two transforms are equivalent\n\nDescribes how to obtain the user-to-device-space transform\n\nDiscusses the math behind affine transforms\n\nAbout Quartz Transformation Functions\n\nYou can easily translate, scale, and rotate your drawing using the Quartz 2D built-in transformation functions. With just a few lines of code, you can apply these transformations in any order and in any combination. Figure 5-1 illustrates the effects of scaling and rotating an image. Each transformation you apply updates the CTM. The CTM always represents the current mapping between user space and device space. This mapping ensures that the output from your application looks great on any display screen or printer.\n\nFigure 5-1  Applying scaling and rotation\n\nThe Quartz 2D API provides five functions that allow you to obtain and modify the CTM. You can rotate, translate, and scale the CTM, and you can concatenate an affine transformation matrix with the CTM. See Modifying the Current Transformation Matrix.\n\nQuartz also allows you to create affine transforms that don’t operate on user space until you decide to apply the transform to the CTM. You use another set of functions to create affine transforms, which can then be concatenated with the CTM. See Creating Affine Transforms.\n\nYou can use either set of functions without understanding anything about matrix math. However if you want to understand what Quartz does when you call one of the transform functions, read The Math Behind the Matrices.\n\nModifying the Current Transformation Matrix\n\nYou manipulate the CTM to rotate, scale, or translate the page before drawing an image, thereby transforming the object you are about to draw. Before you transform the CTM, you need to save the graphics state so that you can restore it after drawing. You can also concatenate the CTM with an affine transform (see Creating Affine Transforms). Each of these four operations—translation, rotation, scaling, and concatenation—is described in this section along with the CTM functions that perform each operation.\n\nThe following line of code draws an image, assuming that you provide a valid graphics context, a pointer to the rectangle to draw the image to, and a valid CGImage object. The code draws an image, such as the sample rooster image shown in Figure 5-2. As you read the rest of this section, you’ll see how the image changes as you apply transformations.\n\nCGContextDrawImage (myContext, rect, myImage);\nFigure 5-2  An image that is not transformed\n\nTranslation moves the origin of the coordinate space by the amount you specify for the x and y axes. You call the function CGContextTranslateCTM to modify the x and y coordinates of each point by a specified amount. Figure 5-3 shows an image translated by 100 units in the x-axis and 50 units in the y-axis, using the following line of code:\n\nCGContextTranslateCTM (myContext, 100, 50);\nFigure 5-3  A translated image\n\nRotation moves the coordinate space by the angle you specify. You call the function CGContextRotateCTM to specify the rotation angle, in radians. Figure 5-4 shows an image rotated by –45 degrees about the origin, which is the lower left of the window, using the following line of code:\n\nCGContextRotateCTM (myContext, radians(–45.));\n\nThe image is clipped because the rotation moved part of the image to a location outside the context. You need to specify the rotation angle in radians.\n\nIt’s useful to write a radians routine if you plan to perform many rotations.\n\n#include <math.h>\n\n\nstatic inline double radians (double degrees) {return degrees * M_PI/180;}\nFigure 5-4  A rotated image\n\nScaling changes the scale of the coordinate space by the x and y factors you specify, effectively stretching or shrinking the image. The magnitude of the x and y factors governs whether the new coordinates are larger or smaller than the original. In addition, by making the x factor negative, you can flip the coordinates along the x-axis; similarly, you can flip coordinates horizontally, along the y-axis, by making the y factor negative. You call the function CGContextScaleCTM to specify the x and y scaling factors. Figure 5-5 shows an image whose x values are scaled by .5 and whose y values are scaled by .75, using the following line of code:\n\nCGContextScaleCTM (myContext, .5, .75);\nFigure 5-5  A scaled image\n\nConcatenation combines two matrices by multiplying them together. You can concatenate several matrices to form a single matrix that contains the cumulative effects of the matrices. You call the function CGContextConcatCTM to combine the CTM with an affine transform. Affine transforms, and the functions that create them, are discussed in Creating Affine Transforms.\n\nAnother way to achieve a cumulative effect is to perform two or more transformations without restoring the graphics state between transformation calls. Figure 5-6 shows an image that results from translating an image and then rotating it, using the following lines of code:\n\nCGContextTranslateCTM (myContext, w,h);\n\n\nCGContextRotateCTM (myContext, radians(-180.));\nFigure 5-6  An image that is translated and rotated\n\nFigure 5-7 shows an image that is translated, scaled, and rotated, using the following lines of code:\n\nCGContextTranslateCTM (myContext, w/4, 0);\n\n\nCGContextScaleCTM (myContext, .25,  .5);\n\n\nCGContextRotateCTM (myContext, radians ( 22.));\nFigure 5-7  An image that is translated, scaled, and then rotated\n\nThe order in which you perform multiple transformations matters; you get different results if you reverse the order. Reverse the order of transformations used to create Figure 5-7 and you get the results shown in Figure 5-8, which is produced with this code:\n\nCGContextRotateCTM (myContext, radians ( 22.));\n\n\nCGContextScaleCTM (myContext, .25,  .5);\n\n\nCGContextTranslateCTM (myContext, w/4, 0);\nFigure 5-8  An image that is rotated, scaled, and then translated\nCreating Affine Transforms\n\nThe affine transform functions available in Quartz operate on matrices, not on the CTM. You can use these functions to construct a matrix that you later apply to the CTM by calling the function CGContextConcatCTM. The affine transform functions either operate on, or return, a CGAffineTransform data structure. You can construct simple or complex affine transforms that are reusable.\n\nThe affine transform functions perform the same operations as the CTM functions—translation, rotation, scaling, and concatenation. Table 5-1 lists the functions that perform these operations along with information on their use. Note that there are two functions for each of the translation, rotation, and scaling operations.\n\nTable 5-1  Affine transform functions for translation, rotation, and scaling\n\nFunction\n\n\t\n\nUse\n\n\n\n\nCGAffineTransformMakeTranslation\n\n\t\n\nTo construct a new translation matrix from x and y values that specify how much to move the origin.\n\n\n\n\nCGAffineTransformTranslate\n\n\t\n\nTo apply a translation operation to an existing affine transform.\n\n\n\n\nCGAffineTransformMakeRotation\n\n\t\n\nTo construct a new rotation matrix from a value that specifies in radians how much to rotate the coordinate system.\n\n\n\n\nCGAffineTransformRotate\n\n\t\n\nTo apply a rotation operation to an existing affine transform.\n\n\n\n\nCGAffineTransformMakeScale\n\n\t\n\nTo construct a new scaling matrix from x and y values that specify how much to stretch or shrink coordinates.\n\n\n\n\nCGAffineTransformScale\n\n\t\n\nTo apply a scaling operation to an existing affine transform.\n\nQuartz also provides an affine transform function that inverts a matrix, CGAffineTransformInvert. Inversion is generally used to provide reverse transformation of points within transformed objects. Inversion can be useful when you need to recover a value that has been transformed by a matrix: Invert the matrix, and multiply the value by the inverted matrix, and the result is the original value. You usually don’t need to invert transforms because you can reverse the effects of transforming the CTM by saving and restoring the graphics state.\n\nIn some situations you might not want to transform the entire space, but just a point or a size. You operate on a CGPoint structure by calling the function CGPointApplyAffineTransform. You operate on a CGSize structure by calling the function CGSizeApplyAffineTransform. You can operate on a CGRect structure by calling the function CGRectApplyAffineTransform. This function returns the smallest rectangle that contains the transformed corner points of the rectangle passed to it. If the affine transform that operates on the rectangle performs only scaling and translation operations, the returned rectangle coincides with the rectangle constructed from the four transformed corners.\n\nYou can create a new affine transform by calling the function CGAffineTransformMake, but unlike the other functions that make new affine transforms, this one requires you to supply matrix entries. To effectively use this function, you need to have an understanding of matrix math. See The Math Behind the Matrices.\n\nEvaluating Affine Transforms\n\nYou can determine whether one affine transform is equal to another by calling the function CGAffineTransformEqualToTransform. This function returns true if the two transforms passed to it are equal and false otherwise.\n\nThe function CGAffineTransformIsIdentity is a useful function for checking whether a transform is the identity transform. The identity transform performs no translation, scaling, or rotation. Applying this transform to the input coordinates always returns the input coordinates. The Quartz constant CGAffineTransformIdentity represents the identity transform.\n\nGetting the User to Device Space Transform\n\nTypically when you draw with Quartz 2D, you work only in user space. Quartz takes care of transforming between user and device space for you. If your application needs to obtain the affine transform that Quartz uses to convert between user and device space, you can call the function CGContextGetUserSpaceToDeviceSpaceTransform.\n\nQuartz provides a number of convenience functions to transform the following geometries between user space and device space. You might find these functions easier to use than applying the affine transform returned from the function CGContextGetUserSpaceToDeviceSpaceTransform.\n\nPoints. The functions CGContextConvertPointToDeviceSpace and CGContextConvertPointToUserSpace transform a CGPoint data type from one space to the other.\n\nSizes. The functions CGContextConvertSizeToDeviceSpace and CGContextConvertSizeToUserSpace transform a CGSize data type from one space to the other.\n\nRectangles. The functions CGContextConvertRectToDeviceSpace and CGContextConvertRectToUserSpace transform a CGRect data type from one space to the other.\n\nThe Math Behind the Matrices\n\nThe only Quartz 2D function for which you need an understanding of matrix math is the function CGAffineTransformMake, which makes an affine transform from the six critical entries in a 3 x 3 matrix. Even if you never plan to construct an affine transformation matrix from scratch, you might find the math behind the transform functions interesting. If not, you can skip the rest of this chapter.\n\nThe six critical values of a 3 x 3 transformation matrix —a, b, c, d, tx and ty— are shown in the following matrix:\n\nNote: The rightmost column of the matrix always contains the constant values 0, 0, 1. Mathematically, this third column is required to allow concatenation, which is explained later in this section. It appears in this section for the sake of mathematical correctness only.\n\nGiven the 3 x 3 transformation matrix described above, Quartz uses this equation to transform a point (x, y) into a resultant point (x’, y’):\n\nThe result is in a different coordinate system, the one transformed by the variable values in the transformation matrix. The following equations are the definition of the previous matrix transform:\n\nThe following matrix is the identity matrix. It performs no translation, scaling, or rotation. Multiplying this matrix by the input coordinates always returns the input coordinates.\n\nUsing the formulas discussed earlier, you can see that this matrix would generate a new point (x’, y’) that is the same as the old point (x, y):\n\nThis matrix describes a translation operation:\n\nThese are the resulting equations that Quartz uses to apply the translation:\n\nThis matrix describes a scaling operation on a point (x, y):\n\nThese are the resulting equations that Quartz uses to scale the coordinates:\n\nThis matrix describes a rotation operation, rotating the point (x, y) counterclockwise by an angle a:\n\nThese are the resulting equations that Quartz uses to apply the rotation:\n\nThis equation concatenates a rotation operation with a translation operation:\n\nThese are the resulting equations that Quartz uses to apply the transform:\n\nNote that the order in which you concatenate matrices is important—matrix multiplication is not commutative. That is, the result of multiplying matrix A by matrix B does not necessarily equal the result of multiplying matrix B by matrix A.\n\nAs previously mentioned, concatenation is the reason the affine transformation matrix contains a third column with the constant values 0, 0, 1. To multiply one matrix against another matrix, the number of columns of one matrix must match the number of rows of the other. This means that a 2 x 3 matrix cannot be multiplied against a 2 x 3 matrix. Thus we need the extra column containing the constant values.\n\nAn inversion operation produces original coordinates from transformed ones. Given the coordinates (x, y), which have been transformed by a given matrix A to new coordinates (x’, y’), transforming the coordinates (x’, y’) by the inverse of matrix A produces the original coordinates (x, y). When a matrix is multiplied by its inverse, the result is the identity matrix.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Patterns",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_patterns/dq_patterns.html#//apple_ref/doc/uid/TP30001066-CH206-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nPatterns\n\nA pattern is a sequence of drawing operations that is repeatedly painted to a graphics context. You can use patterns in the same way as you use colors. When you paint using a pattern, Quartz divides the page into a set of pattern cells, with each cell the size of the pattern image, and draws each cell using a callback you provide. Figure 6-1 shows a pattern drawn to a window graphics context.\n\nFigure 6-1  A pattern drawn to a window\nThe Anatomy of a Pattern\n\nThe pattern cell is the basic component of a pattern. The pattern cell for the pattern shown in Figure 6-1 is shown in Figure 6-2. The black rectangle is not part of the pattern; it’s drawn to show where the pattern cell ends.\n\nFigure 6-2  A pattern cell\n\nThe size of this particular pattern cell includes the area of the four colored rectangles and space above and to the right of the rectangles, as shown in Figure 6-3. The black rectangle surrounding each pattern cell in the figure is not part of the cell; it’s drawn to indicate the bounds of the cell. When you create a pattern cell, you define the bounds of the cell and draw within the bounds.\n\nFigure 6-3  Pattern cells with black rectangles drawn to show the bounds of each cell\n\nYou can specify how far apart Quartz draws the start of each pattern cell from the next in the horizontal and vertical directions. The pattern cells in Figure 6-3 are drawn so that the start of one pattern cell is exactly a pattern width apart from the next pattern cell, resulting in each pattern cell abutting on the next. The pattern cells in Figure 6-4 have space added in both directions, horizontal and vertical. You can specify different spacing values for each direction. If you make the spacing less than the width or height of a pattern cell, the pattern cells overlap.\n\nFigure 6-4  Spacing between pattern cells\n\nWhen you draw a pattern cell, Quartz uses pattern space as the coordinate system. Pattern space is an abstract space that maps to the default user space by the transformation matrix you specify when you create the pattern—the pattern matrix.\n\nNote: Pattern space is separate from user space. The untransformed pattern space maps to the base (untransformed) user space, regardless of the state of the current transformation matrix. When you apply a transformation to pattern space, Quartz applies the transform only to pattern space.\n\nThe default conventions for a pattern’s coordinate systems are those of the underlying graphics context. By default, Quartz uses a coordinate system where a positive x value represents a displacement to the right and a positive y value represents an upward displacement. However, a graphics context created by UIKit uses a different convention, where positive y values indicate a downward displacement. While this convention is normally applied to the graphics context by concatenating a transformation onto the coordinate system, in this case, Quartz also modifies the default conventions of pattern space to match.\n\nIf you don’t want Quartz to transform the pattern cell, you can specify the identity matrix. However, you can achieve interesting effects by supplying a transformation matrix. Figure 6-5 shows the effect of scaling the pattern cell shown in Figure 6-2. Figure 6-6 demonstrates rotating the pattern cell. Translating the pattern cell is a bit more subtle. Figure 6-7 shows the origin of the pattern, with the pattern cell translated in both directions, horizontal and vertical, so that the pattern no longer abuts the window as it does in Figure 6-1.\n\nFigure 6-5  A scaled pattern cell\nFigure 6-6  A rotated pattern cell\nFigure 6-7  A translated pattern cell\nColored Patterns and Stencil (Uncolored) Patterns\n\nColored patterns have inherent colors associated with them. Change the coloring used to create the pattern cell, and the pattern loses its meaning. A Scottish tartan (such as the sample one shown in Figure 6-8) is an example of a colored pattern. The color in a colored pattern is specified as part of the pattern cell creation process, not as part of the pattern drawing process.\n\nFigure 6-8  A colored pattern has inherent color\n\nOther patterns are defined solely on their shape and, for that reason, can be thought of as stencil patterns, uncolored patterns, or even as an image mask. The red and black stars shown in Figure 6-9 are each renditions of the same pattern cell. The cell itself consists of one shape—a filled star. When the pattern cell was defined, no color was associated with it. The color is specified as part of the pattern drawing process, not as part of the pattern cell creation.\n\nFigure 6-9  A stencil pattern does not have inherent color\n\nYou can create either kind of pattern—colored or stencil—in Quartz 2D.\n\nTiling\n\nTiling is the process of rendering pattern cells to a portion of a page. When Quartz renders a pattern to a device, Quartz may need to adjust the pattern to fit the device space. That is, the pattern cell as defined in user space might not fit perfectly when rendered to the device because of differences between user space units and device pixels.\n\nQuartz has three tiling options it can use to adjust patterns when necessary. Quartz can preserve:\n\nThe pattern, at the expense of adjusting the spacing between pattern cells slightly, but by no more than one device pixel. This is referred to as no distortion.\n\nSpacing between cells, at the expense of distorting the pattern cell slightly, but by no more than one device pixel. This is referred to as constant spacing with minimal distortion.\n\nSpacing between cells (as for the minimal distortion option) at the expense of distorting the pattern cell as much as needed to get fast tiling. This is referred to as constant spacing.\n\nHow Patterns Work\n\nPatterns operate similarly to colors, in that you set a fill or stroke pattern and then call a painting function. Quartz uses the pattern you set as the “paint.” For example, if you want to paint a filled rectangle with a solid color, you first call a function, such as CGContextSetFillColor, to set the fill color. Then you call the function CGContextFillRect to paint the filled rectangle with the color you specify. To paint with a pattern, you first call the function CGContextSetFillPattern to set the pattern. Then you call CGContextFillRect to actually paint the filled rectangle with the pattern you specify. The difference between painting with colors and with patterns is that you must define the pattern. You supply the pattern and color information to the function CGContextSetFillPattern. You’ll see how to create, set, and paint patterns in Painting Colored Patterns and Painting Stencil Patterns.\n\nHere’s an example of how Quartz works behind the scenes to paint with a pattern you provide. When you fill or stroke with a pattern, Quartz conceptually performs the following tasks to draw each pattern cell:\n\nSaves the graphics state.\n\nTranslates the current transformation matrix to the origin of the pattern cell.\n\nConcatenates the CTM with the pattern matrix.\n\nClips to the bounding rectangle of the pattern cell.\n\nCalls your drawing callback to draw the pattern cell.\n\nRestores the graphics state.\n\nQuartz takes care of all the tiling for you, repeatedly rendering the pattern cell to the drawing space until the entire space is painted. You can fill or stroke with a pattern. The pattern cell can be of any size you specify. If you want to see the pattern, you should make sure the pattern cell fits in the drawing space. For example, if your pattern cell is 8 units by 10 units, and you use the pattern to stroke a line that has a width of 2 units, the pattern cell will be clipped since it is 10 units wide. In this case, you might not recognize the pattern.\n\nPainting Colored Patterns\n\nThe five steps you need to perform to paint a colored pattern are described in the following sections:\n\nWrite a Callback Function That Draws a Colored Pattern Cell\n\nSet Up the Colored Pattern Color Space\n\nSet Up the Anatomy of the Colored Pattern\n\nSpecify the Colored Pattern as a Fill or Stroke Pattern\n\nDraw With the Colored Pattern\n\nThese are the same steps you use to paint a stencil pattern. The difference between the two is how you set up color information. You can see how all the steps fit together in A Complete Colored Pattern Painting Function.\n\nWrite a Callback Function That Draws a Colored Pattern Cell\n\nWhat a pattern cell looks like is entirely up to you. For this example, the code in Listing 6-1 draws the pattern cell shown in Figure 6-2. Recall that the black line surrounding the pattern cell is not part of the cell; it’s drawn to show that the bounds of the pattern cell are larger than the rectangles painted by the code. You specify the pattern size to Quartz later.\n\nYour pattern cell drawing function is a callback that follows this form:\n\ntypedef void (*CGPatternDrawPatternCallback) (\n\n\n                        void *info,\n\n\n                        CGContextRef context\n\n\n    );\n\nYou can name your callback whatever you like. The one in Listing 6-1 is named MyDrawColoredPattern. The callback takes two parameters:\n\ninfo, a generic pointer to private data associated with the pattern. This parameter is optional; you can pass NULL. The data passed to your callback is the same data you supply later, when you create the pattern.\n\ncontext, the graphics context for drawing the pattern cell.\n\nThe pattern cell drawn by the code in Listing 6-1 is arbitrary. Your code draws whatever is appropriate for the pattern you create. These details about the code are important:\n\nThe pattern size is declared. You need to keep the pattern size in mind as you write your drawing code. Here, the size is declared as a global. The drawing function doesn’t specifically refer to the size, except in a comment. Later, you specify the pattern size to Quartz 2D. See Set Up the Anatomy of the Colored Pattern.\n\nThe drawing function follows the prototype defined by the CGPatternDrawPatternCallback callback type definition.\n\nThe drawing performed in the code sets colors, which makes this a colored pattern.\n\nListing 6-1  A drawing callback that draws a colored pattern cell\n\n#define H_PATTERN_SIZE 16\n\n\n#define V_PATTERN_SIZE 18\n\n\n \n\n\nvoid MyDrawColoredPattern (void *info, CGContextRef myContext)\n\n\n{\n\n\n    CGFloat subunit = 5; // the pattern cell itself is 16 by 18\n\n\n \n\n\n    CGRect  myRect1 = {{0,0}, {subunit, subunit}},\n\n\n            myRect2 = {{subunit, subunit}, {subunit, subunit}},\n\n\n            myRect3 = {{0,subunit}, {subunit, subunit}},\n\n\n            myRect4 = {{subunit,0}, {subunit, subunit}};\n\n\n \n\n\n    CGContextSetRGBFillColor (myContext, 0, 0, 1, 0.5);\n\n\n    CGContextFillRect (myContext, myRect1);\n\n\n    CGContextSetRGBFillColor (myContext, 1, 0, 0, 0.5);\n\n\n    CGContextFillRect (myContext, myRect2);\n\n\n    CGContextSetRGBFillColor (myContext, 0, 1, 0, 0.5);\n\n\n    CGContextFillRect (myContext, myRect3);\n\n\n    CGContextSetRGBFillColor (myContext, .5, 0, .5, 0.5);\n\n\n    CGContextFillRect (myContext, myRect4);\n\n\n}\nSet Up the Colored Pattern Color Space\n\nThe code in Listing 6-1 uses colors to draw the pattern cell. You must ensure that Quartz paints with the colors you use in your drawing routine by setting the base pattern color space to NULL, as shown in Listing 6-2. A detailed explanation for each numbered line of code follows the listing.\n\nListing 6-2  Creating a base pattern color space\n\nCGColorSpaceRef patternSpace;\n\n\n \n\n\npatternSpace = CGColorSpaceCreatePattern (NULL);\n// 1\n\n\nCGContextSetFillColorSpace (myContext, patternSpace);\n// 2\n\n\nCGColorSpaceRelease (patternSpace);\n// 3\n\nHere’s what the code does:\n\nCreates a pattern color space appropriate for a colored pattern by calling the function CGColorSpaceCreatePattern, passing NULL as the base color space.\n\nSets the fill color space to the pattern color space. If you are stroking your pattern, call CGContextSetStrokeColorSpace.\n\nReleases the pattern color space.\n\nSet Up the Anatomy of the Colored Pattern\n\nInformation about the anatomy of a pattern is kept in a CGPattern object. You create a CGPattern object by calling the function CGPatternCreate, whose prototype is shown in Listing 6-3.\n\nListing 6-3  The CGPatternCreate function prototype\n\nCGPatternRef CGPatternCreate (  void *info,\n\n\n                                CGRect bounds,\n\n\n                                CGAffineTransform matrix,\n\n\n                                CGFloat xStep,\n\n\n                                CGFloat yStep,\n\n\n                                CGPatternTiling tiling,\n\n\n                                bool isColored,\n\n\n                                const CGPatternCallbacks *callbacks );\n\nThe info parameter is a pointer to data you want to pass to your drawing callback. This is the same pointer discussed in Write a Callback Function That Draws a Colored Pattern Cell.\n\nYou specify the size of the pattern cell in the bounds parameter. The matrix parameter is where you specify the pattern matrix, which maps the pattern coordinate system to the default coordinate system of the graphics context. Use the identity matrix if you want to draw the pattern using the same coordinate system as the graphics context. The xStep and yStep parameters specify the horizontal and vertical spacing between cells in the pattern coordinate system. See The Anatomy of a Pattern to review information on bounds, pattern matrix, and spacing.\n\nThe tiling parameter can be one of three values:\n\nkCGPatternTilingNoDistortion\n\nkCGPatternTilingConstantSpacingMinimalDistortion\n\nkCGPatternTilingConstantSpacing\n\nSee Tiling to review information on tiling.\n\nThe isColored parameter specifies whether the pattern cell is a colored pattern (true) or a stencil pattern (false). If you pass true here, your drawing pattern callback specifies the pattern color, and you must set the pattern color space to the colored pattern color space (see Set Up the Colored Pattern Color Space).\n\nThe last parameter you pass to the function CGPatternCreate is a pointer to a CGPatternCallbacks data structure. This structure has three fields:\n\nstruct CGPatternCallbacks\n\n\n{\n\n\n    unsigned int version;\n\n\n    CGPatternDrawPatternCallback drawPattern;\n\n\n    CGPatternReleaseInfoCallback releaseInfo;\n\n\n};\n\nYou set the version field to 0. The drawPattern field is a pointer to your drawing callback. The releaseInfo field is a pointer to a callback that’s invoked when the CGPattern object is released, to release storage for the info parameter you passed to your drawing callback. If you didn’t pass any data in this parameter, you set this field to NULL.\n\nSpecify the Colored Pattern as a Fill or Stroke Pattern\n\nYou can use your pattern for filling or stroking by calling the appropriate function—CGContextSetFillPattern or CGContextSetStrokePattern. Quartz uses your pattern for any subsequent filling or stroking.\n\nThese functions each take three parameters:\n\nThe graphics context\n\nThe CGPattern object that you created previously\n\nAn array of color components\n\nAlthough colored patterns supply their own color, you must pass a single alpha value to inform Quartz of the overall opacity of the pattern when it’s drawn. Alpha can vary from 1 (completely opaque) to 0 (completely transparent). These lines of code show an example of how to set opacity for a colored pattern used to fill.\n\nCGFloat alpha = 1;\n\n\n \n\n\nCGContextSetFillPattern (myContext, myPattern, &alpha);\nDraw With the Colored Pattern\n\nAfter you’ve completed the previous steps, you can call any Quartz 2D function that paints. Your pattern is used as the “paint.” For example, you can call CGContextStrokePath, CGContextFillPath, CGContextFillRect, or any other function that paints.\n\nA Complete Colored Pattern Painting Function\n\nThe code in Listing 6-4 contains a function that paints a colored pattern. The function incorporates all the steps discussed previously. A detailed explanation for each numbered line of code follows the listing.\n\nListing 6-4  A function that paints a colored pattern\n\nvoid MyColoredPatternPainting (CGContextRef myContext,\n\n\n                 CGRect rect)\n\n\n{\n\n\n    CGPatternRef    pattern;\n// 1\n\n\n    CGColorSpaceRef patternSpace;\n// 2\n\n\n    CGFloat         alpha = 1,\n// 3\n\n\n                    width, height;\n// 4\n\n\n    static const    CGPatternCallbacks callbacks = {0, \n// 5\n\n\n                                        &MyDrawPattern,\n\n\n                                        NULL};\n\n\n \n\n\n    CGContextSaveGState (myContext);\n\n\n    patternSpace = CGColorSpaceCreatePattern (NULL);\n// 6\n\n\n    CGContextSetFillColorSpace (myContext, patternSpace);\n// 7\n\n\n    CGColorSpaceRelease (patternSpace);\n// 8\n\n\n \n\n\n    pattern = CGPatternCreate (NULL, \n// 9\n\n\n                    CGRectMake (0, 0, H_PSIZE, V_PSIZE),\n// 10\n\n\n                    CGAffineTransformMake (1, 0, 0, 1, 0, 0),\n// 11\n\n\n                    H_PATTERN_SIZE, \n// 12\n\n\n                    V_PATTERN_SIZE, \n// 13\n\n\n                    kCGPatternTilingConstantSpacing,\n// 14\n\n\n                    true, \n// 15\n\n\n                    &callbacks);\n// 16\n\n\n \n\n\n    CGContextSetFillPattern (myContext, pattern, &alpha);\n// 17\n\n\n    CGPatternRelease (pattern);\n// 18\n\n\n    CGContextFillRect (myContext, rect);\n// 19\n\n\n    CGContextRestoreGState (myContext);\n\n\n}\n\nHere’s what the code does:\n\nDeclares storage for a CGPattern object that is created later.\n\nDeclares storage for a pattern color space that is created later.\n\nDeclares a variable for alpha and sets it to 1, which specifies the opacity of the pattern as completely opaque.\n\nDeclares variable to hold the height and width of the window. In this example, the pattern is painted over the area of a window.\n\nDeclares and fills a callbacks structure, passing 0 as the version and a pointer to a drawing callback function. This example does not provide a release info callback, so that field is set to NULL.\n\nCreates a pattern color space object, setting the pattern’s base color space to NULL. When you paint a colored pattern, the pattern supplies its own color in the drawing callback, which is why you set the color space to NULL.\n\nSets the fill color space to the pattern color space object you just created.\n\nReleases the pattern color space object.\n\nPasses NULL because the pattern does not need any additional information passed to the drawing callback.\n\nPasses a CGRect object that specifies the bounds of the pattern cell.\n\nPasses a CGAffineTransform matrix that specifies how to translate the pattern space to the default user space of the context in which the pattern is used. This example passes the identity matrix.\n\nPasses the horizontal pattern size as the horizontal displacement between the start of each cell. In this example, one cell is painted adjacent to the next.\n\nPasses the vertical pattern size as the vertical displacement between start of each cell.\n\nPasses the constant kCGPatternTilingConstantSpacing to specify how Quartz should render the pattern. For more information, see Tiling.\n\nPasses true for the isColored parameter, to specify that the pattern is a colored pattern.\n\nPasses a pointer to the callbacks structure that contains version information, and a pointer to your drawing callback function.\n\nSets the fill pattern, passing the context, the CGPattern object you just created, and a pointer to the alpha value that specifies an opacity for Quartz to apply to the pattern.\n\nReleases the CGPattern object.\n\nFills a rectangle that is the size of the window passed to the MyColoredPatternPainting routine. Quartz fills the rectangle using the pattern you just set up.\n\nPainting Stencil Patterns\n\nThe five steps you need to perform to paint a stencil pattern are described in the following sections:\n\nWrite a Callback Function That Draws a Stencil Pattern Cell\n\nSet Up the Stencil Pattern Color Space\n\nSet Up the Anatomy of the Stencil Pattern\n\nSpecify the Stencil Pattern as a Fill or Stroke Pattern\n\nDrawing with the Stencil Pattern\n\nThese are actually the same steps you use to paint a colored pattern. The difference between the two is how you set up color information. You can see how all the steps fit together in A Complete Stencil Pattern Painting Function.\n\nWrite a Callback Function That Draws a Stencil Pattern Cell\n\nThe callback you write for drawing a stencil pattern follows the same form as that described for a colored pattern cell. See Write a Callback Function That Draws a Colored Pattern Cell. The only difference is that your drawing callback does not specify any color. The pattern cell shown in Figure 6-10 does not get its color from the drawing callback. The color is set outside the drawing color in the pattern color space.\n\nFigure 6-10  A stencil pattern cell\n\nTake a look at the code in Listing 6-5, which draws the pattern cell shown in Figure 6-10. Notice that the code simply creates a path and fills the path. The code does not set color.\n\nListing 6-5  A drawing callback that draws a stencil pattern cell\n\n#define PSIZE 16    // size of the pattern cell\n\n\n \n\n\nstatic void MyDrawStencilStar (void *info, CGContextRef myContext)\n\n\n{\n\n\n    int k;\n\n\n    double r, theta;\n\n\n \n\n\n    r = 0.8 * PSIZE / 2;\n\n\n    theta = 2 * M_PI * (2.0 / 5.0); // 144 degrees\n\n\n \n\n\n    CGContextTranslateCTM (myContext, PSIZE/2, PSIZE/2);\n\n\n \n\n\n    CGContextMoveToPoint(myContext, 0, r);\n\n\n    for (k = 1; k < 5; k++) {\n\n\n        CGContextAddLineToPoint (myContext,\n\n\n                    r * sin(k * theta),\n\n\n                    r * cos(k * theta));\n\n\n    }\n\n\n    CGContextClosePath(myContext);\n\n\n    CGContextFillPath(myContext);\n\n\n}\nSet Up the Stencil Pattern Color Space\n\nStencil patterns require that you set up a pattern color space for Quartz to paint with, as shown in Listing 6-6. A detailed explanation for each numbered line of code follows the listing.\n\nListing 6-6  Code that creates a pattern color space for a stencil pattern\n\nCGPatternRef pattern;\n\n\nCGColorSpaceRef baseSpace;\n\n\nCGColorSpaceRef patternSpace;\n\n\n \n\n\nbaseSpace = CGColorSpaceCreateWithName (kCGColorSpaceGenericRGB);\n// 1\n\n\npatternSpace = CGColorSpaceCreatePattern (baseSpace);\n// 2\n\n\nCGContextSetFillColorSpace (myContext, patternSpace);\n// 3\n\n\nCGColorSpaceRelease(patternSpace);\n// 4\n\n\nCGColorSpaceRelease(baseSpace);\n// 5\n\nHere’s what the code does:\n\nThis function creates a generic RGB space. Generic color spaces leave color matching to the system. For more information, see Creating Generic Color Spaces.\n\nCreates a pattern color space. The color space you supply specifies how colors are represented for the pattern. Later, when you set colors for the pattern, you must set them using the pattern color space. For this example, you will need to specify color using RGB values.\n\nSets the color space to use when filling a pattern. You can set a stroke color space by calling the function CGContextSetStrokeColorSpace.\n\nReleases the pattern color space object.\n\nReleases the base color space object.\n\nSet Up the Anatomy of the Stencil Pattern\n\nYou specify information about the anatomy of a pattern the way you would for a colored pattern—by calling the function CGPatternCreate. The only difference is that you pass false for the isColored parameter. See Set Up the Anatomy of the Colored Pattern for more information on the parameters you supply to the CGPatternCreate function.\n\nSpecify the Stencil Pattern as a Fill or Stroke Pattern\n\nYou can use your pattern for filling or stroking by calling the appropriate function, CGContextSetFillPattern or CGContextSetStrokePattern. Quartz uses your pattern for any subsequent filling or stroking.\n\nThese functions each take three parameters:\n\nThe graphics context\n\nThe CGPattern object that you created previously\n\nAn array of color components\n\nA stencil pattern does not supply a color in the drawing callback, so you must pass a color to the fill or stroke functions to inform Quartz what color to use. Listing 6-7 shows an example of how to set color for a stencil pattern. The values in the color array are interpreted by Quartz in the color space you set up earlier. Because this example uses device RGB, the color array contains values for red, green, and blue components. The fourth value specifies the opacity of the color.\n\nListing 6-7  Code that sets opacity for a colored pattern\n\nstatic const CGFloat color[4] = { 0, 1, 1, 0.5 }; //cyan, 50% transparent\n\n\n \n\n\nCGContextSetFillPattern (myContext, myPattern, color);\nDrawing with the Stencil Pattern\n\nAfter you’ve completed the previous steps, you can call any Quartz 2D function that paints. Your pattern is used as the “paint.” For example, you can call CGContextStrokePath, CGContextFillPath, CGContextFillRect, or any other function that paints.\n\nA Complete Stencil Pattern Painting Function\n\nThe code in Listing 6-8 contains a function that paints a stencil pattern. The function incorporates all the steps discussed previously. A detailed explanation for each numbered line of code follows the listing.\n\nListing 6-8  A function that paints a stencil pattern\n\n#define PSIZE 16\n\n\n \n\n\nvoid MyStencilPatternPainting (CGContextRef myContext,\n\n\n                                const Rect *windowRect)\n\n\n{\n\n\n    CGPatternRef pattern;\n\n\n    CGColorSpaceRef baseSpace;\n\n\n    CGColorSpaceRef patternSpace;\n\n\n    static const CGFloat color[4] = { 0, 1, 0, 1 };\n// 1\n\n\n    static const CGPatternCallbacks callbacks = {0, &drawStar, NULL};\n// 2\n\n\n \n\n\n    baseSpace = CGColorSpaceCreateDeviceRGB ();\n// 3\n\n\n    patternSpace = CGColorSpaceCreatePattern (baseSpace);\n// 4\n\n\n    CGContextSetFillColorSpace (myContext, patternSpace);\n// 5\n\n\n    CGColorSpaceRelease (patternSpace);\n\n\n    CGColorSpaceRelease (baseSpace);\n\n\n    pattern = CGPatternCreate(NULL, CGRectMake(0, 0, PSIZE, PSIZE),\n// 6\n\n\n                  CGAffineTransformIdentity, PSIZE, PSIZE,\n\n\n                  kCGPatternTilingConstantSpacing,\n\n\n                  false, &callbacks);\n\n\n    CGContextSetFillPattern (myContext, pattern, color);\n// 7\n\n\n    CGPatternRelease (pattern);\n// 8\n\n\n    CGContextFillRect (myContext,CGRectMake (0,0,PSIZE*20,PSIZE*20));\n// 9\n\n\n}\n\nHere’s what the code does:\n\nDeclares an array to hold a color value and sets the value (which will be in RGB color space) to opaque green.\n\nDeclares and fills a callbacks structure, passing 0 as the version and a pointer to a drawing callback function. This example does not provide a release info callback, so that field is set to NULL.\n\nCreates an RGB device color space. If the pattern is drawn to a display, you need to supply this type of color space.\n\nCreates a pattern color space object from the RGB device color space.\n\nSets the fill color space to the pattern color space object you just created.\n\nCreates a pattern object. Note that the second to last parameter—the isColored parameter—is false. Stencil patterns do not supply color, so you must pass false for this parameter. All other parameters are similar to those passed for the colored pattern example. See A Complete Colored Pattern Painting Function.\n\nSets the fill pattern, passing the color array declared previously.\n\nReleases the CGPattern object.\n\nFills a rectangle. Quartz fills the rectangle using the pattern you just set up.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Color and Color Spaces",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_color/dq_color.html#//apple_ref/doc/uid/TP30001066-CH205-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nColor and Color Spaces\n\nDevices (displays, printers, scanners, cameras) don’t treat color the same way; each has its own range of colors that the device can produce faithfully. A color produced on one device might not be able to be produced on another device.\n\nTo work with color effectively and to understand the Quartz 2D functions for using color spaces and color, you should be familiar with the terminology discussed in Color Management Overview. That document discusses color perception, color values, device-independent and device color spaces, the color-matching problem, rendering intent, color management modules, and ColorSync.\n\nIn this chapter, you’ll learn how Quartz represents color and color spaces, and what the alpha component is. This chapter also discusses how to:\n\nCreate color spaces\n\nCreate and set colors\n\nSet rendering intent\n\nAbout Color and Color Spaces\n\nA color in Quartz is represented by a set of values. The values are meaningless without a color space that dictates how to interpret color information. For example, the values in Table 4-1 all represent the color blue at full intensity. But without knowing the color space or the allowable range of values for each color space, you have no way of knowing which color each set of values represents.\n\nTable 4-1  Color values in different color spaces\n\nValues\n\n\t\n\nColor space\n\n\t\n\nComponents\n\n\n\n\n240 degrees, 100%, 100%\n\n\t\n\nHSB\n\n\t\n\nHue, saturation, brightness\n\n\n\n\n0, 0, 1\n\n\t\n\nRGB\n\n\t\n\nRed, green, blue\n\n\n\n\n1, 1, 0, 0\n\n\t\n\nCMYK\n\n\t\n\nCyan, magenta, yellow, black\n\n\n\n\n1, 0, 0\n\n\t\n\nBGR\n\n\t\n\nBlue, green, red\n\nIf you provide the wrong color space, you can get quite dramatic differences, as shown in Figure 4-1. Although the green color is interpreted the same in BGR and RGB color spaces, the red and blue values are flipped.\n\nFigure 4-1  Applying a BGR and an RGB color profile to the same image\n\nColor spaces can have different numbers of components. Three of the color spaces in the table have three components, while the CMYK color space has four. Value ranges are relative to that color space. For most color spaces, color values in Quartz range from 0.0 to 1.0, with 1.0 meaning full intensity. For example, the color blue at full intensity, specified in the RGB color space in Quartz, has the values (0, 0, 1.0). In Quartz, color also has an alpha value that specifies the transparency of a color. The color values in Table 4-1 don’t show an alpha value.\n\nThe Alpha Value\n\nThe alpha value is the graphics state parameter that Quartz uses to determine how to composite newly painted objects to the existing page. At full intensity, newly painted objects are opaque. At zero intensity, newly painted objects are invisible. Figure 4-2 shows five large rectangles, drawn using alpha values of 1.0, 0.75, 0.5, 0.1, and 0.0. As the large rectangle becomes transparent, it exposes a smaller, opaque red rectangle drawn underneath.\n\nFigure 4-2  A comparison of large rectangles painted using various alpha values\n\nYou can make both the objects on the page and the page itself transparent by setting the alpha value globally in the graphics context before painting. Figure 4-3 compares a global alpha setting of 0.5 with the default value of 1.0.\n\nFigure 4-3  A comparison of global alpha values\n\nIn the normal blend mode (which is the default for the graphics state) Quartz performs alpha blending by combining the components of the source color with the components of the destination color using the formula:\n\ndestination = (alpha * source) + (1 - alpha) * destination\n\nwhere source is one component of the new paint color and destination is one component of the background color. This formula is executed for each newly painted shape or image.\n\nFor object transparency, set the alpha value to 1.0 to specify that objects you draw should be fully opaque; set it to 0.0 to specify that newly drawn objects are fully transparent. An alpha value between 0.0 and 1.0 specifies a partially transparent object. You can supply an alpha value as the last color component to all routines that accept colors. You can also set the global alpha value using the CGContextSetAlpha function. Keep in mind that if you set both, Quartz multiplies the alpha color component by the global alpha value.\n\nTo allow the page itself to be fully transparent, you can explicitly clear the alpha channel of the graphics context using the CGContextClearRect function, as long as the graphics context is a window or bitmap graphics context. You might want to do this when creating a transparency mask for an icon, for example, or to make the background of a window transparent.\n\nCreating Color Spaces\n\nQuartz supports the standard color spaces used by color management systems for device-independent color spaces and also supports generic, indexed, and pattern color spaces. Device-independent color spaces represent color in a way that is portable between devices. They are used for the interchanges of color data from the native color space of one device to the native color space of another device. Colors in a device-independent color space appear the same when displayed on different devices, to the extent that the capabilities of the device allow. For that reason, device-independent color spaces are your best choice for representing color.\n\nApplications that have precise color requirements should always use a device-independent color space. A common device independent color space is the generic color space. Generic color spaces let the operating system provide the best color space for your application. Drawing to the display looks as good as printing the same content to a printer.\n\nImportant: iOS does not support device-independent or generic color spaces. iOS applications must use device color spaces instead.\n\nCreating Device-Independent Color Spaces\n\nTo create a device-independent color space, you provide Quartz with the reference white point, reference black point, and gamma values for a particular device. Quartz uses this information to convert colors from your source color space into the color space of the output device.\n\nThe device-independent color spaces supported by Quartz, and the functions that create them are:\n\nL*a*b* is a nonlinear transformation of the Munsell color notation system (a system that specifies colors by hue, value, and saturation—or chroma —values). This color space matches perceived color difference with quantitative distance in color space. The L* component represents the lightness value, the a* component represents values from green to red, and the b* component represents values from blue to yellow. This color space is designed to mimic how the human brain decodes color. Use the function CGColorSpaceCreateLab.\n\nICC is a color space from an ICC color profile, as defined by the International Color Consortium. ICC profiles define the gamut of colors supported by a device along with other device characteristics so that this information can be used to accurately transform the color space of one device to the color space of another. The manufacturer of the device typically provides an ICC profile. Some color monitors and printers contain embedded ICC profile information, as do some bitmap formats such as TIFF. Use the function CGColorSpaceCreateICCBased.\n\nCalibrated RGB is a device-independent RGB color space that represents colors relative to a reference white point that is based on the whitest light that can be generated by the output device. Use the function CGColorSpaceCreateCalibratedRGB.\n\nCalibrated gray is a device-independent grayscale color space that represents colors relative to a reference white point that is based on the whitest light that can be generated by the output device. Use the function CGColorSpaceCreateCalibratedGray.\n\nCreating Generic Color Spaces\n\nGeneric color spaces leave color matching to the system. For most cases, the result is acceptable. Although the name may imply otherwise, each “generic” color space—generic gray, generic RGB, and generic CMYK—is a specific device-independent color space.\n\nGeneric color spaces are easy to use; you don’t need to supply any reference point information. You create a generic color space by using the function CGColorSpaceCreateWithName along with one of the following constants:\n\nkCGColorSpaceGenericGray, which specifies generic gray, a monochromatic color space that permits the specification of a single value ranging from absolute black (value 0.0) to absolute white (value 1.0).\n\nkCGColorSpaceGenericRGB, which specifies generic RGB, a three-component color space (red, green, and blue) that models the way an individual pixel is composed on a color monitor. Each component of the RGB color space ranges in value from 0.0 (zero intensity) to 1.0 (full intensity).\n\nkCGColorSpaceGenericCMYK, which specifies generic CMYK, a four-component color space (cyan, magenta, yellow, and black) that models the way ink builds up during printing. Each component of the CMYK color space ranges in value from 0.0 (does not absorb the color) to 1.0 (fully absorbs the color).\n\nCreating Device Color Spaces\n\nDevice color spaces are primarily used by iOS applications because other options are not available. In most cases, a Mac OS X application should use a generic color space instead of creating a device color space. However, some Quartz routines expect images with a device color space. For example, if you call CGImageCreateWithMask and specify an image as the mask, the image must be defined with the device gray color space.\n\nYou create a device color space by using one of the following functions:\n\nCGColorSpaceCreateDeviceGray for a device-dependent grayscale color space.\n\nCGColorSpaceCreateDeviceRGB for a device-dependent RGB color space.\n\nCGColorSpaceCreateDeviceCMYK for a device-dependent CMYK color space.\n\nCreating Indexed and Pattern Color Spaces\n\nIndexed color spaces contain a color table with up to 256 entries, and a base color space to which the color table entries are mapped. Each entry in the color table specifies one color in the base color space. Use the function CGColorSpaceCreateIndexed.\n\nPattern color spaces, discussed in Patterns, are used when painting with patterns. Use the function CGColorSpaceCreatePattern.\n\nSetting and Creating Colors\n\nQuartz provides a suite of functions for setting fill color, stroke color, color spaces, and alpha. Each of these color parameters apply to the graphics state, which means that once set, that setting remains in effect until set to another value.\n\nA color must have an associated color space. Otherwise, Quartz won’t know how to interpret color values. Further, you need to supply an appropriate color space for the drawing destination. Compare the blue fill color on the left side of Figure 4-4, which is a CMYK fill color, with the blue color shown on the right side, which is an RGB fill color. If you view the onscreen version of this document, you’ll see a large difference between the fill colors. The colors are theoretically identical, but appear identical only if the RGB color is used for an RGB device and the CMYK color is used for a CMYK device.\n\nFigure 4-4  A CMYK fill color and an RGB fill color\n\nYou can use the functions CGContextSetFillColorSpace and CGContextSetStrokeColorSpace to set the fill and stroke color spaces, or you can use one of the convenience functions (listed in Table 4-2) that set color for a device color space.\n\nTable 4-2  Color-setting functions\n\nFunction\n\n\t\n\nUse to set color for\n\n\n\n\nCGContextSetRGBStrokeColor\n\nCGContextSetRGBFillColor\n\n\t\n\nDevice RGB. At PDF-generation time, Quartz writes the colors as if they were in the corresponding generic color space.\n\n\n\n\nCGContextSetCMYKStrokeColor\n\nCGContextSetCMYKFillColor\n\n\t\n\nDevice CMYK. (Remains device CMYK at PDF-generation time.)\n\n\n\n\nCGContextSetGrayStrokeColor\n\nCGContextSetGrayFillColor\n\n\t\n\nDevice Gray. At PDF-generation time, Quartz writes the colors as if they were in the corresponding generic color space.\n\n\n\n\nCGContextSetStrokeColorWithColor\n\nCGContextSetFillColorWithColor\n\n\t\n\nAny color space; you supply a CGColor object that specifies the color space. Use these functions for colors you need repeatedly.\n\n\n\n\nCGContextSetStrokeColor\n\nCGContextSetFillColor\n\n\t\n\nThe current color space. Not recommended. Instead, set color using a CGColor object and the functions CGContextSetStrokeColorWithColor and CGContextSetFillColorWithColor.\n\nYou specify the fill and stroke colors as values located within the fill and stroke color spaces. For example, a fully saturated red color in the RGB color space is specified as an array of four numbers: (1.0, 0.0, 0.0, 1.0). The first three numbers specify full red intensity and no green or blue intensity. The fourth number is the alpha value, which is used to specify the opacity of the color.\n\nIf you reuse colors in your application, the most efficient way to set fill and stroke colors is to create a CGColor object, which you then pass as a parameter to the functions CGContextSetFillColorWithColor and CGContextSetStrokeColorWithColor. You can keep the CGColor object around as long as you need it. You can improve your application’s performance by using CGColor objects directly.\n\nYou create a CGColor object by calling the function CGColorCreate, passing a CGColorspace object and an array of floating-point values that specify the intensity values for the color. The last component in the array specifies the alpha value.\n\nSetting Rendering Intent\n\nThe rendering intent specifies how Quartz maps colors from the source color space to those that are within the gamut of the destination color space of a graphics context. If you don’t explicitly set the rendering intent, Quartz uses relative colorimetric rendering intent for all drawing except bitmap (sampled) images. Quartz uses perceptual rendering intent for those.\n\nTo set the rendering intent, call the function CGContextSetRenderingIntent, passing a graphics context and one of the following constants:\n\nkCGRenderingIntentDefault. Uses the default rendering intent for the context.\n\nkCGRenderingIntentAbsoluteColorimetric. Maps colors outside of the gamut of the output device to the closest possible match inside the gamut of the output device. This can produce a clipping effect, where two different color values in the gamut of the graphics context are mapped to the same color value in the output device’s gamut. This is the best choice when the colors used in the graphics are within the gamut of both the source and the destination, as is often the case with logos or when spot colors are used.\n\nkCGRenderingIntentRelativeColorimetric. The relative colorimetric shifts all colors (including those within the gamut) to account for the difference between the white point of the graphics context and the white point of the output device.\n\nkCGRenderingIntentPerceptual. Preserves the visual relationship between colors by compressing the gamut of the graphics context to fit inside the gamut of the output device. Perceptual intent is good for photographs and other complex, detailed images.\n\nkCGRenderingIntentSaturation. Preserves the relative saturation value of the colors when converting into the gamut of the output device. The result is an image with bright, saturated colors. Saturation intent is good for reproducing images with low detail, such as presentation charts and graphs.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Overview of Quartz 2D",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_overview/dq_overview.html#//apple_ref/doc/uid/TP30001066-CH202-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nPrevious\nOverview of Quartz 2D\n\nQuartz 2D is a two-dimensional drawing engine accessible in the iOS environment and from all Mac OS X application environments outside of the kernel. You can use the Quartz 2D application programming interface (API) to gain access to features such as path-based drawing, painting with transparency, shading, drawing shadows, transparency layers, color management, anti-aliased rendering, PDF document generation, and PDF metadata access. Whenever possible, Quartz 2D leverages the power of the graphics hardware.\n\nIn Mac OS X, Quartz 2D can work with all other graphics and imaging technologies—Core Image, Core Video, OpenGL, and QuickTime. It’s possible to create an image in Quartz from a QuickTime graphics importer, using the QuickTime function GraphicsImportCreateCGImage. See QuickTime Framework Reference for details. Moving Data Between Quartz 2D and Core Image in Mac OS X describes how you can provide images to Core Image, which is a framework that supports image processing.\n\nSimilarly, in iOS, Quartz 2D works with all available graphics and animation technologies, such as Core Animation, OpenGL ES, and the UIKit classes.\n\nThe Page\n\nQuartz 2D uses the painter’s model for its imaging. In the painter’s model, each successive drawing operation applies a layer of “paint” to an output “canvas,” often called a page. The paint on the page can be modified by overlaying more paint through additional drawing operations. An object drawn on the page cannot be modified except by overlaying more paint. This model allows you to construct extremely sophisticated images from a small number of powerful primitives.\n\nFigure 1-1 shows how the painter’s model works. To get the image in the top part of the figure, the shape on the left was drawn first followed by the solid shape. The solid shape overlays the first shape, obscuring all but the perimeter of the first shape. The shapes are drawn in the opposite order in the bottom of the figure, with the solid shape drawn first. As you can see, in the painter’s model the drawing order is important.\n\nFigure 1-1  The painter’s model\n\nThe page may be a real sheet of paper (if the output device is a printer); it may be a virtual sheet of paper (if the output device is a PDF file); it may even be a bitmap image. The exact nature of the page depends on the particular graphics context you use.\n\nDrawing Destinations: The Graphics Context\n\nA graphics context is an opaque data type (CGContextRef) that encapsulates the information Quartz uses to draw images to an output device, such as a PDF file, a bitmap, or a window on a display. The information inside a graphics context includes graphics drawing parameters and a device-specific representation of the paint on the page. All objects in Quartz are drawn to, or contained by, a graphics context.\n\nYou can think of a graphics context as a drawing destination, as shown in Figure 1-2. When you draw with Quartz, all device-specific characteristics are contained within the specific type of graphics context you use. In other words, you can draw the same image to a different device simply by providing a different graphics context to the same sequence of Quartz drawing routines. You do not need to perform any device-specific calculations; Quartz does it for you.\n\nFigure 1-2  Quartz drawing destinations\n\nThese graphics contexts are available to your application:\n\nA bitmap graphics context allows you to paint RGB colors, CMYK colors, or grayscale into a bitmap. A bitmap is a rectangular array (or raster) of pixels, each pixel representing a point in an image. Bitmap images are also called sampled images. See Creating a Bitmap Graphics Context.\n\nA PDF graphics context allows you to create a PDF file. In a PDF file, your drawing is preserved as a sequence of commands. There are some significant differences between PDF files and bitmaps:\n\nPDF files, unlike bitmaps, may contain more than one page.\n\nWhen you draw a page from a PDF file on a different device, the resulting image is optimized for the display characteristics of that device.\n\nPDF files are resolution independent by nature—the size at which they are drawn can be increased or decreased infinitely without sacrificing image detail. The user-perceived quality of a bitmap image is tied to the resolution at which the bitmap is intended to be viewed.\n\nSee Creating a PDF Graphics Context.\n\nA window graphics context is a graphics context that you can use to draw into a window. Note that because Quartz 2D is a graphics engine and not a window management system, you use one of the application frameworks to obtain a graphics context for a window. See Creating a Window Graphics Context in Mac OS X for details.\n\nA layer context (CGLayerRef) is an offscreen drawing destination associated with another graphics context. It is designed for optimal performance when drawing the layer to the graphics context that created it. A layer context can be a much better choice for offscreen drawing than a bitmap graphics context. See Core Graphics Layer Drawing.\n\nWhen you want to print in Mac OS X, you send your content to a PostScript graphics context that is managed by the printing framework. See Obtaining a Graphics Context for Printing for more information.\n\nQuartz 2D Opaque Data Types\n\nThe Quartz 2D API defines a variety of opaque data types in addition to graphics contexts. Because the API is part of the Core Graphics framework, the data types and the routines that operate on them use the CG prefix.\n\nQuartz 2D creates objects from opaque data types that your application operates on to achieve a particular drawing output. Figure 1-3 shows the sorts of results you can achieve when you apply drawing operations to three of the objects provided by Quartz 2D. For example:\n\nYou can rotate and display a PDF page by creating a PDF page object, applying a rotation operation to the graphics context, and asking Quartz 2D to draw the page to a graphics context.\n\nYou can draw a pattern by creating a pattern object, defining the shape that makes up the pattern, and setting up Quartz 2D to use the pattern as paint when it draws to a graphics context.\n\nYou can fill an area with an axial or radial shading by creating a shading object, providing a function that determines the color at each point in the shading, and then asking Quartz 2D to use the shading as a fill color.\n\nFigure 1-3  Opaque data types are the basis of drawing primitives in Quartz 2D\n\nThe opaque data types available in Quartz 2D include the following:\n\nCGPathRef, used for vector graphics to create paths that you fill or stroke. See Paths.\n\nCGImageRef, used to represent bitmap images and bitmap image masks based on sample data that you supply. See Bitmap Images and Image Masks.\n\nCGLayerRef, used to represent a drawing layer that can be used for repeated drawing (such as for backgrounds or patterns) and for offscreen drawing. See Core Graphics Layer Drawing\n\nCGPatternRef, used for repeated drawing. See Patterns.\n\nCGShadingRef and CGGradientRef, used to paint gradients. See Gradients.\n\nCGFunctionRef, used to define callback functions that take an arbitrary number of floating-point arguments. You use this data type when you create gradients for a shading. See Gradients.\n\nCGColorRef and CGColorSpaceRef, used to inform Quartz how to interpret color. See Color and Color Spaces.\n\nCGImageSourceRef and CGImageDestinationRef, which you use to move data into and out of Quartz. See Data Management in Quartz 2D and Image I/O Programming Guide.\n\nCGFontRef, used to draw text. See Text.\n\nCGPDFDictionaryRef, CGPDFObjectRef, CGPDFPageRef, CGPDFStream, CGPDFStringRef, and CGPDFArrayRef, which provide access to PDF metadata. See PDF Document Creation, Viewing, and Transforming.\n\nCGPDFScannerRef and CGPDFContentStreamRef, which parse PDF metadata. See PDF Document Parsing.\n\nCGPSConverterRef, used to convert PostScript to PDF. It is not available in iOS. See PostScript Conversion.\n\nGraphics States\n\nQuartz modifies the results of drawing operations according to the parameters in the current graphics state. The graphics state contains parameters that would otherwise be taken as arguments to drawing routines. Routines that draw to a graphics context consult the graphics state to determine how to render their results. For example, when you call a function to set the fill color, you are modifying a value stored in the current graphics state. Other commonly used elements of the current graphics state include the line width, the current position, and the text font size.\n\nThe graphics context contains a stack of graphics states. When Quartz creates a graphics context, the stack is empty. When you save the graphics state, Quartz pushes a copy of the current graphics state onto the stack. When you restore the graphics state, Quartz pops the graphics state off the top of the stack. The popped state becomes the current graphics state.\n\nTo save the current graphics state, use the function CGContextSaveGState to push a copy of the current graphics state onto the stack. To restore a previously saved graphics state, use the function CGContextRestoreGState to replace the current graphics state with the graphics state that’s on top of the stack.\n\nNote that not all aspects of the current drawing environment are elements of the graphics state. For example, the current path is not considered part of the graphics state and is therefore not saved when you call the function CGContextSaveGState. The graphics state parameters that are saved when you call this function are listed in Table 1-1.\n\nTable 1-1  Parameters that are associated with the graphics state\n\nParameters\n\n\t\n\nDiscussed in this chapter\n\n\n\n\nCurrent transformation matrix (CTM)\n\n\t\n\nTransforms\n\n\n\n\nClipping area\n\n\t\n\nPaths\n\n\n\n\nLine: width, join, cap, dash, miter limit\n\n\t\n\nPaths\n\n\n\n\nAccuracy of curve estimation (flatness)\n\n\t\n\nPaths\n\n\n\n\nAnti-aliasing setting\n\n\t\n\nGraphics Contexts\n\n\n\n\nColor: fill and stroke settings\n\n\t\n\nColor and Color Spaces\n\n\n\n\nAlpha value (transparency)\n\n\t\n\nColor and Color Spaces\n\n\n\n\nRendering intent\n\n\t\n\nColor and Color Spaces\n\n\n\n\nColor space: fill and stroke settings\n\n\t\n\nColor and Color Spaces\n\n\n\n\nText: font, font size, character spacing, text drawing mode\n\n\t\n\nText\n\n\n\n\nBlend mode\n\n\t\n\nPaths and Bitmap Images and Image Masks\n\nQuartz 2D Coordinate Systems\n\nA coordinate system, shown in Figure 1-4, defines the range of locations used to express the location and sizes of objects to be drawn on the page. You specify the location and size of graphics in the user-space coordinate system, or, more simply, the user space. Coordinates are defined as floating-point values.\n\nFigure 1-4  The Quartz coordinate system\n\nBecause different devices have different underlying imaging capabilities, the locations and sizes of graphics must be defined in a device-independent manner. For example, a screen display device might be capable of displaying no more than 96 pixels per inch, while a printer might be capable of displaying 300 pixels per inch. If you define the coordinate system at the device level (in this example, either 96 pixels or 300 pixels), objects drawn in that space cannot be reproduced on other devices without visible distortion. They will appear too large or too small.\n\nQuartz accomplishes device independence with a separate coordinate system—user space—mapping it to the coordinate system of the output device—device space—using the current transformation matrix, or CTM. A matrix is a mathematical construct used to efficiently describe a set of related equations. The current transformation matrix is a particular type of matrix called an affine transform, which maps points from one coordinate space to another by applying translation, rotation, and scaling operations (calculations that move, rotate, and resize a coordinate system).\n\nThe current transformation matrix has a secondary purpose: It allows you to transform how objects are drawn. For example, to draw a box rotated by 45 degrees, you rotate the coordinate system of the page (the CTM) before you draw the box. Quartz draws to the output device using the rotated coordinate system.\n\nA point in user space is represented by a coordinate pair (x,y), where x represents the location along the horizontal axis (left and right) and y represents the vertical axis (up and down). The origin of the user coordinate space is the point (0,0). The origin is located at the lower-left corner of the page, as shown in Figure 1-4. In the default coordinate system for Quartz, the x-axis increases as it moves from the left toward the right of the page. The y-axis increases in value as it moves from the bottom toward the top of the page.\n\nSome technologies set up their graphics contexts using a different default coordinate system than the one used by Quartz. Relative to Quartz, such a coordinate system is a modified coordinate system and must be compensated for when performing some Quartz drawing operations. The most common modified coordinate system places the origin in the upper-left corner of the context and changes the y-axis to point towards the bottom of the page. A few places where you might see this specific coordinate system used are the following:\n\nIn Mac OS X, a subclass of NSView that overrides its isFlipped method to return YES.\n\nIn iOS, a drawing context returned by an UIView.\n\nIn iOS, a drawing context created by calling the UIGraphicsBeginImageContextWithOptions function.\n\nThe reason UIKit returns Quartz drawing contexts with modified coordinate systems is that UIKit uses a different default coordinate convention; it applies the transform to Quartz contexts it creates so that they match its conventions. If your application wants to use the same drawing routines to draw to both a UIView object and a PDF graphics context (which is created by Quartz and uses the default coordinate system), you need to apply a transform so that the PDF graphics context receives the same modified coordinate system. To do this, apply a transform that translates the origin to the upper-left corner of the PDF context and scales the y-coordinate by -1.\n\nUsing a scaling transform to negate the y-coordinate alters some conventions in Quartz drawing. For example, if you call CGContextDrawImage to draw an image into the context, the image is modified by the transform when it is drawn into the destination. Similarly, path drawing routines accept parameters that specify whether an arc is drawn in a clockwise or counterclockwise direction in the default coordinate system. If a coordinate system is modified, the result is also modified, as if the image were reflected in a mirror. In Figure 1-5, passing the same parameters into Quartz results in a clockwise arc in the default coordinate system and a counterclockwise arc after the y-coordinate is negated by the transform.\n\nFigure 1-5  Modifying the coordinate system creates a mirrored image.\n\nIt is up to your application to adjust any Quartz calls it makes to a context that has a transform applied to it. For example, if you want an image or PDF to draw correctly into a graphics context, your application may need to temporarily adjust the CTM of the graphics context. In iOS, if you use a UIImage object to wrap a CGImage object you create, you do not need to modify the CTM. The UIImage object automatically compensates for the modified coordinate system applied by UIKit.\n\nImportant: The above discussion is essential to understand if you plan to write applications that directly target Quartz on iOS, but it is not sufficient. On iOS 3.2 and later, when UIKit creates a drawing context for your application, it also makes additional changes to the context to match the default UIKIt conventions. In particular, patterns and shadows, which are not affected by the CTM, are adjusted separately so that their conventions match UIKit’s coordinate system. In this case, there is no equivalent mechanism to the CTM that your application can use to change a context created by Quartz to match the behavior for a context provided by UIKit; your application must recognize the what kind of context it is drawing into and adjust its behavior to match the expectations of the context.\n\nMemory Management: Object Ownership\n\nQuartz uses the Core Foundation memory management model, in which objects are reference counted. When created, Core Foundation objects start out with a reference count of 1. You can increment the reference count by calling a function to retain the object, and decrement the reference count by calling a function to release the object. When the reference count is decremented to 0, the object is freed. This model allows objects to safely share references to other objects.\n\nThere are a few simple rules to keep in mind:\n\nIf you create or copy an object, you own it, and therefore you must release it. That is, in general, if you obtain an object from a function with the words “Create” or “Copy” in its name, you must release the object when you’re done with it. Otherwise, a memory leak results.\n\nIf you obtain an object from a function that does not contain the words “Create” or “Copy” in its name, you do not own a reference to the object, and you must not release it. The object will be released by its owner at some point in the future.\n\nIf you do not own an object and you need to keep it around, you must retain it and release it when you’re done with it. You use the Quartz 2D functions specific to an object to retain and release that object. For example, if you receive a reference to a CGColorspace object, you use the functions CGColorSpaceRetain and CGColorSpaceRelease to retain and release the object as needed. You can also use the Core Foundation functions CFRetain and CFRelease, but you must be careful not to pass NULL to these functions.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Core Image Kernel Language",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Reference/CIKernelLangRef/ci_gslang_ext.html#//apple_ref/doc/uid/TP40004397-CH206-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Kernel Language Reference\nTable of Contents\nIntroduction\nCore Image Kernel Language\nRevision History\nNext\nPrevious\nCore Image Kernel Language\n\nThe following sections list the symbols provided by the Core Image Kernel Language:\n\nFunctions\n\nData Types\n\nKeywords\n\nYou can use these symbols together with any of the OpenGL Shading Language routines that Core Image supports. See Unsupported Items for those you can’t use.\n\nFunctions\n\nThis section describes the following functions:\n\ncompare\n\ncos_\n\ncossin\n\ncossin_\n\ndestCoord\n\npremultiply\n\nsample\n\nsamplerCoord\n\nsamplerExtent\n\nsamplerOrigin\n\nsamplerSize\n\nsamplerTransform\n\nsin_\n\nsincos\n\nsincos_\n\ntan_\n\nunpremultiply\n\ncompare\ngenType compare (genType x, genType y, genType z)\n\nFor each component, returns x < 0 ? y : z. Note that genType is a placeholder for an arbitrary vector type.\n\ncos_\ngenType cos_ (genType x)\n\nSimilar to cos (x) except that x must be in the [–pi, pi] range. Note that genType is a placeholder for an arbitrary vector type.\n\ncossin\nvec2 cossin (float x)\n\nReturns vec2 (cos (x), sin (x)).\n\ncossin_\nvec2 cossin_ (float x)\n\nReturns vec2 (cos (x), sin (x)). This function expects x to be in the [–pi, pi] range.\n\ndestCoord\nvarying vec2 destCoord ()\n\nReturns the position, in working space coordinates, of the pixel currently being computed. The destination space refers to the coordinate space of the image you are rendering.\n\npremultiply\nvec4 premultiply (vec4 color)\n\nMultiplies the red, green, and blue components of the color parameter by its alpha component.\n\nsample\nvec4 sample (uniform sampler src, vec2 point)\n\nReturns the pixel value produced from sampler src at the position point, where point is specified in sampler space.\n\nsamplerCoord\nvarying vec2 samplerCoord (uniform sampler src)\n\nReturns the position, in sampler space, of the sampler src that is associated with the current output pixel (that is, after any transformation matrix associated with src is applied). The sample space refers to the coordinate space of that you are texturing from.\n\nNote that if your source data is tiled, the sample coordinate will have an offset (dx/dy). You can convert a destination location to the sampler location using the samplerTransform function.\n\nsamplerExtent\nuniform vec4 samplerExtent (uniform sampler src)\n\nReturns the extent of the sampler in world coordinates, as a four-element vector [x, y, width, height].\n\nsamplerOrigin\nuniform vec2 samplerOrigin (uniform sampler src)\n\nEquivalent to samplerExtent (src).xy.\n\nsamplerSize\nuniform vec2 samplerSize (uniform sampler src)\n\nEquivalent to samplerExtent (src).zw.\n\nsamplerTransform\nvec2 samplerTransform (uniform sampler src, vec2 point)\n\nReturns the position in the coordinate space of the source (the first argument) that is associated with the position defined in working-space coordinates (the second argument). (Keep in mind that the working space coordinates reflect any transformations that you applied to the working space.)\n\nFor example, if you are modifying a pixel in the working space, and you need to retrieve the pixels that surround this pixel in the original image, you would make calls similar to the following, where d is the location of the pixel you are modifying in the working space, and image is the image source for the pixels.\n\nsamplerTransform(image, d + vec2(-1.0,-1.0));\n\n\nsamplerTransform(image, d + vec2(+1.0,-1.0));\n\n\nsamplerTransform(image, d + vec2(-1.0,+1.0));\n\n\nsamplerTransform(image, d + vec2(+1.0,+1.0));\nsin_\ngenType sin_ (genType x)\n\nSimilar to sin (x) except that x must be in the [–pi, pi] range. Note that genType is a placeholder for an arbitrary vector type.\n\nsincos\nvec2 sincos (float x)\n\nReturns vec2 (sin (x), cos (x)).\n\nsincos_\nvec2 sincos_ (float x)\n\nReturns vec2 (sin (x), cos (x)). This function expects x to be in the [–pi, pi] range.\n\ntan_\ngenType tan_ (genType x)\n\nSimilar to tan (x) except that x must be in the [–pi, pi] range. Note that genType is a placeholder for an arbitrary vector type.\n\nunpremultiply\nvec4 unpremultiply (vec4 color)\n\nIf the alpha component of the color parameter is greater than 0, divides the red, green and blue components by alpha. If alpha is 0, this function returns color.\n\nData Types\nsampler\n\nSpecifies a sampler passed in from CISampler that is used to get samples from data.\n\n__color\n\nSpecifies a type for kernel parameters that need to be color matched to the current CIContext working color space.\n\n__table\n\nSpecifies a flag for a sampler that fetches values from a lookup table.\n\nThe __table flag must precede the sampler type. The flag ensures that Core Image does not sample the table values using world coordinates.\n\nFor example, to use a lookup table sampler in a kernel named shadedmaterial, the kernel declaration would be:\n\nkernel vec4 shadedmaterial(sampler heightfield, __table sampler envmap, float surfaceScale, vec2 envscaling)\n\nUsing the __table flag prevents the envmap sampler values from being transformed, even if the shaded material kernel gets inserted into a filter chain with an affine transform. If you don’t tag the sampler this way and you chain the shaded material filter to an affine transform for rotation, then looking up values in the environment map results in getting rotated values, which is not correct because the lookup table is simply a data collection.\n\nKeywords\nkernel\n\nSpecifies a kernel routine. Kernel routines are extracted and compiled by the CIKernel class. A kernel encapsulates the computation required to compute a single pixel in the output image.\n\nEach kernel is tagged by the kernel keyword in its return type. The underlying return type of the kernel must be vec4 . Core Image requires this type in order to return the output pixel for the input pixel currently being evaluated.\n\nAll parameters to the kernel are implicitly marked uniform. Parameters marked out and inout are not allowed.\n\nYou can pass the following types to a kernel routine:\n\nsampler: Requires a CISampler object when applied.\n\n__table: A qualifier for a sampler type.\n\nfloat, vec2, vec3, vec4: Requires an NSNumber or CIVector.\n\n__color: A color that will be matched to the CIContext working color space when passed into the program. It requires a CIColor object when applied. To the kernel program it appears to be a vec4 type in premultiplied RGBA format.\n\nUnsupported Items\n\nCore Image does not support the OpenGL Shading Language source code preprocessor. In addition, the following are not implemented:\n\nData types: mat2, mat3, mat4, struct, arrays\n\nStatements: continue, break, discard. Other flow control statements (if, for, while, do while) are supported only when the loop condition can be inferred at the time the code compiles.\n\nExpression operators: % << >> | & ^ || && ^^ ~\n\nBuilt-in functions: ftransform, matrixCompMult, dfdx, dfdy, fwidth, noise1, noise2, noise3, noise4, refract\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-01-12\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Document Revision History",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Reference/CIKernelLangRef/RevisionHistory.html#//apple_ref/doc/uid/TP40004397-CH2-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Kernel Language Reference\nTable of Contents\nIntroduction\nCore Image Kernel Language\nRevision History\nPrevious\nDocument Revision History\n\nThis table describes the changes to Core Image Kernel Language Reference.\n\nDate\tNotes\n2015-01-12\t\n\nUpdated to reflect the availability of custom filter kernels for Core Image in iOS 8 and later.\n\n\n2012-09-19\t\n\nAdded an example on using the samplerTransform function.\n\n\n2008-06-09\t\n\nUpdated for OS X v10.5.\n\n\n \t\n\nAdded information about the destination and sampler coordinate spaces.\n\n\n2006-06-28\t\n\nNew document that describes the symbols for writing image-processing kernels.\n\n\n \t\n\nThe content in this book was published previously as an appendix in Core Image Programming Guide.\n\n\n\nPrevious\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-01-12\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Introduction",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/Introduction/Introduction.html#//apple_ref/doc/uid/TP30001066",
    "html": "Documentation Archive\nDeveloper\nSearch\nQuartz 2D Programming Guide\nTable of Contents\nIntroduction\nOverview of Quartz 2D\nGraphics Contexts\nPaths\nColor and Color Spaces\nTransforms\nPatterns\nShadows\nGradients\nTransparency Layers\nData Management in Quartz 2D\nBitmap Images and Image Masks\nCore Graphics Layer Drawing\nPDF Document Creation, Viewing, and Transforming\nPDF Document Parsing\nPostScript Conversion\nText\nGlossary\nRevision History\nNext\nIntroduction\n\nCore Graphics, also known as Quartz 2D, is an advanced, two-dimensional drawing engine available for iOS, tvOS and macOS application development. Quartz 2D provides low-level, lightweight 2D rendering with unmatched output fidelity regardless of display or printing device. Quartz 2D is resolution- and device-independent.\n\nThe Quartz 2D API is easy to use and provides access to powerful features such as transparency layers, path-based drawing, offscreen rendering, advanced color management, anti-aliased rendering, and PDF document creation, display, and parsing.\n\nWho Should Read This Document?\n\nThis document is intended for developers who need to perform any of the following tasks:\n\nDraw graphics\n\nProvide graphics editing capabilities in an application\n\nCreate or display bitmap images\n\nWork with PDF documents\n\nOrganization of This Document\n\nThis document is organized into the following chapters:\n\nOverview of Quartz 2D describes the page, drawing destinations, Quartz opaque data types, graphics states, coordinates, and memory management, and it takes a look at how Quartz works “under the hood.”\n\nGraphics Contexts describes the kinds of drawing destinations and provides step-by-step instructions for creating all flavors of graphics contexts.\n\nPaths discusses the basic elements that make up paths, shows how to create and paint them, shows how to set up a clipping area, and explains how blend modes affect painting.\n\nColor and Color Spaces discusses color values and using alpha values for transparency, and it describes how to create a color space, set colors, create color objects, and set rendering intent.\n\nTransforms describes the current transformation matrix and explains how to modify it, shows how to set up affine transforms, shows how to convert between user and device space, and provides background information on the mathematical operations that Quartz performs.\n\nPatterns defines what a pattern and its parts are, tells how Quartz renders them, and shows how to create colored and stenciled patterns.\n\nShadows describes what shadows are, explains how they work, and shows how to paint with them.\n\nGradients discusses axial and radial gradients and shows how to create and use CGShading and CGGradient objects.\n\nTransparency Layers gives examples of what transparency layers look like, discusses how they work, and provides step-by-step instructions for implementing them.\n\nData Management in Quartz 2D discusses how to move data into and out of Quartz.\n\nBitmap Images and Image Masks describes what makes up a bitmap image definition and shows how to use a bitmap image as a Quartz drawing primitive. It also describes masking techniques you can use on images and shows the various effects you can achieve by using blend modes when drawing images.\n\nCore Graphics Layer Drawing describes how to create and use drawing layers to achieve high-performance patterned drawing or to draw offscreen.\n\nPDF Document Creation, Viewing, and Transforming shows how to open and view PDF documents, apply transforms to them, create a PDF file, access PDF metadata, add links, and add security features (such as password protection).\n\nPDF Document Parsing describes how to use CGPDFScanner and CGPDFContentStream objects to parse and inspect PDF documents.\n\nPostScript Conversion gives an overview of the functions you can use in Mac OS X to convert a PostScript file to a PDF document. These functions are not available in iOS.\n\nText describes Quartz 2D low-level support for text and glyphs, and alternatives that provide higher-level and Unicode text support. It also discusses how to copy font variations.\n\nGlossary defines the terms used in this guide.\n\nSee Also\n\nThese items are essential reading for anyone using Quartz 2D:\n\nCore Graphics Framework Reference provides a complete reference for the Quartz 2D application programming interface.\n\nColor Management Overview is a brief introduction to the principles of color perception, color spaces, and color management systems.\n\nMailing lists. Join the quartz-dev mailing list to discuss problems using Quartz 2D.\n\nNext\n\n\n\n\n\nCopyright © 2001, 2017 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2017-03-21\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Introduction",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Reference/CIKernelLangRef/Introduction/Introduction.html#//apple_ref/doc/uid/TP40004397",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Kernel Language Reference\nTable of Contents\nIntroduction\nCore Image Kernel Language\nRevision History\nNext\nIntroduction\n\nThe Core Image kernel language defines functions, data types, and keywords that you can use to specify image processing operations for custom Core Image filters that you write. You can also use a subset of the OpenGL Shading Language (glslang).\n\nThis document defines the symbols in the Core Image kernel language and lists the symbols in the OpenGL Shading Language that are unsupported in Core Image filters.\n\nAny developer who wants to use the Core Image API to write custom image processing filters should read this document. Before reading this document you should be familiar with the documents listed in the See Also section.\n\nOrganization of This Document\n\nCore Image Kernel Language defines the functions, data types, and keywords available for writing image processing routines and lists symbols in the OpenGL Shading Language that are not supported.\n\nSee Also\n\nCore Image Reference Collection defines the classes used to define and access image processing filters.\n\nCore Image Programming Guide describes how to write custom image processing filters and package them as image units. It also provides several examples of kernel routines.\n\nOpenGL Shading Language, available from the OpenGL website, provides a reference to glslang.\n\nNext\n\n\n\n\n\nCopyright © 2015 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2015-01-12\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Document Revision History",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_revhistory/ci_revhistory.html#//apple_ref/doc/uid/TP30001185-CH99-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Programming Guide\nTable of Contents\nIntroduction\nProcessing Images\nDetecting Faces in an Image\nAuto Enhancing Images\nQuerying the System for Filters\nSubclassing CIFilter: Recipes for Custom Effects\nGetting the Best Performance\nUsing Feedback to Process Images\nWhat You Need to Know Before Writing a Custom Filter\nCreating Custom Filters\nPackaging and Loading Image Units\nRevision History\nPrevious\nDocument Revision History\n\nThis table describes the changes to Core Image Programming Guide.\n\nDate\tNotes\n2016-09-13\t\n\nUpdated for iOS 10.0, tvOS 10.0, and OS X v10.12.\n\n\n \t\n\nRewrote the Processing Images chapter to reflect modern Core Image best practices.\n\n\n2016-03-21\t\n\nCorrected typos and references to features that were formerly for OS X only and are now available in iOS.\n\n\n2015-10-21\t\n\nCorrected errors in example code listings.\n\n\n2015-09-16\t\n\nFixed errors in code examples.\n\n\n2014-11-18\t\n\nFixed errors in code examples.\n\n\n2013-10-22\t\n\nCorrected several minor errors.\n\n\n \t\n\nUpdated code examples to use number, array, and dictionary literals.\n\n\n \t\n\nUpdated code examples to use standard key constants where available (for example, kCIInputImageKey instead of @\"inputImage\").\n\n\n2013-01-28\t\n\nCorrected code listing in \"The color cube in code\".\n\n\n \t\n\nCorrected code line c[3] = rgb[3] * alpha in Listing 5-3 to read simply c[3] = alpha.\n\n\n2012-09-19\t\n\nUpdated for iOS 6.0 and OS X v10.7.\n\n\n \t\n\nUpdated the introduction chapter.\n\n\n \t\n\nAdded chapters on face detection and auto enhancement filters.\n\n\n \t\n\nAdded information on performance best practices.\n\n\n \t\n\nAdded recipes for subclassing CIFilter to get custom effects.\n\n\n \t\n\nMoved information on using the CIImageAccumulator class to its own chapter and revised the content to bring it up to date.\n\n\n \t\n\nAdded clarification on naming input parameters for custom filters.\n\n\n \t\n\nAdded information on thread safety.\n\n\n \t\n\nFixed broken link to external article.\n\n\n2011-10-12\t\n\nIncluded for Core Image on iOS 5.\n\n\n2008-06-09\t\n\nAdded details on coordinate spaces.\n\n\n \t\n\nAdded information to Building a Dictionary of Filters.\n\n\n2007-10-31\t\n\nUpdated for OS X v10.5.\n\n\n \t\n\nAdded a note to Building a Dictionary of Filters.\n\n\n \t\n\nAdd information about CIFilter Image Kit Additions.\n\n\n \t\n\nAdded information about support for RAW images.\n\n\n \t\n\nUpdated links to references and added links in several places to Image Unit Tutorial.\n\n\n \t\n\nRevised Executable and Nonexecutable Filters.\n\n\n \t\n\nRevised Write an Output Image Method.\n\n\n2007-05-29\t\n\nAdded link to Cocoa memory management.\n\n\n \t\n\nRemoved section on memory management. Instead, see Advanced Memory Management Programming Guide.\n\n\n \t\n\nAdded a note to Building a Dictionary of Filters.\n\n\n \t\n\nFixed a typographical error.\n\n\n2007-01-08\t\n\nFixed minor technical and typographical errors.\n\n\n2006-09-05\t\n\nFixed minor technical problem.\n\n\n \t\n\nCorrected the angular values for colors in Creating a CIFilter Object and Setting Values.\n\n\n2006-06-28\t\n\nReorganized content and added task information.\n\n\n \t\n\nRemoved the appendix “Core Image Filters” and created a new document named Core Image Filter Reference.\n\n\n \t\n\nRemoved the appendix “Core Image Kernel Language” and created a new document named Core Image Kernel Language Reference.\n\n\n \t\n\nAdded Kernel Routine Examples to Creating Custom Filters and changed some of the short variable names to long ones in the code listings. Added information to Computing a Hole Distortion to clarify the purpose of the example.\n\n\n \t\n\nMoved information about packaging filters as image units into its own chapter. Added additional information about the files needed in the project and where to install the image unit. See Before You Get Started, Build and Test the Image Unit, and See Also.\n\n\n \t\n\nUpdated the book introduction and some of the chapter introductions to reflect the chapter and appendix changes.\n\n\n \t\n\nRevised Creating Custom Filters. In particular, see Write a Custom Attributes Method and Register the Filter.\n\n\n \t\n\nAdded additional information on how to create nonexecutable filters. See Writing Nonexecutable Filters.\n\n\n \t\n\nRevised information on creating a CIContext object from an OpenGL graphics context.\n\n\n \t\n\nFixed formatting and, in online versions of this document, provided hyperlinks to the image creation functions in “Methods used to create a CIImage object from existing image sources.”\n\n\n \t\n\nAdded hyperlinks to most symbols and to sample code available in the ADC Reference Library.\n\n\n \t\n\nNumerous small formatting and grammatical changes throughout.\n\n\n2005-12-06\t\n\nMade minor corrections to a few filter parameters. Added information on the CIFilterBrowser widget.\n\n\n2005-11-09\t\n\nFixed several typographical errors and a broken hyperlink.\n\n\n2005-08-11\t\n\nUpdated a figure in the PDF version of this document.\n\n\n2005-07-07\t\n\nCorrected typographical errors.\n\n\n2005-04-29\t\n\nUpdated for public release of OS X v10.4. First public version.\n\n\n \t\n\nChanged the title from Image Processing With Core Image to make it more consistent with the titles of similar documentation.\n\n\n \t\n\nCompletely revised Querying the System for Filters to provide more in-depth information about how Core Image works.\n\n\n \t\n\nSplit the chapter titled Core Image Tasks into two chapters: Processing Images and Creating Custom Filters. Completely updated the content in each to reflect additions to the API and to provide more in-depth information.\n\n\n \t\n\nAdded Using Transition Effects.\n\n\n \t\n\nAdded Using Feedback to Process Images.\n\n\n \t\n\nAdded Applying a Filter to Video.\n\n\n \t\n\nAdded Expressing Image Processing Operations in Core Image.\n\n\n \t\n\nAdded Use Quartz Composer to Test the Kernel Routine.\n\n\n \t\n\nProvided more information on the region of interest and ROI functions. See The Region of Interest and Supplying an ROI Function.\n\n\n \t\n\nProvided more information on executable and nonexecutable filters. See Executable and Nonexecutable Filters and Writing Nonexecutable Filters.\n\n\n \t\n\nUpdated the appendix “Core Image Filters to include recently-added built-in Core Image filters. Also replaced many of the figures to provide a better idea of the result produced by a filter.\n\n\n \t\n\nUpdated the appendix “Core Image Kernel Language” to reflect changes in the kernel language. Added explanations for the kernel routine examples.\n\n\n2004-06-29\t\n\nNew seed draft that describes an image processing technology, built into OS X v10.4, that provides access to built-in image filters for both video and still images and support for custom filters and real-time processing.\n\n\n\nPrevious\n\n\n\n\n\nCopyright © 2004, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Packaging and Loading Image Units",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_image_units/ci_image_units.html#//apple_ref/doc/uid/TP30001185-CH7-SW12",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Programming Guide\nTable of Contents\nIntroduction\nProcessing Images\nDetecting Faces in an Image\nAuto Enhancing Images\nQuerying the System for Filters\nSubclassing CIFilter: Recipes for Custom Effects\nGetting the Best Performance\nUsing Feedback to Process Images\nWhat You Need to Know Before Writing a Custom Filter\nCreating Custom Filters\nPackaging and Loading Image Units\nRevision History\nNext\nPrevious\nPackaging and Loading Image Units\n\nAn image unit represents the plug-in architecture for Core Image filters. Image units use the NSBundle class as the packaging mechanism to allow you to make the filters that you create available to other apps. An image unit can contain filters that are executable or nonexecutable. (See Executable and Nonexecutable Filters for details.)\n\nTo create an image unit from a custom filter, you must perform the following tasks:\n\nWrite the filter by following the instructions in Creating a Custom Filter.\n\nCreate an Image Unit Project in Xcode.\n\nAdd Your Filter Files to the Project.\n\nCustomize the Load Method.\n\nModify the Description Property List.\n\nBuild and Test the Image Unit\n\nAfter reading this chapter, you may also want to\n\nRead Image Unit Tutorial for in-depth information on writing kernels and creating image units.\n\nVisit Apple’s Image Units Licensing and Trademarks webpage to find out how to validate image units and obtain the Image Unit logo.\n\nBefore You Get Started\n\nDownload the CIDemoImageUnit sample. When you create an image unit, you should have similar files. This image unit contains one filter, FunHouseMirror. Each filter in an image unit typically has three files: an interface file for the filter class, the associated implementation file, and a kernel file. As you can see in sample code project, this is true for the FunHouseMirror filter: FunHouseMirrorFilter.h, FunHouseMirrorFilter.m, and funHouseMirror.cikernel.\n\nEach image unit should also have interface and implementation files for the CIPlugInRegistration protocol. In the figure, see MyPlugInLoader.h and MyPlugInLoader.m. The other important file that you’ll need to modify is the Description.plist file.\n\nNow that you know a bit about the files in an image unit project, you’re ready to create one.\n\nCreate an Image Unit Project in Xcode\n\nXcode provides a template for creating image units. After you create an image unit project, you’ll have most of the files you need to get started and the project will be linked to the appropriate frameworks.\n\nTo create an image unit project in Xcode\n\nThe MyImageUnitKernelFilter.cikernel file provided in the image unit project is a sample kernel file. If you’ve already created a filter you won’t need this file, so you can delete it. You’ll add your own to the project in just a moment.\n\nCustomize the Load Method\n\nOpen the file that implements the CIPlugInRegistration protocol. In it you’ll find a load method, as shown in Listing 10-1. You have the option to add code to this method to perform any initialization that’s needed, such as a registration check. The method returns true if the filter is loaded successfully. If you don’t need any custom initialization, you can leave the load method as it is.\n\nListing 10-1  The load method provided by the image unit template\n\n-(BOOL)load:(void*)host\n\n\n{\n\n\n    // Custom image unit initialization code goes here\n\n\n    return YES;\n\n\n}\n\nIf you want, you can write an unload method to perform any cleanup tasks that might be required by your filter.\n\nAdd Your Filter Files to the Project\n\nAdd the filter files you created previously to the image unit project. Recall that you’ll need the interface and implementation files for each filter and the associated kernel file. If you haven’t written the filter yet, see Creating Custom Filters.\n\nKeep in mind that you can package more than one filter in an image unit, and you can have as many kernel files as needed for your filters. Just make sure that you include all of the filter and kernel files that you want to package.\n\nModify the Description Property List\n\nFor executable filters, only the version number, filter class, and filter name are read from the Description.plist file. You provide a list of attributes for the filter in your code (see Write a Custom Attributes Method). You need to check the Description.plist file provided in the image unit template to make sure the filter name is correct and to enter the version number.\n\nFor CPU–nonexecutable filters, the image unit host reads the Description.plist file to obtain information about the filter attributes listed inTable 10-1. You need to modify the Description.plist file so it contains the appropriate information. (For information about filter keys, see also Core Image Reference Collection.)\n\nTable 10-1  Keys in the filter description property list\n\nKey\n\n\t\n\nAssociated values\n\n\n\n\nCIPlugInFilterList\n\n\t\n\nA dictionary of filter dictionaries. If this key is present, it indicates that there is at least one Core Image filter defined in the image unit.\n\n\n\n\nCIFilterDisplayName\n\n\t\n\nThe localized filter name available in the Description.strings file.\n\n\n\n\nCIFilterClass\n\n\t\n\nThe class name in the binary that contains the filter implementation, if available.\n\n\n\n\nCIKernelFile\n\n\t\n\nThe filename of the filter kernel in the bundle, if available. Use this key to define a nonexecutable filter.\n\n\n\n\nCIFilterAttributes\n\n\t\n\nA dictionary of attributes that describe the filter. This is the same as the attributes dictionary that you provided when you wrote the filter.\n\n\n\n\nCIInputs\n\n\t\n\nAn array of input keys and associated attributes. The input keys must be in the same order as the parameters of the kernel function. Each attribute must contain its parameter class (see Table 10-2) and name.\n\n\n\n\nCIOutputs\n\n\t\n\nReserved for future use.\n\n\n\n\nCIHasCustomInterface\n\n\t\n\nNone. Use this key to specify that the filter has a custom user interface. The host provides a view for the user interface.\n\n\n\n\nCIPlugInVersion\n\n\t\n\nThe version of the CIPlugIn architecture, which is 1.0.\n\nTable 10-2 lists the input parameter classes and the value associated with each class. For a nonexecutable filter, you provide the parameter class for each input and output parameter.\n\nTable 10-2  Input parameter classes and expected values\n\nInput parameter class\n\n\t\n\nAssociated value\n\n\n\n\nCIColor\n\n\t\n\nA string that specifies a color.\n\n\n\n\nCIVector\n\n\t\n\nA string that specifies a vector. See vectorWithString:.\n\n\n\n\nCIImage\n\n\t\n\nAn NSString object that describes either the relative path of the image to the bundle or the absolute path of the image.\n\n\n\n\nAll scalar types\n\n\t\n\nAn NSNumber value.\n\nBuild and Test the Image Unit\n\nBefore you start creating an image unit, you should test the kernel code to make sure that it works properly. (See Use Quartz Composer to Test the Kernel Routine.) After you successfully build the image unit, you’ll want to copy it to the following directories:\n\n/Library/Graphics/Image Units\n\n~/Library/Graphics/Image Units\n\nThen, you should try loading the image unit from an app and using the filter (or filters) that are packaged in the unit. See Loading Image Units, Querying the System for Filters, and Processing Images.\n\nLoading Image Units\n\nThe built-in filters supplied by Apple are loaded automatically. The only filters you need to load are third-party filters packaged as image units. An image unit, which is simply a bundle, can contain one or more image processing filters. If the image unit is installed in one of the locations discussed in Build and Test the Image Unit, then it can be used by any app\\ that calls one of the load methods provided by the CIPlugin class and shown in Table 10-3. You need to load image units only once. For example, to load all globally installed image units, you could add the following line of code to an initialization routine in your app.\n\n  [CIPlugIn loadAllPlugIns];\n\nAfter calling the load method, you proceed the same as you would for using any of the image processing filters provided by Apple. Follow the instructions in the rest of this chapter.\n\nTable 10-3  Methods used to load image units\n\nMethod\n\n\t\n\nComment\n\n\n\n\nloadAllPlugIns\n\n\t\n\nScans image unit directories (/Library/Graphics/Image Units and ~/Library/Graphics/Image Units) for files that have the .plugin extension and then loads the image unit.\n\n\n\n\nloadNonExecutablePlugIns\n\n\t\n\nScans image unit directories (/Library/Graphics/Image Units and ~/Library/Graphics/Image Units) for files that have the .plugin extension and then loads only the kernels of the image unit. That is, it loads only those files that have the .cikernel extension. This call does not execute any of the image unit code.\n\n\n\n\nloadPlugIn:allowNonExecutable:\n\n\t\n\nLoads the image unit at the location specified by the url argument. Pass true for the allowNonExecutable argument if you want to load only the kernels of the image unit without executing any of the image unit code.\n\nSee Also\n\nImage Unit Tutorial which provides step-by-step instructions for writing a variety of kernels and packaging them as image units.\n\nCIDemoImageUnit is a sample image unit Xcode project.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Creating Custom Filters",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_custom_filters/ci_custom_filters.html#//apple_ref/doc/uid/TP30001185-CH6-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Programming Guide\nTable of Contents\nIntroduction\nProcessing Images\nDetecting Faces in an Image\nAuto Enhancing Images\nQuerying the System for Filters\nSubclassing CIFilter: Recipes for Custom Effects\nGetting the Best Performance\nUsing Feedback to Process Images\nWhat You Need to Know Before Writing a Custom Filter\nCreating Custom Filters\nPackaging and Loading Image Units\nRevision History\nNext\nPrevious\nCreating Custom Filters\n\nIf the filters provided by Core Image don’t provide the functionality you need, you can write your own filter. You can include a filter as part of an application project, or (in macOS only) you can package one or more filters as a standalone image unit. Image units use the NSBundle class and allow an app to host external plug-in filters.\n\nThe following sections provide detailed information on how to create and use custom filters and image units:\n\nExpressing Image Processing Operations in Core Image\n\nCreating a Custom Filter describes the methods that you need to implement and other filter requirements.\n\nUsing Your Own Custom Filter tells what’s need for you to use the filter in your own app. (If you want to package it as a standalone image unit, see Packaging and Loading Image Units.)\n\nSupplying an ROI Function provides information about the region of interest and when you must supply a method to calculate this region. (It’s not always needed.)\n\nWriting Nonexecutable Filters is a must-read section for anyone who plans to write a filter that is CPU nonexecutable, as it lists the requirements for such filters. An image unit can contain both kinds of filters. CPU nonexecutable filters are secure because they cannot harbor viruses and Trojan horses. Filter clients who are security conscious may want to use only those filters that are CPU nonexecutable.\n\nKernel Routine Examples provides kernel routines for three sample filters: brightening, multiply, and hole distortion.\n\nExpressing Image Processing Operations in Core Image\n\nCore Image works such that a kernel (that is, a per-pixel processing routine) is written as a computation where an output pixel is expressed using an inverse mapping back to the corresponding pixels of the kernel’s input images. Although you can express most pixel computations this way—some more naturally than others—there are some image processing operations for which this is difficult, if not impossible. Before you write a filter, you may want to consider whether the image processing operation can be expressed in Core Image. For example, computing a histogram is difficult to describe as an inverse mapping to the source image.\n\nCreating a Custom Filter\n\nThis section shows how to create a Core Image filter that has an Objective-C portion and a kernel portion. By following the steps in this section, you’ll create a filter that is CPU executable. You can package this filter, along with other filters if you’d like, as an image unit by following the instructions in Packaging and Loading Image Units. Or, you can simply use the filter from within your own app. See Using Your Own Custom Filter for details.\n\nCore Image provides three types of kernel-based filters: color filters, warp filters, and general filters. A general filter includes a GPU kernel routine that can modify pixel colors and pixel locations. However, if you’re designing a filter that modifies only pixel colors, or that changes image geometry without otherwise modifying pixels, creating a color or warp filter lets Core Image provide better filter performance across the wide variety of iOS and Mac hardware. For details, see the reference documentation for the CIColorKernel and CIWarpKernel classes.\n\nThe general filter in this section assumes that the region of interest (ROI) and the domain of definition coincide. If you want to write a filter for which this assumption isn’t true, make sure you also read Supplying an ROI Function. Before you create your own custom filter, make sure you understand Core Image coordinate spaces. See Building a Dictionary of Filters.\n\nTo create a custom CPU executable filter, perform the following steps:\n\nWrite the Kernel Code\n\nUse Quartz Composer to Test the Kernel Routine\n\nDeclare an Interface for the Filter\n\nWrite an Init Method for the CIKernel Object\n\nWrite a Custom Attributes Method\n\nWrite an Output Image Method\n\nRegister the Filter\n\nWrite a Method to Create Instances of the Filter\n\nEach step is described in detail in the sections that follow using a haze removal filter as an example. The effect of the haze removal filter is to adjust the brightness and contrast of an image, and to apply sharpening to it. This filter is useful for correcting images taken through light fog or haze, which is typically the case when taking an image from an airplane. Figure 9-1 shows an image before and after processing with the haze removal filter. The app using the filter provides sliders that enable the user to adjust the filter’s input parameters.\n\nFigure 9-1  An image before and after processing with the haze removal filter\nWrite the Kernel Code\n\nThe code that performs per-pixel processing resides in a file with the .cikernel extension. You can include more than one kernel routine in this file. You can also include other routines if you want to make your code modular. You specify a kernel using a subset of OpenGL Shading Language (glslang) and the Core Image extensions to it. See Core Image Kernel Language Reference for information on allowable elements of the language.\n\nA kernel routine signature must return a vector (vec4) that contains the result of mapping the source pixel to a destination pixel. Core Image invokes a kernel routine once for each pixel. Keep in mind that your code can’t accumulate knowledge from pixel to pixel. A good strategy when writing your code is to move as much invariant calculation as possible from the actual kernel and place it in the Objective-C portion of the filter.\n\nListing 9-1 shows the kernel routine for a haze removal filter. A detailed explanation for each numbered line of code follows the listing. (There are examples of other pixel-processing routines in Kernel Routine Examples and in Image Unit Tutorial.)\n\nListing 9-1  A kernel routine for the haze removal filter\n\nkernel vec4 myHazeRemovalKernel(sampler src,             // 1\n\n\n                     __color color,\n\n\n                    float distance,\n\n\n                    float slope)\n\n\n{\n\n\n    vec4   t;\n\n\n    float  d;\n\n\n \n\n\n    d = destCoord().y * slope  +  distance;              // 2\n\n\n    t = unpremultiply(sample(src, samplerCoord(src)));   // 3\n\n\n    t = (t - d*color) / (1.0-d);                         // 4\n\n\n \n\n\n    return premultiply(t);                               // 5\n\n\n}\n\nHere’s what the code does:\n\nTakes four input parameters and returns a vector. When you declare the interface for the filter, you must make sure to declare the same number of input parameters as you specify in the kernel. The kernel must return a vec4 data type.\n\nCalculates a value based on the y-value of the destination coordinate and the slope and distance input parameters. The destCoord routine (provided by Core Image) returns the position, in working space coordinates, of the pixel currently being computed.\n\nGets the pixel value, in sampler space, of the sampler src that is associated with the current output pixel after any transformation matrix associated with the src is applied. Recall that Core Image uses color components with premultiplied alpha values. Before processing, you need to unpremultiply the color values you receive from the sampler.\n\nCalculates the output vector by applying the haze removal formula, which incorporates the slope and distance calculations and adjusts for color.\n\nReturns a vec4 vector, as required. The kernel performs a premultiplication operation before returning the result because Core Image uses color components with premultiplied alpha values.\n\nA few words about samplers and sample coordinate space: The samplers you set up for providing samples to custom kernels can contain any values necessary for the filter calculation, not just color values. For example, a sampler can provide values from numerical tables, vector fields in which the x and y values are represented by the red and green components respectively, height fields, and so forth. This means that you can store any vector-value field with up to four components in a sampler. To avoid confusion on the part of the filter client, it’s best to provide documentation that states when a vector is not used for color. When you use a sampler that doesn’t provide color, you can bypass the color correction that Core Image usually performs by providing a nil colorspace.\n\nUse Quartz Composer to Test the Kernel Routine\n\nQuartz Composer is an easy-to-use development tool that you can use to test kernel routines.\n\nTo download Quartz Composer\n\nQuartz Composer provides a patch—Core Image Filter—into which you can place your kernel routine.You simply open the Inspector for the Core Image Filter patch, and either paste or type your code into the text field, as shown in Figure 9-2.\n\nFigure 9-2  The haze removal kernel routine pasted into the Settings pane\n\nAfter you enter the code, the patch’s input ports are automatically created according to the prototype of the kernel function, as you can see in Figure 9-3. The patch always has a single output port, which represents the resulting image produced by the kernel.\n\nThe simple composition shown in Figure 9-3 imports an image file using the Image Importer patch, processes it through the kernel, then renders the result onscreen using the Billboard patch. Your kernel can use more than one image or, if it generates output, it might not require any input images.\n\nThe composition you build to test your kernel can be more complex than that shown in Figure 9-3. For example, you might want to chain your kernel routine with other built-in Core Image filters or with other kernel routines. Quartz Composer provides many other patches that you can use in the course of testing your kernel routine.\n\nFigure 9-3  A Quartz Composer composition that tests a kernel routine\nDeclare an Interface for the Filter\n\nThe .h file for the filter contains the interface that specifies the filter inputs, as shown in Listing 9-2. The haze removal kernel has four input parameters: a source, color, distance, and slope. The interface for the filter must also contain these input parameters. The input parameters must be in the same order as specified for the filter, and the data types must be compatible between the two.\n\nNote: Make sure to prefix the names of input parameters with input as shown in Listing 9-2.\n\nListing 9-2  Code that declares the interface for a haze removal filter\n\n@interface MyHazeFilter: CIFilter\n\n\n{\n\n\n    CIImage   *inputImage;\n\n\n    CIColor   *inputColor;\n\n\n    NSNumber  *inputDistance;\n\n\n    NSNumber  *inputSlope;\n\n\n}\n\n\n \n\n\n@end\nWrite an Init Method for the CIKernel Object\n\nThe implementation file for the filter contains a method that initializes a Core Image kernel object (CIKernel) with the kernel routine specified in the .cikernel file. A .cikernel file can contain more than one kernel routine. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 9-3  An init method that initializes the kernel\n\nstatic CIKernel *hazeRemovalKernel = nil;\n\n\n \n\n\n- (id)init\n\n\n{\n\n\n    if(hazeRemovalKernel == nil)                                         // 1\n\n\n    {\n\n\n        NSBundle    *bundle = [NSBundle bundleForClass: [self class]];   // 2\n\n\n        NSString    *code = [NSString stringWithContentsOfFile: [bundle\n\n\n                                pathForResource: @\"MyHazeRemoval\"\n\n\n                                ofType: @\"cikernel\"]];                   // 3\n\n\n        NSArray     *kernels = [CIKernel kernelsWithString: code];       // 4\n\n\n        hazeRemovalKernel = kernels[0];                                  // 5\n\n\n    }\n\n\n    return [super init];\n\n\n}\n\nHere’s what the code does:\n\nChecks whether the CIKernel object is already initialized.\n\nReturns the bundle that dynamically loads the CIFilter class.\n\nReturns a string created from the file name at the specified path, which in this case is the MyHazeRemoval.cikernel file.\n\nCreates a CIKernel object from the string specified by the code argument. Each routine in the .cikernel file that is marked as a kernel is returned in the kernels array. This example has only one kernel in the .cikernel file, so the array contains only one item.\n\nSets hazeRemovalKernel to the first kernel in the kernels array. If the .cikernel file contains more than one kernel, you would also initialize those kernels in this routine.\n\nWrite a Custom Attributes Method\n\nA customAttributes method allows clients of the filter to obtain the filter attributes such as the input parameters, default values, and minimum and maximum values. (See CIFilter Class Reference for a complete list of attributes.) A filter is not required to provide any information about an attribute other than its class, but a filter must behave in a reasonable manner if attributes are not present.\n\nTypically, these are the attributes that your customAttributes method would return:\n\nInput and output parameters\n\nAttribute class for each parameter you supply (mandatory)\n\nMinimum, maximum, and default values for each parameter (optional)\n\nOther information as appropriate, such as slider minimum and maximum values (optional)\n\nListing 9-4 shows the customAttributes method for the Haze filter. The input parameters inputDistance and inputSlope each have minimum, maximum, slider minimum, slider maximum, default and identity values set. The slider minimum and maximum values are used to set up the sliders shown in Figure 9-1. The inputColor parameter has a default value set.\n\nListing 9-4  The customAttributes method for the Haze filter\n\n- (NSDictionary *)customAttributes\n\n\n{\n\n\n    return @{\n\n\n        @\"inputDistance\" :  @{\n\n\n            kCIAttributeMin       : @0.0,\n\n\n            kCIAttributeMax       : @1.0,\n\n\n            kCIAttributeSliderMin : @0.0,\n\n\n            kCIAttributeSliderMax : @0.7,\n\n\n            kCIAttributeDefault   : @0.2,\n\n\n            kCIAttributeIdentity  : @0.0,\n\n\n            kCIAttributeType      : kCIAttributeTypeScalar\n\n\n            },\n\n\n        @\"inputSlope\" : @{\n\n\n            kCIAttributeSliderMin : @-0.01,\n\n\n            kCIAttributeSliderMax : @0.01,\n\n\n            kCIAttributeDefault   : @0.00,\n\n\n            kCIAttributeIdentity  : @0.00,\n\n\n            kCIAttributeType      : kCIAttributeTypeScalar\n\n\n            },\n\n\n         kCIInputColorKey : @{\n\n\n         kCIAttributeDefault : [CIColor colorWithRed:1.0\n\n\n                                               green:1.0\n\n\n                                                blue:1.0\n\n\n                                               alpha:1.0]\n\n\n           },\n\n\n   };\n\n\n}\nWrite an Output Image Method\n\nAn outputImage method creates a CISampler object for each input image (or image mask), creates a CIFilterShape object (if appropriate), and applies the kernel method. Listing 9-5 shows an outputImage method for the haze removal filter. The first thing the code does is to set up a sampler to fetch pixels from the input image. Because this filter uses only one input image, the code sets up only one sampler.\n\nThe code calls the apply:arguments:options: method of CIFilter to produce a CIImage object. The first parameter to the apply method is the CIKernel object that contains the haze removal kernel function. (See Write the Kernel Code.) Recall that the haze removal kernel function takes four arguments: a sampler, a color, a distance, and the slope. These arguments are passed as the next four parameters to the apply:arguments:options: method in Listing 9-5. The remaining arguments to the apply method specify options (key-value pairs) that control how Core Image should evaluate the function. You can pass one of three keys: kCIApplyOptionExtent, kCIApplyOptionDefinition, or kCIApplyOptionUserInfo. This example uses the kCIApplyOptionDefinition key to specify the domain of definition (DOD) of the output image. See CIFilter Class Reference for a description of these keys and for more information on using the apply:arguments:options: method.\n\nThe final argument nil, specifies the end of the options list.\n\nListing 9-5  A method that returns the image output from a haze removal filter\n\n- (CIImage *)outputImage\n\n\n{\n\n\n    CISampler *src = [CISampler samplerWithImage: inputImage];\n\n\n \n\n\n    return [self apply: hazeRemovalKernel, src, inputColor, inputDistance,\n\n\n        inputSlope, kCIApplyOptionDefinition, [src definition], nil];\n\n\n}\n\nListing 9-5 is a simple example. The implementation for your outputImage method needs to be tailored to your filter. If your filter requires loop-invariant calculations, you would include them in the outputImage method rather than in the kernel.\n\nRegister the Filter\n\nIdeally, you’ll package the filter as an image unit, regardless of whether you plan to distribute the filter to others or use it only in your own app. If you plan to package this filter as an image unit, you’ll register your filter using the CIPlugInRegistration protocol described in Packaging and Loading Image Units. You can skip the rest of this section.\n\nNote: Packaging your custom filter as an image unit promotes modular programming and code maintainability.\n\nIf for some reason you do not want to package the filter as an image unit (which is not recommended), you’ll need to register your filter using the registration method of the CIFilter class described shown in Listing 9-6. The initialize method calls registerFilterName:constructor:classAttributes:. You should register only the display name (kCIAttributeFilterDisplayName) and the filter categories (kCIAttributeFilterCategories). All other filters attributes should be specified in the customAttributes method. (See Write a Custom Attributes Method).\n\nThe filter name is the string for creating the haze removal filter when you want to use it. The constructor object specified implements the filterWithName: method (see Write a Method to Create Instances of the Filter). The filter class attributes are specified as an NSDictionary object. The display name—what you’d show in the user interface—for this filter is Haze Remover.\n\nListing 9-6  Registering a filter that is not part of an image unit\n\n+ (void)initialize\n\n\n{\n\n\n    [CIFilter registerFilterName: @\"MyHazeRemover\"\n\n\n                     constructor: self\n\n\n                 classAttributes:\n\n\n     @{kCIAttributeFilterDisplayName : @\"Haze Remover\",\n\n\n       kCIAttributeFilterCategories : @[\n\n\n               kCICategoryColorAdjustment, kCICategoryVideo,\n\n\n               kCICategoryStillImage, kCICategoryInterlaced,\n\n\n               kCICategoryNonSquarePixels]}\n\n\n     ];\n\n\n}\nWrite a Method to Create Instances of the Filter\n\nIf you plan to use this filter only in your own app, then you’ll need to implement a filterWithName: method as described in this section. If you plan to package this filter as an image unit for use by third-party developers, then you can skip this section because your packaged filters can use the filterWithName: method provided by the CIFilter class.\n\nThe filterWithName: method shown in Listing 9-7 creates instances of the filter when they are requested.\n\nListing 9-7  A method that creates instance of a filter\n\n+ (CIFilter *)filterWithName: (NSString *)name\n\n\n{\n\n\n    CIFilter  *filter;\n\n\n    filter = [[self alloc] init];\n\n\n    return filter;\n\n\n}\n\nAfter you follow these steps to create a filter, you can use the filter in your own app. See Using Your Own Custom Filter for details. If you want to make a filter or set of filters available as a plug-in for other apps, see Packaging and Loading Image Units.\n\nUsing Your Own Custom Filter\n\nThe procedure for using your own custom filter is the same as the procedure for using any filter provided by Core Image except that you must initialize the filter class. You initialize the haze removal filter class created in the last section with this line of code:\n\n[MyHazeFilter class];\n\nListing 9-8 shows how to use the haze removal filter. Note the similarity between this code and the code discussed in Processing Images.\n\nNote: If you’ve packaged your filter as an image unit, you need to load it. See Processing Images for details.\n\nListing 9-8  Using your own custom filter\n\n- (void)drawRect: (NSRect)rect\n\n\n{\n\n\n    CGRect  cg = CGRectMake(NSMinX(rect), NSMinY(rect),\n\n\n                            NSWidth(rect), NSHeight(rect));\n\n\n    CIContext *context = [[NSGraphicsContext currentContext] CIContext];\n\n\n \n\n\n    if(filter == nil) {\n\n\n        NSURL       *url;\n\n\n \n\n\n        [MyHazeFilter class];\n\n\n \n\n\n        url = [NSURL fileURLWithPath: [[NSBundle mainBundle]\n\n\n                    pathForResource: @\"CraterLake\"  ofType: @\"jpg\"]];\n\n\n        filter = [CIFilter filterWithName: @\"MyHazeRemover\"\n\n\n                            withInputParameters:@{\n\n\n                  kCIInputImageKey: [CIImage imageWithContentsOfURL: url],\n\n\n                  kCIInputColorKey: [CIColor colorWithRed:0.7 green:0.9 blue:1],\n\n\n                  }];\n\n\n    }\n\n\n \n\n\n    [filter setValue: @(distance) forKey: @\"inputDistance\"];\n\n\n    [filter setValue: @(slope) forKey: @\"inputSlope\"];\n\n\n \n\n\n    [context drawImage: [filter valueForKey: kCIOutputImageKey]\n\n\n        atPoint: cg.origin  fromRect: cg];\n\n\n}\nSupplying an ROI Function\n\nThe region of interest, or ROI, defines the area in the source from which a sampler takes pixel information to provide to the kernel for processing. Recall from the The Region of Interest discussion in Querying the System for Filters that the working space coordinates of the ROI and the DOD either coincide exactly, are dependent on one another, or not related. Core Image always assumes that the ROI and the DOD coincide. If that’s the case for the filter you write, then you don’t need to supply an ROI function. But if this assumption is not true for the filter you write, then you must supply an ROI function. Further, you can supply an ROI function only for CPU executable filters.\n\nNote:  The ROI and domain of definition for a CPU nonexecutable filter must coincide. This is also the case for color kernels described by the CIColorKernel class. You can’t supply an ROI function for these types of filter. See Writing Nonexecutable Filters.\n\nThe ROI function you supply calculates the region of interest for each sampler that is used by the kernel. Core Image invokes your ROI function, passing to it the sampler index, the extent of the region being rendered, and any data that is needed by your routine. In OS X v10.11 and later and iOS 8.0 and later, the recommended way to apply a filter is the applyWithExtent:roiCallback:arguments: or applyWithExtent:roiCallback:inputImage:arguments: method, to which you supply a callback function as a block (Objective-C) or closure (Swift).\n\nNote: In OS X v10.10 and earlier, use the setROISelector: method to provide an ROI function before calling the filter’s apply: or apply:arguments:options: method. The discussions below assume the OS X v10.11 and iOS 8.0 API; however, the inner workings of each example ROI function are the same for both the newer and older APIs.\n\nFor details on the selector form of an ROI function, see the reference documentation for the setROISelector: method.\n\nAn ROI callback is a block or closure whose signature conforms to the CIKernelROICallback type. This block takes two parameters: the first, index, specifies the sampler for which the method calculates the ROI, and the second, rect, specifies the extent of the region for which ROI information is desired. Core Image calls your routine for each pass through the filter. Your method calculates the ROI based on the rectangle passed to it, and returns the ROI specified as a CGRect structure.\n\nThe next sections provide examples of ROI functions.\n\nA Simple ROI Function\n\nIf your ROI function does not require data to be passed to it in the userInfo parameter, then you don’t need to include that argument, as shown in Listing 9-9. The code in Listing 9-9 outsets the sampler by one pixel, which is a calculation used by an edge-finding filter or any 3x3 convolution.\n\nListing 9-9  A simple ROI function\n\nCIKernelROICallback callback = ^(int index, CGRect rect) {\n\n\n    return CGRectInset(rect, -1.0, -1.0);\n\n\n};\n\nNote that this function ignores the index value. If your kernel uses only one sampler, then you can ignore the index. If your kernel uses more than one sampler, you must make sure that you return the ROI that’s appropriate for the specified sampler. You’ll see how to do that in the sections that follow.\n\nAn ROI Function for a Glass Distortion Filter\n\nListing 9-10 shows an ROI function for a glass distortion filter. This function returns an ROI for two samplers. Sampler 0 represents the image to distort and sampler 1 represents the texture used for the glass.\n\nAs with other uses of blocks (Objective-C) or closures (Swift), an ROI callback can capture state from the context in which is defined. You can use this behavior to provide additional parameters to your routine, as seen in this example: an external value scale controls the inset applied by the ROI function. (When using the older setROISelector: API, you provide such values with the kCIApplyOptionUserInfo key in the options dictionary you pass to the apply:arguments:options: method.\n\nAll of the glass texture (sampler 1) needs to be referenced because the filter uses the texture as a rectangular pattern. As a result, the function returns an infinite rectangle as the ROI. An infinite rectangle is a convention that indicates to use all of a sampler. (The constant CGRectInfinite is defined in the Quartz 2D API.)\n\nNote: If you use an infinite ROI make sure that the sampler’s domain of definition is not also infinite. Otherwise, Core Image will not be able to render the image.\n\nListing 9-10  An ROI function for a glass distortion filter\n\nfloat scale = 1.0f;\n\n\nCIKernelROICallback distortionCallback = ^(int index, CGRect rect) {\n\n\n    if (index == 0) {\n\n\n        CGFloat s = scale * 0.5f;\n\n\n        return CGRectInset(rect, -s,-s);\n\n\n    }\n\n\n    return CGRectInfinite;\n\n\n};\nAn ROI Function for an Environment Map\n\nListing 9-11 shows an ROI function that returns the ROI for a kernel that uses three samplers, one of which is an environment map. The ROI for sampler 0 and sampler 1 coincide with the DOD. For that reason, the code returns the destination rectangle passed to it for samplers other than sampler 2.\n\nSampler 2 uses captured values that specify the size of the environment map to create the rectangle that specifies the region of interest.\n\nListing 9-11  Supplying a routine that calculates the region of interest\n\nCGRect sampler2ROI = CGRectMake(0, 0, envMapWidth, envMapHeight);\n\n\nCIKernelROICallback envMapROICallback = ^(int index, CGRect rect) {\n\n\n    if (samplerIndex == 2) {\n\n\n        return sampler2ROI;\n\n\n    }\n\n\n    return destination;\n\n\n};\nSpecifying Sampler Order\n\nAs you saw from the previous examples, a sampler has an index associated with it. When you supply an ROI function, Core Image passes a sampler index to you. A sampler index is assigned on the basis of its order when passed to the apply method for the filter. You call apply from within the filter’s outputImage routine, as shown in Listing 9-12.\n\nIn this listing, notice especially the numbered lines of code that set up the samplers and show how to provide them to the kernel. A detailed explanation for each of these lines appears following Listing 9-12.\n\nListing 9-12  An output image routine for a filter that uses an environment map\n\n- (CIImage *)outputImage\n\n\n{\n\n\n    int i;\n\n\n    CISampler *src, *blur, *env;                                      // 1\n\n\n    CIVector *envscale;\n\n\n    CIKernel *kernel;\n\n\n \n\n\n    src = [CISampler samplerWithImage:inputImage];                    // 2\n\n\n    blur = [CISampler samplerWithImage:inputHeightImage];             // 3\n\n\n    env = [CISampler samplerWithImage:inputEnvironmentMap];           // 4\n\n\n    envscale = [CIVector vectorWithX:[inputEMapWidth floatValue]\n\n\n                     Y:[inputEMapHeight floatValue]];\n\n\n    i = [inputKind intValue];\n\n\n    if ([inputHeightInAlpha boolValue]) {\n\n\n        i += 8;\n\n\n    }\n\n\n    kernel = roundLayerKernels[i];\n\n\n    return [kernel applyWithExtent: [self extent]                      // 5\n\n\n                       roiCallback: envMapROICallback                  // 6\n\n\n                         arguments: @[                                 // 7\n\n\n                               blur,\n\n\n                               env,\n\n\n                               @( pow(10.0, [inputSurfaceScale floatValue]) ),\n\n\n                               envscale,\n\n\n                               inputEMapOpacity,\n\n\n                         ]];\n\n\n }\n\nHere’s what the code does:\n\nDeclares variables for each of the three samplers that are needed for the kernel.\n\nSets up a sampler for the input image. The ROI for this sampler coincides with the DOD.\n\nSets up a sampler for an image used for input height. The ROI for this sampler coincides with the DOD.\n\nSets up a sampler for an environment map. The ROI for this sampler does not coincide with the DOD, which means you must supply an ROI function.\n\nApplies the kernel to produce a Core Image image (CIImage object), using the options that follow.\n\nThe ROI function with the kernel that needs to use it.\n\nThe arguments to the kernel function, which must be type compatible with the function signature of the kernel function. (The kernel function source is not shown here; assume they are type compatible in this example).\n\nThe order of the sampler arguments determine its index. The first sampler supplied to the kernel is index 0. In this case, that’s the src sampler. The second sampler supplied to the kernel—blur— is assigned index 1. The third sampler—env—is assigned index 2. It’s important to check your ROI function to make sure that you provide the appropriate ROI for each sampler.\n\nWriting Nonexecutable Filters\n\nA filter that is CPU nonexecutable is guaranteed to be secure. Because this type of filter runs only on the GPU, it cannot engage in virus or Trojan horse activity or other malicious behavior. To guarantee security, CPU nonexecutable filters have these restrictions:\n\nThis type of filter is a pure kernel, meaning that it is fully contained in a .cikernel file. As such, it doesn’t have a filter class and is restricted in the types of processing it can provide. Sampling instructions of the following form are the only types of sampling instructions that are valid for a nonexecutable filter:\n\ncolor = sample (someSrc, samplerCoord(someSrc));\n\nCPU nonexecutable filters must be packaged as part of an image unit.\n\nCore Image assumes that the ROI coincides with the DOD. This means that nonexecutable filters are not suited for such effects as blur or distortion.\n\nThe CIDemoImageUnit sample contains a nonexecutable filter in the MyKernelFilter.cikernel file. When the image unit is loaded, the MyKernelFilter filter is loaded along with the FunHouseMirror filter that’s also in the image unit. FunHouseMirror, however, is an executable filter. It has an Objective-C portion as well as a kernel portion.\n\nWhen you write a nonexecutable filter, you need to provide all filter attributes in the Descriptions.plist file for the image unit bundle. Listing 9-13 shows the attributes for the MyKernelFilter in the CIDemoImageUnit sample.\n\nListing 9-13  The property list for the MyKernelFilter nonexecutable filter\n\n<key>MyKernelFilter</key>\n\n\n        <dict>\n\n\n            <key>CIFilterAttributes</key>\n\n\n            <dict>\n\n\n                <key>CIAttributeFilterCategories</key>\n\n\n                <array>\n\n\n                    <string>CICategoryStylize</string>\n\n\n                    <string>CICategoryVideo</string>\n\n\n                    <string>CICategoryStillImage</string>\n\n\n                </array>\n\n\n                <key>CIAttributeFilterDisplayName</key>\n\n\n                <string>MyKernelFilter</string>\n\n\n                <key>CIInputs</key>\n\n\n                <array>\n\n\n                    <dict>\n\n\n                        <key>CIAttributeClass</key>\n\n\n                        <string>CIImage</string>\n\n\n                        <key>CIAttributeDisplayName</key>\n\n\n                        <string>inputImage</string>\n\n\n                        <key>CIAttributeName</key>\n\n\n                        <string>inputImage</string>\n\n\n                    </dict>\n\n\n                    <dict>\n\n\n                        <key>CIAttributeClass</key>\n\n\n                        <string>NSNumber</string>\n\n\n                        <key>CIAttributeDefault</key>\n\n\n                        <real>8</real>\n\n\n                        <key>CIAttributeDisplayName</key>\n\n\n                        <string>inputScale</string>\n\n\n                        <key>CIAttributeIdentity</key>\n\n\n                        <real>8</real>\n\n\n                        <key>CIAttributeMin</key>\n\n\n                        <real>1</real>\n\n\n                        <key>CIAttributeName</key>\n\n\n                        <string>inputScale</string>\n\n\n                        <key>CIAttributeSliderMax</key>\n\n\n                        <real>16</real>\n\n\n                        <key>CIAttributeSliderMin</key>\n\n\n                        <real>1</real>\n\n\n                    </dict>\n\n\n                    <dict>\n\n\n                        <key>CIAttributeClass</key>\n\n\n                        <string>NSNumber</string>\n\n\n                        <key>CIAttributeDefault</key>\n\n\n                        <real>1.2</real>\n\n\n                        <key>CIAttributeDisplayName</key>\n\n\n                        <string> inputGreenWeight </string>\n\n\n                        <key>CIAttributeIdentity</key>\n\n\n                        <real>1.2</real>\n\n\n                        <key>CIAttributeMin</key>\n\n\n                        <real>1</real>\n\n\n                        <key>CIAttributeName</key>\n\n\n                        <string>inputGreenWeight</string>\n\n\n                        <key>CIAttributeSliderMax</key>\n\n\n                        <real>3.0</real>\n\n\n                        <key>CIAttributeSliderMin</key>\n\n\n                        <real>1</real>\n\n\n                    </dict>\n\n\n                </array>\n\n\n            </dict>\n\n\n            <key>CIFilterClass</key>\n\n\n            <string>MyKernelFilter</string>\n\n\n            <key>CIHasCustomInterface</key>\n\n\n            <false/>\n\n\n            <key>CIKernelFile</key>\n\n\n            <string>MyKernelFilter</string>\nKernel Routine Examples\n\nThe essence of any image processing filter is the kernel that performs the pixel calculations. The code listings in this section show some typical kernel routines for these filters: brighten, multiply, and hole distortion. By looking at these you can get an idea of how to write your own kernel routine. Note, however, that these routines are examples. Don’t assume that the code shown here is what Core Image uses for the filters it supplies.\n\nBefore you write your own kernel routine, you may want to read Expressing Image Processing Operations in Core Image to see which operations pose a challenge in Core Image. You’ll also want to take a look at Core Image Kernel Language Reference.\n\nYou can find in-depth information on writing kernels as well as more examples in Image Unit Tutorial.\n\nComputing a Brightening Effect\n\nListing 9-14 computes a brightening effect. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 9-14  A kernel routine that computes a brightening effect\n\nkernel vec4 brightenEffect (sampler src, float k)\n\n\n{\n\n\n  vec4 currentSource = sample (src, samplerCoord (src));         // 1\n\n\n  currentSource.rgb = currentSource.rgb + k * currentSource.a;   // 2\n\n\n  return currentSource;                                          // 3\n\n\n}\n\nHere’s what the code does:\n\nLooks up the source pixel in the sampler that is associated with the current output position.\n\nAdds a bias to the pixel value. The bias is k scaled by the alpha value of the pixel to make sure the pixel value is premultiplied.\n\nReturns the changed pixel.\n\nComputing a Multiply Effect\n\nListing 9-15 shows a kernel routine that computes a multiply effect. The code looks up the source pixel in the sampler and then multiplies it by the value passed to the routine.\n\nListing 9-15  A kernel routine that computes a multiply effect\n\nkernel vec4 multiplyEffect (sampler src, __color mul)\n\n\n{\n\n\n  return sample (src, samplerCoord (src)) * mul;\n\n\n}\n\n\n \nComputing a Hole Distortion\n\nListing 9-16 shows a kernel routine that computes a hole distortion. Note that there are many ways to compute a hole distortion. A detailed explanation for each numbered line of code appears following the listing.\n\nListing 9-16  A kernel routine that computes a hole distortion\n\nkernel vec4 holeDistortion (sampler src, vec2 center, vec2 params)   // 1\n\n\n{\n\n\n  vec2 t1;\n\n\n  float distance0, distance1;\n\n\n \n\n\n  t1 = destCoord () - center;                                        // 2\n\n\n  distance0 = dot (t1, t1);                                          // 3\n\n\n  t1 = t1 * inversesqrt (distance0);                                 // 4\n\n\n  distance0 = distance0 * inversesqrt (distance0) * params.x;        // 5\n\n\n  distance1 = distance0 - (1.0 / distance0);                         // 6\n\n\n  distance0 = (distance0 < 1.0 ? 0.0 : distance1) * params.y;        // 7\n\n\n  t1 = t1 * distance0 + center;                                      // 8\n\n\n \n\n\n  return sample (src, samplerTransform (src, t1));                   // 9\n\n\n}\n\nHere’s what the code does:\n\nTakes three parameters—a sampler, a vector that specifies the center of the hole distortion, and the params vector, which contains (1/radius, radius).\n\nCreates the vector t1 from the center to the current working coordinates.\n\nSquares the distance from the center and assigns the value to the distance0 variable.\n\nNormalizes t1. (Makes t1 a unit vector.)\n\nComputes the parametric distance from the center (distance squared * 1/distance) * 1/radius. This value is 0 at the center and 1 where the distance is equal to the radius.\n\nCreates a hole with the appropriate distortion around it. (x – 1/sqrt (x))\n\nMakes sure that all pixels within the hole map from the pixel at the center, then scales up the distorted distance function by the radius.\n\nScales the vector to create the distortion and then adds the center back in.\n\nReturns the distorted sample from the source texture.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "What You Need to Know Before Writing a Custom Filter",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_advanced_concepts/ci.advanced_concepts.html#//apple_ref/doc/uid/TP30001185-CH9-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Programming Guide\nTable of Contents\nIntroduction\nProcessing Images\nDetecting Faces in an Image\nAuto Enhancing Images\nQuerying the System for Filters\nSubclassing CIFilter: Recipes for Custom Effects\nGetting the Best Performance\nUsing Feedback to Process Images\nWhat You Need to Know Before Writing a Custom Filter\nCreating Custom Filters\nPackaging and Loading Image Units\nRevision History\nNext\nPrevious\nWhat You Need to Know Before Writing a Custom Filter\n\nCore Image provides support for writing custom filters. A custom filter is one for which you write a routine, called a kernel, that specifies the calculations to perform on each source image pixel. If you plan to use the built-in Core Image filters, either as they are or by subclassing them, you don’t need to read this chapter. If you plan to write a custom filter, you should read this chapter so you understand the processing path and the components in a custom filter. After reading this chapter, you can find out how to write a filter in Creating Custom Filters. If you are interested in packaging your custom filter for distribution, you should also read Packaging and Loading Image Units.\n\nFilter Clients and Filter Creators\n\nCore Image is designed for two types of developers: filter clients and filter creators. If you plan only to use Core Image filters, you are a filter client. If you plan to write your own filter, you are a filter creator.\n\nFigure 8-1 shows the components of a typical filter. The shaded area of the figure indicates parts that are “under the hood”—the parts that a filter client does not need to know anything about but which a filter creator must understand. The portion that’s not shaded shows two methods—attributes and outputImage—that provide data to the filter client. The filter’s attributes method returns a list of key-value pairs that describe a filter. The outputImage method produces an image using:\n\nA sampler to fetch pixels from a source\n\nA kernel that processes pixels\n\nFigure 8-1  The components of a typical filter\n\nAt the heart of every custom filter is a kernel. The kernel specifies the calculations that are performed on each source image pixel. Kernel calculations can be very simple or complex. A very simple kernel for a “do nothing” filter could simply return the source pixel:\n\ndestination pixel = source pixel\n\nFilter creators use a variant of OpenGL Shading Language (glslang) to specify per-pixel calculations. (See Core Image Kernel Language Reference.) The kernel is opaque to a filter client. A filter can actually use several kernel routines, passing the output of one to the input of another. For instructions on how to write a custom filter, see Creating Custom Filters.\n\nNote:  A kernel is the actual routine, written using the Core Image variant of glslang, that a filter uses to process pixels. A CIKernel object is a Core Image object that contains a kernel routine. When you create a filter, you’ll see that the kernel routine exists in its own file—one that has a .cikernel extension. You create a CIKernel object programmatically by passing a string that contains the kernel routine.\n\nFilter creators can make their custom filters available to any app by packaging them as a plug-in, or image unit, using the architecture specified by the NSBundle class. An image unit can contain more than one filter, as shown in Figure 8-2. For example, you could write a set of filters that perform different kinds of edge detection and package them as a single image unit. Filter clients can use the Core Image API to load the image unit and to obtain a list of the filters contained in that image unit. See Loading Image Units for basic information. See Image Unit Tutorial for in-depth examples and detailed information on writing filters and packaging them as standalone image units.\n\nFigure 8-2  An image unit contains packaging information along with one or more filter definitions\nThe Processing Path\n\nFigure 8-3 shows the pixel processing path for a filter that operates on two source images. Source images are always specified as CIImage objects. Core Image provides a variety of ways to get image data. You can supply a URL to an image, read raw image data (using the NSData class), or convert a Quartz 2D image (CGContextRef), an OpenGL texture, or a Core Video image buffer (CVImageBufferRef) to a CIImage object.\n\nNote that the actual number of input images, and whether or not the filter requires an input image, depends on the filter. Filters are very flexible—a filter can:\n\nWork without an input image. Some filters generate an image based on input parameters that aren’t images. (For example, see the CICheckerboardGenerator and CIConstantColorGenerator filters in Core Image Filter Reference.)\n\nRequire one image. (For example, see the CIColorPosterize and CICMYKHalftone filters in Core Image Filter Reference.)\n\nRequire two or more images. Filters that composite images or use the values in one image to control how the pixels in another image are processed typically require two or more images. One input image can act as a shading image, an image mask, a background image, or provide a source of lookup values that control some aspect of how the other image is processed. (For example, see the CIShadedMaterial filter in Core Image Filter Reference.)\n\nWhen you process an image, it is your responsibility to create a CIImage object that contains the appropriate input data.\n\nNote:  Although a CIImage object has image data associated with it, it is not an image. You can think of a CIImage object as an image “recipe.” A CIImage object has all the information necessary to produce an image, but Core Image doesn’t actually render an image until it is told to do so.\n\nFigure 8-3  The pixel processing path\n\nPixels from each source image are fetched by a CISampler object, or simply a sampler. As its name suggests, a sampler retrieves samples of an image and provides them to a kernel. A filter creator provides a sampler for each source image. Filter clients don’t need to know anything about samplers.\n\nA sampler defines:\n\nA coordinate transform, which can be the identity transform if no transformation is needed.\n\nAn interpolation mode, which can be nearest neighbor sampling or bilinear interpolation (which is the default).\n\nA wrapping mode that specifies how to produce pixels when the sampled area is outside of the source image—either to use transparent black or clamp to the extent.\n\nThe filter creator defines the per-pixel image processing calculations in the kernel, but Core Image handles the actual implementation of those calculations. Core Image determines whether the calculations are performed using the GPU or the CPU. Core Image implements hardware rasterization using Metal, OpenGL, or OpenGL ES depending on device capabilities. It implements software rasterization through an emulation environment specifically tuned for evaluating fragment programs with nonprojective texture lookups over large quadrilaterals (quads).\n\nAlthough the pixel processing path is from source image to destination, the calculation path that Core Image uses begins at the destination and works its way back to the source pixels, as shown in Figure 8-4. This backward calculation might seem unwieldy, but it actually minimizes the number of pixels used in any calculation. The alternative, which Core Image does not use, is the brute force method of processing all source pixels, then later deciding what’s needed for the destination. Let’s take a closer look at Figure 8-4.\n\nFigure 8-4  The Core Image calculation path\n\nAssume that the filter in Figure 8-4 performs some kind of compositing operation, such as source-over compositing. The filter client wants to overlap the two images so that only a small portion of each image is composited to achieve the result shown at the left side of Figure 8-4. By looking ahead to what the destination ought to be, Core Image can determine which data from the source images affect the final image and then restrict calculations only to those source pixels. As a result, the samplers fetch sample pixels only from shaded areas in the source images, shown in Figure 8-4.\n\nNote the box in Figure 8-4 that’s labeled Domain of definition. The domain of definition is simply a way to further restrict calculations. It is an area outside of which all pixels are transparent (that is, the alpha component is equal to 0). In this example, the domain of definition coincides exactly with the destination image. Core Image lets you supply a CIFilterShape object to define this area. The CIFilterShape class provides a number of methods that can define rectangular shapes, transform shapes, and perform inset, union, and intersection operations on shapes. For example, if you define a filter shape using a rectangle that is smaller than the shaded area shown in Figure 8-4, then Core Image uses that information to further restrict the source pixels used in the calculation.\n\nCore Image promotes efficient processing in other ways. It performs intelligent caching and compiler optimizations that make it well-suited for such tasks as real-time video processing and image analysis. It caches intermediate results for any data set that is evaluated repeatedly. Core Image evicts data in least-recently-used order whenever adding a new image would cause the cache to grow too large. Objects that are reused frequently remain in the cache, while those used once in a while might be moved in and out of the cache as needed. Your app benefits from Core Image caching without needing to know the details of how caching is implemented. However, you get the best performance by reusing objects (images, contexts, and so forth) whenever you can.\n\nCore Image also gets great performance by using traditional compilation techniques at the kernel and pass levels. The method Core Image uses to allocate registers minimizes the number of temporary registers (per kernel) and temporary pixel buffers (per filter graph). The compiler performs several optimizations and automatically distinguishes between reading data-dependent textures, which are based on previous calculations, and those that are not data-dependent. Again, you don’t need to concern yourself with the details of the compilation techniques. The important point is that Core Image is hardware savvy; it uses the power of the GPU and multicore CPUs whenever it can, and it does so in smart ways.\n\nCoordinate Spaces\n\nCore Image performs operations in a device-independent working space. The Core Image working space is, in theory, infinite in extent. A point in working space is represented by a coordinate pair (x, y), where x represents the location along the horizontal axis and y represents the location along the vertical axis. Coordinates are floating-point values. By default, the origin is point (0,0).\n\nWhen Core Image reads an image, it translates the pixel locations into device-independent working space coordinates. When it is time to display a processed image, Core Image translates the working space coordinates to the appropriate coordinates for the destination, such as a display.\n\nWhen you write your own filters, you need to be familiar with two coordinate spaces: the destination coordinate space and the sampler space. The destination coordinate space represents the image you are rendering to. The sampler space represents what you are texturing from (another image, a lookup table, and so on). You obtain the current location in destination space using the destCoord function whereas the samplerCoord function provides the current location in sample space. (See Core Image Kernel Language Reference.)\n\nKeep in mind that if your source data is tiled, the sampler coordinates have an offset (dx/dy). If your sample coordinates have an offset, it may be necessary for you to convert the destination location to the sampler location using the function samplerTransform.\n\nThe Region of Interest\n\nAlthough not explicitly labeled in Figure 8-4, the shaded area in each of the source images is the region of interest for samplers depicted in the figure. The region of interest, or ROI, defines the area in the source from which a sampler takes pixel information to provide to the kernel for processing. If you are a filter client, you don’t need to concern yourself with the ROI. But if you are a filter creator, you’ll want to understand the relationship between the region of interest and the domain of definition.\n\nRecall that the domain of definition describes the bounding shape of a filter. In theory, this shape can be without bounds. Consider, for example, a filter that creates a repeating pattern that could extend to infinity.\n\nThe ROI and the domain of definition can relate to each other in the following ways:\n\nThey coincide exactly—there is a 1:1 mapping between source and destination. For example, a hue filter processes a pixel from the working space coordinate (r,s) in the ROI to produce a pixel at the working space coordinate (r,s) in the domain of definition.\n\nThey are dependent on each other, but modulated in some way. Some of the most interesting filters—blur and distortion, for example—use many source pixels in the calculation of one destination pixel. For example, a distortion filter might use a pixel (r,s) and its neighbors from the working coordinate space in the ROI to produce a single pixel (r,s) in the domain of definition.\n\nThe domain of definition is calculated from values in a lookup table that are provided by the sampler. The location of values in the map or table are unrelated to the working space coordinates in the source image and the destination. A value located at (r,s) in a shading image does not need to be the value that produces a pixel at the working space coordinate (r,s) in the domain of definition. Many filters use values provided in a shading image or lookup table in combination with an image source. For example, a color ramp or a table that approximates a function, such as the arcsin function, provides values that are unrelated to the notion of working coordinates.\n\nUnless otherwise instructed, Core Image assumes that the ROI and the domain of definition coincide. If you write a filter for which this assumption doesn’t hold, you need to provide Core Image with a routine that calculates the ROI for a particular sampler.\n\nSee Supplying an ROI Function for more information.\n\nExecutable and Nonexecutable Filters\n\nYou can categorize custom Core Image filters on the basis of whether or not they require an auxiliary binary executable to be loaded into the address space of the client app. As you use the Core Image API, you’ll notice that these are simply referred to as executable and nonexecutable. Filter creators can choose to write either kind of filter. Filter clients can choose to use only nonexecutable or to use both kinds of filters.\n\nSecurity is the primary motivation for distinguishing CPU executable and CPU nonexecutable filters. Nonexecutable filters consist only of a Core Image kernel program to describe the filter operation. In contrast, an executable filter also contains machine code that runs on the CPU. Core Image kernel programs run within a restricted environment and cannot pose as a virus, Trojan horse, or other security threat, whereas arbitrary code that runs on the CPU can.\n\nNonexecutable filters have special requirements, one of which is that nonexecutable filters must be packaged as part of an image unit. Filter creators can read Writing Nonexecutable Filters for more information. Filter clients can find information on loading each kind of filter in Loading Image Units.\n\nColor Components and Premultiplied Alpha\n\nPremultiplied alpha is a term used to describe a source color, the components of which have already been multiplied by an alpha value. Premultiplying speeds up the rendering of an image by eliminating the need to perform a multiplication operation for each color component. For example, in an RGB color space, rendering an image with premultiplied alpha eliminates three multiplication operations (red times alpha, green times alpha, and blue times alpha) for each pixel in the image.\n\nFilter creators must supply Core Image with color components that are premultiplied by the alpha value. Otherwise, the filter behaves as if the alpha value for a color component is 1.0. Making sure color components are premultiplied is important for filters that manipulate color.\n\nBy default, Core Image assumes that processing nodes are 128 bits-per-pixel, linear light, premultiplied RGBA floating-point values that use the GenericRGB color space. You can specify a different working color space by providing a Quartz 2D CGColorSpace object. Note that the working color space must be RGB-based. If you have YUV data as input (or other data that is not RGB-based), you can use ColorSync functions to convert to the working color space. (See Quartz 2D Programming Guide for information on creating and using CGColorspace objects.)\n\nWith 8-bit YUV 4:2:2 sources, Core Image can process 240 HD layers per gigabyte. Eight-bit YUV is the native color format for video source such as DV, MPEG, uncompressed D1, and JPEG. You need to convert YUV color spaces to an RGB color space for Core Image.\n\nSee Also\n\nShantzis, Michael A., “A Model for Efficient and Flexible Image Computing,” (1994), Proceedings of the 21st Annual Conference on Computer Graphics and Interactive Techniques.\n\nSmith, Alvy Ray, “Image Compositing Fundamentals,” Memo 4, Microsoft, July 1995. Available from http://alvyray.com/Memos/MemosCG.htm#ImageCompositing\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Using Feedback to Process Images",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_feedback_based/ci_feedback_based.html#//apple_ref/doc/uid/TP30001185-CH5-SW5",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Programming Guide\nTable of Contents\nIntroduction\nProcessing Images\nDetecting Faces in an Image\nAuto Enhancing Images\nQuerying the System for Filters\nSubclassing CIFilter: Recipes for Custom Effects\nGetting the Best Performance\nUsing Feedback to Process Images\nWhat You Need to Know Before Writing a Custom Filter\nCreating Custom Filters\nPackaging and Loading Image Units\nRevision History\nNext\nPrevious\nUsing Feedback to Process Images\n\nThe CIImageAccumulator class is ideally suited for feedback-based processing. As it’s name suggests, it accumulates image data over time. This chapter shows how to use a CIImageAccumulator object to implement a simple painting app called MicroPaint that allows users to paint on a canvas to create images similar to that shown in Figure 7-1.\n\nFigure 7-1  Output from MicroPaint\n\nThe “image” starts as a blank canvas. MicroPaint uses an image accumulator to collect the paint applied by the user. When the user clicks Clear, MicroPaint resets the image accumulator to a white canvas. The color well allows the user to change paint colors. The user can change the brush size using the slider.\n\nThe essential tasks for creating setting up an image accumulator for the MicroPaint app are:\n\nSet Up the Interface for the MicroPaint App\n\nInitialize Filters and Default Values for Painting\n\nTrack and Accumulate Painting Operations\n\nThis chapter describes only the code that is essential to creating an image accumulator and supporting drawing to it. The methods for drawing to the view and for handling view size changes aren’t discussed here. For that, see CIMicroPaint, which is a complete sample code project that you can download and examine in more detail. CIMicroPaint has several interesting details. It shows how to draw to an OpenGL view and to maintain backward compatibility for previous versions of OS X.\n\nSet Up the Interface for the MicroPaint App\n\nThe interface to MicroPaint needs the following:\n\nAn image accumulator\n\nA “brush” for the user. The brush is a Core Image filter (CIRadialGradient) that applies color in a way that simulates an air brush.\n\nA composite filter (CISourceOverCompositing) that allows new paint to be composited over previously applied paint.\n\nVariables for keeping track of the current paint color and brush size.\n\nBuilding a Dictionary of Filters declares MircoPaintView as a subclass of SampleCIView. The SampleCIView class isn’t discussed here; it is a subclass of the NSOpenGLView class. See the CIMicroPaint sample app for details.\n\nListing 7-1  The interface for the MicroPaint app\n\n@interface MicroPaintView : SampleCIView {\n\n\n    CIImageAccumulator *imageAccumulator;\n\n\n    CIFilter *brushFilter;\n\n\n    CIFilter *compositeFilter;\n\n\n    NSColor *color;\n\n\n    CGFloat brushSize;\n\n\n}\n\n\n@end\nInitialize Filters and Default Values for Painting\n\nWhen you initialize the MicroPaint app (as shown in Listing 7-2), you need to create the brush and composite filters, and set the initial brush size and paint color. The code in Listing 7-2 is created and initialized to transparent black with an input radius of 0. When the user drags the cursor, the brush filter takes on the current values for brush size and color.\n\nListing 7-2  Initializing filters, brush size, and paint color\n\n    brushFilter = [CIFilter filterWithName: @\"CIRadialGradient\" withInputParameters:@{\n\n\n         @\"inputColor1\": [CIColor colorWithRed:0.0 green:0.0 blue:0.0 alpha:0.0],\n\n\n         @\"inputRadius0\": @0.0,\n\n\n         }];\n\n\n    compositeFilter = [CIFilter filterWithName: @\"CISourceOverCompositing\"];\n\n\n    brushSize = 25.0;\n\n\n    color = [NSColor colorWithDeviceRed: 0.0 green: 0.0 blue: 0.0 alpha: 1.0];\nTrack and Accumulate Painting Operations\n\nThe mouseDragged: method is called whenever the user either clicks or drags the cursor over the canvas. It updates the brush and compositing filter values and adds new painting operations to the accumulated image.\n\nAfter setting the image, you need to trigger an update of the display. Your drawRect: method handles drawing the image. When drawing to a CIContext object, make sure to use drawImage:inRect:fromRect: rather than the deprecated method drawImage:atPoint:fromRect:.\n\nListing 7-3  Setting up and applying the brush filter to the accumulated image\n\n- (void)mouseDragged:(NSEvent *)event\n\n\n{\n\n\n    CGRect   rect;\n\n\n    NSPoint  loc = [self convertPoint: [event locationInWindow] fromView: nil];\n\n\n    CIColor   *cicolor;\n\n\n \n\n\n    // Make a rectangle that is centered on the drag location and\n\n\n    // whose dimensions are twice of the current brush size\n\n\n    rect = CGRectMake(loc.x-brushSize, loc.y-brushSize,\n\n\n                                   2.0*brushSize, 2.0*brushSize);\n\n\n    // Set the size of the brush\n\n\n    // Recall this is really a radial gradient filter\n\n\n    [brushFilter setValue: @(brushSize)\n\n\n                   forKey: @\"inputRadius1\"];\n\n\n    cicolor = [[CIColor alloc] initWithColor: color];\n\n\n    [brushFilter setValue: cicolor  forKey: @\"inputColor0\"];\n\n\n    [brushFilter setValue: [CIVector vectorWithX: loc.x Y:loc.y]\n\n\n                   forKey: kCIInputCenterKey];\n\n\n    // Composite the output from the brush filter with the image\n\n\n    // accummulated by the image accumulator\n\n\n    [compositeFilter setValue: [brushFilter valueForKey: kCIOutputImageKey]\n\n\n                       forKey: kCIInputImageKey];\n\n\n    [compositeFilter setValue: [imageAccumulator image]\n\n\n                       forKey: kCIInputBackgroundImageKey];\n\n\n    // Set the image accumluator to the composited image\n\n\n    [imageAccumulator setImage: [compositeFilter valueForKey: kCIOutputImageKey]\n\n\n                     dirtyRect: rect];\n\n\n    // After setting the image, you need to trigger an update of the display\n\n\n    [self setImage: [imageAccumulator image] dirtyRect: rect];\n\n\n}\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Subclassing CIFilter: Recipes for Custom Effects",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_filer_recipes/ci_filter_recipes.html#//apple_ref/doc/uid/TP30001185-CH4-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Programming Guide\nTable of Contents\nIntroduction\nProcessing Images\nDetecting Faces in an Image\nAuto Enhancing Images\nQuerying the System for Filters\nSubclassing CIFilter: Recipes for Custom Effects\nGetting the Best Performance\nUsing Feedback to Process Images\nWhat You Need to Know Before Writing a Custom Filter\nCreating Custom Filters\nPackaging and Loading Image Units\nRevision History\nNext\nPrevious\nSubclassing CIFilter: Recipes for Custom Effects\n\nYou can create custom effects by using the output of one image filter as the input of another, chaining as many filters together as you’d like. When you create an effect this way that you want to use multiple times, consider subclassing CIFilter to encapsulate the effect as a filter.\n\nThis chapter shows how Core Image subclasses CIFilter to create the CIColorInvert filter. Then it describes recipes for chaining together a variety of filters to achieve interesting effects. By following the subclassing procedure in Subclassing CIFilter to Create the CIColorInvert Filter, you should be able to create filters from the recipes in this chapter or venture forth to create your own interesting combinations of the built-in filters provided by Core Image.\n\nSubclassing CIFilter to Create the CIColorInvert Filter\n\nWhen you subclass CIFilter you can modify existing filters by coding them with preset values or by chaining them together. Core Image implements some of its built-in filters using this technique.\n\nTo subclass a filter you need to perform the following tasks:\n\nDeclare properties for the filter’s input parameters. You must prefix each input parameter name with input, such as inputImage.\n\nOverride the setDefaults method, if necessary. (It’s not necessary in this example because the input parameters are set values.)\n\nOverride the outputImage method.\n\nThe CIColorInvert filter provided by Core Image is a variation on the CIColorMatrix filter. As its name suggests, CIColorInvert supplies vectors to CIColorMatrix that invert the colors of the input image. Follow the simple example shown in Listing 5-1 and Listing 5-2 to build your own filters.\n\nListing 5-1  The interface for the CIColorInvert filter\n\n@interface CIColorInvert: CIFilter {\n\n\n    CIImage *inputImage;\n\n\n}\n\n\n@property (retain, nonatomic) CIImage *inputImage;\n\n\n@end\n\nListing 5-2  The outputImage method for the CIColorInvert filter\n\n@implementation CIColorInvert\n\n\n@synthesize inputImage;\n\n\n- (CIImage *) outputImage\n\n\n{\n\n\n    CIFilter *filter = [CIFilter filterWithName:@\"CIColorMatrix\"\n\n\n                            withInputParameters: @{\n\n\n            kCIInputImageKey: inputImage,\n\n\n        @\"inputRVector\": [CIVector vectorWithX:-1 Y:0 Z:0],\n\n\n        @\"inputGVector\": [CIVector vectorWithX:0 Y:-1 Z:0],\n\n\n        @\"inputBVector\": [CIVector vectorWithX:0 Y:0 Z:-1],\n\n\n        @\"inputBiasVector\": [CIVector vectorWithX:1 Y:1 Z:1],\n\n\n        }];\n\n\n    return filter.outputImage;\n\n\n}\nChroma Key Filter Recipe\n\nRemoves a color or range of colors from a source image and then composites the source image with a background image.\n\nFigure 5-1  The Chroma Key filter processing chain\n\nTo create a chroma key filter:\n\nCreate a cube map of data that maps the color values you want to remove so they are transparent (alpha value is 0.0).\n\nUse the CIColorCube filter and the cube map to remove chroma-key color from the source image.\n\nUse the CISourceOverCompositing filter to blend the processed source image over a background image\n\nThe sections that follow show how to perform each step.\n\nCreate a Cube Map\n\nA color cube is a 3D color lookup table. The Core Image filter CIColorCube takes color values as input and applies a lookup table to the values. The default lookup table for CIColorCube is an identity matrix—meaning that is does nothing to its supplied data. However, this recipe requires that you remove all green from the image. (You can remove a different color if you’d like.)\n\nYou need to remove all the green from the image by setting green to alpha = 0.0, which makes it transparent. “Green” encompasses a range of colors. The most straightforward way to proceed is to convert the color values in the image from RGBA to HSV values. In HSV, hue is represented as an angle around the central axis of a cylinder. In that representation, you can visualize color as a pie slice and then simply remove the slice that represents the chroma-key color.\n\nTo remove green, you need to define the minimum and maximum angles around the central access that contain green hues. Then, for anything that’s green, you set its alpha value to 0.0. Pure green is at a value corresponding to 120º. The minimum and maximum angles need to center around that value.\n\nCube map data must be premultiplied alpha, so the final step for creating the cube map is to multiply the RGB values by the alpha value you just computed, which is either 0.0 for green hues or 1.0 otherwise. Listing 5-3 shows how to create the color cube needed for this filter recipe.\n\nListing 5-3  The color cube in code\n\n// Allocate memory\n\n\nconst unsigned int size = 64;\n\n\nfloat *cubeData = (float *)malloc (size * size * size * sizeof (float) * 4);\n\n\nfloat rgb[3], hsv[3], *c = cubeData;\n\n\n \n\n\n// Populate cube with a simple gradient going from 0 to 1\n\n\nfor (int z = 0; z < size; z++){\n\n\n    rgb[2] = ((double)z)/(size-1); // Blue value\n\n\n    for (int y = 0; y < size; y++){\n\n\n        rgb[1] = ((double)y)/(size-1); // Green value\n\n\n        for (int x = 0; x < size; x ++){\n\n\n            rgb[0] = ((double)x)/(size-1); // Red value\n\n\n            // Convert RGB to HSV\n\n\n            // You can find publicly available rgbToHSV functions on the Internet\n\n\n            rgbToHSV(rgb, hsv);\n\n\n            // Use the hue value to determine which to make transparent\n\n\n            // The minimum and maximum hue angle depends on\n\n\n            // the color you want to remove\n\n\n            float alpha = (hsv[0] > minHueAngle && hsv[0] < maxHueAngle) ? 0.0f: 1.0f;\n\n\n            // Calculate premultiplied alpha values for the cube\n\n\n            c[0] = rgb[0] * alpha;\n\n\n            c[1] = rgb[1] * alpha;\n\n\n            c[2] = rgb[2] * alpha;\n\n\n            c[3] = alpha;\n\n\n            c += 4; // advance our pointer into memory for the next color value\n\n\n        }\n\n\n    }\n\n\n}\n\n\n// Create memory with the cube data\n\n\nNSData *data = [NSData dataWithBytesNoCopy:cubeData\n\n\n                       length:cubeDataSize\n\n\n                       freeWhenDone:YES];\n\n\nCIColorCube *colorCube = [CIFilter filterWithName:@\"CIColorCube\"];\n\n\n[colorCube setValue:@(size) forKey:@\"inputCubeDimension\"];\n\n\n// Set data for cube\n\n\n[colorCube setValue:data forKey:@\"inputCubeData\"];\nRemove green from the source image\n\nNow that you have the color map data, supply the foreground image—the one you want the green removed from—to the CIColorCube filter and get the output image.\n\n[colorCube setValue:myInputImage forKey:kCIInputImageKey];\n\n\nCIImage *result = [colorCube valueForKey:kCIOutputImageKey];\nBlend the processed source image over a background image\n\nSet the input parameters of the CISourceOverCompositing filter as follows:\n\nSet inputImage to the image produced from the CIColorCube filter.\n\nSet inputBackgroundImage to the image that shows the new background. This example uses a beach image.\n\nThe foreground image now appears as if it is on the beach.\n\nWhite Vignette for Faces Filter Recipe\n\nIncreases the brightness of an image at the periphery of a face detected in an image.\n\nFigure 5-2  The White Vignette filter processing chain\n\nTo create a white vignette filter:\n\nFind the human face in an image.\n\nCreate a base shade map using CIRadialGradient centered on the face.\n\nBlend the base shade map with the original image.\n\nThe sections that follow show how to perform each step.\n\nFind the Face\n\nUse the CIDetector class to locate a face in an image. The first item in the array that featuresInImage:options: returns is the face the filter operates on. After you have the face, calculate the center of the face from the bounds provided by the detector. You need the center value to create the shade map. Listing 5-4 shows how to locate a face using CIDetector.\n\nListing 5-4  Using CIDetector to locate one face\n\nCIDetector *detector = [CIDector detectorOfType:CIDetectorTypeFace\n\n\n                                        context:nil\n\n\n                                        options:nil];\n\n\nNSArray *faceArray = [detector featuresInImage:image options:nil];\n\n\nCIFeature *face = faceArray[0];\n\n\nCGFloat xCenter = face.bounds.origin.x + face.bounds.size.width/2.0;\n\n\nCGFloat yCenter = face.bounds.origin.y + face.bounds.size.height/2.0;\n\n\nCIVector *center = [CIVector vectorWithX:xCenter Y:yCenter];\nCreate a Shade Map\n\nUse the CIRadialGradient filter to create a shade map centered on the face. The center of the shade map should be transparent so that the face in the image remains untouched. The edges of the map should be opaque white. Areas in between should have varying degrees of transparency.\n\nTo achieve this effect, set the input parameters to CIRadialGradient as follows:\n\nSet inputRadius0 to a value larger than the longest dimension of the the image.\n\nSet inputRadius1 to a value larger than the face, such as face.bounds.size.height + 50.\n\nSet inputColor0 to opaque white.\n\nSet inputColor1 to transparent white.\n\nSet the inputCenter to the center of the face bounds that you computed with Listing 5-4.\n\nBlend the Gradient with the Face\n\nSet the input parameters of the CISourceOverCompositing filter as follows:\n\nSet inputImage to the original image.\n\nSet inputBackgroundImage to the shade map produced in the last step.\n\nTilt-Shift Filter Recipe\n\nSelectively focuses an image to simulate a miniature scene.\n\nFigure 5-3  The Tilt-Shift filter processing chain\n\nTo create a tilt-shift filter :\n\nCreate a blurred version of the image.\n\nCreate two linear gradients.\n\nCreate a mask by compositing the linear gradients.\n\nComposite the blurred image, the mask, and the original image.\n\nThe sections that follow show how to perform each step.\n\nCreate a Blurred Version of the image\n\nSet the input parameters of the CIGaussianBlur filter as follows:\n\nSet inputImage to the image you want to process.\n\nSet inputRadius to 10.0 (which is the default value).\n\nCreate Two Linear Gradients\n\nCreate a linear gradient from a single color (such as green or gray) that varies from top to bottom. Set the input parameters of CILinearGradient as follows:\n\nSet inputPoint0 to (0, 0.75 * h)\n\nSet inputColor0 to (0,1,0,1)\n\nSet inputPoint1 to (0, 0.5*h)\n\nSet inputColor1 to (0,1,0,0)\n\nCreate a green linear gradient that varies from bottom to top. Set the input parameters of CILinearGradient as follows:\n\nSet inputPoint0 to (0, 0.25 * h)\n\nSet inputColor0 to (0,1,0,1)\n\nSet inputPoint1 to (0, 0.5*h)\n\nSet inputColor1 to (0,1,0,0)\n\nCreate a Mask from the Linear Gradients\n\nTo create a mask, set the input parameters of the CIAdditionCompositing filter as follows:\n\nSet inputImage to the first linear gradient you created.\n\nSet inputBackgroundImage to the second linear gradient you created.\n\nCombine the Blurred Image, Source Image, and the Gradients\n\nThe final step is to use the CIBlendWithMask filter, setting the input parameters as follows:\n\nSet inputImage to the blurred version of the image.\n\nSet inputBackgroundImage to the original, unprocessed image.\n\nSet inputMaskImage to the mask, that is, the combined gradients.\n\nThe mask will affect only the outer portion of an image. The transparent portions of the mask will show through the original, unprocessed image. The opaque portions of the mask allow the blurred image to show.\n\nAnonymous Faces Filter Recipe\n\nFinds faces in an image and pixellates them so they can’t be recognized.\n\nFigure 5-4  The Anonymous Faces filter processing chain\n\nTo create an anonymous faces filter:\n\nCreate a pixellated version of the image.\n\nBuild a mask using the faces detected in the image.\n\nBlend the pixellated image with the original image using the mask.\n\nThe sections that follow show how to perform each step.\n\nCreate a Pixellated version of the image\n\nSet the input parameters of the CIPixellate filter as follows:\n\nSet inputImage to the image that contains the faces.\n\nSet inputScale to max(width, height)/60 or another value that seems pleasing to you, where width and height refer to the image’s width and height.\n\nBuild a Mask From the Faces Detected in the Image\n\nUse the CIDetector class for find the faces in the image. For each face:\n\nUse the CIRadialGradient filter to create a circle that surrounds the face.\n\nUse the CISourceOverCompositing filter to add the gradient to the mask.\n\nListing 5-5  Building a mask for the faces detected in an image\n\nCIDetector *detector = [CIDetector detectorOfType:CIDetectorTypeFace\n\n\n                                          context:nil\n\n\n                                          options:nil];\n\n\nNSArray *faceArray = [detector featuresInImage:image options:nil];\n\n\n \n\n\n// Create a green circle to cover the rects that are returned.\n\n\n \n\n\nCIImage *maskImage = nil;\n\n\n \n\n\nfor (CIFeature *f in faceArray) {\n\n\n    CGFloat centerX = f.bounds.origin.x + f.bounds.size.width / 2.0;\n\n\n    CGFloat centerY = f.bounds.origin.y + f.bounds.size.height / 2.0;\n\n\n    CGFloat radius = MIN(f.bounds.size.width, f.bounds.size.height) / 1.5);\n\n\n    CIFilter *radialGradient = [CIFilter filterWithName:@\"CIRadialGradient\" withInputParameters:@{\n\n\n        @\"inputRadius0\": @(radius),\n\n\n        @\"inputRadius1\": @(radius + 1.0f),\n\n\n        @\"inputColor0\": [CIColor colorWithRed:0.0 green:1.0 blue:0.0 alpha:1.0],\n\n\n        @\"inputColor1\": [CIColor colorWithRed:0.0 green:0.0 blue:0.0 alpha:0.0],\n\n\n        kCIInputCenterKey: [CIVector vectorWithX:centerX Y:centerY],\n\n\n        }];\n\n\n    CIImage *circleImage = [radialGradient valueForKey:kCIOutputImageKey];\n\n\n    if (nil == maskImage)\n\n\n        maskImage = circleImage;\n\n\n    else\n\n\n        maskImage = [[CIFilter filterWithName:@\"CISourceOverCompositing\" withInputParameters:@{\n\n\n            kCIInputImageKey: circleImage,\n\n\n            kCIInputBackgroundImageKey: maskImage,\n\n\n            }] valueForKey:kCIOutputImageKey];\n\n\n}\nBlend the Pixellated Image, the Mask, and the Original Image\n\nSet the input parameters of the CIBlendWithMask filter to the following:\n\nSet inputImage to the pixellated version of the image.\n\nSet inputBackgroundImage to the original image.\n\nSet inputMaskImage to to the composited green circles.\n\nPixellate Transition Filter Recipe\n\nTransitions from one image to another by pixelating each image.\n\nFigure 5-5  The Pixellate Transition filter processing chain\n\nTo create a pixellate-transition filter:\n\nUse CIDissolveTransition to transition between the source and destination images.\n\nPixellate the result of the transition filter.\n\nThe sections that follow show how to perform each step.\n\nCreate a Dissolve Transition\n\nSet the input parameters of the CIDissolveTransition filter as follows:\n\nSet inputImage to the image from which you want to transition.\n\nSet inputTagetImage to the image to which you want to transition.\n\nSet inputTime to a value similar to min(max(2*(time - 0.25), 0), 1), which is a ramp function that’s clamped between two values.\n\nPixellate the Result of the Transition\n\nSet the CIPixellate filter to vary the scale of the pixels over time by setting its input parameters as:\n\nSet inputImage to the output image from the CIDissolveTransition filter.\n\nSet inputScale to change over time by supplying values from a triangle function: 90*(1 - 2*abs(time - 0.5))\n\nUse the default value for inputCenter.\n\nOld Film Filter Recipe\n\nDecreases the quality of a video image to make it look like an old, scratchy analog film.\n\nFigure 5-6  The Old Film filter processing chain\n\nTo create an old-film filter:\n\nApply the CISepiaTone filter to the original video image.\n\nCreate randomly varying white specks.\n\nCreate randomly varying dark scratches.\n\nComposite the specks and scratches onto the sepia-toned image.\n\nThe sections that follow show how to perform each step.\n\nApply Sepia to the Video Image\n\nSet the input parameters of the CISepiaTone as follows:\n\nSet inputImage to the video image to apply the effect to.\n\nSet inputIntensity to 1.0.\n\nCreate Randomly Varying White Specks\n\nUse the CIRandomGenerator filter, which produces colored noise. It does not have any input parameters.\n\nTo process the noise so that you get only white specks, use the CIColorMatrix filter with the input parameters set as follows:\n\nSet inputImage to the output produced by the random generator.\n\nSet inputRVector, inputGVector, and inputBVector to (0,1,0,0).\n\nSet inputBiasVector to (0,0,0,0).\n\nUse the CISourceOverCompositing filter to blend the specks with the video image by setting the filter’s input parameters as follows:\n\nSet inputImage to the white-specks image produced by the CIColorMatrix filter.\n\nSet inputBackgroundImage to the image produced by the CISepiaTone filter.\n\nCreate Randomly Varying Dark Scratches\n\nUse the CIRandomGenerator filter again to generate colored noise. Then process its output using the CIAffineTransform filter with these input parameters:\n\nSet inputImage to the noise generated by the CIRandomGenerator filter.\n\nSet inputTransform to scale x by 1.5 and y by 25. This makes the pixels thick and long, but they will still be colored.\n\nAn alternative to using CIAffineTransform is to transform the noise using the imageByApplyingTransform: method.\n\nTo make the pixels dark, set the input parameters of the CIColorMatrix filter as follows:\n\nSet inputImage to the transformed video image.\n\nSet inputRVector to (4,0,0,0).\n\nSet inputGVector, inputBVector, and inputAVector to (0,0,0,0).\n\nSet inputBiasVector to (0,1,1,1).\n\nThis results in cyan-colored scratches.\n\nTo make the scratches dark, apply the CIMinimumComponent filter to the cyan-colored scratches. This filter uses the minimum value of the r,g,b, values to produce a grayscale image.\n\nComposite the Specks and Scratches to the Sepia Video Image\n\nSet the input parameters of the CIMultiplyCompositing filter as follows:\n\nSetinputBackgroundImage to the processed video image (sepia tone, white specks).\n\nSetinputImage to the dark scratches, that is, the output from the CIMinimumComponent filter.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Getting the Best Performance",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_performance/ci_performance.html#//apple_ref/doc/uid/TP30001185-CH10-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Programming Guide\nTable of Contents\nIntroduction\nProcessing Images\nDetecting Faces in an Image\nAuto Enhancing Images\nQuerying the System for Filters\nSubclassing CIFilter: Recipes for Custom Effects\nGetting the Best Performance\nUsing Feedback to Process Images\nWhat You Need to Know Before Writing a Custom Filter\nCreating Custom Filters\nPackaging and Loading Image Units\nRevision History\nNext\nPrevious\nGetting the Best Performance\n\nCore Image provides many options for creating images, contexts, and rendering content. How you choose to accomplish a task depends on:\n\nHow often your app needs to perform a task\n\nWhether your app works with still or video images\n\nWhether you need to support real-time processing or analysis\n\nHow important color fidelity is to your users\n\nYou should read over the performance best practices to ensure your app runs as efficiently as possible.\n\nPerformance Best Practices\n\nFollow these practices for best performance:\n\nDon’t create a CIContext object every time you render.\n\nContexts store a lot of state information; it’s more efficient to reuse them.\n\nEvaluate whether you app needs color management. Don’t use it unless you need it. See Does Your App Need Color Management?.\n\nAvoid Core Animation animations while rendering CIImage objects with a GPU context.\n\nIf you need to use both simultaneously, you can set up both to use the CPU.\n\nMake sure images don’t exceed CPU and GPU limits.\n\nImage size limits for CIContext objects differ depending on whether Core Image uses the CPU or GPU. Check the limit by using the methods inputImageMaximumSize and outputImageMaximumSize.\n\nUser smaller images when possible.\n\nPerformance scales with the number of output pixels. You can have Core Image render into a smaller view, texture, or framebuffer. Allow Core Animation to upscale to display size.\n\nUse Core Graphics or Image I/O functions to crop or downsample, such as the functions CGImageCreateWithImageInRect or CGImageSourceCreateThumbnailAtIndex.\n\nThe UIImageView class works best with static images.\n\nIf your app needs to get the best performance, use lower-level APIs.\n\nAvoid unnecessary texture transfers between the CPU and GPU.\n\nRender to a rectangle that is the same size as the source image before applying a contents scale factor.\n\nConsider using simpler filters that can produce results similar to algorithmic filters.\n\nFor example, CIColorCube can produce output similar to CISepiaTone, and do so more efficiently.\n\nTake advantage of the support for YUV image in iOS 6.0 and later.\n\nCamera pixel buffers are natively YUV but most image processing algorithms expect RBGA data. There is a cost to converting between the two. Core Image supports reading YUB from CVPixelBuffer objects and applying the appropriate color transform.\n\noptions = @{ (id)kCVPixelBufferPixelFormatTypeKey :\n\n\n    @(kCVPixelFormatType_420YpCbCr88iPlanarFullRange) };\nDoes Your App Need Color Management?\n\nBy default, Core Image applies all filters in light-linear color space. This provides the most accurate and consistent results.\n\nThe conversion to and from sRGB adds to filter complexity, and requires Core Image to apply these equations:\n\nrgb = mix(rgb.0.0774, pow(rgb*0.9479 + 0.05213, 2.4), step(0.04045, rgb))\n\n\nrgb = mix(rgb12.92, pow(rgb*0.4167) * 1.055 - 0.055, step(0.00313, rgb))\n\nConsider disabling color management if:\n\nYour app needs the absolute highest performance.\n\nUsers won't notice the quality differences after exaggerated manipulations.\n\nTo disable color management, set the kCIImageColorSpace key to null. If you are using an EAGL context, also set the context colorspace to null when you create the EAGL context. See Building Your Own Workflow with a Core Image Context.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Querying the System for Filters",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_concepts/ci_concepts.html#//apple_ref/doc/uid/TP30001185-CH2-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Programming Guide\nTable of Contents\nIntroduction\nProcessing Images\nDetecting Faces in an Image\nAuto Enhancing Images\nQuerying the System for Filters\nSubclassing CIFilter: Recipes for Custom Effects\nGetting the Best Performance\nUsing Feedback to Process Images\nWhat You Need to Know Before Writing a Custom Filter\nCreating Custom Filters\nPackaging and Loading Image Units\nRevision History\nNext\nPrevious\nQuerying the System for Filters\n\nCore Image provides methods that let you query the system for the available built-in filters and the associated information about each filter—display name, input parameters, parameter types, defaults values, and so on. Querying the system provides you with the most up-to-date information about available filters. If your app supports letting users choose and set filters, you can use this information when creating a user interface for a filter.\n\nGetting a List of Filters and Attributes\n\nUse the filterNamesInCategory: and filterNamesInCategories: methods to discover exactly which filters are available. Filters are categorized to make the list more manageable. If you know a filter category, you can find out the filters available for that category by calling the method filterNamesInCategory: and supplying one of the category constants listed in Table 4-1, Table 4-2, or Table 4-3.\n\nIf you want to find all available filters for a list of categories, you can call the method filterNamesInCategories:, supplying an array of category constants from those listed in the tables. The method returns an NSArray object populated with the filter names for each category. You can obtain a list of all filters for all categories by supplying nil instead of an array of category constants.\n\nA filter can be a member of more than one category. A category can specify:\n\nThe type of effect produced by the filter (color adjustment, distortion, and so forth). See Table 4-1.\n\nThe usage of the filter (still image, video, high dynamic range, and so forth). See Table 4-2.\n\nWhether the filter is provided by Core Image (built-in). See Table 4-3.\n\nTable 4-1  Filter category constants for effect types\n\nEffect type\n\n\t\n\nIndicates\n\n\n\n\nkCICategoryDistortionEffect\n\n\t\n\nDistortion effects, such as bump, twirl, hole\n\n\n\n\nkCICategoryGeometryAdjustment\n\n\t\n\nGeometry adjustment, such as affine transform, crop, perspective transform\n\n\n\n\nkCICategoryCompositeOperation\n\n\t\n\nCompositing, such as source over, minimum, source atop, color dodge blend mode\n\n\n\n\nkCICategoryHalftoneEffect\n\n\t\n\nHalftone effects, such as screen, line screen, hatched\n\n\n\n\nkCICategoryColorAdjustment\n\n\t\n\nColor adjustment, such as gamma adjust, white point adjust, exposure\n\n\n\n\nkCICategoryColorEffect\n\n\t\n\nColor effect, such as hue adjust, posterize\n\n\n\n\nkCICategoryTransition\n\n\t\n\nTransitions between images, such as dissolve, disintegrate with mask, swipe\n\n\n\n\nkCICategoryTileEffect\n\n\t\n\nTile effect, such as parallelogram, triangle\n\n\n\n\nkCICategoryGenerator\n\n\t\n\nImage generator, such as stripes, constant color, checkerboard\n\n\n\n\nkCICategoryGradient\n\n\t\n\nGradient, such as axial, radial, Gaussian\n\n\n\n\nkCICategoryStylize\n\n\t\n\nStylize, such as pixellate, crystallize\n\n\n\n\nkCICategorySharpen\n\n\t\n\nSharpen, luminance\n\n\n\n\nkCICategoryBlur\n\n\t\n\nBlur, such as Gaussian, zoom, motion\n\nTable 4-2  Filter category constants for filter usage\n\nUse\n\n\t\n\nIndicates\n\n\n\n\nkCICategoryStillImage\n\n\t\n\nCan be used for still images\n\n\n\n\nkCICategoryVideo\n\n\t\n\nCan be used for video\n\n\n\n\nkCICategoryInterlaced\n\n\t\n\nCan be used for interlaced images\n\n\n\n\nkCICategoryNonSquarePixels\n\n\t\n\nCan be used for nonsquare pixels\n\n\n\n\nkCICategoryHighDynamicRange\n\n\t\n\nCan be used for high-dynamic range pixels\n\nTable 4-3  Filter category constants for filter origin\n\nFilter origin\n\n\t\n\nIndicates\n\n\n\n\nkCICategoryBuiltIn\n\n\t\n\nA filter provided by Core Image\n\nAfter you obtain a list of filter names, you can retrieve the attributes for a filter by creating a CIFilter object and calling the method attributes as follows:\n\nCIFilter *myFilter = [CIFilter filterWithName:@\"<# Filter Name Here #>\"];\n\n\nNSDictionary *myFilterAttributes = [myFilter attributes];\n\nYou replace the string \"<# Filter Name Here #>\" with the name of the filter you are interested in. Attributes include such things as name, categories, class, minimum, and maximum. See CIFilter Class Reference for the complete list of attributes that can be returned.\n\nBuilding a Dictionary of Filters\n\nIf your app provides a user interface, it can consult a filter dictionary to create and update the user interface. For example, filter attributes that are Boolean would require a checkbox or similar user interface element, and attributes that vary continuously over a range could use a slider. You can use the maximum and minimum values as the basis for text labels. The default attribute setting would dictate the initial setting in the user interface.\n\nFilter names and attributes provide all the information you need to build a user interface that allows users to choose a filter and control its input parameters. The attributes for a filter tell you how many input parameters the filter has, the parameter names, the data type, and the minimum, maximum, and default values.\n\nNote: If you are interested in building a user interface for a Core Image filter, see IKFilterUIView Class Reference, which provides a view that contains input parameter controls for a Core Image filter.\n\nListing 4-1 shows code that gets filter names and builds a dictionary of filters by functional categories. The code retrieves filters in these categories—kCICategoryGeometryAdjustment, kCICategoryDistortionEffect, kCICategorySharpen, and kCICategoryBlur—but builds the dictionary based on app-defined functional categories, such as Distortion and Focus. Functional categories are useful for organizing filter names in a menu that makes sense for the user. The code does not iterate through all possible Core Image filter categories, but you can easily extend this code by following the same process.\n\nListing 4-1  Code that builds a dictionary of filters by functional categories\n\nNSMutableDictionary *filtersByCategory = [NSMutableDictionary dictionary];\n\n\n \n\n\nNSMutableArray *filterNames = [NSMutableArray array];\n\n\n[filterNames addObjectsFromArray:\n\n\n    [CIFilter filterNamesInCategory:kCICategoryGeometryAdjustment]];\n\n\n[filterNames addObjectsFromArray:\n\n\n    [CIFilter filterNamesInCategory:kCICategoryDistortionEffect]];\n\n\nfiltersByCategory[@\"Distortion\"] = [self buildFilterDictionary: filterNames];\n\n\n \n\n\n[filterNames removeAllObjects];\n\n\n[filterNames addObjectsFromArray:\n\n\n    [CIFilter filterNamesInCategory:kCICategorySharpen]];\n\n\n[filterNames addObjectsFromArray:\n\n\n    [CIFilter filterNamesInCategory:kCICategoryBlur]];\n\n\nfiltersByCategory[@\"Focus\"] = [self buildFilterDictionary: filterNames];\n\nListing 4-2 shows the buildFilterDictionary routine called in Listing 4-1. This routine builds a dictionary of attributes for each of the filters in a functional category. A detailed explanation for each numbered line of code follows the listing.\n\nListing 4-2  Building a dictionary of filters by functional name\n\n- (NSMutableDictionary *)buildFilterDictionary:(NSArray *)filterClassNames  // 1\n\n\n{\n\n\n    NSMutableDictionary *filters = [NSMutableDictionary dictionary];\n\n\n    for (NSString *className in filterClassNames) {                         // 2\n\n\n        CIFilter *filter = [CIFilter filterWithName:className];             // 3\n\n\n \n\n\n        if (filter) {\n\n\n            filters[className] = [filter attributes];                       // 4\n\n\n        } else {\n\n\n            NSLog(@\"could not create '%@' filter\", className);\n\n\n        }\n\n\n    }\n\n\n    return filters;\n\n\n}\n\nHere’s what the code does:\n\nTakes an array of filter names as an input parameter. Recall from Listing 4-1 that this array can be a concatenation of filter names from more than one Core Image filter category. In this example, the array is based upon functional categories set up by the app (Distortion or Focus).\n\nIterates through the array of filter names.\n\nRetrieves the filter object for the filter name.\n\nRetrieves the attributes dictionary for a filter and adds it to the dictionary returned by the routine.\n\nNote: Apps that run in OS X v10.5 and later can use the CIFilter Image Kit additions to provide a filter browser and a view for setting filter input parameters. See CIFilter Image Kit Additions and ImageKit Programming Guide.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Auto Enhancing Images",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_autoadjustment/ci_autoadjustmentSAVE.html#//apple_ref/doc/uid/TP30001185-CH11-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Programming Guide\nTable of Contents\nIntroduction\nProcessing Images\nDetecting Faces in an Image\nAuto Enhancing Images\nQuerying the System for Filters\nSubclassing CIFilter: Recipes for Custom Effects\nGetting the Best Performance\nUsing Feedback to Process Images\nWhat You Need to Know Before Writing a Custom Filter\nCreating Custom Filters\nPackaging and Loading Image Units\nRevision History\nNext\nPrevious\nAuto Enhancing Images\n\nThe auto enhancement feature of Core Image analyzes an image for its histogram, face region contents, and metadata properties. It then returns an array of CIFilter objects whose input parameters are already set to values that will improve the analyzed image.\n\nAuto enhancement is available in iOS v5.0 and later and in OS X v10.8 and later.\n\nAuto Enhancement Filters\n\nTable 3-1 shows the filters Core Image uses for automatically enhancing images. These filters remedy some of the most common issues found in photos.\n\nTable 3-1  Filters that Core Image uses to enhance an image\n\nFilter\n\n\t\n\nPurpose\n\n\n\n\nCIRedEyeCorrection\n\n\t\n\nRepairs red/amber/white eye due to camera flash\n\n\n\n\nCIFaceBalance\n\n\t\n\nAdjusts the color of a face to give pleasing skin tones\n\n\n\n\nCIVibrance\n\n\t\n\nIncreases the saturation of an image without distorting the skin tones\n\n\n\n\nCIToneCurve\n\n\t\n\nAdjusts image contrast\n\n\n\n\nCIHighlightShadowAdjust\n\n\t\n\nAdjusts shadow details\n\nUsing Auto Enhancement Filters\n\nThe auto enhancement API has only two methods: autoAdjustmentFilters and autoAdjustmentFiltersWithOptions:. In most cases, you’ll want to use the method that provides an options dictionary.\n\nYou can set these options:\n\nThe image orientation, which is important for the CIRedEyeCorrection and CIFaceBalance filters, so that Core Image can find faces accurately.\n\nWhether to apply only red eye correction. (Set kCIImageAutoAdjustEnhance to false.)\n\nWhether to apply all filters except red eye correction. (Set kCIImageAutoAdjustRedEye to false.)\n\nThe autoAdjustmentFiltersWithOptions: method returns an array of options filters that you’ll then want to chain together and apply to the analyzed image, as shown in Listing 3-1. The code first creates an options dictionary. It then gets the orientation of the image and sets that as the value for the key CIDetectorImageOrientation.\n\nListing 3-1  Getting auto enhancement filters and applying them to an image\n\nNSDictionary *options = @{ CIDetectorImageOrientation :\n\n\n                 [[image properties] valueForKey:kCGImagePropertyOrientation] };\n\n\nNSArray *adjustments = [myImage autoAdjustmentFiltersWithOptions:options];\n\n\nfor (CIFilter *filter in adjustments) {\n\n\n     [filter setValue:myImage forKey:kCIInputImageKey];\n\n\n     myImage = filter.outputImage;\n\n\n}\n\nRecall that the input parameter values are already set by Core Image to produce the best result.\n\nYou don’t have to apply the auto adjustment filters right away. You can save the filter names and parameter values for later. Saving them allows your app to perform the enhancements later without the cost of analyzing the image again.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Detecting Faces in an Image",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_detect_faces/ci_detect_faces.html#//apple_ref/doc/uid/TP30001185-CH8-SW1",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Programming Guide\nTable of Contents\nIntroduction\nProcessing Images\nDetecting Faces in an Image\nAuto Enhancing Images\nQuerying the System for Filters\nSubclassing CIFilter: Recipes for Custom Effects\nGetting the Best Performance\nUsing Feedback to Process Images\nWhat You Need to Know Before Writing a Custom Filter\nCreating Custom Filters\nPackaging and Loading Image Units\nRevision History\nNext\nPrevious\nDetecting Faces in an Image\n\nCore Image can analyze and find human faces in an image. It performs face detection, not recognition. Face detection is the identification of rectangles that contain human face features, whereas face recognition is the identification of specific human faces (John, Mary, and so on). After Core Image detects a face, it can provide information about face features, such as eye and mouth positions. It can also track the position an identified face in a video.\n\nFigure 2-1  Core Image identifies face bounds in an image\n\nKnowing where the faces are in an image lets you perform other operations, such as cropping or adjusting the image quality of the face (tone balance, red-eye correction and so on). You can also perform other interesting operations on the faces; for example:\n\nAnonymous Faces Filter Recipe shows how to apply a pixellate filter only to the faces in an image.\n\nWhite Vignette for Faces Filter Recipe shows how to place a vignette around a face.\n\nNote: Face detection is available in iOS v5.0 and later and in OS X v10.7 and later.\n\nDetecting Faces\n\nUse the CIDetector class to find faces in an image as shown in Listing 2-1.\n\nListing 2-1  Creating a face detector\n\nCIContext *context = [CIContext context];                    // 1\n\n\nNSDictionary *opts = @{ CIDetectorAccuracy : CIDetectorAccuracyHigh };      // 2\n\n\nCIDetector *detector = [CIDetector detectorOfType:CIDetectorTypeFace\n\n\n                                          context:context\n\n\n                                          options:opts];                    // 3\n\n\n \n\n\nopts = @{ CIDetectorImageOrientation :\n\n\n          [[myImage properties] valueForKey:kCGImagePropertyOrientation] }; // 4\n\n\nNSArray *features = [detector featuresInImage:myImage options:opts];        // 5\n\nHere’s what the code does:\n\nCreates a context with default options. You can use any of the context-creation functions described in Processing Images.) You also have the option of supplying nil instead of a context when you create the detector.)\n\nCreates an options dictionary to specify accuracy for the detector. You can specify low or high accuracy. Low accuracy (CIDetectorAccuracyLow) is fast; high accuracy, shown in this example, is thorough but slower.\n\nCreates a detector for faces. The only type of detector you can create is one for human faces.\n\nSets up an options dictionary for finding faces. It’s important to let Core Image know the image orientation so the detector knows where it can find upright faces. Most of the time you’ll read the image orientation from the image itself, and then provide that value to the options dictionary.\n\nUses the detector to find features in an image. The image you provide must be a CIImage object. Core Image returns an array of CIFeature objects, each of which represents a face in the image.\n\nAfter you get an array of faces, you’ll probably want to find out their characteristics, such as where the eyes and mouth are located. The next sections describes how.\n\nGetting Face and Face Feature Bounds\n\nFace features include:\n\nleft and right eye positions\n\nmouth position\n\ntracking ID and tracking frame count which Core Image uses to follow a face in a video segment (available in iOS v6.0 and later and in OS X v10.8 and later)\n\nAfter you get an array of face features from a CIDetector object, you can loop through the array to examine the bounds of each face and each feature in the faces, as shown in Listing 2-2.\n\nListing 2-2  Examining face feature bounds\n\nfor (CIFaceFeature *f in features) {\n\n\n    NSLog(@\"%@\", NSStringFromRect(f.bounds));\n\n\n \n\n\n    if (f.hasLeftEyePosition) {\n\n\n        NSLog(@\"Left eye %g %g\", f.leftEyePosition.x, f.leftEyePosition.y);\n\n\n    }\n\n\n    if (f.hasRightEyePosition) {\n\n\n        NSLog(@\"Right eye %g %g\", f.rightEyePosition.x, f.rightEyePosition.y);\n\n\n    }\n\n\n    if (f.hasMouthPosition) {\n\n\n        NSLog(@\"Mouth %g %g\", f.mouthPosition.x, f.mouthPosition.y);\n\n\n    }\n\n\n}\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "Processing Images",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_tasks/ci_tasks.html#//apple_ref/doc/uid/TP30001185-CH3-TPXREF101",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Programming Guide\nTable of Contents\nIntroduction\nProcessing Images\nDetecting Faces in an Image\nAuto Enhancing Images\nQuerying the System for Filters\nSubclassing CIFilter: Recipes for Custom Effects\nGetting the Best Performance\nUsing Feedback to Process Images\nWhat You Need to Know Before Writing a Custom Filter\nCreating Custom Filters\nPackaging and Loading Image Units\nRevision History\nNext\nPrevious\nProcessing Images\n\nProcessing images means applying filters—an image filter is a piece of software that examines an input image pixel by pixel and algorithmically applies some effect in order to create an output image. In Core Image, image processing relies on the CIFilter and CIImage classes, which describe filters and their input and output. To apply filters and display or export results, you can make use of the integration between Core Image and other system frameworks, or create your own rendering workflow with the CIContext class. This chapter covers the key concepts for working with these classes to apply filters and render results.\n\nOverview\n\nThere are many ways to use Core Image for image processing in your app. Listing 1-1 shows a basic example and provides pointers to further explanations in this chapter.\n\nListing 1-1  The basics of applying a filter to an image\n\nimport CoreImage\n\n\n \n\n\nlet context = CIContext()                                           // 1\n\n\n \n\n\nlet filter = CIFilter(name: \"CISepiaTone\")!                         // 2\n\n\nfilter.setValue(0.8, forKey: kCIInputIntensityKey)\n\n\nlet image = CIImage(contentsOfURL: myURL)                           // 3\n\n\nfilter.setValue(image, forKey: kCIInputImageKey)\n\n\nlet result = filter.outputImage!                                    // 4\n\n\nlet cgImage = context.createCGImage(result, from: result.extent)    // 5\n\nHere’s what the code does:\n\nCreate a CIContext object (with default options). You don’t always need your own Core Image context—often you can integrate with other system frameworks that manage rendering for you. Creating your own context lets you more precisely control the rendering process and the resources involved in rendering. Contexts are heavyweight objects, so if you do create one, do so as early as possible, and reuse it each time you need to process images. (See Building Your Own Workflow with a Core Image Context.)\n\nInstantiate a CIFilter object representing the filter to apply, and provide values for its parameters. (See Filters Describe Image Processing Effects.)\n\nCreate a CIImage object representing the image to be processed, and provide it as the input image parameter to the filter. Reading image data from a URL is just one of many ways to create an image object. (See Images are the Input and Output of Filters.)\n\nGet a CIImage object representing the filter’s output. The filter has not yet executed at this point—the image object is a “recipe” specifying how to create an image with the specified filter, parameters, and input. Core Image performs this recipe only when you request rendering. (See Images are the Input and Output of Filters.)\n\nRender the output image to a Core Graphics image that you can display or save to a file. (See Building Your Own Workflow with a Core Image Context.)\n\nImages are the Input and Output of Filters\n\nCore Image filters process and produce Core Image images. A CIImage instance is an immutable object representing an image. These objects don’t directly represent image bitmap data—instead, a CIImage object is a “recipe” for producing an image. One recipe might call for loading an image from a file; another might represent output from a filter, or from a chain of filters. Core Image performs these recipes only when you request that an image be rendered for display or output.\n\nTo apply a filter, create one or more CIImage objects representing the images to be processed by the filter, and assign them to the input parameters of the filter (such as kCIInputImageKey). You can create a Core Image image object from nearly any source of image data, including:\n\nURLs referencing image files to be loaded or NSData objects containing image file data\n\nQuartz2D, UIKit, or AppKit image representations (CGImageRef, UIImage, or NSBitmapImageRep objects)\n\nMetal, OpenGL, or OpenGL ES textures\n\nCoreVideo image or pixel buffers (CVImageBufferRef or CVPixelBufferRef)\n\nIOSurfaceRef objects that share image data between processes\n\nImage bitmap data in memory (a pointer to such data, or a CIImageProvider object that provides data on demand)\n\nFor a full list of ways to create a CIImage object, see CIImage Class Reference.\n\nBecause a CIImage object describes how to produce an image (instead of containing image data), it can also represent filter output. When you access the outputImage property of a CIFilter object, Core Image merely identifies and stores the steps needed to execute the filter. Those steps are performed only when you request that the image be rendered for display or output. You can request rendering either explicitly, using one of the CIContextrender or draw methods (see Building Your Own Workflow with a Core Image Context), or implicitly, by displaying an image using one of the many system frameworks that work with Core Image (see Integrating with Other Frameworks).\n\nDeferring processing until rendering time makes Core Image fast and efficient. At rendering time, Core Image can see if more than one filter needs to be applied to an image. If so, it automatically concatenates multiple “recipes” and organizes them to eliminate redundant operations, so that each pixel is processed only once rather than many times.\n\nFilters Describe Image Processing Effects\n\nAn instance of the CIFilter class is a mutable object representing an image processing effect and any parameters that control that effect’s behavior. To use a filter, you create a CIFilter object, set its input parameters, and then access its output image (see Images are the Input and Output of Filters below). Call the filterWithName: initializer to instantiate a filter object using the name of a filter known to the system (see Querying the System for Filters or Core Image Filter Reference).\n\nMost filters have one or more input parameters that let you control how processing is done. Each input parameter has an attribute class that specifies its data type, such as NSNumber. An input parameter can optionally have other attributes, such as its default value, the allowable minimum and maximum values, the display name for the parameter, and other attributes described in CIFilter Class Reference. For example, the CIColorMonochrome filter has three input parameters—the image to process, a monochrome color, and the color intensity.\n\nFilter parameters are defined as key-value pairs; to work with parameters, you typically use the valueForKey: and setValue:forKey: methods or other features that build upon key-value coding (such as Core Animation). The key is a constant that identifies the attribute and the value is the setting associated with the key. Core Image attribute values typically use one of the data types listed in Attribute value data types .\n\nTable 1-1  Attribute value data types\n\nData Type\n\n\t\n\nObject\n\n\t\n\nDescription\n\n\n\n\nStrings\n\n\t\n\nNSString\n\n\t\n\nText, typically for display to the user\n\n\n\n\nFloating-point values\n\n\t\n\nNSNumber\n\n\t\n\nA scalar value, such as an intensity level or radius\n\n\n\n\nVectors\n\n\t\n\nCIVector\n\n\t\n\nA set of floating-point values that can specify positions, sizes, rectangles, or untagged color component values\n\n\n\n\nColors\n\n\t\n\nCIColor\n\n\t\n\nA set of color component values, tagged with a color space specifying how to interpret them\n\n\n\n\nImages\n\n\t\n\nCIImage\n\n\t\n\nAn image; see Images are the Input and Output of Filters\n\n\n\n\nTransforms\n\n\t\n\nNSData, NSAffineTransform\n\n\t\n\nA coordinate transformation to apply to an image\n\nImportant: CIFilter objects are mutable, so you cannot safely share them between different threads. Each thread must create its own CIFilter objects. However, a filter’s input and output CIImage objects are immutable, and thus safe to pass between threads.\n\nChaining Filters for Complex Effects\n\nEvery Core Image filter produces an output CIImage object, so you can use this object as input to another filter. For example, the sequence of filters illustrated in Figure 1-1 applies a color effect to an image, then adds a glow effect, and finally crops a section out of the result.\n\nFigure 1-1  Construct a filter chain by connecting filter inputs and outputs\n\nCore Image optimizes the application of filter chains such as this one to render results quickly and efficiently. Each CIImage object in the chain isn’t a fully rendered image, but instead merely a “recipe” for rendering. Core Image doesn’t need to execute each filter individually, wasting time and memory rendering intermediate pixel buffers that will never be seen. Instead, Core Image combines filters into a single operation, and can even reorganize filters when applying them in a different order will produce the same result more efficiently. Figure 1-2 shows a more accurate rendition of the example filter chain from Figure 1-1.\n\nFigure 1-2  Core Image optimizes a filter chain to a single operation\n\nNotice that in Figure 1-2, the crop operation has moved from last to first. That filter results in large areas of the original image being cropped out of the final output. As such, there’s no need to apply the color and sharpen filters to those pixels. By performing the crop first, Core Image ensures that expensive image processing operations apply only to pixels that will be visible in final output.\n\nListing 1-2 shows how to set up a filter chain like that illustrated above.\n\nListing 1-2  Creating a filter chain\n\nfunc applyFilterChain(to image: CIImage) -> CIImage {\n\n\n    // The CIPhotoEffectInstant filter takes only an input image\n\n\n    let colorFilter = CIFilter(name: \"CIPhotoEffectProcess\", withInputParameters:\n\n\n        [kCIInputImageKey: image])!\n\n\n    \n\n\n    // Pass the result of the color filter into the Bloom filter\n\n\n    // and set its parameters for a glowy effect.\n\n\n    let bloomImage = colorFilter.outputImage!.applyingFilter(\"CIBloom\",\n\n\n                                                             withInputParameters: [\n\n\n                                                                kCIInputRadiusKey: 10.0,\n\n\n                                                                kCIInputIntensityKey: 1.0\n\n\n        ])\n\n\n    \n\n\n    // imageByCroppingToRect is a convenience method for\n\n\n    // creating the CICrop filter and accessing its outputImage.\n\n\n    let cropRect = CGRect(x: 350, y: 350, width: 150, height: 150)\n\n\n    let croppedImage = bloomImage.cropping(to: cropRect)\n\n\n    \n\n\n    return croppedImage\n\n\n}\n\nListing 1-2 also shows a few different convenience methods for configuring filters and accessing their results. In summary, you can use any of these methods to apply a filter, either individually or as part of a filter chain:\n\nCreate a CIFilter instance with the filterWithName: initializer, set parameters using the setValue:forKey: method (including the kCIInputImageKey for the image to process), and access the output image with the outputImage property. (See Listing 1-1.)\n\nCreate a CIFilter instance and set its parameters (including the input image) in one call with the filterWithName:withInputParameters: initializer, then use the the outputImage property to access output. (See the colorFilter example in Listing 1-2.)\n\nApply a filter without creating a CIFilter instance by using the imageByApplyingFilter:withInputParameters: method to a CIImage object. (See the bloomImage example in Listing 1-2.)\n\nFor certain commonly used filter operations, such as cropping, clamping, and applying coordinate transforms, use other CIImage instance methods listed in Creating an Image by Modifying an Existing Image. (See the croppedImage example in Listing 1-2.)\n\nUsing Special Filter Types for More Options\n\nMost of the built-in Core Image filters operate on a main input image (possibly with additional input images that affect processing) and create a single output image. But there are several additional types of that you can use to create interesting effects or combine with other filters to produce more complex workflows.\n\nA compositing (or blending) filters combine two images according to a preset formula. For example:\n\nThe CISourceInCompositing filter combines images such that only the areas that are opaque in both input images are visible in the output image.\n\nThe CIMultiplyBlendMode filter multiplies pixel colors from both images, producing a darkened output image.\n\nFor the complete list of compositing filters, query the CICategoryCompositeOperation category.\n\nNote: You can arrange input images before compositing them by applying geometry adjustments to each. See the CICategoryGeometryAdjustment filter category or the imageByApplyingTransform: method.\n\nA generator filters take no input images. Instead, these filters use other input parameters to create a new image from scratch. Some generators produce output that can be useful on its own, and others can be combined in filter chains to produce more interesting images. Some examples from among the built-in Core Image filters include:\n\nFilters like CIQRCodeGenerator and CICode128BarcodeGenerator generate barcode images that encode specified input data.\n\nFilters like CIConstantColorGenerator, CICheckerboardGenerator, and CILinearGradient generate simple procedural images from specified colors. You can combine these with other filters for interesting effects—for example, the CIRadialGradient filter can create a mask for use with the CIMaskedVariableBlur filter.\n\nFilters like CILenticularHaloGenerator and CISunbeamsGenerator create standalone visual effects—combine these with compositing filters to add special effects to an image.\n\nTo find generator filters, query the CICategoryGenerator and CICategoryGradient categories.\n\nA reduction filter operates on an input image, but instead of creating an output image in the traditional sense, its output describes information about the input image. For example:\n\nThe CIAreaMaximum filter outputs a single color value representing the brightest of all pixel colors in a specified area of an image.\n\nThe CIAreaHistogram filter outputs information about the numbers of pixels for each intensity value in a specified area of an image.\n\nAll Core Image filters must produce a CIImage object as their output, so the information produced by a reduction filter is still an image. However, you usually don’t display these images—instead, you read color values from single-pixel or single-row images, or use them as input to other filters.\n\nFor the complete list of reduction filters, query the CICategoryReduction category.\n\nA transition filter takes two input images and varies its output between them in response to an independent variable—typically, this variable is time, so you can use a transition filter to create an animation that starts with one image, ends on another, and progresses from one to the other using an interesting visual effect. Core Image provides several built-in transition filters, including:\n\nThe CIDissolveTransition filter produces a simple cross-dissolve, fading from one image to another.\n\nThe CICopyMachineTransition filter simulates a photocopy machine, swiping a bar of bright light across one image to reveal another.\n\nFor the complete list of transition filters, query the CICategoryTransition category.\n\nIntegrating with Other Frameworks\n\nCore Image interoperates with several other technologies in iOS, macOS, and tvOS. Thanks to this tight integration, you can use Core Image to easily add visual effects to games, video, or images in your app’s user interface without needing to build complex rendering code. The following sections cover several of the common ways to use Core Image in an app and the conveniences system frameworks provide for each.\n\nProcessing Still Images in UIKit and AppKit\n\nUIKit and AppKit provide easy ways to add Core Image processing to still images, whether those images appear in your app’s UI or are part of its workflow. For example:\n\nA travel app might present stock photography of destinations in a list, then apply filters to those images to create a subtle background for each destination’s detail page.\n\nA social app might apply filters to user avatar pictures to indicate mood for each post.\n\nA photography app might allow the user to customize images with filters upon capture, or offer a Photos app extension for adding effects to pictures in the user’s Photos library (see Photo Editing in App Extension Programming Guide).\n\nNote: Don’t use Core Image to create blur effects that are part of a user interface design (like those seen in the translucent sidebars, toolbars, and backgrounds of the macOS, iOS, and tvOS system interfaces). Instead, see the NSVisualEffectView (macOS) or UIVisualEffectView (iOS/tvOS) classes, which automatically match the system appearance and provide efficient real-time rendering.\n\nIn iOS and tvOS, you can apply Core Image filters anywhere you work with UIImage objects. Listing 1-3 shows a simple method for using filters with an image view.\n\nListing 1-3  Applying a filter to an image view (iOS/tvOS)\n\nclass ViewController: UIViewController {\n\n\n    let filter = CIFilter(name: \"CISepiaTone\",\n\n\n                          withInputParameters: [kCIInputIntensityKey: 0.5])!\n\n\n    @IBOutlet var imageView: UIImageView!\n\n\n    \n\n\n    func displayFilteredImage(image: UIImage) {\n\n\n        // Create a Core Image image object for the input image.\n\n\n        let inputImage = CIImage(image: image)!\n\n\n        // Set that image as the filter's input image parameter.\n\n\n        filter.setValue(inputImage, forKey: kCIInputImageKey)\n\n\n        // Get a UIImage representation of the filter's output and display it.\n\n\n        imageView.image = UIImage(CIImage: filter.outputImage!)\n\n\n    }\n\n\n}\n\nIn macOS, use the initWithBitmapImageRep: method to create CIImage objects from bitmap images and the NSCIImageRep class to create images you can use anywhere NSImage objects are supported.\n\nProcessing Video with AV Foundation\n\nThe AVFoundation framework provides a number of high level utilities for working with video and audio content. Among these is the AVVideoComposition class, which you can use to combine or edit video and audio tracks into a single presentation. (For general information on compositions, see Editing in AVFoundation Programming Guide.) You can use an AVVideoComposition object to apply Core Image filters to each frame of a video during playback or export, as shown in Listing 1-4.\n\nListing 1-4  Applying a filter to a video composition\n\nlet filter = CIFilter(name: \"CIGaussianBlur\")!\n\n\nlet composition = AVVideoComposition(asset: asset, applyingCIFiltersWithHandler: { request in\n\n\n    \n\n\n    // Clamp to avoid blurring transparent pixels at the image edges\n\n\n    let source = request.sourceImage.clampingToExtent()\n\n\n    filter.setValue(source, forKey: kCIInputImageKey)\n\n\n    \n\n\n    // Vary filter parameters based on video timing\n\n\n    let seconds = CMTimeGetSeconds(request.compositionTime)\n\n\n    filter.setValue(seconds * 10.0, forKey: kCIInputRadiusKey)\n\n\n    \n\n\n    // Crop the blurred output to the bounds of the original image\n\n\n    let output = filter.outputImage!.cropping(to: request.sourceImage.extent)\n\n\n    \n\n\n    // Provide the filter output to the composition\n\n\n    request.finish(with: output, context: nil)\n\n\n})\n\nWhen you create a composition with the videoCompositionWithAsset:applyingCIFiltersWithHandler: initializer, you supply a handler that’s responsible for applying filters to each frame of video. AVFoundation automatically calls your handler during playback or export. In the handler, you use the provided AVAsynchronousCIImageFilteringRequest object first to retrieve the video frame to be filtered (and supplementary information such as the frame time), then to provide the filtered image for use by the composition.\n\nTo use the created video composition for playback, create an AVPlayerItem object from the same asset used as the composition’s source, then assign the composition to the player item’s videoComposition property. To export the composition to a new movie file, create an AVAssetExportSession object from the same source asset, then assign the composition to the export session’s videoComposition property.\n\nTip: Listing 1-4 also shows another useful Core Image technique. By default, a blur filter also softens the edges of an image by blurring image pixels together with the transparent pixels that (in the filter’s image processing space) surround the image. This effect can be undesirable in some circumstances, such as when filtering video.\n\nTo avoid this effect, use the imageByClampingToExtent method (or the CIAffineClamp filter) to extend the edge pixels of the image infinitely in all directions before blurring. Clamping creates an image of infinite size, so you should also crop the image after blurring.\n\nProcessing Game Content with SpriteKit and SceneKit\n\nSpriteKit is a technology for building 2D games and other types of apps that feature highly dynamic animated content; SceneKit is for working with 3D assets, rendering and animating 3D scenes, and building 3D games. (For more information on each technology, see SpriteKit Programming Guide and SceneKit Framework Reference.) Both frameworks provide high-performance real-time rendering, with easy ways to add Core Image processing to all or part of a scene.\n\nIn SpriteKit, you can add Core Image filters using the SKEffectNode class. To see an example of this class in use, create a new Xcode project using the Game template (for iOS or tvOS), select SpriteKit as the game technology, and modify the touchesBegan:withEvent: method in the GameScene class to use the code in Listing 1-5. (For the macOS Game template, you can make similar modifications to the mouseDown: method.)\n\nListing 1-5  Applying filters in SpriteKit\n\noverride func touchesBegan(touches: Set<UITouch>, withEvent event: UIEvent?) {\n\n\n    for touch in touches {\n\n\n        let sprite = SKSpriteNode(imageNamed:\"Spaceship\")\n\n\n        sprite.setScale(0.5)\n\n\n        sprite.position = touch.location(in: self)\n\n\n        sprite.run(.repeatForever(.rotate(byAngle: 1, duration:1)))\n\n\n        \n\n\n        let effect = SKEffectNode()\n\n\n        effect.addChild(sprite)\n\n\n        effect.shouldEnableEffects = true\n\n\n        effect.filter = CIFilter(name: \"CIPixellate\",\n\n\n                                 withInputParameters: [kCIInputScaleKey: 20.0])\n\n\n        \n\n\n        self.addChild(effect)\n\n\n    }\n\n\n}\n\nNote that the SKScene class itself is a subclass of SKEffectNode, so you can also apply a Core Image filter to an entire SpriteKit scene.\n\nIn SceneKit, the filters property of the SCNNode class can apply Core Image filters to any element of a 3D scene. To see this property in action, create a new Xcode project using the Game template (for iOS, tvOS, or macOS), select SceneKit as the game technology, and modify the viewDidLoad method in the GameViewController class to use the code in Listing 1-6.\n\nListing 1-6  Applying filters in SceneKit\n\n// Find this line in the template code:\n\n\nlet ship = rootNode.childNode(withName: \"ship\", recursively: true)!\n\n\n \n\n\n// Add these lines after it:\n\n\nlet pixellate = CIFilter(name: \"CIPixellate\",\n\n\n                         withInputParameters: [kCIInputScaleKey: 20.0])!\n\n\nship.filters = [ pixellate ]\n\nYou can also animate filter parameters on a SceneKit node—for details, see the reference documentation for the filters property.\n\nIn both SpriteKit and SceneKit, you can use transitions to change a view’s scene with added visual flair. (See the presentScene:transition: method for SpriteKit and the presentScene:withTransition:incomingPointOfView:completionHandler: method for SceneKit.) Use the SKTransition class and its transitionWithCIFilter:duration: initializer to create a transition animation from any Core Image transition filter.\n\nProcessing Core Animation Layers (macOS)\n\nIn macOS, you can use the filters property to apply filters to the contents of any CALayer-backed view, and add animations that vary filter parameters over time. See Filters Add Visual Effects to OS X Views and Advanced Animation Tricks in Core Animation Programming Guide.\n\nBuilding Your Own Workflow with a Core Image Context\n\nWhen you apply Core Image filters using the technologies listed in the previous section, those frameworks automatically manage the underlying resources that Core Image uses to process images and render results for display. This approach both maximizes performance for those workflows and makes them easier to set up. However, in some cases it’s more prudent to manage those resources yourself using the CIContext class. By managing a Core Image context directly, you can precisely control your app’s performance characteristics or integrate Core Image with lower-level rendering technologies.\n\nA Core Image context represents the CPU or GPU computing technology, resources, and settings needed to execute filters and produce images. Several kinds of contexts are available, so you should choose the option that best fits your app’s workflow and the other technologies you may be working with. The sections below discuss some common scenarios; for the full set of options, see CIContext Class Reference.\n\nImportant: A Core Image context is a heavyweight object managing a large amount of resources and state. Repeatedly creating and destroying contexts has a large performance cost, so if you plan to perform multiple image processing operations, create a context early on and store it for future reuse.\n\nRendering with an Automatic Context\n\nIf you don’t have constraints on how your app interoperates with other graphics technologies, creating a Core Image context is simple: just use the basic init or initWithOptions: initializer. When you do so, Core Image automatically manages resources internally, choosing the appropriate or best available CPU or GPU rendering technology based on the current device and any options you specify. This approach is well-suited to tasks such as rendering a processed image for output to a file (for example, with the writeJPEGRepresentationOfImage:toURL:colorSpace:options:error: method).\n\nNote: A context without an explicitly specified rendering destination cannot use the drawImage:inRect:fromRect: method, because that method’s behavior changes depending on the rendering destination in use. Instead, use the CIContext methods whose names begin with render or create to specify an explicit destination.\n\nTake care when using this approach if you intend to render Core Image results in real time—that is, to animate changes in filter parameters, to produce an animated transition effect, or to process video or other visual content that already renders many times per second. Even though a CIContext object created with this approach can automatically render using the GPU, presenting the rendered results may involve expensive copy operations between CPU and GPU memory.\n\nReal-Time Rendering with Metal\n\nThe Metal framework provides low-overhead access to the GPU, enabling high performance for graphics rendering and parallel compute workflows. Such workflows are integral to image processing, so Core Image builds upon Metal wherever possible. If you’re building an app that renders graphics with Metal, or if you want to leverage Metal to get real-time performance for animating filter output or filtering animated input (such as live video), use a Metal device to create your Core Image context.\n\nListing 1-7 and Listing 1-8 show an example of using a MetalKit view (MTKView) to render Core Image output. (Important steps are numbered in each listing and described afterward.)\n\nListing 1-7  Setting up a Metal view for Core Image rendering\n\nclass ViewController: UIViewController, MTKViewDelegate {  // 1\n\n\n    \n\n\n    // Metal resources\n\n\n    var device: MTLDevice!\n\n\n    var commandQueue: MTLCommandQueue!\n\n\n    var sourceTexture: MTLTexture!                         // 2\n\n\n    \n\n\n    // Core Image resources\n\n\n    var context: CIContext!\n\n\n    let filter = CIFilter(name: \"CIGaussianBlur\")!\n\n\n    let colorSpace = CGColorSpaceCreateDeviceRGB()\n\n\n    \n\n\n    override func viewDidLoad() {\n\n\n        super.viewDidLoad()\n\n\n        \n\n\n        device = MTLCreateSystemDefaultDevice()            // 3\n\n\n        commandQueue = device.newCommandQueue()\n\n\n        \n\n\n        let view = self.view as! MTKView                   // 4\n\n\n        view.delegate = self\n\n\n        view.device = device\n\n\n        view.framebufferOnly = false\n\n\n        \n\n\n        context = CIContext(mtlDevice: device)             // 5\n\n\n        \n\n\n        // other setup\n\n\n    }\n\n\n}\n\nThis example uses a UIViewController subclass for iOS or tvOS. To use with macOS, subclass NSViewController instead.\n\nThe sourceTexture property holds a Metal texture containing the image to be processed by the filter. This example doesn’t show loading the texture’s content because there are many ways to fill a texture—for example, you could load an image file using the MTKTextureLoader class, or use the texture as output from an earlier rendering pass of your own.\n\nCreate the Metal objects needed for rendering—a MTLDevice object representing the GPU to use, and a command queue to execute render and compute commands on that GPU. (This command queue can handle both the render or compute commands encoded by Core Image and those from any additional rendering passes of your own.)\n\nConfigure the MetalKit view.\n\nImportant: Always set the framebufferOnly property to NO when using a Metal view, layer, or texture as a Core Image rendering destination.\n\nCreate a Core Image context that uses the same Metal device as the view. By sharing Metal resources, Core Image can process texture contents and render to the view without the performance costs of copying image data to and from separate CPU or GPU memory buffers. CIContext objects are expensive to create, so you do so only once and reuse it each time you process images.\n\nMetalKit calls the drawInMTKView: method each time the view needs displaying. (By default, MetalKit may call this method as many as 60 times per second. For details, see the view’s preferredFramesPerSecond property.) Listing 1-8 shows a basic implementation of that method for rendering from a Core Image context.\n\nListing 1-8  Drawing with Core Image filters in a Metal view\n\npublic func draw(in view: MTKView) {\n\n\n    if let currentDrawable = view.currentDrawable {              // 1\n\n\n        let commandBuffer = commandQueue.commandBuffer()\n\n\n        \n\n\n        let inputImage = CIImage(mtlTexture: sourceTexture)!     // 2\n\n\n        filter.setValue(inputImage, forKey: kCIInputImageKey)\n\n\n        filter.setValue(20.0, forKey: kCIInputRadiusKey)\n\n\n        \n\n\n        context.render(filter.outputImage!,                      // 3\n\n\n            to: currentDrawable.texture,\n\n\n            commandBuffer: commandBuffer,\n\n\n            bounds: inputImage.extent,\n\n\n            colorSpace: colorSpace)\n\n\n        \n\n\n        commandBuffer.present(currentDrawable)                   // 4\n\n\n        commandBuffer.commit()\n\n\n    }\n\n\n}\n\nObtain a Metal drawable texture to render into and a command buffer to encode rendering commands.\n\nConfigure the filter’s input parameters, including the input image sourced from a Metal texture. This example uses constant parameters, but remember that this method runs up to 60 times per second—you can use this opportunity to vary filter parameters over time to create smooth animations.\n\nTell the Core Image context to render the filter output into the view’s drawable texture. The bounds parameter tells Core Image what portion of the image to draw—this example uses the input image’s dimensions.\n\nTell Metal to display the rendered image when the command buffer finishes executing.\n\nThis example shows only the minimal code needed to render with Core Image using Metal. In a real application, you’d likely perform additional rendering passes before or after the one managed by Core Image, or render Core Image output into a secondary texture and use that texture in another rendering pass. For more information on drawing with Metal, see Metal Programming Guide.\n\nReal-Time Rendering with OpenGL or OpenGL ES\n\nCore Image can also use OpenGL (macOS) or OpenGL ES (iOS and tvOS) for high-performance, GPU-based rendering. Use this option if you need to support older hardware where Metal is not available, or if you want to integrate Core Image into an existing OpenGL or OpenGL ES workflow.\n\nIf you draw using OpenGL ES (in iOS or tvOS), use the contextWithEAGLContext:options: initializer to create a Core Image context from the EAGLContext you use for rendering.\n\nIf you draw using OpenGL (in macOS), use the contextWithCGLContext:pixelFormat:colorSpace:options: initializer create a Core Image context from the OpenGL context you use for rendering. (See the reference documentation for that method for important details on pixel formats.)\n\nIn either scenario, use the imageWithTexture:size:flipped:colorSpace: initializer to create CIImage objects from OpenGL or OpenGL ES textures. Working with image data that’s already in GPU memory improves performance by removing redundant copy operations.\n\nTo render Core Image output in OpenGL or OpenGL ES, make your GL context current and set a destination framebuffer, then call the drawImage:inRect:fromRect: method.\n\nCPU-Based Rendering with Quartz 2D\n\nIf your app doesn’t require real-time performance and draws view content using CoreGraphics (for example, in the drawRect: method of a UIKit or AppKit view), use the contextWithCGContext:options: initializer to create a Core Image context that works directly with the Core Graphics context you’re already using for other drawing. (In macOS, use the CIContext property of the current NSGraphicsContext object instead.) For information on CoreGraphics contexts, see Quartz 2D Programming Guide.\n\nNext\nPrevious\n\n\n\n\n\nCopyright © 2004, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  },
  {
    "title": "About Core Image",
    "url": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_intro/ci_intro.html",
    "html": "Documentation Archive\nDeveloper\nSearch\nCore Image Programming Guide\nTable of Contents\nIntroduction\nProcessing Images\nDetecting Faces in an Image\nAuto Enhancing Images\nQuerying the System for Filters\nSubclassing CIFilter: Recipes for Custom Effects\nGetting the Best Performance\nUsing Feedback to Process Images\nWhat You Need to Know Before Writing a Custom Filter\nCreating Custom Filters\nPackaging and Loading Image Units\nRevision History\nNext\nAbout Core Image\n\nCore Image is an image processing and analysis technology designed to provide near real-time processing for still and video images. It operates on image data types from the Core Graphics, Core Video, and Image I/O frameworks, using either a GPU or CPU rendering path. Core Image hides the details of low-level graphics processing by providing an easy-to-use application programming interface (API). You don’t need to know the details of OpenGL, OpenGL ES, or Metal to leverage the power of the GPU, nor do you need to know anything about Grand Central Dispatch (GCD) to get the benefit of multicore processing. Core Image handles the details for you.\n\nFigure I-1  Core Image in relation to the operating system\nAt a Glance\n\nThe Core Image framework provides:\n\nAccess to built-in image processing filters\n\nFeature detection capability\n\nSupport for automatic image enhancement\n\nThe ability to chain multiple filters together to create custom effects\n\nSupport for creating custom filters that run on a GPU\n\nFeedback-based image processing capabilities\n\nOn macOS, Core Image also provides a means for packaging custom filters for use by other apps.\n\nCore Image is Efficient and Easy to Use for Processing and Analyzing Images\n\nCore Image provides hundreds of built-in filters. You set up filters by supplying key-value pairs for a filter’s input parameters. The output of one filter can be the input of another, making it possible to chain numerous filters together to create amazing effects. If you create a compound effect that you want to use again, you can subclass CIFilter to capture the effect “recipe.”\n\nThere are more than a dozen categories of filters. Some are designed to achieve artistic results, such as the stylize and halftone filter categories. Others are optimal for fixing image problems, such as color adjustment and sharpen filters.\n\nCore Image can analyze the quality of an image and provide a set of filters with optimal settings for adjusting such things as hue, contrast, and tone color, and for correcting for flash artifacts such as red eye. It does all this with one method call on your part.\n\nCore Image can detect human face features in still images and track them over time in video images. Knowing where faces are can help you determine where to place a vignette or apply other special filters.\n\nRelevant chapters: Processing Images, Detecting Faces in an Image, Auto Enhancing Images, Subclassing CIFilter: Recipes for Custom Effects\n\nQuery Core Image to Get a List of Filters and Their Attributes\n\nCore Image has “built-in” reference documentation for its filters. You can query the system to find out which filters are available. Then, for each filter, you can retrieve a dictionary that contains its attributes, such as its input parameters, defaults parameter values, minimum and maximum values, display name, and more.\n\nRelevant chapter:  Querying the System for Filters\n\nCore Image Can Achieve Real-Time Video Performance\n\nIf your app needs to process video in real-time, there are several things you can do to optimize performance.\n\nRelevant chapter: Getting the Best Performance\n\nUse an Image Accumulator to Support Feedback-Based Processing\n\nThe CIImageAccumulator class is designed for efficient feedback-based image processing, which you might find useful if your app needs to image dynamical systems.\n\nRelevant chapter:  Using Feedback to Process Images\n\nCreate and Distribute Custom Kernels and Filters\n\nIf none of the built-in filters suits your needs, even when chained together, consider creating a custom filter. You’ll need to understand kernels—programs that operate at the pixel level—because they are at the heart of every filter.\n\nIn macOS, you can package one or more custom filter as an image unit so that other apps can load and use them.\n\nRelevant chapters: What You Need to Know Before Writing a Custom Filter, Creating Custom Filters, Packaging and Loading Image Units\n\nSee Also\n\nOther important documentation for Core Image includes:\n\nCore Image Reference Collection provides a detailed description of the classes available in the Core Image framework.\n\nCore Image Filter Reference describes the built-in image processing filters that Apple provides, and shows how images appear before and after processing with a filter.\n\nCore Image Kernel Language Reference describes the language for creating kernel routines for custom filters.\n\nNext\n\n\n\n\n\nCopyright © 2004, 2016 Apple Inc. All Rights Reserved. Terms of Use | Privacy Policy | Updated: 2016-09-13\n\nHow helpful is this document?\n*\n Very helpful\n Somewhat helpful\n Not helpful\nHow can we improve this document?\n Fix typos or links\n Fix incorrect information\n Add or update code samples\n Add or update illustrations\n Add information about...\n \n*\n\n* Required information\n\nTo submit a product bug or enhancement request, please visit the Bug Reporter page.\n\nPlease read Apple's Unsolicited Idea Submission Policy before you send us your feedback."
  }
]
[
  {
    "title": "VNImageOptionCameraIntrinsics | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimageoptioncameraintrinsics",
    "html": "Discussion\n\nThe camera intrinsics matrix is a CFDataRef instance containing a matrix_float3x3, which is a column-major matrix:\n\nfx and fy are the focal length in pixels. For square pixels, they have the same value.\n\nox and oy are the coordinates of the principal point. The origin is the upper-left corner of the frame.\n\nSee Also\nOptions Dictionary Keys\nVNImageOptionProperties\nThe dictionary from CGImageSourceCopyPropertiesAtIndex containing metadata for algorithms like horizon detection.\nVNImageOptionCIContext\nAn option key to specify the CIContext to be used in the handler's Core Image operations."
  },
  {
    "title": "VNImageOptionCIContext | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimageoptioncicontext",
    "html": "Discussion\n\nIf this key isn't specified, Vision will create its own CIContext.\n\nSpecify a CIContext when you've used one in processing an input CIImage or executing a CIFilter chain, so you can save the cost of creating a new context.\n\nSee Also\nOptions Dictionary Keys\nVNImageOptionProperties\nThe dictionary from CGImageSourceCopyPropertiesAtIndex containing metadata for algorithms like horizon detection.\nVNImageOptionCameraIntrinsics\nAn option to specify the camera intrinstics as an NSData or CFDataRef object representing a matrix_float3x3."
  },
  {
    "title": "VNImageOptionProperties | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimageoptionproperties",
    "html": "See Also\nOptions Dictionary Keys\nVNImageOptionCameraIntrinsics\nAn option to specify the camera intrinstics as an NSData or CFDataRef object representing a matrix_float3x3.\nVNImageOptionCIContext\nAn option key to specify the CIContext to be used in the handler's Core Image operations."
  },
  {
    "title": "initWithURL:orientation:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2869645-initwithurl",
    "html": "Parameters\nimageURL\n\nA URL pointing to the image to be used for performing the requests. The image must be in a format supported by Image I/O. Image content is immutable.\n\norientation\n\nThe orientation of the input image.\n\noptions\n\nAn optional dictionary containing VNImageOption keys to auxiliary image data.\n\nSee Also\nCreating a Request Handler\n- initWithCGImage:options:\nCreates a handler to be used for performing requests on Core Graphics images.\n- initWithCGImage:orientation:options:\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\n- initWithCIImage:options:\nCreates a handler to be used for performing requests on CIImage data.\n- initWithCIImage:orientation:options:\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\n- initWithCVPixelBuffer:options:\nCreates a handler for performing requests on a Core Video pixel buffer.\n- initWithCVPixelBuffer:orientation:options:\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\n- initWithCVPixelBuffer:depthData:orientation:options:\n- initWithCMSampleBuffer:options:\nCreates a request handler that performs requests on an image contained within a sample buffer.\n- initWithCMSampleBuffer:orientation:options:\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\n- initWithCMSampleBuffer:depthData:orientation:options:\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\n- initWithData:options:\nCreates a handler to be used for performing requests on an image contained in an NSData object.\n- initWithData:orientation:options:\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\n- initWithURL:options:\nCreates a handler to be used for performing requests on an image at the specified URL."
  },
  {
    "title": "initWithURL:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2866553-initwithurl",
    "html": "Parameters\nimageURL\n\nA URL pointing to the image to be used for performing the requests. The image must be in a format supported by Image I/O. Image content is immutable.\n\noptions\n\nAn optional dictionary containing VNImageOption keys to auxiliary image data.\n\nSee Also\nCreating a Request Handler\n- initWithCGImage:options:\nCreates a handler to be used for performing requests on Core Graphics images.\n- initWithCGImage:orientation:options:\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\n- initWithCIImage:options:\nCreates a handler to be used for performing requests on CIImage data.\n- initWithCIImage:orientation:options:\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\n- initWithCVPixelBuffer:options:\nCreates a handler for performing requests on a Core Video pixel buffer.\n- initWithCVPixelBuffer:orientation:options:\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\n- initWithCVPixelBuffer:depthData:orientation:options:\n- initWithCMSampleBuffer:options:\nCreates a request handler that performs requests on an image contained within a sample buffer.\n- initWithCMSampleBuffer:orientation:options:\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\n- initWithCMSampleBuffer:depthData:orientation:options:\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\n- initWithData:options:\nCreates a handler to be used for performing requests on an image contained in an NSData object.\n- initWithData:orientation:options:\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\n- initWithURL:orientation:options:\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "initWithData:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2866551-initwithdata",
    "html": "Parameters\nimageData\n\nData containing the image to be used for performing the requests. Image content is immutable.\n\noptions\n\nAn optional dictionary containing VNImageOption keys to auxiliary image data.\n\nDiscussion\n\nThe intended use cases of this type of initializer include compressed images and network downloads, where a client may receive a JPEG from a website or the cloud.\n\nSee Also\nCreating a Request Handler\n- initWithCGImage:options:\nCreates a handler to be used for performing requests on Core Graphics images.\n- initWithCGImage:orientation:options:\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\n- initWithCIImage:options:\nCreates a handler to be used for performing requests on CIImage data.\n- initWithCIImage:orientation:options:\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\n- initWithCVPixelBuffer:options:\nCreates a handler for performing requests on a Core Video pixel buffer.\n- initWithCVPixelBuffer:orientation:options:\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\n- initWithCVPixelBuffer:depthData:orientation:options:\n- initWithCMSampleBuffer:options:\nCreates a request handler that performs requests on an image contained within a sample buffer.\n- initWithCMSampleBuffer:orientation:options:\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\n- initWithCMSampleBuffer:depthData:orientation:options:\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\n- initWithData:orientation:options:\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\n- initWithURL:options:\nCreates a handler to be used for performing requests on an image at the specified URL.\n- initWithURL:orientation:options:\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "initWithCMSampleBuffer:depthData:orientation:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/4144382-initwithcmsamplebuffer",
    "html": "Parameters\nsampleBuffer\n\nThe sample buffer that contains the image to analyze. If the sample buffer doesn’t contain an image buffer with image data, the system raises an error.\n\ndepthData\n\nThe depth data to associate with the image.\n\norientation\n\nThe EXIF orientation of the image.\n\noptions\n\nA dictionary that specifies auxiliary information about the image.\n\nDiscussion\n\nSample buffers may contain metadata, like the camera intrinsics. Vision algorithms that support this metadata use it in their analysis, unless overwritten by the options you specify.\n\nImportant\n\nUse a physical device to perform your testing. Performing requests in Simulator may produce inaccurate results due to the inability of Core Image to render certain pixel formats in this environment.\n\nSee Also\nCreating a Request Handler\n- initWithCGImage:options:\nCreates a handler to be used for performing requests on Core Graphics images.\n- initWithCGImage:orientation:options:\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\n- initWithCIImage:options:\nCreates a handler to be used for performing requests on CIImage data.\n- initWithCIImage:orientation:options:\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\n- initWithCVPixelBuffer:options:\nCreates a handler for performing requests on a Core Video pixel buffer.\n- initWithCVPixelBuffer:orientation:options:\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\n- initWithCVPixelBuffer:depthData:orientation:options:\n- initWithCMSampleBuffer:options:\nCreates a request handler that performs requests on an image contained within a sample buffer.\n- initWithCMSampleBuffer:orientation:options:\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\n- initWithData:options:\nCreates a handler to be used for performing requests on an image contained in an NSData object.\n- initWithData:orientation:options:\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\n- initWithURL:options:\nCreates a handler to be used for performing requests on an image at the specified URL.\n- initWithURL:orientation:options:\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "initWithData:orientation:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2869635-initwithdata",
    "html": "Parameters\nimageData\n\nData containing the image to be used for performing the requests. Image content is immutable.\n\norientation\n\nThe orientation of the input image.\n\noptions\n\nAn optional dictionary containing VNImageOption keys to auxiliary image data.\n\nDiscussion\n\nThe intended use cases of this type of initializer include compressed images and network downloads, where a client may receive a JPEG from a website or the cloud.\n\nSee Also\nCreating a Request Handler\n- initWithCGImage:options:\nCreates a handler to be used for performing requests on Core Graphics images.\n- initWithCGImage:orientation:options:\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\n- initWithCIImage:options:\nCreates a handler to be used for performing requests on CIImage data.\n- initWithCIImage:orientation:options:\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\n- initWithCVPixelBuffer:options:\nCreates a handler for performing requests on a Core Video pixel buffer.\n- initWithCVPixelBuffer:orientation:options:\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\n- initWithCVPixelBuffer:depthData:orientation:options:\n- initWithCMSampleBuffer:options:\nCreates a request handler that performs requests on an image contained within a sample buffer.\n- initWithCMSampleBuffer:orientation:options:\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\n- initWithCMSampleBuffer:depthData:orientation:options:\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\n- initWithData:options:\nCreates a handler to be used for performing requests on an image contained in an NSData object.\n- initWithURL:options:\nCreates a handler to be used for performing requests on an image at the specified URL.\n- initWithURL:orientation:options:\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "initWithCMSampleBuffer:orientation:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/3548374-initwithcmsamplebuffer",
    "html": "Parameters\nsampleBuffer\n\nThe sample buffer that contains the image to analyze. If the sample buffer doesn’t contain an image buffer with image data, the system raises an error.\n\norientation\n\nThe EXIF orientation of the image.\n\noptions\n\nA dictionary that specifies auxiliary information about the image.\n\nDiscussion\n\nSample buffers may contain metadata, like the camera intrinsics. Vision algorithms that support this metadata use it in their analysis, unless overwritten by the options you specify.\n\nImportant\n\nUse a physical device to perform your testing. Performing requests in Simulator may produce inaccurate results due to the inability of Core Image to render certain pixel formats in this environment.\n\nSee Also\nCreating a Request Handler\n- initWithCGImage:options:\nCreates a handler to be used for performing requests on Core Graphics images.\n- initWithCGImage:orientation:options:\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\n- initWithCIImage:options:\nCreates a handler to be used for performing requests on CIImage data.\n- initWithCIImage:orientation:options:\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\n- initWithCVPixelBuffer:options:\nCreates a handler for performing requests on a Core Video pixel buffer.\n- initWithCVPixelBuffer:orientation:options:\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\n- initWithCVPixelBuffer:depthData:orientation:options:\n- initWithCMSampleBuffer:options:\nCreates a request handler that performs requests on an image contained within a sample buffer.\n- initWithCMSampleBuffer:depthData:orientation:options:\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\n- initWithData:options:\nCreates a handler to be used for performing requests on an image contained in an NSData object.\n- initWithData:orientation:options:\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\n- initWithURL:options:\nCreates a handler to be used for performing requests on an image at the specified URL.\n- initWithURL:orientation:options:\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "initWithCMSampleBuffer:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/3548373-initwithcmsamplebuffer",
    "html": "Parameters\nsampleBuffer\n\nThe sample buffer that contains the image to analyze. If the sample buffer doesn’t contain an image buffer with image data, the system raises an error.\n\noptions\n\nA dictionary that specifies auxiliary information about the image.\n\nDiscussion\n\nSample buffers may contain metadata, like the camera intrinsics. Vision algorithms that support this metadata use it in their analysis, unless overwritten by the options you specify.\n\nImportant\n\nUse a physical device to perform your testing. Performing requests in Simulator may produce inaccurate results due to the inability of Core Image to render certain pixel formats in this environment.\n\nSee Also\nCreating a Request Handler\n- initWithCGImage:options:\nCreates a handler to be used for performing requests on Core Graphics images.\n- initWithCGImage:orientation:options:\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\n- initWithCIImage:options:\nCreates a handler to be used for performing requests on CIImage data.\n- initWithCIImage:orientation:options:\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\n- initWithCVPixelBuffer:options:\nCreates a handler for performing requests on a Core Video pixel buffer.\n- initWithCVPixelBuffer:orientation:options:\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\n- initWithCVPixelBuffer:depthData:orientation:options:\n- initWithCMSampleBuffer:orientation:options:\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\n- initWithCMSampleBuffer:depthData:orientation:options:\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\n- initWithData:options:\nCreates a handler to be used for performing requests on an image contained in an NSData object.\n- initWithData:orientation:options:\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\n- initWithURL:options:\nCreates a handler to be used for performing requests on an image at the specified URL.\n- initWithURL:orientation:options:\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "initWithCVPixelBuffer:depthData:orientation:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/4144383-initwithcvpixelbuffer",
    "html": "See Also\nCreating a Request Handler\n- initWithCGImage:options:\nCreates a handler to be used for performing requests on Core Graphics images.\n- initWithCGImage:orientation:options:\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\n- initWithCIImage:options:\nCreates a handler to be used for performing requests on CIImage data.\n- initWithCIImage:orientation:options:\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\n- initWithCVPixelBuffer:options:\nCreates a handler for performing requests on a Core Video pixel buffer.\n- initWithCVPixelBuffer:orientation:options:\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\n- initWithCMSampleBuffer:options:\nCreates a request handler that performs requests on an image contained within a sample buffer.\n- initWithCMSampleBuffer:orientation:options:\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\n- initWithCMSampleBuffer:depthData:orientation:options:\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\n- initWithData:options:\nCreates a handler to be used for performing requests on an image contained in an NSData object.\n- initWithData:orientation:options:\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\n- initWithURL:options:\nCreates a handler to be used for performing requests on an image at the specified URL.\n- initWithURL:orientation:options:\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "initWithCVPixelBuffer:orientation:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2880303-initwithcvpixelbuffer",
    "html": "Parameters\npixelBuffer\n\nA pixel buffer that contains the image to use for performing the requests. The contents can’t change for the lifetime of the request handler.\n\norientation\n\nThe orientation of the input image.\n\noptions\n\nA dictionary that specifies auxiliary information about the image.\n\nSee Also\nCreating a Request Handler\n- initWithCGImage:options:\nCreates a handler to be used for performing requests on Core Graphics images.\n- initWithCGImage:orientation:options:\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\n- initWithCIImage:options:\nCreates a handler to be used for performing requests on CIImage data.\n- initWithCIImage:orientation:options:\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\n- initWithCVPixelBuffer:options:\nCreates a handler for performing requests on a Core Video pixel buffer.\n- initWithCVPixelBuffer:depthData:orientation:options:\n- initWithCMSampleBuffer:options:\nCreates a request handler that performs requests on an image contained within a sample buffer.\n- initWithCMSampleBuffer:orientation:options:\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\n- initWithCMSampleBuffer:depthData:orientation:options:\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\n- initWithData:options:\nCreates a handler to be used for performing requests on an image contained in an NSData object.\n- initWithData:orientation:options:\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\n- initWithURL:options:\nCreates a handler to be used for performing requests on an image at the specified URL.\n- initWithURL:orientation:options:\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "initWithCIImage:orientation:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2869641-initwithciimage",
    "html": "Parameters\nimage\n\nA CIImage containing the image to be used for performing the requests. Image content is immutable.\n\norientation\n\nThe orientation of the input image.\n\noptions\n\nAn optional dictionary containing VNImageOption keys to auxiliary image data.\n\nSee Also\nCreating a Request Handler\n- initWithCGImage:options:\nCreates a handler to be used for performing requests on Core Graphics images.\n- initWithCGImage:orientation:options:\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\n- initWithCIImage:options:\nCreates a handler to be used for performing requests on CIImage data.\n- initWithCVPixelBuffer:options:\nCreates a handler for performing requests on a Core Video pixel buffer.\n- initWithCVPixelBuffer:orientation:options:\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\n- initWithCVPixelBuffer:depthData:orientation:options:\n- initWithCMSampleBuffer:options:\nCreates a request handler that performs requests on an image contained within a sample buffer.\n- initWithCMSampleBuffer:orientation:options:\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\n- initWithCMSampleBuffer:depthData:orientation:options:\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\n- initWithData:options:\nCreates a handler to be used for performing requests on an image contained in an NSData object.\n- initWithData:orientation:options:\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\n- initWithURL:options:\nCreates a handler to be used for performing requests on an image at the specified URL.\n- initWithURL:orientation:options:\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "initWithCVPixelBuffer:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2880309-initwithcvpixelbuffer",
    "html": "Parameters\npixelBuffer\n\nA pixel buffer that contains the image to use for performing the requests. The contents can’t change for the lifetime of the request handler.\n\noptions\n\nA dictionary that specifies auxiliary information about the image.\n\nSee Also\nCreating a Request Handler\n- initWithCGImage:options:\nCreates a handler to be used for performing requests on Core Graphics images.\n- initWithCGImage:orientation:options:\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\n- initWithCIImage:options:\nCreates a handler to be used for performing requests on CIImage data.\n- initWithCIImage:orientation:options:\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\n- initWithCVPixelBuffer:orientation:options:\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\n- initWithCVPixelBuffer:depthData:orientation:options:\n- initWithCMSampleBuffer:options:\nCreates a request handler that performs requests on an image contained within a sample buffer.\n- initWithCMSampleBuffer:orientation:options:\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\n- initWithCMSampleBuffer:depthData:orientation:options:\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\n- initWithData:options:\nCreates a handler to be used for performing requests on an image contained in an NSData object.\n- initWithData:orientation:options:\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\n- initWithURL:options:\nCreates a handler to be used for performing requests on an image at the specified URL.\n- initWithURL:orientation:options:\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "initWithCIImage:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2866549-initwithciimage",
    "html": "Parameters\nimage\n\nA CIImage containing the image to be used for performing the requests. Image content is immutable.\n\noptions\n\nAn optional dictionary containing VNImageOptionProperties keys to auxiliary image data.\n\nSee Also\nCreating a Request Handler\n- initWithCGImage:options:\nCreates a handler to be used for performing requests on Core Graphics images.\n- initWithCGImage:orientation:options:\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\n- initWithCIImage:orientation:options:\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\n- initWithCVPixelBuffer:options:\nCreates a handler for performing requests on a Core Video pixel buffer.\n- initWithCVPixelBuffer:orientation:options:\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\n- initWithCVPixelBuffer:depthData:orientation:options:\n- initWithCMSampleBuffer:options:\nCreates a request handler that performs requests on an image contained within a sample buffer.\n- initWithCMSampleBuffer:orientation:options:\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\n- initWithCMSampleBuffer:depthData:orientation:options:\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\n- initWithData:options:\nCreates a handler to be used for performing requests on an image contained in an NSData object.\n- initWithData:orientation:options:\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\n- initWithURL:options:\nCreates a handler to be used for performing requests on an image at the specified URL.\n- initWithURL:orientation:options:\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "initWithCGImage:orientation:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2869629-initwithcgimage",
    "html": "Parameters\nimage\n\nA CGImageRef containing the image to be used for performing the requests. Image content is immutable.\n\norientation\n\nThe orientation of the input image.\n\noptions\n\nAn optional dictionary containing VNImageOption keys to auxiliary image data.\n\nSee Also\nCreating a Request Handler\n- initWithCGImage:options:\nCreates a handler to be used for performing requests on Core Graphics images.\n- initWithCIImage:options:\nCreates a handler to be used for performing requests on CIImage data.\n- initWithCIImage:orientation:options:\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\n- initWithCVPixelBuffer:options:\nCreates a handler for performing requests on a Core Video pixel buffer.\n- initWithCVPixelBuffer:orientation:options:\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\n- initWithCVPixelBuffer:depthData:orientation:options:\n- initWithCMSampleBuffer:options:\nCreates a request handler that performs requests on an image contained within a sample buffer.\n- initWithCMSampleBuffer:orientation:options:\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\n- initWithCMSampleBuffer:depthData:orientation:options:\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\n- initWithData:options:\nCreates a handler to be used for performing requests on an image contained in an NSData object.\n- initWithData:orientation:options:\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\n- initWithURL:options:\nCreates a handler to be used for performing requests on an image at the specified URL.\n- initWithURL:orientation:options:\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "initWithCGImage:options: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2866541-initwithcgimage",
    "html": "Parameters\nimage\n\nA CGImageRef containing the image to be used for performing the requests. Image content is immutable.\n\noptions\n\nAn optional dictionary containing VNImageOption keys to auxiliary image data.\n\nSee Also\nCreating a Request Handler\n- initWithCGImage:orientation:options:\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\n- initWithCIImage:options:\nCreates a handler to be used for performing requests on CIImage data.\n- initWithCIImage:orientation:options:\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\n- initWithCVPixelBuffer:options:\nCreates a handler for performing requests on a Core Video pixel buffer.\n- initWithCVPixelBuffer:orientation:options:\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\n- initWithCVPixelBuffer:depthData:orientation:options:\n- initWithCMSampleBuffer:options:\nCreates a request handler that performs requests on an image contained within a sample buffer.\n- initWithCMSampleBuffer:orientation:options:\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\n- initWithCMSampleBuffer:depthData:orientation:options:\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\n- initWithData:options:\nCreates a handler to be used for performing requests on an image contained in an NSData object.\n- initWithData:orientation:options:\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\n- initWithURL:options:\nCreates a handler to be used for performing requests on an image at the specified URL.\n- initWithURL:orientation:options:\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessingoption/3551606-init",
    "html": "Parameters\nrawValue\n\nA string value that represents a frameCadence or timeInterval."
  },
  {
    "title": "timeInterval | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessingoption/3548381-timeinterval",
    "html": "See Also\nOptions\nstatic let frameCadence: VNVideoProcessingOption\nA value that indicates the video frame cadence at which to perform the video processing.\nDeprecated"
  },
  {
    "title": "frameCadence | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessingoption/3548380-framecadence",
    "html": "See Also\nOptions\nstatic let timeInterval: VNVideoProcessingOption\nA value that indicates that the video processor should perform a request every n-seconds.\nDeprecated"
  },
  {
    "title": "init(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor/frameratecadence/3675682-init",
    "html": "Parameters\nframeRate\n\nThe frame rate at which to process video."
  },
  {
    "title": "init(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor/timeintervalcadence/3675686-init",
    "html": "Parameters\ntimeInterval\n\nThe time interval at which to process video."
  },
  {
    "title": "performRequests:error: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2880297-performrequests",
    "html": "Parameters\nrequests\n\nA nonempty array of VNRequest instances to perform.\n\nerror\n\nAn optional error parameter populated when problems arise in scheduling the requests. Check if the return value is NO.\n\nReturn Value\n\nReturns YES if all requests were scheduled and performed. If the return value is NO, check the error parameter.\n\nDiscussion\n\nThe function returns after all requests have either completed or failed. Check individual requests and errors for their respective successes and failures."
  },
  {
    "title": "code93 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2923468-code93",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "childContourCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontour/3548322-childcontourcount",
    "html": "See Also\nAccessing Child Contours\nvar childContours: [VNContour]\nAn array of contours that this contour encloses.\nfunc childContour(at: Int) -> VNContour\nRetrieves the child contour object at the specified index."
  },
  {
    "title": "VNGenerateOpticalFlowRequest.ComputationAccuracy.medium | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateopticalflowrequest/computationaccuracy/medium",
    "html": "See Also\nAccuracy Levels\ncase low\nLow accuracy.\ncase high\nHigh accuracy.\ncase veryHigh\nVery high accuracy."
  },
  {
    "title": "rightPupil | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks2d/2879441-rightpupil",
    "html": "Discussion\n\nThis value may be inaccurate if the eye is blinking.\n\nSee Also\nFace Landmark Points\nvar allPoints: VNFaceLandmarkRegion2D?\nThe region containing all face landmark points.\nvar faceContour: VNFaceLandmarkRegion2D?\nThe region containing points that trace the face contour from the left cheek, over the chin, to the right cheek.\nvar leftEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the left eye.\nvar rightEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the right eye.\nvar leftEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the left eyebrow.\nvar rightEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the right eyebrow.\nvar nose: VNFaceLandmarkRegion2D?\nThe region containing points that outline the nose.\nvar noseCrest: VNFaceLandmarkRegion2D?\nThe region containing points that trace the center crest of the nose.\nvar medianLine: VNFaceLandmarkRegion2D?\nThe region containing points that trace a vertical line down the center of the face.\nvar outerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the outside of the lips.\nvar innerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the space between the lips.\nvar leftPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the left pupil is located."
  },
  {
    "title": "Code39FullASCII | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795572-code39fullascii",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "leftEarTop | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173250-lefteartop",
    "html": "See Also\nGetting the Head Joint Names\nstatic let leftEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the left ear.\nstatic let leftEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the left ear.\nstatic let leftEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the left eye.\nstatic let neck: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the neck.\nstatic let nose: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the nose.\nstatic let rightEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the right eye.\nstatic let rightEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the right ear.\nstatic let rightEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the right ear.\nstatic let rightEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the right ear."
  },
  {
    "title": "childContour(at:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontour/3548321-childcontour",
    "html": "Parameters\nchildContourIndex\n\nThe child contour index value.\n\nReturn Value\n\nThe child contour object.\n\nSee Also\nAccessing Child Contours\nvar childContourCount: Int\nThe total number of detected child contours.\nvar childContours: [VNContour]\nAn array of contours that this contour encloses."
  },
  {
    "title": "confidence | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks/2909054-confidence",
    "html": "Discussion\n\nA value of 0 indicates no confidence. A value of 1 indicates full confidence."
  },
  {
    "title": "code39FullASCIIChecksum | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2923470-code39fullasciichecksum",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "VNElementType.float | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnelementtype/float",
    "html": "See Also\nElement Types\ncase unknown\nThe element type isn't known.\ncase double\nThe elements are double-precision floating-point numbers."
  },
  {
    "title": "VNElementType.double | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnelementtype/double",
    "html": "See Also\nElement Types\ncase unknown\nThe element type isn't known.\ncase float\nThe elements are floating-point numbers."
  },
  {
    "title": "indexPath | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontour/3548324-indexpath",
    "html": "See Also\nInspecting the Contour\nvar aspectRatio: Float\nThe aspect ratio of the contour.\nvar normalizedPath: CGPath\nThe contour object as a path in normalized coordinates.\nvar pointCount: Int\nThe contour’s number of points.\nvar normalizedPoints: [simd_float2]\nThe contour’s array of points in normalized coordinates.\nfunc polygonApproximation(epsilon: Float) -> VNContour\nSimplifies the contour to a polygon using a Ramer-Douglas-Peucker algorithm."
  },
  {
    "title": "VNElementType.unknown | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnelementtype/unknown",
    "html": "See Also\nElement Types\ncase float\nThe elements are floating-point numbers.\ncase double\nThe elements are double-precision floating-point numbers."
  },
  {
    "title": "VNVideoProcessingOption | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessingoption",
    "html": "Topics\nOptions\nstatic let frameCadence: VNVideoProcessingOption\nA value that indicates the video frame cadence at which to perform the video processing.\nstatic let timeInterval: VNVideoProcessingOption\nA value that indicates that the video processor should perform a request every n-seconds.\nInitializers\ninit(rawValue: String)\nCreates an option with a string value.\nRelationships\nConforms To\nEquatable\nHashable\nRawRepresentable\nSendable"
  },
  {
    "title": "aspectRatio | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontour/3600614-aspectratio",
    "html": "Discussion\n\nThe aspect ratio is the original image’s width divided by its height.\n\nSee Also\nInspecting the Contour\nvar indexPath: IndexPath\nThe contour object’s index path.\nvar normalizedPath: CGPath\nThe contour object as a path in normalized coordinates.\nvar pointCount: Int\nThe contour’s number of points.\nvar normalizedPoints: [simd_float2]\nThe contour’s array of points in normalized coordinates.\nfunc polygonApproximation(epsilon: Float) -> VNContour\nSimplifies the contour to a polygon using a Ramer-Douglas-Peucker algorithm."
  },
  {
    "title": "normalizedPath | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontour/3548325-normalizedpath",
    "html": "See Also\nInspecting the Contour\nvar aspectRatio: Float\nThe aspect ratio of the contour.\nvar indexPath: IndexPath\nThe contour object’s index path.\nvar pointCount: Int\nThe contour’s number of points.\nvar normalizedPoints: [simd_float2]\nThe contour’s array of points in normalized coordinates.\nfunc polygonApproximation(epsilon: Float) -> VNContour\nSimplifies the contour to a polygon using a Ramer-Douglas-Peucker algorithm."
  },
  {
    "title": "normalizedPoints | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontour/3650333-normalizedpoints",
    "html": "Discussion\n\nThis property value provides the address of the buffer that contain the array of CGPoint values.\n\nSee Also\nInspecting the Contour\nvar aspectRatio: Float\nThe aspect ratio of the contour.\nvar indexPath: IndexPath\nThe contour object’s index path.\nvar normalizedPath: CGPath\nThe contour object as a path in normalized coordinates.\nvar pointCount: Int\nThe contour’s number of points.\nfunc polygonApproximation(epsilon: Float) -> VNContour\nSimplifies the contour to a polygon using a Ramer-Douglas-Peucker algorithm."
  },
  {
    "title": "polygonApproximation(epsilon:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontour/3618958-polygonapproximation",
    "html": "Parameters\nepsilon\n\nThis parameter defines the distance threshold the algorithm uses. It preserves points whose perpendicular distance to the line segment they are on is greater than epsilon, and removes all others.\n\nReturn Value\n\nA simplified polygon contour from the points of the original contour.\n\nSee Also\nInspecting the Contour\nvar aspectRatio: Float\nThe aspect ratio of the contour.\nvar indexPath: IndexPath\nThe contour object’s index path.\nvar normalizedPath: CGPath\nThe contour object as a path in normalized coordinates.\nvar pointCount: Int\nThe contour’s number of points.\nvar normalizedPoints: [simd_float2]\nThe contour’s array of points in normalized coordinates."
  },
  {
    "title": "cadence | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor/requestprocessingoptions/3675684-cadence",
    "html": "Discussion\n\nBy default, the system processes every frame of video if you don’t provide a value for this property.\n\nSee Also\nConfiguring Options\nclass VNVideoProcessor.Cadence\nAn object that defines the cadence at which to process video.\nclass VNVideoProcessor.FrameRateCadence\nAn object that defines a frame-based cadence for processing a video stream.\nclass VNVideoProcessor.TimeIntervalCadence\nAn object that defines a time-based cadence for processing a video stream."
  },
  {
    "title": "VNVideoProcessor.FrameRateCadence | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor/frameratecadence",
    "html": "Topics\nCreating a Cadence\ninit(Int)\nCreates a new frame-based cadence with a frame rate.\nInspecting the Frame Rate\nvar frameRate: Int\nThe frame rate at which to process video.\nRelationships\nInherits From\nVNVideoProcessor.Cadence\nSee Also\nConfiguring Options\nvar cadence: VNVideoProcessor.Cadence?\nThe cadence the video processor maintains to process the request.\nclass VNVideoProcessor.Cadence\nAn object that defines the cadence at which to process video.\nclass VNVideoProcessor.TimeIntervalCadence\nAn object that defines a time-based cadence for processing a video stream."
  },
  {
    "title": "allPoints | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks2d/2879430-allpoints",
    "html": "See Also\nFace Landmark Points\nvar faceContour: VNFaceLandmarkRegion2D?\nThe region containing points that trace the face contour from the left cheek, over the chin, to the right cheek.\nvar leftEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the left eye.\nvar rightEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the right eye.\nvar leftEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the left eyebrow.\nvar rightEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the right eyebrow.\nvar nose: VNFaceLandmarkRegion2D?\nThe region containing points that outline the nose.\nvar noseCrest: VNFaceLandmarkRegion2D?\nThe region containing points that trace the center crest of the nose.\nvar medianLine: VNFaceLandmarkRegion2D?\nThe region containing points that trace a vertical line down the center of the face.\nvar outerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the outside of the lips.\nvar innerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the space between the lips.\nvar leftPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the left pupil is located.\nvar rightPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the right pupil is located."
  },
  {
    "title": "VNVideoProcessor.TimeIntervalCadence | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor/timeintervalcadence",
    "html": "Topics\nCreating a Cadence\ninit(CFTimeInterval)\nCreates a new time-based cadence with a time interval.\nInspecting the Time Interval\nvar timeInterval: CFTimeInterval\nThe time interval of the cadence.\nRelationships\nInherits From\nVNVideoProcessor.Cadence\nSee Also\nConfiguring Options\nvar cadence: VNVideoProcessor.Cadence?\nThe cadence the video processor maintains to process the request.\nclass VNVideoProcessor.Cadence\nAn object that defines the cadence at which to process video.\nclass VNVideoProcessor.FrameRateCadence\nAn object that defines a frame-based cadence for processing a video stream."
  },
  {
    "title": "rightEyebrow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks2d/2879432-righteyebrow",
    "html": "See Also\nFace Landmark Points\nvar allPoints: VNFaceLandmarkRegion2D?\nThe region containing all face landmark points.\nvar faceContour: VNFaceLandmarkRegion2D?\nThe region containing points that trace the face contour from the left cheek, over the chin, to the right cheek.\nvar leftEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the left eye.\nvar rightEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the right eye.\nvar leftEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the left eyebrow.\nvar nose: VNFaceLandmarkRegion2D?\nThe region containing points that outline the nose.\nvar noseCrest: VNFaceLandmarkRegion2D?\nThe region containing points that trace the center crest of the nose.\nvar medianLine: VNFaceLandmarkRegion2D?\nThe region containing points that trace a vertical line down the center of the face.\nvar outerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the outside of the lips.\nvar innerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the space between the lips.\nvar leftPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the left pupil is located.\nvar rightPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the right pupil is located."
  },
  {
    "title": "leftEyebrow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks2d/2879438-lefteyebrow",
    "html": "See Also\nFace Landmark Points\nvar allPoints: VNFaceLandmarkRegion2D?\nThe region containing all face landmark points.\nvar faceContour: VNFaceLandmarkRegion2D?\nThe region containing points that trace the face contour from the left cheek, over the chin, to the right cheek.\nvar leftEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the left eye.\nvar rightEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the right eye.\nvar rightEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the right eyebrow.\nvar nose: VNFaceLandmarkRegion2D?\nThe region containing points that outline the nose.\nvar noseCrest: VNFaceLandmarkRegion2D?\nThe region containing points that trace the center crest of the nose.\nvar medianLine: VNFaceLandmarkRegion2D?\nThe region containing points that trace a vertical line down the center of the face.\nvar outerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the outside of the lips.\nvar innerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the space between the lips.\nvar leftPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the left pupil is located.\nvar rightPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the right pupil is located."
  },
  {
    "title": "leftEye | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks2d/2879426-lefteye",
    "html": "See Also\nFace Landmark Points\nvar allPoints: VNFaceLandmarkRegion2D?\nThe region containing all face landmark points.\nvar faceContour: VNFaceLandmarkRegion2D?\nThe region containing points that trace the face contour from the left cheek, over the chin, to the right cheek.\nvar rightEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the right eye.\nvar leftEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the left eyebrow.\nvar rightEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the right eyebrow.\nvar nose: VNFaceLandmarkRegion2D?\nThe region containing points that outline the nose.\nvar noseCrest: VNFaceLandmarkRegion2D?\nThe region containing points that trace the center crest of the nose.\nvar medianLine: VNFaceLandmarkRegion2D?\nThe region containing points that trace a vertical line down the center of the face.\nvar outerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the outside of the lips.\nvar innerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the space between the lips.\nvar leftPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the left pupil is located.\nvar rightPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the right pupil is located."
  },
  {
    "title": "rightEye | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks2d/2879442-righteye",
    "html": "See Also\nFace Landmark Points\nvar allPoints: VNFaceLandmarkRegion2D?\nThe region containing all face landmark points.\nvar faceContour: VNFaceLandmarkRegion2D?\nThe region containing points that trace the face contour from the left cheek, over the chin, to the right cheek.\nvar leftEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the left eye.\nvar leftEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the left eyebrow.\nvar rightEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the right eyebrow.\nvar nose: VNFaceLandmarkRegion2D?\nThe region containing points that outline the nose.\nvar noseCrest: VNFaceLandmarkRegion2D?\nThe region containing points that trace the center crest of the nose.\nvar medianLine: VNFaceLandmarkRegion2D?\nThe region containing points that trace a vertical line down the center of the face.\nvar outerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the outside of the lips.\nvar innerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the space between the lips.\nvar leftPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the left pupil is located.\nvar rightPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the right pupil is located."
  },
  {
    "title": "medianLine | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks2d/2879427-medianline",
    "html": "See Also\nFace Landmark Points\nvar allPoints: VNFaceLandmarkRegion2D?\nThe region containing all face landmark points.\nvar faceContour: VNFaceLandmarkRegion2D?\nThe region containing points that trace the face contour from the left cheek, over the chin, to the right cheek.\nvar leftEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the left eye.\nvar rightEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the right eye.\nvar leftEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the left eyebrow.\nvar rightEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the right eyebrow.\nvar nose: VNFaceLandmarkRegion2D?\nThe region containing points that outline the nose.\nvar noseCrest: VNFaceLandmarkRegion2D?\nThe region containing points that trace the center crest of the nose.\nvar outerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the outside of the lips.\nvar innerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the space between the lips.\nvar leftPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the left pupil is located.\nvar rightPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the right pupil is located."
  },
  {
    "title": "nose | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks2d/2879443-nose",
    "html": "See Also\nFace Landmark Points\nvar allPoints: VNFaceLandmarkRegion2D?\nThe region containing all face landmark points.\nvar faceContour: VNFaceLandmarkRegion2D?\nThe region containing points that trace the face contour from the left cheek, over the chin, to the right cheek.\nvar leftEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the left eye.\nvar rightEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the right eye.\nvar leftEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the left eyebrow.\nvar rightEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the right eyebrow.\nvar noseCrest: VNFaceLandmarkRegion2D?\nThe region containing points that trace the center crest of the nose.\nvar medianLine: VNFaceLandmarkRegion2D?\nThe region containing points that trace a vertical line down the center of the face.\nvar outerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the outside of the lips.\nvar innerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the space between the lips.\nvar leftPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the left pupil is located.\nvar rightPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the right pupil is located."
  },
  {
    "title": "faceContour | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks2d/2879437-facecontour",
    "html": "See Also\nFace Landmark Points\nvar allPoints: VNFaceLandmarkRegion2D?\nThe region containing all face landmark points.\nvar leftEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the left eye.\nvar rightEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the right eye.\nvar leftEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the left eyebrow.\nvar rightEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the right eyebrow.\nvar nose: VNFaceLandmarkRegion2D?\nThe region containing points that outline the nose.\nvar noseCrest: VNFaceLandmarkRegion2D?\nThe region containing points that trace the center crest of the nose.\nvar medianLine: VNFaceLandmarkRegion2D?\nThe region containing points that trace a vertical line down the center of the face.\nvar outerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the outside of the lips.\nvar innerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the space between the lips.\nvar leftPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the left pupil is located.\nvar rightPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the right pupil is located."
  },
  {
    "title": "noseCrest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks2d/2879431-nosecrest",
    "html": "See Also\nFace Landmark Points\nvar allPoints: VNFaceLandmarkRegion2D?\nThe region containing all face landmark points.\nvar faceContour: VNFaceLandmarkRegion2D?\nThe region containing points that trace the face contour from the left cheek, over the chin, to the right cheek.\nvar leftEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the left eye.\nvar rightEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the right eye.\nvar leftEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the left eyebrow.\nvar rightEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the right eyebrow.\nvar nose: VNFaceLandmarkRegion2D?\nThe region containing points that outline the nose.\nvar medianLine: VNFaceLandmarkRegion2D?\nThe region containing points that trace a vertical line down the center of the face.\nvar outerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the outside of the lips.\nvar innerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the space between the lips.\nvar leftPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the left pupil is located.\nvar rightPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the right pupil is located."
  },
  {
    "title": "computeDeviceForComputeStage: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/4173241-computedeviceforcomputestage",
    "html": "Parameters\ncomputeStage\n\nThe compute stage to inspect.\n\nReturn Value\n\nThe current compute device; otherwise, nil if one isn’t assigned.\n\nSee Also\nConfiguring the Compute Device\n- setComputeDevice:forComputeStage:\nAssigns a compute device for a compute stage.\n- supportedComputeStageDevicesAndReturnError:\nThe collection of compute devices per stage that a request supports."
  },
  {
    "title": "outerLips | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks2d/2879440-outerlips",
    "html": "See Also\nFace Landmark Points\nvar allPoints: VNFaceLandmarkRegion2D?\nThe region containing all face landmark points.\nvar faceContour: VNFaceLandmarkRegion2D?\nThe region containing points that trace the face contour from the left cheek, over the chin, to the right cheek.\nvar leftEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the left eye.\nvar rightEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the right eye.\nvar leftEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the left eyebrow.\nvar rightEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the right eyebrow.\nvar nose: VNFaceLandmarkRegion2D?\nThe region containing points that outline the nose.\nvar noseCrest: VNFaceLandmarkRegion2D?\nThe region containing points that trace the center crest of the nose.\nvar medianLine: VNFaceLandmarkRegion2D?\nThe region containing points that trace a vertical line down the center of the face.\nvar innerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the space between the lips.\nvar leftPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the left pupil is located.\nvar rightPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the right pupil is located."
  },
  {
    "title": "innerLips | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks2d/2879434-innerlips",
    "html": "See Also\nFace Landmark Points\nvar allPoints: VNFaceLandmarkRegion2D?\nThe region containing all face landmark points.\nvar faceContour: VNFaceLandmarkRegion2D?\nThe region containing points that trace the face contour from the left cheek, over the chin, to the right cheek.\nvar leftEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the left eye.\nvar rightEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the right eye.\nvar leftEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the left eyebrow.\nvar rightEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the right eyebrow.\nvar nose: VNFaceLandmarkRegion2D?\nThe region containing points that outline the nose.\nvar noseCrest: VNFaceLandmarkRegion2D?\nThe region containing points that trace the center crest of the nose.\nvar medianLine: VNFaceLandmarkRegion2D?\nThe region containing points that trace a vertical line down the center of the face.\nvar outerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the outside of the lips.\nvar leftPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the left pupil is located.\nvar rightPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the right pupil is located."
  },
  {
    "title": "leftBackPaw | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173247-leftbackpaw",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left elbow.\nstatic let leftFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left elbow.\nstatic let rightFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right elbow.\nstatic let rightBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right elbow.\nstatic let leftBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left knee.\nstatic let leftFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left knee.\nstatic let rightBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right knee.\nstatic let rightFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right knee.\nstatic let leftFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left paw.\nstatic let rightBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right paw.\nstatic let rightFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right paw."
  },
  {
    "title": "microPDF417 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3751014-micropdf417",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "itf14 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2869644-itf14",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "qr | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2869620-qr",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "head | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointsgroupname/4173273-head",
    "html": "See Also\nGetting the Group Names\nstatic let all: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents all joints.\nstatic let forelegs: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the forelegs.\nstatic let hindlegs: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the hindlegs.\nstatic let tail: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the tail.\nstatic let trunk: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the trunk."
  },
  {
    "title": "upce | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2869627-upce",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology."
  },
  {
    "title": "Aztec | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795568-aztec",
    "html": "See Also\nDeprecated Symbols\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "Code39 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795570-code39",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "Code39FullASCIIChecksum | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795573-code39fullasciichecksum",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "leftEye | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173251-lefteye",
    "html": "See Also\nGetting the Head Joint Names\nstatic let leftEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the left ear.\nstatic let leftEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the left ear.\nstatic let leftEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the left ear.\nstatic let neck: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the neck.\nstatic let nose: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the nose.\nstatic let rightEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the right eye.\nstatic let rightEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the right ear.\nstatic let rightEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the right ear.\nstatic let rightEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the right ear."
  },
  {
    "title": "leftEarMiddle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173249-leftearmiddle",
    "html": "See Also\nGetting the Head Joint Names\nstatic let leftEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the left ear.\nstatic let leftEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the left ear.\nstatic let leftEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the left eye.\nstatic let neck: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the neck.\nstatic let nose: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the nose.\nstatic let rightEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the right eye.\nstatic let rightEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the right ear.\nstatic let rightEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the right ear.\nstatic let rightEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the right ear."
  },
  {
    "title": "VNChirality.unknown | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnchirality/unknown",
    "html": "See Also\nChirality Values\ncase left\nIndicates a left-handed pose.\ncase right\nIndicates a right-handed pose."
  },
  {
    "title": "VNTrackOpticalFlowRequest.ComputationAccuracy.low | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackopticalflowrequest/computationaccuracy/low",
    "html": "See Also\nOptions\ncase medium\nAn option that indicates a moderate level of computational accuracy.\ncase high\nAn option that indicates a high level of computational accuracy.\ncase veryHigh\nAn option that indicates a very high level of computational accuracy."
  },
  {
    "title": "VNTrackOpticalFlowRequest.ComputationAccuracy.veryHigh | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackopticalflowrequest/computationaccuracy/veryhigh",
    "html": "See Also\nOptions\ncase low\nAn option that indicates a low level of computational accuracy.\ncase medium\nAn option that indicates a moderate level of computational accuracy.\ncase high\nAn option that indicates a high level of computational accuracy."
  },
  {
    "title": "VNTrackOpticalFlowRequest.ComputationAccuracy.high | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackopticalflowrequest/computationaccuracy/high",
    "html": "See Also\nOptions\ncase low\nAn option that indicates a low level of computational accuracy.\ncase medium\nAn option that indicates a moderate level of computational accuracy.\ncase veryHigh\nAn option that indicates a very high level of computational accuracy."
  },
  {
    "title": "VNTrackOpticalFlowRequest.ComputationAccuracy.medium | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackopticalflowrequest/computationaccuracy/medium",
    "html": "See Also\nOptions\ncase low\nAn option that indicates a low level of computational accuracy.\ncase high\nAn option that indicates a high level of computational accuracy.\ncase veryHigh\nAn option that indicates a very high level of computational accuracy."
  },
  {
    "title": "VNGenerateOpticalFlowRequest.ComputationAccuracy.low | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateopticalflowrequest/computationaccuracy/low",
    "html": "See Also\nAccuracy Levels\ncase medium\nMedium accuracy.\ncase high\nHigh accuracy.\ncase veryHigh\nVery high accuracy."
  },
  {
    "title": "pointCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarkregion/2909053-pointcount",
    "html": "Discussion\n\nThe value is zero if no points for a region could be found."
  },
  {
    "title": "VNGenerateOpticalFlowRequest.ComputationAccuracy.veryHigh | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateopticalflowrequest/computationaccuracy/veryhigh",
    "html": "See Also\nAccuracy Levels\ncase low\nLow accuracy.\ncase medium\nMedium accuracy.\ncase high\nHigh accuracy."
  },
  {
    "title": "VNGenerateOpticalFlowRequest.ComputationAccuracy.high | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateopticalflowrequest/computationaccuracy/high",
    "html": "See Also\nAccuracy Levels\ncase low\nLow accuracy.\ncase medium\nMedium accuracy.\ncase veryHigh\nVery high accuracy."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2869634-init",
    "html": "Parameters\nrawValue\n\nThe raw string value."
  },
  {
    "title": "init(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncoremlmodel/2890148-init",
    "html": "Discussion\n\nThis method may fail if Vision does not support the created CoreML model. For example, a model that does not accept an image as any of its inputs will yield an VNErrorCode.invalidModel error."
  },
  {
    "title": "I2of5Checksum | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795580-i2of5checksum",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "PDF417 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795582-pdf417",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "ITF14 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795581-itf14",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "I2of5 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795579-i2of5",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "EAN13 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795577-ean13",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "EAN8 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795578-ean8",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "Code93 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795574-code93",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "aztec | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2869621-aztec",
    "html": "See Also\nSupported Symbologies\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "Code93i | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795575-code93i",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "codabar | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3751010-codabar",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "DataMatrix | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795576-datamatrix",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "code39 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2923473-code39",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "code39Checksum | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2923467-code39checksum",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "code39FullASCII | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2923469-code39fullascii",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "all | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointsgroupname/3675664-all",
    "html": "See Also\nGroup Names\nstatic let thumb: VNHumanHandPoseObservation.JointsGroupName\nThe thumb.\nstatic let indexFinger: VNHumanHandPoseObservation.JointsGroupName\nThe index finger.\nstatic let littleFinger: VNHumanHandPoseObservation.JointsGroupName\nThe little finger.\nstatic let middleFinger: VNHumanHandPoseObservation.JointsGroupName\nThe middle finger.\nstatic let ringFinger: VNHumanHandPoseObservation.JointsGroupName\nThe ring finger."
  },
  {
    "title": "pointCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontour/3548327-pointcount",
    "html": "See Also\nInspecting the Contour\nvar aspectRatio: Float\nThe aspect ratio of the contour.\nvar indexPath: IndexPath\nThe contour object’s index path.\nvar normalizedPath: CGPath\nThe contour object as a path in normalized coordinates.\nvar normalizedPoints: [simd_float2]\nThe contour’s array of points in normalized coordinates.\nfunc polygonApproximation(epsilon: Float) -> VNContour\nSimplifies the contour to a polygon using a Ramer-Douglas-Peucker algorithm."
  },
  {
    "title": "VNRequestFaceLandmarksConstellation.constellation76Points | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequestfacelandmarksconstellation/constellation76points",
    "html": "See Also\nTypes of Constellations\ncase constellationNotDefined\nAn undefined constellation.\ncase constellation65Points\nA constellation with 65 points."
  },
  {
    "title": "VNRequestFaceLandmarksConstellation.constellation65Points | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequestfacelandmarksconstellation/constellation65points",
    "html": "See Also\nTypes of Constellations\ncase constellationNotDefined\nAn undefined constellation.\ncase constellation76Points\nA constellation with 76 points."
  },
  {
    "title": "childContours | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontour/3548323-childcontours",
    "html": "See Also\nAccessing Child Contours\nvar childContourCount: Int\nThe total number of detected child contours.\nfunc childContour(at: Int) -> VNContour\nRetrieves the child contour object at the specified index."
  },
  {
    "title": "VNVideoProcessor.Cadence | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor/cadence",
    "html": "Relationships\nInherits From\nNSObject\nConforms To\nNSCopying\nSee Also\nConfiguring Options\nvar cadence: VNVideoProcessor.Cadence?\nThe cadence the video processor maintains to process the request.\nclass VNVideoProcessor.FrameRateCadence\nAn object that defines a frame-based cadence for processing a video stream.\nclass VNVideoProcessor.TimeIntervalCadence\nAn object that defines a time-based cadence for processing a video stream."
  },
  {
    "title": "setComputeDevice:forComputeStage: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/4173242-setcomputedevice",
    "html": "Parameters\ncomputeDevice\n\nThe compute device to assign to the compute stage.\n\ncomputeStage\n\nThe compute stage.\n\nDiscussion\n\nIf the parameter computeDevice is nil, the framework removes any explicit compute device assignment and allows the framework to select the device.\n\nConfigure any compute device for a given compute stage. When performing a request, the system makes a validity check. Call supportedComputeStageDevicesAndReturnError: to get valid compute devices for a request’s compute stages.\n\nSee Also\nConfiguring the Compute Device\n- computeDeviceForComputeStage:\nReturns the compute device for a compute stage.\n- supportedComputeStageDevicesAndReturnError:\nThe collection of compute devices per stage that a request supports."
  },
  {
    "title": "initWithCompletionHandler: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/2875416-initwithcompletionhandler",
    "html": "Parameters\ncompletionHandler\n\nThe block to invoke after the request finishes processing.\n\nDiscussion\n\nVision executes the completion handler on the same queue that it executes the request; however, this queue differs from the one where you called performRequests:error:.\n\nSee Also\nInitializing a Request\n- init\nCreates a new Vision request with no completion handler."
  },
  {
    "title": "code93i | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2923475-code93i",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "code128 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2923472-code128",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "i2of5 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2923474-i2of5",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "gs1DataBarLimited | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3751013-gs1databarlimited",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "gs1DataBarExpanded | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3751012-gs1databarexpanded",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "gs1DataBar | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3751011-gs1databar",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "ean8 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2869624-ean8",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "bodyLandmarkKeyNose | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548247-bodylandmarkkeynose",
    "html": "See Also\nHead\nstatic let bodyLandmarkKeyLeftEye: VNRecognizedPointKey\nThe left eye.\nDeprecated\nstatic let bodyLandmarkKeyRightEye: VNRecognizedPointKey\nThe right eye.\nDeprecated\nstatic let bodyLandmarkKeyLeftEar: VNRecognizedPointKey\nThe left ear.\nDeprecated\nstatic let bodyLandmarkKeyRightEar: VNRecognizedPointKey\nThe right ear.\nDeprecated\nstatic let bodyLandmarkRegionKeyFace: VNRecognizedPointGroupKey\nA group key identifying the face, which includes the eyes, ears, and nose.\nDeprecated"
  },
  {
    "title": "bodyLandmarkKeyLeftHip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548243-bodylandmarkkeylefthip",
    "html": "See Also\nTorso\nstatic let bodyLandmarkKeyNeck: VNRecognizedPointKey\nThe center point of the neck.\nDeprecated\nstatic let bodyLandmarkKeyLeftShoulder: VNRecognizedPointKey\nThe left shoulder.\nDeprecated\nstatic let bodyLandmarkKeyRightShoulder: VNRecognizedPointKey\nThe right shoulder.\nDeprecated\nstatic let bodyLandmarkKeyRightHip: VNRecognizedPointKey\nThe right hip.\nDeprecated\nstatic let bodyLandmarkKeyRoot: VNRecognizedPointKey\nThe center point of the waist.\nDeprecated\nstatic let bodyLandmarkRegionKeyTorso: VNRecognizedPointGroupKey\nA group key identifying the torso, which includes the neck, shoulders, hips, and root.\nDeprecated"
  },
  {
    "title": "all | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointsgroupname/4173271-all",
    "html": "See Also\nGetting the Group Names\nstatic let forelegs: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the forelegs.\nstatic let head: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the head.\nstatic let hindlegs: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the hindlegs.\nstatic let tail: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the tail.\nstatic let trunk: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the trunk."
  },
  {
    "title": "i2of5Checksum | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2923471-i2of5checksum",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "pdf417 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2869630-pdf417",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "Code128 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795569-code128",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "leftEarBottom | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173248-leftearbottom",
    "html": "See Also\nGetting the Head Joint Names\nstatic let leftEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the left ear.\nstatic let leftEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the left ear.\nstatic let leftEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the left eye.\nstatic let neck: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the neck.\nstatic let nose: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the nose.\nstatic let rightEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the right eye.\nstatic let rightEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the right ear.\nstatic let rightEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the right ear.\nstatic let rightEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the right ear."
  },
  {
    "title": "neck | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173255-neck",
    "html": "See Also\nGetting the Head Joint Names\nstatic let leftEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the left ear.\nstatic let leftEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the left ear.\nstatic let leftEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the left ear.\nstatic let leftEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the left eye.\nstatic let nose: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the nose.\nstatic let rightEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the right eye.\nstatic let rightEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the right ear.\nstatic let rightEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the right ear.\nstatic let rightEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the right ear."
  },
  {
    "title": "rightEarBottom | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173260-rightearbottom",
    "html": "See Also\nGetting the Head Joint Names\nstatic let leftEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the left ear.\nstatic let leftEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the left ear.\nstatic let leftEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the left ear.\nstatic let leftEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the left eye.\nstatic let neck: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the neck.\nstatic let nose: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the nose.\nstatic let rightEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the right eye.\nstatic let rightEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the right ear.\nstatic let rightEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the right ear."
  },
  {
    "title": "leftBackElbow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173245-leftbackelbow",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left elbow.\nstatic let rightFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right elbow.\nstatic let rightBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right elbow.\nstatic let leftBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left knee.\nstatic let leftFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left knee.\nstatic let rightBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right knee.\nstatic let rightFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right knee.\nstatic let leftBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left paw.\nstatic let leftFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left paw.\nstatic let rightBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right paw.\nstatic let rightFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right paw."
  },
  {
    "title": "indexTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675645-indextip",
    "html": "See Also\nIndex\nstatic let indexDIP: VNHumanHandPoseObservation.JointName\nThe index finger’s distal interphalangeal (DIP) joint.\nstatic let indexPIP: VNHumanHandPoseObservation.JointName\nThe index finger’s proximal interphalangeal (PIP) joint.\nstatic let indexMCP: VNHumanHandPoseObservation.JointName\nThe index finger’s metacarpophalangeal (MCP) joint."
  },
  {
    "title": "leftBackKnee | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173246-leftbackknee",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left elbow.\nstatic let leftFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left elbow.\nstatic let rightFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right elbow.\nstatic let rightBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right elbow.\nstatic let leftFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left knee.\nstatic let rightBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right knee.\nstatic let rightFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right knee.\nstatic let leftBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left paw.\nstatic let leftFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left paw.\nstatic let rightBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right paw.\nstatic let rightFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right paw."
  },
  {
    "title": "rightFrontElbow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173264-rightfrontelbow",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left elbow.\nstatic let leftFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left elbow.\nstatic let rightBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right elbow.\nstatic let leftBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left knee.\nstatic let leftFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left knee.\nstatic let rightBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right knee.\nstatic let rightFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right knee.\nstatic let leftBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left paw.\nstatic let leftFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left paw.\nstatic let rightBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right paw.\nstatic let rightFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right paw."
  },
  {
    "title": "rightBackElbow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173257-rightbackelbow",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left elbow.\nstatic let leftFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left elbow.\nstatic let rightFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right elbow.\nstatic let leftBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left knee.\nstatic let leftFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left knee.\nstatic let rightBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right knee.\nstatic let rightFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right knee.\nstatic let leftBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left paw.\nstatic let leftFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left paw.\nstatic let rightBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right paw.\nstatic let rightFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right paw."
  },
  {
    "title": "leftFrontKnee | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173253-leftfrontknee",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left elbow.\nstatic let leftFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left elbow.\nstatic let rightFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right elbow.\nstatic let rightBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right elbow.\nstatic let leftBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left knee.\nstatic let rightBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right knee.\nstatic let rightFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right knee.\nstatic let leftBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left paw.\nstatic let leftFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left paw.\nstatic let rightBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right paw.\nstatic let rightFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right paw."
  },
  {
    "title": "isGS1DataCarrier | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodeobservation/4165646-isgs1datacarrier",
    "html": "See Also\nParsing the Payload\nvar payloadStringValue: String?\nA string value that represents the barcode payload.\nvar payloadData: Data?\nThe raw data representation of the barcode’s payload.\nvar supplementalPayloadString: String?\nThe supplemental code decoded as a string value.\nvar supplementalPayloadData: Data?\nvar supplementalCompositeType: VNBarcodeCompositeType\nThe supplemental composite type."
  },
  {
    "title": "thumbTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675661-thumbtip",
    "html": "See Also\nThumb\nstatic let thumbIP: VNHumanHandPoseObservation.JointName\nThe thumb’s interphalangeal (IP) joint.\nstatic let thumbMP: VNHumanHandPoseObservation.JointName\nThe thumb’s metacarpophalangeal (MP) joint.\nstatic let thumbCMC: VNHumanHandPoseObservation.JointName\nThe thumb’s carpometacarpal (CMC) joint."
  },
  {
    "title": "leftPupil | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks2d/2879436-leftpupil",
    "html": "Discussion\n\nThis value may be inaccurate if the eye is blinking.\n\nSee Also\nFace Landmark Points\nvar allPoints: VNFaceLandmarkRegion2D?\nThe region containing all face landmark points.\nvar faceContour: VNFaceLandmarkRegion2D?\nThe region containing points that trace the face contour from the left cheek, over the chin, to the right cheek.\nvar leftEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the left eye.\nvar rightEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the right eye.\nvar leftEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the left eyebrow.\nvar rightEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the right eyebrow.\nvar nose: VNFaceLandmarkRegion2D?\nThe region containing points that outline the nose.\nvar noseCrest: VNFaceLandmarkRegion2D?\nThe region containing points that trace the center crest of the nose.\nvar medianLine: VNFaceLandmarkRegion2D?\nThe region containing points that trace a vertical line down the center of the face.\nvar outerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the outside of the lips.\nvar innerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the space between the lips.\nvar rightPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the right pupil is located."
  },
  {
    "title": "bodyLandmarkKeyLeftAnkle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548239-bodylandmarkkeyleftankle",
    "html": "See Also\nLegs\nstatic let bodyLandmarkKeyRightKnee: VNRecognizedPointKey\nThe right knee.\nDeprecated\nstatic let bodyLandmarkKeyRightAnkle: VNRecognizedPointKey\nThe right ankle.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right leg.\nDeprecated\nstatic let bodyLandmarkKeyLeftKnee: VNRecognizedPointKey\nThe left knee.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left leg.\nDeprecated"
  },
  {
    "title": "thumbCMC | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675658-thumbcmc",
    "html": "See Also\nThumb\nstatic let thumbTip: VNHumanHandPoseObservation.JointName\nThe tip of the thumb.\nstatic let thumbIP: VNHumanHandPoseObservation.JointName\nThe thumb’s interphalangeal (IP) joint.\nstatic let thumbMP: VNHumanHandPoseObservation.JointName\nThe thumb’s metacarpophalangeal (MP) joint."
  },
  {
    "title": "middleTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675653-middletip",
    "html": "See Also\nMiddle\nstatic let middleDIP: VNHumanHandPoseObservation.JointName\nThe middle finger’s distal interphalangeal (DIP) joint.\nstatic let middlePIP: VNHumanHandPoseObservation.JointName\nThe middle finger’s proximal interphalangeal (PIP) joint.\nstatic let middleMCP: VNHumanHandPoseObservation.JointName\nThe middle finger’s metacarpophalangeal (MCP) joint."
  },
  {
    "title": "thumbMP | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675660-thumbmp",
    "html": "See Also\nThumb\nstatic let thumbTip: VNHumanHandPoseObservation.JointName\nThe tip of the thumb.\nstatic let thumbIP: VNHumanHandPoseObservation.JointName\nThe thumb’s interphalangeal (IP) joint.\nstatic let thumbCMC: VNHumanHandPoseObservation.JointName\nThe thumb’s carpometacarpal (CMC) joint."
  },
  {
    "title": "indexMCP | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675643-indexmcp",
    "html": "See Also\nIndex\nstatic let indexTip: VNHumanHandPoseObservation.JointName\nThe tip of the index finger.\nstatic let indexDIP: VNHumanHandPoseObservation.JointName\nThe index finger’s distal interphalangeal (DIP) joint.\nstatic let indexPIP: VNHumanHandPoseObservation.JointName\nThe index finger’s proximal interphalangeal (PIP) joint."
  },
  {
    "title": "VNElementTypeSize(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/3152658-vnelementtypesize",
    "html": "Parameters\nelementType\n\nThe type of element.\n\nSee Also\nDetermining Types of Feature Prints\nvar elementType: VNElementType\nThe type of each element in the data.\nenum VNElementType\nAn enumeration of the type of element in feature print data."
  },
  {
    "title": "UPCE | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795584-upce",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated"
  },
  {
    "title": "QR | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795583-qr",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "VNRequestFaceLandmarksConstellation.constellationNotDefined | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequestfacelandmarksconstellation/constellationnotdefined",
    "html": "See Also\nTypes of Constellations\ncase constellation65Points\nA constellation with 65 points.\ncase constellation76Points\nA constellation with 76 points."
  },
  {
    "title": "VNRequestTextRecognitionLevel.accurate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequesttextrecognitionlevel/accurate",
    "html": "See Also\nRecognition Levels\ncase fast\nFast text recognition returns results more quickly at the expense of accuracy."
  },
  {
    "title": "inputImageFeatureName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncoremlmodel/3131934-inputimagefeaturename",
    "html": "Discussion\n\nBy default, Vision uses the first input found, but you can manually set that input to another featureName instead.\n\nSee Also\nProviding Features\nvar featureProvider: MLFeatureProvider?\nAn optional object to support inputs outside Vision."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointsgroupname/3675919-init",
    "html": "Parameters\nrawValue\n\nThe recognized point group key."
  },
  {
    "title": "normalizedPath | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontoursobservation/3548363-normalizedpath",
    "html": "See Also\nInspecting the Observation\nvar contourCount: Int\nThe total number of detected contours.\nvar topLevelContours: [VNContour]\nAn array of contours that don’t have another contour enclosing them.\nvar topLevelContourCount: Int\nThe total number of detected top-level contours.\nfunc contour(at: Int) -> VNContour\nRetrieves the contour object at the specified index, irrespective of hierarchy.\nfunc contour(at: IndexPath) -> VNContour\nRetrieves the contour object at the specified index path.\nclass VNContour\nA class that represents a detected contour in an image."
  },
  {
    "title": "middleMCP | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675651-middlemcp",
    "html": "See Also\nMiddle\nstatic let middleTip: VNHumanHandPoseObservation.JointName\nThe tip of the middle finger.\nstatic let middleDIP: VNHumanHandPoseObservation.JointName\nThe middle finger’s distal interphalangeal (DIP) joint.\nstatic let middlePIP: VNHumanHandPoseObservation.JointName\nThe middle finger’s proximal interphalangeal (PIP) joint."
  },
  {
    "title": "littleTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675649-littletip",
    "html": "See Also\nLittle\nstatic let littleDIP: VNHumanHandPoseObservation.JointName\nThe little finger’s distal interphalangeal (DIP) joint.\nstatic let littlePIP: VNHumanHandPoseObservation.JointName\nThe little finger’s proximal interphalangeal (PIP) joint.\nstatic let littleMCP: VNHumanHandPoseObservation.JointName\nThe little finger’s metacarpophalangeal (MCP) joint."
  },
  {
    "title": "littlePIP | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675648-littlepip",
    "html": "See Also\nLittle\nstatic let littleTip: VNHumanHandPoseObservation.JointName\nThe tip of the little finger.\nstatic let littleDIP: VNHumanHandPoseObservation.JointName\nThe little finger’s distal interphalangeal (DIP) joint.\nstatic let littleMCP: VNHumanHandPoseObservation.JointName\nThe little finger’s metacarpophalangeal (MCP) joint."
  },
  {
    "title": "rightBackKnee | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173258-rightbackknee",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left elbow.\nstatic let leftFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left elbow.\nstatic let rightFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right elbow.\nstatic let rightBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right elbow.\nstatic let leftBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left knee.\nstatic let leftFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left knee.\nstatic let rightFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right knee.\nstatic let leftBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left paw.\nstatic let leftFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left paw.\nstatic let rightBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right paw.\nstatic let rightFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right paw."
  },
  {
    "title": "rightFrontKnee | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173265-rightfrontknee",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left elbow.\nstatic let leftFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left elbow.\nstatic let rightFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right elbow.\nstatic let rightBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right elbow.\nstatic let leftBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left knee.\nstatic let leftFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left knee.\nstatic let rightBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right knee.\nstatic let leftBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left paw.\nstatic let leftFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left paw.\nstatic let rightBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right paw.\nstatic let rightFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right paw."
  },
  {
    "title": "rightBackPaw | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173259-rightbackpaw",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left elbow.\nstatic let leftFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left elbow.\nstatic let rightFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right elbow.\nstatic let rightBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right elbow.\nstatic let leftBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left knee.\nstatic let leftFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left knee.\nstatic let rightBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right knee.\nstatic let rightFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right knee.\nstatic let leftBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left paw.\nstatic let leftFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left paw.\nstatic let rightFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right paw."
  },
  {
    "title": "rightFrontPaw | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173266-rightfrontpaw",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left elbow.\nstatic let leftFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left elbow.\nstatic let rightFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right elbow.\nstatic let rightBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right elbow.\nstatic let leftBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left knee.\nstatic let leftFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left knee.\nstatic let rightBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right knee.\nstatic let rightFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right knee.\nstatic let leftBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left paw.\nstatic let leftFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left paw.\nstatic let rightBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right paw."
  },
  {
    "title": "hindlegs | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointsgroupname/4173274-hindlegs",
    "html": "See Also\nGetting the Group Names\nstatic let all: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents all joints.\nstatic let forelegs: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the forelegs.\nstatic let head: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the head.\nstatic let tail: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the tail.\nstatic let trunk: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the trunk."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4176674-init",
    "html": "Parameters\nrawValue\n\nThe point key."
  },
  {
    "title": "forelegs | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointsgroupname/4173272-forelegs",
    "html": "See Also\nGetting the Group Names\nstatic let all: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents all joints.\nstatic let head: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the head.\nstatic let hindlegs: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the hindlegs.\nstatic let tail: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the tail.\nstatic let trunk: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the trunk."
  },
  {
    "title": "tailTop | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173269-tailtop",
    "html": "See Also\nGetting the Tail Joint Names\nstatic let tailMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the tail.\nstatic let tailBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the tail."
  },
  {
    "title": "trunk | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointsgroupname/4173276-trunk",
    "html": "See Also\nGetting the Group Names\nstatic let all: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents all joints.\nstatic let forelegs: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the forelegs.\nstatic let head: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the head.\nstatic let hindlegs: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the hindlegs.\nstatic let tail: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the tail."
  },
  {
    "title": "tailBottom | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173267-tailbottom",
    "html": "See Also\nGetting the Tail Joint Names\nstatic let tailTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the tail.\nstatic let tailMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the tail."
  },
  {
    "title": "tailMiddle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173268-tailmiddle",
    "html": "See Also\nGetting the Tail Joint Names\nstatic let tailTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the tail.\nstatic let tailBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the tail."
  },
  {
    "title": "bodyLandmarkKeyLeftEar | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548240-bodylandmarkkeyleftear",
    "html": "See Also\nHead\nstatic let bodyLandmarkKeyLeftEye: VNRecognizedPointKey\nThe left eye.\nDeprecated\nstatic let bodyLandmarkKeyRightEye: VNRecognizedPointKey\nThe right eye.\nDeprecated\nstatic let bodyLandmarkKeyRightEar: VNRecognizedPointKey\nThe right ear.\nDeprecated\nstatic let bodyLandmarkKeyNose: VNRecognizedPointKey\nThe nose.\nDeprecated\nstatic let bodyLandmarkRegionKeyFace: VNRecognizedPointGroupKey\nA group key identifying the face, which includes the eyes, ears, and nose.\nDeprecated"
  },
  {
    "title": "bodyLandmarkKeyRightEar | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548249-bodylandmarkkeyrightear",
    "html": "See Also\nHead\nstatic let bodyLandmarkKeyLeftEye: VNRecognizedPointKey\nThe left eye.\nDeprecated\nstatic let bodyLandmarkKeyRightEye: VNRecognizedPointKey\nThe right eye.\nDeprecated\nstatic let bodyLandmarkKeyLeftEar: VNRecognizedPointKey\nThe left ear.\nDeprecated\nstatic let bodyLandmarkKeyNose: VNRecognizedPointKey\nThe nose.\nDeprecated\nstatic let bodyLandmarkRegionKeyFace: VNRecognizedPointGroupKey\nA group key identifying the face, which includes the eyes, ears, and nose.\nDeprecated"
  },
  {
    "title": "bodyLandmarkKeyLeftShoulder | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548245-bodylandmarkkeyleftshoulder",
    "html": "See Also\nTorso\nstatic let bodyLandmarkKeyNeck: VNRecognizedPointKey\nThe center point of the neck.\nDeprecated\nstatic let bodyLandmarkKeyRightShoulder: VNRecognizedPointKey\nThe right shoulder.\nDeprecated\nstatic let bodyLandmarkKeyLeftHip: VNRecognizedPointKey\nThe left hip.\nDeprecated\nstatic let bodyLandmarkKeyRightHip: VNRecognizedPointKey\nThe right hip.\nDeprecated\nstatic let bodyLandmarkKeyRoot: VNRecognizedPointKey\nThe center point of the waist.\nDeprecated\nstatic let bodyLandmarkRegionKeyTorso: VNRecognizedPointGroupKey\nA group key identifying the torso, which includes the neck, shoulders, hips, and root.\nDeprecated"
  },
  {
    "title": "bodyLandmarkKeyNeck | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3571269-bodylandmarkkeyneck",
    "html": "See Also\nTorso\nstatic let bodyLandmarkKeyLeftShoulder: VNRecognizedPointKey\nThe left shoulder.\nDeprecated\nstatic let bodyLandmarkKeyRightShoulder: VNRecognizedPointKey\nThe right shoulder.\nDeprecated\nstatic let bodyLandmarkKeyLeftHip: VNRecognizedPointKey\nThe left hip.\nDeprecated\nstatic let bodyLandmarkKeyRightHip: VNRecognizedPointKey\nThe right hip.\nDeprecated\nstatic let bodyLandmarkKeyRoot: VNRecognizedPointKey\nThe center point of the waist.\nDeprecated\nstatic let bodyLandmarkRegionKeyTorso: VNRecognizedPointGroupKey\nA group key identifying the torso, which includes the neck, shoulders, hips, and root.\nDeprecated"
  },
  {
    "title": "bodyLandmarkKeyRightShoulder | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548254-bodylandmarkkeyrightshoulder",
    "html": "See Also\nTorso\nstatic let bodyLandmarkKeyNeck: VNRecognizedPointKey\nThe center point of the neck.\nDeprecated\nstatic let bodyLandmarkKeyLeftShoulder: VNRecognizedPointKey\nThe left shoulder.\nDeprecated\nstatic let bodyLandmarkKeyLeftHip: VNRecognizedPointKey\nThe left hip.\nDeprecated\nstatic let bodyLandmarkKeyRightHip: VNRecognizedPointKey\nThe right hip.\nDeprecated\nstatic let bodyLandmarkKeyRoot: VNRecognizedPointKey\nThe center point of the waist.\nDeprecated\nstatic let bodyLandmarkRegionKeyTorso: VNRecognizedPointGroupKey\nA group key identifying the torso, which includes the neck, shoulders, hips, and root.\nDeprecated"
  },
  {
    "title": "bodyLandmarkKeyRightHip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548252-bodylandmarkkeyrighthip",
    "html": "See Also\nTorso\nstatic let bodyLandmarkKeyNeck: VNRecognizedPointKey\nThe center point of the neck.\nDeprecated\nstatic let bodyLandmarkKeyLeftShoulder: VNRecognizedPointKey\nThe left shoulder.\nDeprecated\nstatic let bodyLandmarkKeyRightShoulder: VNRecognizedPointKey\nThe right shoulder.\nDeprecated\nstatic let bodyLandmarkKeyLeftHip: VNRecognizedPointKey\nThe left hip.\nDeprecated\nstatic let bodyLandmarkKeyRoot: VNRecognizedPointKey\nThe center point of the waist.\nDeprecated\nstatic let bodyLandmarkRegionKeyTorso: VNRecognizedPointGroupKey\nA group key identifying the torso, which includes the neck, shoulders, hips, and root.\nDeprecated"
  },
  {
    "title": "bodyLandmarkKeyRoot | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3571270-bodylandmarkkeyroot",
    "html": "See Also\nTorso\nstatic let bodyLandmarkKeyNeck: VNRecognizedPointKey\nThe center point of the neck.\nDeprecated\nstatic let bodyLandmarkKeyLeftShoulder: VNRecognizedPointKey\nThe left shoulder.\nDeprecated\nstatic let bodyLandmarkKeyRightShoulder: VNRecognizedPointKey\nThe right shoulder.\nDeprecated\nstatic let bodyLandmarkKeyLeftHip: VNRecognizedPointKey\nThe left hip.\nDeprecated\nstatic let bodyLandmarkKeyRightHip: VNRecognizedPointKey\nThe right hip.\nDeprecated\nstatic let bodyLandmarkRegionKeyTorso: VNRecognizedPointGroupKey\nA group key identifying the torso, which includes the neck, shoulders, hips, and root.\nDeprecated"
  },
  {
    "title": "bodyLandmarkKeyRightWrist | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548255-bodylandmarkkeyrightwrist",
    "html": "See Also\nArms\nstatic let bodyLandmarkKeyRightElbow: VNRecognizedPointKey\nThe right elbow.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right arm.\nDeprecated\nstatic let bodyLandmarkKeyLeftWrist: VNRecognizedPointKey\nThe left wrist.\nDeprecated\nstatic let bodyLandmarkKeyLeftElbow: VNRecognizedPointKey\nThe left elbow.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left arm.\nDeprecated"
  },
  {
    "title": "bodyLandmarkKeyRightElbow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548250-bodylandmarkkeyrightelbow",
    "html": "See Also\nArms\nstatic let bodyLandmarkKeyRightWrist: VNRecognizedPointKey\nThe right wrist.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right arm.\nDeprecated\nstatic let bodyLandmarkKeyLeftWrist: VNRecognizedPointKey\nThe left wrist.\nDeprecated\nstatic let bodyLandmarkKeyLeftElbow: VNRecognizedPointKey\nThe left elbow.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left arm.\nDeprecated"
  },
  {
    "title": "dataMatrix | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2869648-datamatrix",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "bodyLandmarkKeyLeftKnee | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548244-bodylandmarkkeyleftknee",
    "html": "See Also\nLegs\nstatic let bodyLandmarkKeyRightKnee: VNRecognizedPointKey\nThe right knee.\nDeprecated\nstatic let bodyLandmarkKeyRightAnkle: VNRecognizedPointKey\nThe right ankle.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right leg.\nDeprecated\nstatic let bodyLandmarkKeyLeftAnkle: VNRecognizedPointKey\nThe left ankle.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left leg.\nDeprecated"
  },
  {
    "title": "ean13 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/2869639-ean13",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "VNErrorCode.missingOption | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/missingoption",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "Code39Checksum | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3795571-code39checksum",
    "html": "See Also\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated"
  },
  {
    "title": "movingAverageRadius | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrajectoryobservation/3751001-movingaverageradius",
    "html": "See Also\nEvaluating an Observation\nvar detectedPoints: [VNPoint]\nThe centroid points of the detected contour along the trajectory.\nvar projectedPoints: [VNPoint]\nThe centroids of the calculated trajectory from the detected points.\nvar equationCoefficients: simd_float3\nThe coefficients of the parabolic equation."
  },
  {
    "title": "msiPlessey | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/4127029-msiplessey",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "projectedPoints | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrajectoryobservation/3564826-projectedpoints",
    "html": "Discussion\n\nThe projected points define the ideal trajectory described by the parabolic equation. The equation’s coefficients and the projected points of the detected trajectory get refined over time. The system limits the maximum number of cached points to the maximum points needed to describe the trajectory together with the parabolic equation.\n\nSee Also\nEvaluating an Observation\nvar detectedPoints: [VNPoint]\nThe centroid points of the detected contour along the trajectory.\nvar equationCoefficients: simd_float3\nThe coefficients of the parabolic equation.\nvar movingAverageRadius: CGFloat\nThe moving average radius of the object the request is tracking."
  },
  {
    "title": "detectedPoints | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrajectoryobservation/3564824-detectedpoints",
    "html": "Discussion\n\nThe detected points may differ slightly from the ideal trajectory because they fall within the allowed tolerance. The system limits the maximum number of points based on the maximum trajectory length set in the request.\n\nSee Also\nEvaluating an Observation\nvar projectedPoints: [VNPoint]\nThe centroids of the calculated trajectory from the detected points.\nvar equationCoefficients: simd_float3\nThe coefficients of the parabolic equation.\nvar movingAverageRadius: CGFloat\nThe moving average radius of the object the request is tracking."
  },
  {
    "title": "equationCoefficients | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrajectoryobservation/3564825-equationcoefficients",
    "html": "Discussion\n\nThis equation describes the parabola on which the detected contour is traveling. The equation and the projected points get refined over time.\n\nSee Also\nEvaluating an Observation\nvar detectedPoints: [VNPoint]\nThe centroid points of the detected contour along the trajectory.\nvar projectedPoints: [VNPoint]\nThe centroids of the calculated trajectory from the detected points.\nvar movingAverageRadius: CGFloat\nThe moving average radius of the object the request is tracking."
  },
  {
    "title": "microQR | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology/3751015-microqr",
    "html": "See Also\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology."
  },
  {
    "title": "nose | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173256-nose",
    "html": "See Also\nGetting the Head Joint Names\nstatic let leftEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the left ear.\nstatic let leftEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the left ear.\nstatic let leftEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the left ear.\nstatic let leftEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the left eye.\nstatic let neck: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the neck.\nstatic let rightEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the right eye.\nstatic let rightEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the right ear.\nstatic let rightEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the right ear.\nstatic let rightEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the right ear."
  },
  {
    "title": "rightEarTop | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173262-righteartop",
    "html": "See Also\nGetting the Head Joint Names\nstatic let leftEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the left ear.\nstatic let leftEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the left ear.\nstatic let leftEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the left ear.\nstatic let leftEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the left eye.\nstatic let neck: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the neck.\nstatic let nose: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the nose.\nstatic let rightEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the right eye.\nstatic let rightEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the right ear.\nstatic let rightEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the right ear."
  },
  {
    "title": "rightEye | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173263-righteye",
    "html": "See Also\nGetting the Head Joint Names\nstatic let leftEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the left ear.\nstatic let leftEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the left ear.\nstatic let leftEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the left ear.\nstatic let leftEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the left eye.\nstatic let neck: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the neck.\nstatic let nose: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the nose.\nstatic let rightEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the right ear.\nstatic let rightEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the right ear.\nstatic let rightEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the right ear."
  },
  {
    "title": "rightEarMiddle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173261-rightearmiddle",
    "html": "See Also\nGetting the Head Joint Names\nstatic let leftEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the left ear.\nstatic let leftEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the left ear.\nstatic let leftEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the left ear.\nstatic let leftEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the left eye.\nstatic let neck: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the neck.\nstatic let nose: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the nose.\nstatic let rightEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the right eye.\nstatic let rightEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the right ear.\nstatic let rightEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the right ear."
  },
  {
    "title": "leftFrontElbow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173252-leftfrontelbow",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left elbow.\nstatic let rightFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right elbow.\nstatic let rightBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right elbow.\nstatic let leftBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left knee.\nstatic let leftFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left knee.\nstatic let rightBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right knee.\nstatic let rightFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right knee.\nstatic let leftBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left paw.\nstatic let leftFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left paw.\nstatic let rightBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right paw.\nstatic let rightFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right paw."
  },
  {
    "title": "thumbIP | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675659-thumbip",
    "html": "See Also\nThumb\nstatic let thumbTip: VNHumanHandPoseObservation.JointName\nThe tip of the thumb.\nstatic let thumbMP: VNHumanHandPoseObservation.JointName\nThe thumb’s metacarpophalangeal (MP) joint.\nstatic let thumbCMC: VNHumanHandPoseObservation.JointName\nThe thumb’s carpometacarpal (CMC) joint."
  },
  {
    "title": "VNErrorCode.unknownError | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/unknownerror",
    "html": "Discussion\n\nFor more information about the error, see the error description and domain.\n\nSee Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "indexDIP | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675642-indexdip",
    "html": "See Also\nIndex\nstatic let indexTip: VNHumanHandPoseObservation.JointName\nThe tip of the index finger.\nstatic let indexPIP: VNHumanHandPoseObservation.JointName\nThe index finger’s proximal interphalangeal (PIP) joint.\nstatic let indexMCP: VNHumanHandPoseObservation.JointName\nThe index finger’s metacarpophalangeal (MCP) joint."
  },
  {
    "title": "middlePIP | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675652-middlepip",
    "html": "See Also\nMiddle\nstatic let middleTip: VNHumanHandPoseObservation.JointName\nThe tip of the middle finger.\nstatic let middleDIP: VNHumanHandPoseObservation.JointName\nThe middle finger’s distal interphalangeal (DIP) joint.\nstatic let middleMCP: VNHumanHandPoseObservation.JointName\nThe middle finger’s metacarpophalangeal (MCP) joint."
  },
  {
    "title": "cameraIntrinsics | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimageoption/2880306-cameraintrinsics",
    "html": "Discussion\n\nThe camera intrinsics matrix is a CFData instance containing a matrix_float3x3, which is a column-major matrix:\n\nfx and fy are the focal length in pixels. For square pixels, they have the same value.\n\nox and oy are the coordinates of the principal point. The origin is the upper-left corner of the frame.\n\nSee Also\nOptions Dictionary Keys\nstatic let properties: VNImageOption\nThe dictionary from CGImageSourceCopyPropertiesAtIndex(_:_:_:) containing metadata for algorithms like horizon detection.\nstatic let ciContext: VNImageOption\nAn option key to specify the CIContext to be used in the handler's Core Image operations."
  },
  {
    "title": "indexPIP | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675644-indexpip",
    "html": "See Also\nIndex\nstatic let indexTip: VNHumanHandPoseObservation.JointName\nThe tip of the index finger.\nstatic let indexDIP: VNHumanHandPoseObservation.JointName\nThe index finger’s distal interphalangeal (DIP) joint.\nstatic let indexMCP: VNHumanHandPoseObservation.JointName\nThe index finger’s metacarpophalangeal (MCP) joint."
  },
  {
    "title": "middleDIP | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675650-middledip",
    "html": "See Also\nMiddle\nstatic let middleTip: VNHumanHandPoseObservation.JointName\nThe tip of the middle finger.\nstatic let middlePIP: VNHumanHandPoseObservation.JointName\nThe middle finger’s proximal interphalangeal (PIP) joint.\nstatic let middleMCP: VNHumanHandPoseObservation.JointName\nThe middle finger’s metacarpophalangeal (MCP) joint."
  },
  {
    "title": "ringTip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675657-ringtip",
    "html": "See Also\nRing\nstatic let ringDIP: VNHumanHandPoseObservation.JointName\nThe ring finger’s distal interphalangeal (DIP) joint.\nstatic let ringPIP: VNHumanHandPoseObservation.JointName\nThe ring finger’s proximal interphalangeal (PIP) joint.\nstatic let ringMCP: VNHumanHandPoseObservation.JointName\nThe ring finger’s metacarpophalangeal (MCP) joint."
  },
  {
    "title": "ringDIP | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675654-ringdip",
    "html": "See Also\nRing\nstatic let ringTip: VNHumanHandPoseObservation.JointName\nThe tip of the ring finger.\nstatic let ringPIP: VNHumanHandPoseObservation.JointName\nThe ring finger’s proximal interphalangeal (PIP) joint.\nstatic let ringMCP: VNHumanHandPoseObservation.JointName\nThe ring finger’s metacarpophalangeal (MCP) joint."
  },
  {
    "title": "ringMCP | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675655-ringmcp",
    "html": "See Also\nRing\nstatic let ringTip: VNHumanHandPoseObservation.JointName\nThe tip of the ring finger.\nstatic let ringDIP: VNHumanHandPoseObservation.JointName\nThe ring finger’s distal interphalangeal (DIP) joint.\nstatic let ringPIP: VNHumanHandPoseObservation.JointName\nThe ring finger’s proximal interphalangeal (PIP) joint."
  },
  {
    "title": "littleMCP | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675647-littlemcp",
    "html": "See Also\nLittle\nstatic let littleTip: VNHumanHandPoseObservation.JointName\nThe tip of the little finger.\nstatic let littleDIP: VNHumanHandPoseObservation.JointName\nThe little finger’s distal interphalangeal (DIP) joint.\nstatic let littlePIP: VNHumanHandPoseObservation.JointName\nThe little finger’s proximal interphalangeal (PIP) joint."
  },
  {
    "title": "VNChirality.left | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnchirality/left",
    "html": "See Also\nChirality Values\ncase right\nIndicates a right-handed pose.\ncase unknown\nIndicates that the pose chirality is unknown."
  },
  {
    "title": "topLevelContourCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontoursobservation/3675675-toplevelcontourcount",
    "html": "See Also\nInspecting the Observation\nvar contourCount: Int\nThe total number of detected contours.\nvar normalizedPath: CGPath\nThe detected contours as a path object in normalized coordinates.\nvar topLevelContours: [VNContour]\nAn array of contours that don’t have another contour enclosing them.\nfunc contour(at: Int) -> VNContour\nRetrieves the contour object at the specified index, irrespective of hierarchy.\nfunc contour(at: IndexPath) -> VNContour\nRetrieves the contour object at the specified index path.\nclass VNContour\nA class that represents a detected contour in an image."
  },
  {
    "title": "leftEar | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675607-leftear",
    "html": "See Also\nHead\nstatic let leftEye: VNHumanBodyPoseObservation.JointName\nThe left eye.\nstatic let rightEar: VNHumanBodyPoseObservation.JointName\nThe right ear.\nstatic let rightEye: VNHumanBodyPoseObservation.JointName\nThe right eye.\nstatic let neck: VNHumanBodyPoseObservation.JointName\nThe neck.\nstatic let nose: VNHumanBodyPoseObservation.JointName\nThe nose."
  },
  {
    "title": "leftEye | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675609-lefteye",
    "html": "See Also\nHead\nstatic let leftEar: VNHumanBodyPoseObservation.JointName\nThe left ear.\nstatic let rightEar: VNHumanBodyPoseObservation.JointName\nThe right ear.\nstatic let rightEye: VNHumanBodyPoseObservation.JointName\nThe right eye.\nstatic let neck: VNHumanBodyPoseObservation.JointName\nThe neck.\nstatic let nose: VNHumanBodyPoseObservation.JointName\nThe nose."
  },
  {
    "title": "topLevelContours | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontoursobservation/3548364-toplevelcontours",
    "html": "Discussion\n\nThis array constitutes the top of the contour hierarchy. You can iterate over each VNContour instance to determine its children.\n\nSee Also\nInspecting the Observation\nvar contourCount: Int\nThe total number of detected contours.\nvar normalizedPath: CGPath\nThe detected contours as a path object in normalized coordinates.\nvar topLevelContourCount: Int\nThe total number of detected top-level contours.\nfunc contour(at: Int) -> VNContour\nRetrieves the contour object at the specified index, irrespective of hierarchy.\nfunc contour(at: IndexPath) -> VNContour\nRetrieves the contour object at the specified index path.\nclass VNContour\nA class that represents a detected contour in an image."
  },
  {
    "title": "contour(at:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontoursobservation/3548361-contour",
    "html": "Parameters\nindexPath\n\nThe hierarchical index path to the contour.\n\nReturn Value\n\nThe contour object at the specified index path.\n\nSee Also\nInspecting the Observation\nvar contourCount: Int\nThe total number of detected contours.\nvar normalizedPath: CGPath\nThe detected contours as a path object in normalized coordinates.\nvar topLevelContours: [VNContour]\nAn array of contours that don’t have another contour enclosing them.\nvar topLevelContourCount: Int\nThe total number of detected top-level contours.\nfunc contour(at: Int) -> VNContour\nRetrieves the contour object at the specified index, irrespective of hierarchy.\nclass VNContour\nA class that represents a detected contour in an image."
  },
  {
    "title": "contour(at:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontoursobservation/3548360-contour",
    "html": "Parameters\ncontourIndex\n\nThe index of the contour to retrieve. Valid values are in the range of 0 to contourCount - 1.\n\nReturn Value\n\nThe contour object at the specified index.\n\nSee Also\nInspecting the Observation\nvar contourCount: Int\nThe total number of detected contours.\nvar normalizedPath: CGPath\nThe detected contours as a path object in normalized coordinates.\nvar topLevelContours: [VNContour]\nAn array of contours that don’t have another contour enclosing them.\nvar topLevelContourCount: Int\nThe total number of detected top-level contours.\nfunc contour(at: IndexPath) -> VNContour\nRetrieves the contour object at the specified index path.\nclass VNContour\nA class that represents a detected contour in an image."
  },
  {
    "title": "rightEye | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675619-righteye",
    "html": "See Also\nHead\nstatic let leftEar: VNHumanBodyPoseObservation.JointName\nThe left ear.\nstatic let leftEye: VNHumanBodyPoseObservation.JointName\nThe left eye.\nstatic let rightEar: VNHumanBodyPoseObservation.JointName\nThe right ear.\nstatic let neck: VNHumanBodyPoseObservation.JointName\nThe neck.\nstatic let nose: VNHumanBodyPoseObservation.JointName\nThe nose."
  },
  {
    "title": "neck | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675614-neck",
    "html": "See Also\nHead\nstatic let leftEar: VNHumanBodyPoseObservation.JointName\nThe left ear.\nstatic let leftEye: VNHumanBodyPoseObservation.JointName\nThe left eye.\nstatic let rightEar: VNHumanBodyPoseObservation.JointName\nThe right ear.\nstatic let rightEye: VNHumanBodyPoseObservation.JointName\nThe right eye.\nstatic let nose: VNHumanBodyPoseObservation.JointName\nThe nose."
  },
  {
    "title": "rightEar | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675617-rightear",
    "html": "See Also\nHead\nstatic let leftEar: VNHumanBodyPoseObservation.JointName\nThe left ear.\nstatic let leftEye: VNHumanBodyPoseObservation.JointName\nThe left eye.\nstatic let rightEye: VNHumanBodyPoseObservation.JointName\nThe right eye.\nstatic let neck: VNHumanBodyPoseObservation.JointName\nThe neck.\nstatic let nose: VNHumanBodyPoseObservation.JointName\nThe nose."
  },
  {
    "title": "nose | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675615-nose",
    "html": "See Also\nHead\nstatic let leftEar: VNHumanBodyPoseObservation.JointName\nThe left ear.\nstatic let leftEye: VNHumanBodyPoseObservation.JointName\nThe left eye.\nstatic let rightEar: VNHumanBodyPoseObservation.JointName\nThe right ear.\nstatic let rightEye: VNHumanBodyPoseObservation.JointName\nThe right eye.\nstatic let neck: VNHumanBodyPoseObservation.JointName\nThe neck."
  },
  {
    "title": "leftShoulder | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675612-leftshoulder",
    "html": "See Also\nArms\nstatic let leftElbow: VNHumanBodyPoseObservation.JointName\nThe left elbow.\nstatic let leftWrist: VNHumanBodyPoseObservation.JointName\nThe left wrist.\nstatic let rightShoulder: VNHumanBodyPoseObservation.JointName\nThe right shoulder.\nstatic let rightElbow: VNHumanBodyPoseObservation.JointName\nThe right elbow.\nstatic let rightWrist: VNHumanBodyPoseObservation.JointName\nThe right wrist."
  },
  {
    "title": "leftAnkle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675606-leftankle",
    "html": "See Also\nLegs\nstatic let leftHip: VNHumanBodyPoseObservation.JointName\nThe left hip.\nstatic let leftKnee: VNHumanBodyPoseObservation.JointName\nThe left knee.\nstatic let rightHip: VNHumanBodyPoseObservation.JointName\nThe right hip.\nstatic let rightKnee: VNHumanBodyPoseObservation.JointName\nThe right knee.\nstatic let rightAnkle: VNHumanBodyPoseObservation.JointName\nThe right ankle."
  },
  {
    "title": "leftElbow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675608-leftelbow",
    "html": "See Also\nArms\nstatic let leftShoulder: VNHumanBodyPoseObservation.JointName\nThe left shoulder.\nstatic let leftWrist: VNHumanBodyPoseObservation.JointName\nThe left wrist.\nstatic let rightShoulder: VNHumanBodyPoseObservation.JointName\nThe right shoulder.\nstatic let rightElbow: VNHumanBodyPoseObservation.JointName\nThe right elbow.\nstatic let rightWrist: VNHumanBodyPoseObservation.JointName\nThe right wrist."
  },
  {
    "title": "leftArm | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointsgroupname/3675628-leftarm",
    "html": "See Also\nArms\nstatic let rightArm: VNHumanBodyPoseObservation.JointsGroupName\nThe right arm."
  },
  {
    "title": "leftFrontPaw | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname/4173254-leftfrontpaw",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left elbow.\nstatic let leftFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left elbow.\nstatic let rightFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right elbow.\nstatic let rightBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right elbow.\nstatic let leftBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left knee.\nstatic let leftFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left knee.\nstatic let rightBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right knee.\nstatic let rightFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right knee.\nstatic let leftBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left paw.\nstatic let rightBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right paw.\nstatic let rightFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right paw."
  },
  {
    "title": "tail | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointsgroupname/4173275-tail",
    "html": "See Also\nGetting the Group Names\nstatic let all: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents all joints.\nstatic let forelegs: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the forelegs.\nstatic let head: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the head.\nstatic let hindlegs: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the hindlegs.\nstatic let trunk: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the trunk."
  },
  {
    "title": "bodyLandmarkKeyLeftEye | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548242-bodylandmarkkeylefteye",
    "html": "See Also\nHead\nstatic let bodyLandmarkKeyRightEye: VNRecognizedPointKey\nThe right eye.\nDeprecated\nstatic let bodyLandmarkKeyLeftEar: VNRecognizedPointKey\nThe left ear.\nDeprecated\nstatic let bodyLandmarkKeyRightEar: VNRecognizedPointKey\nThe right ear.\nDeprecated\nstatic let bodyLandmarkKeyNose: VNRecognizedPointKey\nThe nose.\nDeprecated\nstatic let bodyLandmarkRegionKeyFace: VNRecognizedPointGroupKey\nA group key identifying the face, which includes the eyes, ears, and nose.\nDeprecated"
  },
  {
    "title": "bodyLandmarkKeyRightEye | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548251-bodylandmarkkeyrighteye",
    "html": "See Also\nHead\nstatic let bodyLandmarkKeyLeftEye: VNRecognizedPointKey\nThe left eye.\nDeprecated\nstatic let bodyLandmarkKeyLeftEar: VNRecognizedPointKey\nThe left ear.\nDeprecated\nstatic let bodyLandmarkKeyRightEar: VNRecognizedPointKey\nThe right ear.\nDeprecated\nstatic let bodyLandmarkKeyNose: VNRecognizedPointKey\nThe nose.\nDeprecated\nstatic let bodyLandmarkRegionKeyFace: VNRecognizedPointGroupKey\nA group key identifying the face, which includes the eyes, ears, and nose.\nDeprecated"
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointsgroupname/4176675-init",
    "html": "Parameters\nrawValue\n\nThe point group key."
  },
  {
    "title": "bodyLandmarkKeyRightKnee | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548253-bodylandmarkkeyrightknee",
    "html": "See Also\nLegs\nstatic let bodyLandmarkKeyRightAnkle: VNRecognizedPointKey\nThe right ankle.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right leg.\nDeprecated\nstatic let bodyLandmarkKeyLeftKnee: VNRecognizedPointKey\nThe left knee.\nDeprecated\nstatic let bodyLandmarkKeyLeftAnkle: VNRecognizedPointKey\nThe left ankle.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left leg.\nDeprecated"
  },
  {
    "title": "bodyLandmarkKeyLeftElbow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548241-bodylandmarkkeyleftelbow",
    "html": "See Also\nArms\nstatic let bodyLandmarkKeyRightWrist: VNRecognizedPointKey\nThe right wrist.\nDeprecated\nstatic let bodyLandmarkKeyRightElbow: VNRecognizedPointKey\nThe right elbow.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right arm.\nDeprecated\nstatic let bodyLandmarkKeyLeftWrist: VNRecognizedPointKey\nThe left wrist.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left arm.\nDeprecated"
  },
  {
    "title": "bodyLandmarkKeyLeftWrist | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548246-bodylandmarkkeyleftwrist",
    "html": "See Also\nArms\nstatic let bodyLandmarkKeyRightWrist: VNRecognizedPointKey\nThe right wrist.\nDeprecated\nstatic let bodyLandmarkKeyRightElbow: VNRecognizedPointKey\nThe right elbow.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right arm.\nDeprecated\nstatic let bodyLandmarkKeyLeftElbow: VNRecognizedPointKey\nThe left elbow.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left arm.\nDeprecated"
  },
  {
    "title": "init(xComponent:yComponent:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector/3548341-init",
    "html": "Parameters\nx\n\nThe x-component.\n\ny\n\nThe y-component.\n\nSee Also\nCreating a Vector\ninit(byAdding: VNVector, to: VNVector)\nCreates a new vector by adding the specified vectors.\ninit(bySubtracting: VNVector, from: VNVector)\nCreates a new vector by subtracting the first vector from the second vector.\ninit(byMultiplying: VNVector, byScalar: Double)\nCreates a new vector by multiplying the specified vector’s x-axis and y-axis projections by the scalar value.\ninit(r: Double, theta: Double)\nCreates a new vector in polar coordinate space.\ninit(vectorHead: VNPoint, tail: VNPoint)\nCreates a new vector in Cartesian coordinate space.\nclass var zero: VNVector\nA vector object with zero length."
  },
  {
    "title": "bodyLandmarkKeyRightAnkle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3548248-bodylandmarkkeyrightankle",
    "html": "See Also\nLegs\nstatic let bodyLandmarkKeyRightKnee: VNRecognizedPointKey\nThe right knee.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right leg.\nDeprecated\nstatic let bodyLandmarkKeyLeftKnee: VNRecognizedPointKey\nThe left knee.\nDeprecated\nstatic let bodyLandmarkKeyLeftAnkle: VNRecognizedPointKey\nThe left ankle.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left leg.\nDeprecated"
  },
  {
    "title": "analyze(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor/3675678-analyze",
    "html": "Parameters\ntimeRange\n\nThe time range to analyze. The value must be within the time range of the video asset.\n\nDiscussion\n\nThe system executes this method synchronously, so you typically call it from a separate dispatch queue. It returns when the video processor finishes analyzing the time range or if an error prevents processing.\n\nSee Also\nPerforming Requests\nfunc addRequest(VNRequest, processingOptions: VNVideoProcessor.RequestProcessingOptions)\nAdds a request with processing options to the video processor.\nclass VNVideoProcessor.RequestProcessingOptions\nAn object that defines a video processor’s configuration options.\nfunc removeRequest(VNRequest)\nRemoves a Vision request from the video processor’s request queue.\nfunc cancel()\nCancels the video processing.\nfunc add(VNRequest, withProcessingOptions: [VNVideoProcessingOption : Any])\nAdds a Vision request to perform with the specified configuration.\nDeprecated\nfunc analyze(with: CMTimeRange)\nAnalyzes the specifed time range of the video content.\nDeprecated"
  },
  {
    "title": "VNErrorCode.requestCancelled | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/requestcancelled",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "leftLeg | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointsgroupname/3675629-leftleg",
    "html": "See Also\nLegs\nstatic let rightLeg: VNHumanBodyPoseObservation.JointsGroupName\nThe right leg."
  },
  {
    "title": "rightArm | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointsgroupname/3675630-rightarm",
    "html": "See Also\nArms\nstatic let leftArm: VNHumanBodyPoseObservation.JointsGroupName\nThe left arm."
  },
  {
    "title": "string | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedtext/3152636-string",
    "html": "See Also\nDetermining Recognized Text\nvar confidence: VNConfidence\nA normalized confidence score for the text recognition result."
  },
  {
    "title": "confidence | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedtext/3152635-confidence",
    "html": "Discussion\n\nThe confidence level is a normalized value between 0.0 and 1.0, where 1.0 represents the highest confidence.\n\nSee Also\nDetermining Recognized Text\nvar string: String\nThe top candidate for recognized text."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalidentifier/3367888-init",
    "html": "Parameters\nrawValue\n\nThe raw value string."
  },
  {
    "title": "VNGeneratePersonSegmentationRequest.QualityLevel.balanced | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeneratepersonsegmentationrequest/qualitylevel/balanced",
    "html": "See Also\nQuality Levels\ncase accurate\nPrefers image quality over performance.\ncase fast\nPrefers performance over image quality."
  },
  {
    "title": "VNGeneratePersonSegmentationRequest.QualityLevel.accurate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeneratepersonsegmentationrequest/qualitylevel/accurate",
    "html": "See Also\nQuality Levels\ncase balanced\nPrefers processing that balances image quality and performance.\ncase fast\nPrefers performance over image quality."
  },
  {
    "title": "VNImageCropAndScaleOption.scaleFit | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagecropandscaleoption/scalefit",
    "html": "Discussion\n\nThis option fills the remaining space with transparent pixels to maintain aspect ratio.\n\nSee Also\nCrop and Scale Options\ncase centerCrop\nAn option that scales the image to fit its shorter side within the input dimensions, while preserving its aspect ratio, and center-crops the image.\ncase scaleFill\nAn option that scales the image to fill the input dimensions, resizing it if necessary.\ncase scaleFitRotate90CCW\nAn option that rotates the image 90 degrees counterclockwise and then scales it, while preserving its aspect ratio, to fit on the long side.\ncase scaleFillRotate90CCW\nAn option that rotates the image 90 degrees counterclockwise and then scales it to fill the input dimensions."
  },
  {
    "title": "VNImageCropAndScaleOption.scaleFitRotate90CCW | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagecropandscaleoption/scalefitrotate90ccw",
    "html": "Discussion\n\nThis option optimizes portrait images to fit into landscape buffers for algorithms that are rotation agnostic, and fills the remaining space with transparent pixels.\n\nSee Also\nCrop and Scale Options\ncase centerCrop\nAn option that scales the image to fit its shorter side within the input dimensions, while preserving its aspect ratio, and center-crops the image.\ncase scaleFit\nAn option that scales the image to fit its longer side within the input dimensions, while preserving its aspect ratio, and center-crops the image.\ncase scaleFill\nAn option that scales the image to fill the input dimensions, resizing it if necessary.\ncase scaleFillRotate90CCW\nAn option that rotates the image 90 degrees counterclockwise and then scales it to fill the input dimensions."
  },
  {
    "title": "payloadData | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodeobservation/4183553-payloaddata",
    "html": "See Also\nParsing the Payload\nvar payloadStringValue: String?\nA string value that represents the barcode payload.\nvar supplementalPayloadString: String?\nThe supplemental code decoded as a string value.\nvar supplementalPayloadData: Data?\nvar supplementalCompositeType: VNBarcodeCompositeType\nThe supplemental composite type.\nvar isGS1DataCarrier: Bool\nA Boolean value that indicates whether the barcode carries any global standards data."
  },
  {
    "title": "supplementalPayloadData | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodeobservation/4183554-supplementalpayloaddata",
    "html": "See Also\nParsing the Payload\nvar payloadStringValue: String?\nA string value that represents the barcode payload.\nvar payloadData: Data?\nThe raw data representation of the barcode’s payload.\nvar supplementalPayloadString: String?\nThe supplemental code decoded as a string value.\nvar supplementalCompositeType: VNBarcodeCompositeType\nThe supplemental composite type.\nvar isGS1DataCarrier: Bool\nA Boolean value that indicates whether the barcode carries any global standards data."
  },
  {
    "title": "cat | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalidentifier/3366117-cat",
    "html": "See Also\nAnimals\nstatic let dog: VNAnimalIdentifier\nAn animal identifier for dogs."
  },
  {
    "title": "VNImageCropAndScaleOption.scaleFillRotate90CCW | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagecropandscaleoption/scalefillrotate90ccw",
    "html": "Discussion\n\nThis option optimizes portrait images to fill into landscape buffers for algorithms that are rotation agnostic.\n\nSee Also\nCrop and Scale Options\ncase centerCrop\nAn option that scales the image to fit its shorter side within the input dimensions, while preserving its aspect ratio, and center-crops the image.\ncase scaleFit\nAn option that scales the image to fit its longer side within the input dimensions, while preserving its aspect ratio, and center-crops the image.\ncase scaleFill\nAn option that scales the image to fill the input dimensions, resizing it if necessary.\ncase scaleFitRotate90CCW\nAn option that rotates the image 90 degrees counterclockwise and then scales it, while preserving its aspect ratio, to fit on the long side."
  },
  {
    "title": "ciContext | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimageoption/2923494-cicontext",
    "html": "Discussion\n\nIf this key isn't specified, Vision will create its own CIContext.\n\nSpecify a CIContext when you've used one in processing an input CIImage or executing a CIFilter chain, so you can save the cost of creating a new context.\n\nSee Also\nOptions Dictionary Keys\nstatic let properties: VNImageOption\nThe dictionary from CGImageSourceCopyPropertiesAtIndex(_:_:_:) containing metadata for algorithms like horizon detection.\nstatic let cameraIntrinsics: VNImageOption\nAn option to specify the camera intrinstics as an NSData or CFData object representing a matrix_float3x3."
  },
  {
    "title": "properties | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimageoption/2869618-properties",
    "html": "See Also\nOptions Dictionary Keys\nstatic let cameraIntrinsics: VNImageOption\nAn option to specify the camera intrinstics as an NSData or CFData object representing a matrix_float3x3.\nstatic let ciContext: VNImageOption\nAn option key to specify the CIContext to be used in the handler's Core Image operations."
  },
  {
    "title": "VNElementType | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnelementtype",
    "html": "Topics\nElement Types\ncase unknown\nThe element type isn't known.\ncase float\nThe elements are floating-point numbers.\ncase double\nThe elements are double-precision floating-point numbers.\nRelationships\nConforms To\nSendable\nSee Also\nDetermining Types of Feature Prints\nvar elementType: VNElementType\nThe type of each element in the data.\nfunc VNElementTypeSize(VNElementType) -> Int\nReturns the size of a feature print element."
  },
  {
    "title": "supplementalPayloadString | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodeobservation/4173222-supplementalpayloadstring",
    "html": "See Also\nParsing the Payload\nvar payloadStringValue: String?\nA string value that represents the barcode payload.\nvar payloadData: Data?\nThe raw data representation of the barcode’s payload.\nvar supplementalPayloadData: Data?\nvar supplementalCompositeType: VNBarcodeCompositeType\nThe supplemental composite type.\nvar isGS1DataCarrier: Bool\nA Boolean value that indicates whether the barcode carries any global standards data."
  },
  {
    "title": "data | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfeatureprintobservation/3152630-data",
    "html": "Discussion\n\nThe data is divided into separate elements. Determine the type of element using elementType, and the number of elements using elementCount.\n\nSee Also\nFetching Feature Print Data\nvar elementCount: Int\nThe total number of elements in the data."
  },
  {
    "title": "supplementalCompositeType | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodeobservation/4173221-supplementalcompositetype",
    "html": "See Also\nParsing the Payload\nvar payloadStringValue: String?\nA string value that represents the barcode payload.\nvar payloadData: Data?\nThe raw data representation of the barcode’s payload.\nvar supplementalPayloadString: String?\nThe supplemental code decoded as a string value.\nvar supplementalPayloadData: Data?\nvar isGS1DataCarrier: Bool\nA Boolean value that indicates whether the barcode carries any global standards data."
  },
  {
    "title": "elementCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfeatureprintobservation/3152631-elementcount",
    "html": "See Also\nFetching Feature Print Data\nvar data: Data\nThe feature print data."
  },
  {
    "title": "barcodeDescriptor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodeobservation/2880296-barcodedescriptor",
    "html": "Discussion\n\nUse this object to have Core Image regenerate the observed barcode."
  },
  {
    "title": "ringPIP | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675656-ringpip",
    "html": "See Also\nRing\nstatic let ringTip: VNHumanHandPoseObservation.JointName\nThe tip of the ring finger.\nstatic let ringDIP: VNHumanHandPoseObservation.JointName\nThe ring finger’s distal interphalangeal (DIP) joint.\nstatic let ringMCP: VNHumanHandPoseObservation.JointName\nThe ring finger’s metacarpophalangeal (MCP) joint."
  },
  {
    "title": "middleFinger | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointsgroupname/3675667-middlefinger",
    "html": "See Also\nGroup Names\nstatic let thumb: VNHumanHandPoseObservation.JointsGroupName\nThe thumb.\nstatic let indexFinger: VNHumanHandPoseObservation.JointsGroupName\nThe index finger.\nstatic let littleFinger: VNHumanHandPoseObservation.JointsGroupName\nThe little finger.\nstatic let ringFinger: VNHumanHandPoseObservation.JointsGroupName\nThe ring finger.\nstatic let all: VNHumanHandPoseObservation.JointsGroupName\nAll hand group names."
  },
  {
    "title": "littleDIP | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675646-littledip",
    "html": "See Also\nLittle\nstatic let littleTip: VNHumanHandPoseObservation.JointName\nThe tip of the little finger.\nstatic let littlePIP: VNHumanHandPoseObservation.JointName\nThe little finger’s proximal interphalangeal (PIP) joint.\nstatic let littleMCP: VNHumanHandPoseObservation.JointName\nThe little finger’s metacarpophalangeal (MCP) joint."
  },
  {
    "title": "thumb | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointsgroupname/3675669-thumb",
    "html": "See Also\nGroup Names\nstatic let indexFinger: VNHumanHandPoseObservation.JointsGroupName\nThe index finger.\nstatic let littleFinger: VNHumanHandPoseObservation.JointsGroupName\nThe little finger.\nstatic let middleFinger: VNHumanHandPoseObservation.JointsGroupName\nThe middle finger.\nstatic let ringFinger: VNHumanHandPoseObservation.JointsGroupName\nThe ring finger.\nstatic let all: VNHumanHandPoseObservation.JointsGroupName\nAll hand group names."
  },
  {
    "title": "indexFinger | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointsgroupname/3675665-indexfinger",
    "html": "See Also\nGroup Names\nstatic let thumb: VNHumanHandPoseObservation.JointsGroupName\nThe thumb.\nstatic let littleFinger: VNHumanHandPoseObservation.JointsGroupName\nThe little finger.\nstatic let middleFinger: VNHumanHandPoseObservation.JointsGroupName\nThe middle finger.\nstatic let ringFinger: VNHumanHandPoseObservation.JointsGroupName\nThe ring finger.\nstatic let all: VNHumanHandPoseObservation.JointsGroupName\nAll hand group names."
  },
  {
    "title": "ringFinger | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointsgroupname/3675668-ringfinger",
    "html": "See Also\nGroup Names\nstatic let thumb: VNHumanHandPoseObservation.JointsGroupName\nThe thumb.\nstatic let indexFinger: VNHumanHandPoseObservation.JointsGroupName\nThe index finger.\nstatic let littleFinger: VNHumanHandPoseObservation.JointsGroupName\nThe little finger.\nstatic let middleFinger: VNHumanHandPoseObservation.JointsGroupName\nThe middle finger.\nstatic let all: VNHumanHandPoseObservation.JointsGroupName\nAll hand group names."
  },
  {
    "title": "littleFinger | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointsgroupname/3675666-littlefinger",
    "html": "See Also\nGroup Names\nstatic let thumb: VNHumanHandPoseObservation.JointsGroupName\nThe thumb.\nstatic let indexFinger: VNHumanHandPoseObservation.JointsGroupName\nThe index finger.\nstatic let middleFinger: VNHumanHandPoseObservation.JointsGroupName\nThe middle finger.\nstatic let ringFinger: VNHumanHandPoseObservation.JointsGroupName\nThe ring finger.\nstatic let all: VNHumanHandPoseObservation.JointsGroupName\nAll hand group names."
  },
  {
    "title": "VNChirality.right | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnchirality/right",
    "html": "See Also\nChirality Values\ncase left\nIndicates a left-handed pose.\ncase unknown\nIndicates that the pose chirality is unknown."
  },
  {
    "title": "contourCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontoursobservation/3548362-contourcount",
    "html": "Discussion\n\nUse this value to determine the number of indices available for calling contour(at:).\n\nSee Also\nInspecting the Observation\nvar normalizedPath: CGPath\nThe detected contours as a path object in normalized coordinates.\nvar topLevelContours: [VNContour]\nAn array of contours that don’t have another contour enclosing them.\nvar topLevelContourCount: Int\nThe total number of detected top-level contours.\nfunc contour(at: Int) -> VNContour\nRetrieves the contour object at the specified index, irrespective of hierarchy.\nfunc contour(at: IndexPath) -> VNContour\nRetrieves the contour object at the specified index path.\nclass VNContour\nA class that represents a detected contour in an image."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname/3675918-init",
    "html": "Parameters\nrawValue\n\nThe recognized point key."
  },
  {
    "title": "leftHip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675610-lefthip",
    "html": "See Also\nLegs\nstatic let leftKnee: VNHumanBodyPoseObservation.JointName\nThe left knee.\nstatic let leftAnkle: VNHumanBodyPoseObservation.JointName\nThe left ankle.\nstatic let rightHip: VNHumanBodyPoseObservation.JointName\nThe right hip.\nstatic let rightKnee: VNHumanBodyPoseObservation.JointName\nThe right knee.\nstatic let rightAnkle: VNHumanBodyPoseObservation.JointName\nThe right ankle."
  },
  {
    "title": "leftWrist | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675613-leftwrist",
    "html": "See Also\nArms\nstatic let leftShoulder: VNHumanBodyPoseObservation.JointName\nThe left shoulder.\nstatic let leftElbow: VNHumanBodyPoseObservation.JointName\nThe left elbow.\nstatic let rightShoulder: VNHumanBodyPoseObservation.JointName\nThe right shoulder.\nstatic let rightElbow: VNHumanBodyPoseObservation.JointName\nThe right elbow.\nstatic let rightWrist: VNHumanBodyPoseObservation.JointName\nThe right wrist."
  },
  {
    "title": "rightShoulder | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675622-rightshoulder",
    "html": "See Also\nArms\nstatic let leftShoulder: VNHumanBodyPoseObservation.JointName\nThe left shoulder.\nstatic let leftElbow: VNHumanBodyPoseObservation.JointName\nThe left elbow.\nstatic let leftWrist: VNHumanBodyPoseObservation.JointName\nThe left wrist.\nstatic let rightElbow: VNHumanBodyPoseObservation.JointName\nThe right elbow.\nstatic let rightWrist: VNHumanBodyPoseObservation.JointName\nThe right wrist."
  },
  {
    "title": "rightElbow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675618-rightelbow",
    "html": "See Also\nArms\nstatic let leftShoulder: VNHumanBodyPoseObservation.JointName\nThe left shoulder.\nstatic let leftElbow: VNHumanBodyPoseObservation.JointName\nThe left elbow.\nstatic let leftWrist: VNHumanBodyPoseObservation.JointName\nThe left wrist.\nstatic let rightShoulder: VNHumanBodyPoseObservation.JointName\nThe right shoulder.\nstatic let rightWrist: VNHumanBodyPoseObservation.JointName\nThe right wrist."
  },
  {
    "title": "rightWrist | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675623-rightwrist",
    "html": "See Also\nArms\nstatic let leftShoulder: VNHumanBodyPoseObservation.JointName\nThe left shoulder.\nstatic let leftElbow: VNHumanBodyPoseObservation.JointName\nThe left elbow.\nstatic let leftWrist: VNHumanBodyPoseObservation.JointName\nThe left wrist.\nstatic let rightShoulder: VNHumanBodyPoseObservation.JointName\nThe right shoulder.\nstatic let rightElbow: VNHumanBodyPoseObservation.JointName\nThe right elbow."
  },
  {
    "title": "rightAnkle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675616-rightankle",
    "html": "See Also\nLegs\nstatic let leftHip: VNHumanBodyPoseObservation.JointName\nThe left hip.\nstatic let leftKnee: VNHumanBodyPoseObservation.JointName\nThe left knee.\nstatic let leftAnkle: VNHumanBodyPoseObservation.JointName\nThe left ankle.\nstatic let rightHip: VNHumanBodyPoseObservation.JointName\nThe right hip.\nstatic let rightKnee: VNHumanBodyPoseObservation.JointName\nThe right knee."
  },
  {
    "title": "leftKnee | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675611-leftknee",
    "html": "See Also\nLegs\nstatic let leftHip: VNHumanBodyPoseObservation.JointName\nThe left hip.\nstatic let leftAnkle: VNHumanBodyPoseObservation.JointName\nThe left ankle.\nstatic let rightHip: VNHumanBodyPoseObservation.JointName\nThe right hip.\nstatic let rightKnee: VNHumanBodyPoseObservation.JointName\nThe right knee.\nstatic let rightAnkle: VNHumanBodyPoseObservation.JointName\nThe right ankle."
  },
  {
    "title": "rightKnee | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675621-rightknee",
    "html": "See Also\nLegs\nstatic let leftHip: VNHumanBodyPoseObservation.JointName\nThe left hip.\nstatic let leftKnee: VNHumanBodyPoseObservation.JointName\nThe left knee.\nstatic let leftAnkle: VNHumanBodyPoseObservation.JointName\nThe left ankle.\nstatic let rightHip: VNHumanBodyPoseObservation.JointName\nThe right hip.\nstatic let rightAnkle: VNHumanBodyPoseObservation.JointName\nThe right ankle."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675916-init",
    "html": "Parameters\nrawValue\n\nThe recognized point key."
  },
  {
    "title": "rightHip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname/3675620-righthip",
    "html": "See Also\nLegs\nstatic let leftHip: VNHumanBodyPoseObservation.JointName\nThe left hip.\nstatic let leftKnee: VNHumanBodyPoseObservation.JointName\nThe left knee.\nstatic let leftAnkle: VNHumanBodyPoseObservation.JointName\nThe left ankle.\nstatic let rightKnee: VNHumanBodyPoseObservation.JointName\nThe right knee.\nstatic let rightAnkle: VNHumanBodyPoseObservation.JointName\nThe right ankle."
  },
  {
    "title": "init(byAdding:to:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector/3548347-init",
    "html": "Parameters\nv1\n\nThe first vector.\n\nv2\n\nThe second vector.\n\nSee Also\nCreating a Vector\ninit(bySubtracting: VNVector, from: VNVector)\nCreates a new vector by subtracting the first vector from the second vector.\ninit(byMultiplying: VNVector, byScalar: Double)\nCreates a new vector by multiplying the specified vector’s x-axis and y-axis projections by the scalar value.\ninit(r: Double, theta: Double)\nCreates a new vector in polar coordinate space.\ninit(vectorHead: VNPoint, tail: VNPoint)\nCreates a new vector in Cartesian coordinate space.\ninit(xComponent: Double, yComponent: Double)\nCreates a new vector in Cartesian coordinate space, based on its x-axis and y-axis projections.\nclass var zero: VNVector\nA vector object with zero length."
  },
  {
    "title": "boundingCircle(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeometryutils/3548354-boundingcircle",
    "html": "Parameters\ncontour\n\nA contour around which to calculate the bounding circle.\n\nReturn Value\n\nThe bounding VNCircle object.\n\nSee Also\nCalculating Bounding Circles\nclass func boundingCircle(for: [VNPoint]) -> VNCircle\nCalculates a bounding circle for the specified array of points.\nclass func boundingCircle(forSIMDPoints: UnsafePointer<simd_float2>, pointCount: Int) -> VNCircle\nCalculates a bounding circle for the specified points."
  },
  {
    "title": "dotProduct(of:vector:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector/3548338-dotproduct",
    "html": "Parameters\nv1\n\nThe first vector.\n\nv2\n\nThe second vector.\n\nReturn Value\n\nThe dot product value.\n\nSee Also\nInspecting a Vector\nvar length: Double\nThe length, or absolute value, of the vector.\nvar r: Double\nThe radius, absolute value, or length of the vector.\nvar theta: Double\nThe angle between the vector direction and the positive direction of the x-axis.\nvar squaredLength: Double\nThe squared length of the vector.\nvar x: Double\nA signed projection that indicates the vector’s direction on the x-axis.\nvar y: Double\nA signed projection that indicates the vector’s direction on the y-axis.\nclass func unitVector(for: VNVector) -> VNVector\nCalculates a vector that’s normalized by preserving its direction, so that the vector length equals 1.0."
  },
  {
    "title": "add(_:withProcessingOptions:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor/3548383-add",
    "html": "Deprecated\n\nUse addRequest(_:processingOptions:) instead.\n\nParameters\nrequest\n\nThe request to add to the processing queue.\n\nconfiguration\n\nA dictionary containing the request configuration.\n\nTopics\nVideo Processing Options\nstruct VNVideoProcessingOption\nOptions to pass to the video processor when adding requests.\nSee Also\nPerforming Requests\nfunc addRequest(VNRequest, processingOptions: VNVideoProcessor.RequestProcessingOptions)\nAdds a request with processing options to the video processor.\nclass VNVideoProcessor.RequestProcessingOptions\nAn object that defines a video processor’s configuration options.\nfunc removeRequest(VNRequest)\nRemoves a Vision request from the video processor’s request queue.\nfunc analyze(CMTimeRange)\nAnalyzes a time range of video content.\nfunc cancel()\nCancels the video processing.\nfunc analyze(with: CMTimeRange)\nAnalyzes the specifed time range of the video content.\nDeprecated"
  },
  {
    "title": "removeRequest(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor/3548387-removerequest",
    "html": "Parameters\nrequest\n\nThe request to remove.\n\nSee Also\nPerforming Requests\nfunc addRequest(VNRequest, processingOptions: VNVideoProcessor.RequestProcessingOptions)\nAdds a request with processing options to the video processor.\nclass VNVideoProcessor.RequestProcessingOptions\nAn object that defines a video processor’s configuration options.\nfunc analyze(CMTimeRange)\nAnalyzes a time range of video content.\nfunc cancel()\nCancels the video processing.\nfunc add(VNRequest, withProcessingOptions: [VNVideoProcessingOption : Any])\nAdds a Vision request to perform with the specified configuration.\nDeprecated\nfunc analyze(with: CMTimeRange)\nAnalyzes the specifed time range of the video content.\nDeprecated"
  },
  {
    "title": "cancel() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor/3548385-cancel",
    "html": "See Also\nPerforming Requests\nfunc addRequest(VNRequest, processingOptions: VNVideoProcessor.RequestProcessingOptions)\nAdds a request with processing options to the video processor.\nclass VNVideoProcessor.RequestProcessingOptions\nAn object that defines a video processor’s configuration options.\nfunc removeRequest(VNRequest)\nRemoves a Vision request from the video processor’s request queue.\nfunc analyze(CMTimeRange)\nAnalyzes a time range of video content.\nfunc add(VNRequest, withProcessingOptions: [VNVideoProcessingOption : Any])\nAdds a Vision request to perform with the specified configuration.\nDeprecated\nfunc analyze(with: CMTimeRange)\nAnalyzes the specifed time range of the video content.\nDeprecated"
  },
  {
    "title": "analyze(with:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor/3548384-analyze",
    "html": "Deprecated\n\nUse analyze(_:) instead.\n\nParameters\ntimeRange\n\nThe time range to analyze. The value specified must be within the time range of the video asset.\n\nDiscussion\n\nThe system executes this method synchronously, so you should typically call it from a separate dispatch queue. It returns when the video processor finishes analyzing the specified time range, or if an error prevented the processing.\n\nSee Also\nPerforming Requests\nfunc addRequest(VNRequest, processingOptions: VNVideoProcessor.RequestProcessingOptions)\nAdds a request with processing options to the video processor.\nclass VNVideoProcessor.RequestProcessingOptions\nAn object that defines a video processor’s configuration options.\nfunc removeRequest(VNRequest)\nRemoves a Vision request from the video processor’s request queue.\nfunc analyze(CMTimeRange)\nAnalyzes a time range of video content.\nfunc cancel()\nCancels the video processing.\nfunc add(VNRequest, withProcessingOptions: [VNVideoProcessingOption : Any])\nAdds a Vision request to perform with the specified configuration.\nDeprecated"
  },
  {
    "title": "init(requestRevision:topLeft:topRight:bottomRight:bottomLeft:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrectangleobservation/4144381-init",
    "html": "Parameters\nrequestRevision\n\nThe rectangle detector revision number. A higher revision indicates more recent iterations of the framework.\n\ntopLeft\n\nThe upper-left corner point.\n\ntopRight\n\nThe upper-right corner point.\n\nbottomRight\n\nThe lower-right corner point.\n\nbottomLeft\n\nThe lower-left corner point.\n\nSee Also\nCreating an Observation\ninit(requestRevision: Int, topLeft: CGPoint, bottomLeft: CGPoint, bottomRight: CGPoint, topRight: CGPoint)\nCreates a rectangle observation from its corner points.\nDeprecated"
  },
  {
    "title": "bottomLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrectangleobservation/2867201-bottomleft",
    "html": "See Also\nAccessing the Coordinates\nvar bottomRight: CGPoint\nThe coordinates of the lower-right corner of the observation bounding box.\nvar topLeft: CGPoint\nThe coordinates of the upper-left corner of the observation bounding box.\nvar topRight: CGPoint\nThe coordinates of the upper-right corner of the observation bounding box."
  },
  {
    "title": "topLeft | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrectangleobservation/2867210-topleft",
    "html": "See Also\nAccessing the Coordinates\nvar bottomLeft: CGPoint\nThe coordinates of the lower-left corner of the observation bounding box.\nvar bottomRight: CGPoint\nThe coordinates of the lower-right corner of the observation bounding box.\nvar topRight: CGPoint\nThe coordinates of the upper-right corner of the observation bounding box."
  },
  {
    "title": "bottomRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrectangleobservation/2867226-bottomright",
    "html": "See Also\nAccessing the Coordinates\nvar bottomLeft: CGPoint\nThe coordinates of the lower-left corner of the observation bounding box.\nvar topLeft: CGPoint\nThe coordinates of the upper-left corner of the observation bounding box.\nvar topRight: CGPoint\nThe coordinates of the upper-right corner of the observation bounding box."
  },
  {
    "title": "VNContour | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontour",
    "html": "Topics\nInspecting the Contour\nvar aspectRatio: Float\nThe aspect ratio of the contour.\nvar indexPath: IndexPath\nThe contour object’s index path.\nvar normalizedPath: CGPath\nThe contour object as a path in normalized coordinates.\nvar pointCount: Int\nThe contour’s number of points.\nvar normalizedPoints: [simd_float2]\nThe contour’s array of points in normalized coordinates.\nfunc polygonApproximation(epsilon: Float) -> VNContour\nSimplifies the contour to a polygon using a Ramer-Douglas-Peucker algorithm.\nAccessing Child Contours\nvar childContourCount: Int\nThe total number of detected child contours.\nvar childContours: [VNContour]\nAn array of contours that this contour encloses.\nfunc childContour(at: Int) -> VNContour\nRetrieves the child contour object at the specified index.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nVNRequestRevisionProviding\nSee Also\nInspecting the Observation\nvar contourCount: Int\nThe total number of detected contours.\nvar normalizedPath: CGPath\nThe detected contours as a path object in normalized coordinates.\nvar topLevelContours: [VNContour]\nAn array of contours that don’t have another contour enclosing them.\nvar topLevelContourCount: Int\nThe total number of detected top-level contours.\nfunc contour(at: Int) -> VNContour\nRetrieves the contour object at the specified index, irrespective of hierarchy.\nfunc contour(at: IndexPath) -> VNContour\nRetrieves the contour object at the specified index path."
  },
  {
    "title": "topRight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrectangleobservation/2867233-topright",
    "html": "See Also\nAccessing the Coordinates\nvar bottomLeft: CGPoint\nThe coordinates of the lower-left corner of the observation bounding box.\nvar bottomRight: CGPoint\nThe coordinates of the lower-right corner of the observation bounding box.\nvar topLeft: CGPoint\nThe coordinates of the upper-left corner of the observation bounding box."
  },
  {
    "title": "model | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncoremlrequest/2890150-model",
    "html": "Discussion\n\nThis object wraps a Core ML model.\n\nSee Also\nInitializing with a Core ML Model\ninit(model: VNCoreMLModel)\nCreates a model container to use with an image analysis request based on the model you provide.\ninit(model: VNCoreMLModel, completionHandler: VNRequestCompletionHandler?)\nCreates a model container to use with an image analysis request based on the model you provide, with an optional completion handler.\nclass VNCoreMLModel\nA container for the model to use with Vision requests."
  },
  {
    "title": "init(model:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncoremlrequest/2890146-init",
    "html": "Parameters\nmodel\n\nThe Core ML model on which to base the Vision request.\n\nDiscussion\n\nInitialization can fail if the Core ML model you provide isn’t supported in Vision, such as if the model doesn’t accept an image as input.\n\nSee Also\nInitializing with a Core ML Model\ninit(model: VNCoreMLModel, completionHandler: VNRequestCompletionHandler?)\nCreates a model container to use with an image analysis request based on the model you provide, with an optional completion handler.\nvar model: VNCoreMLModel\nThe model to base the image analysis request on.\nclass VNCoreMLModel\nA container for the model to use with Vision requests."
  },
  {
    "title": "VNErrorCode.OK | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/ok",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNCoreMLRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncoremlrequestrevision1",
    "html": "Discussion\n\nThe revision number is a constant that you pass on a per-request basis to indicate to the Vision framework which version of the Core ML algorithm to use for that request. Each OS release in which the framework improves aspects of the algorithm (recognition speed, accuracy, number of languages supported, and so forth), the revision number increments by 1.\n\nBy default, recognition requests use the latest—the highest—revision number for the SDK that your app links against. If you don’t recompile your app against a newer SDK, your app binary uses the revision that was the default at the time you last compiled it. If you do recompile, your app uses the default of the new SDK.\n\nIf your app must support users on older OS versions that don’t have access to the latest Vision framework, you may want to specify an earlier revision. For example, your algorithm may depend on specific behavior from a Vision request, such as writing your image processing algorithm to assume the size or aspect ratio of bounding boxes from an older revision of the face detector. In such a scenario, you can support earlier versions of the algorithm by specifying lower numbers:\n\nvisionRequest.revision = VNCoreMLRequestRevision1\n"
  },
  {
    "title": "VNRequestRevisionUnspecified | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequestrevisionunspecified",
    "html": "See Also\nDetermining Revision Type\nlet VNDetectRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the rectangle detection request.\nlet VNTrackRectangleRequestRevision1: Int\nA constant for specifying revision 1 of the rectangling tracking request.\nlet VNTrackObjectRequestRevision1: Int\nA constant for specifying revision 1 of the object tracking request.\nlet VNDetectFaceRectanglesRequestRevision2: Int\nA constant for specifying revision 2 of the face rectangles detection request.\nlet VNDetectFaceRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the face rectangles detection request.\nDeprecated\nlet VNDetectFaceLandmarksRequestRevision3: Int\nA constant for specifying revision 3 of the face landmarks detection request.\nlet VNDetectFaceLandmarksRequestRevision2: Int\nA constant for specifying revision 2 of the face landmarks detection request.\nlet VNDetectFaceLandmarksRequestRevision1: Int\nA constant for specifying revision 1 of the face landmarks detection request.\nDeprecated\nlet VNRecognizeTextRequestRevision1: Int\nA constant for specifying revision 1 of the text recognition request.\nlet VNDetectTextRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the text rectangles detection request.\nlet VNDetectBarcodesRequestRevision1: Int\nA constant for specifying revision 1 of the barcode detection request.\nDeprecated\nlet VNDetectHorizonRequestRevision1: Int\nA constant for specifying revision 1 of the horizon detection request.\nlet VNTranslationalImageRegistrationRequestRevision1: Int\nA constant for specifying revision 1 of the translational image registration request.\nlet VNHomographicImageRegistrationRequestRevision1: Int\nA constant for specifying revision 1 of the homographic image registration request.\nlet VNCoreMLRequestRevision1: Int\nA constant for specifying revision 1 of a Core ML request.\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision1: Int\nA constant for specifying revision 1 of the image saliency request.\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision1: Int\nA constant for specifying revision 1 of the image saliency request.\nlet VNClassifyImageRequestRevision1: Int\nA constant for specifying revision 1 of the image classification request.\nlet VNGenerateImageFeaturePrintRequestRevision1: Int\nA constant for specifying revision 1 of the feature print request.\nlet VNDetectFaceCaptureQualityRequestRevision1: Int\nA constant for specifying revision 1 of the face capture detection request.\nlet VNDetectHumanRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the human rectangles detection request."
  },
  {
    "title": "VNErrorCode.turiCoreErrorCode | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/turicoreerrorcode",
    "html": "See Also\nError Codes\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "supportedComputeStageDevicesAndReturnError: | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/4173243-supportedcomputestagedevicesandr",
    "html": "Return Value\n\nA dictionary of per-stage compute devices; otherwise, nil if an error occurs.\n\nSee Also\nConfiguring the Compute Device\n- setComputeDevice:forComputeStage:\nAssigns a compute device for a compute stage.\n- computeDeviceForComputeStage:\nReturns the compute device for a compute stage."
  },
  {
    "title": "VNErrorCode.dataUnavailable | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/dataunavailable",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNHumanBodyPose3DObservation.HeightEstimation.measured | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/heightestimation/measured",
    "html": "See Also\nTechniques\ncase reference\nA technique that uses a reference height."
  },
  {
    "title": "requestRevision | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequestrevisionproviding/2967114-requestrevision",
    "html": "Required"
  },
  {
    "title": "VNErrorCode.invalidModel | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/invalidmodel",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNErrorCode.invalidOption | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/invalidoption",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNErrorCode.internalError | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/internalerror",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNErrorCode.invalidArgument | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/invalidargument",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNErrorCode.invalidFormat | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/invalidformat",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNErrorCode.invalidOperation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/invalidoperation",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNErrorCode.ioError | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/ioerror",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNErrorCode.operationFailed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/operationfailed",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNErrorCode.outOfBoundsError | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/outofboundserror",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNErrorCode.notImplemented | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/notimplemented",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNErrorCode.outOfMemory | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/outofmemory",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "rightLeg | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointsgroupname/3675631-rightleg",
    "html": "See Also\nLegs\nstatic let leftLeg: VNHumanBodyPoseObservation.JointsGroupName\nThe left leg."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointsgroupname/3675917-init",
    "html": "Parameters\nrawValue\n\nThe recognized point group key."
  },
  {
    "title": "VNRequestTrackingLevel.fast | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequesttrackinglevel/fast",
    "html": "Discussion\n\nThis is the default option used by trackers."
  },
  {
    "title": "boundingBox(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedtext/3213755-boundingbox",
    "html": "Parameters\nrange\n\nThe range of the characters in the text string to draw a bounding box around.\n\nReturn Value\n\nA bounding box.\n\nDiscussion\n\nBounding boxes aren’t always an exact fit around the characters. Use them to display in user interfaces to provide general guidance, but avoid using their contents for image processing.\n\nThe bounding box that returns from this method differs based on the value of recognitionLevel, as follows:\n\nVNRequestTextRecognitionLevel.fast\n\n\t\n\nThe bounding box has character precision. When you specify a range from 0 to 1, the value that returns is the first character’s bounding box.\n\n\n\n\nVNRequestTextRecognitionLevel.accurate\n\n\t\n\nThe bounding box has word precision. When you specify a range from 0 to 1, or 1 to 2, the value that returns is the bounding box of the whole word containing the first character."
  },
  {
    "title": "VNGeneratePersonSegmentationRequest.QualityLevel.fast | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeneratepersonsegmentationrequest/qualitylevel/fast",
    "html": "See Also\nQuality Levels\ncase accurate\nPrefers image quality over performance.\ncase balanced\nPrefers processing that balances image quality and performance."
  },
  {
    "title": "dog | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalidentifier/3366118-dog",
    "html": "See Also\nAnimals\nstatic let cat: VNAnimalIdentifier\nAn animal identifier for cats."
  },
  {
    "title": "computeDistance(_:to:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfeatureprintobservation/3182823-computedistance",
    "html": "Parameters\noutDistance\n\nA pointer to store the calculated distance value.\n\nfeaturePrint\n\nThe feature print object whose distance to calculate.\n\nReturn Value\n\ntrue if the operation is successful, otherwise false.\n\nDiscussion\n\nShorter distances indicate greater similarity between feature prints."
  },
  {
    "title": "elementType | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfeatureprintobservation/3152632-elementtype",
    "html": "See Also\nDetermining Types of Feature Prints\nenum VNElementType\nAn enumeration of the type of element in feature print data.\nfunc VNElementTypeSize(VNElementType) -> Int\nReturns the size of a feature print element."
  },
  {
    "title": "pointsInImage(imageSize:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarkregion2d/2928156-pointsinimage",
    "html": "Parameters\nimageSize\n\nThe pixel dimensions of the image in which to present landmark points.\n\nReturn Value\n\nAn array containing a CGPoint for each landmark the system detects in the image, expressed in the coordinate space of the specified image size."
  },
  {
    "title": "init(x:y:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpoint/3548331-init",
    "html": "Parameters\nx\n\nThe x-coordinate value.\n\ny\n\nThe y-coordinate value.\n\nSee Also\nCreating a Point\ninit(location: CGPoint)\nCreates a point object from the specified Core Graphics point.\nclass func apply(VNVector, to: VNPoint) -> VNPoint\nCreates a point object that’s shifted by the X and Y offsets of the specified vector.\nclass var zero: VNPoint\nA point object that represents the origin."
  },
  {
    "title": "init(requestRevision:boundingBox:roll:yaw:pitch:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfaceobservation/3763078-init",
    "html": "Parameters\nrequestRevision\n\nThe revision of the request.\n\nboundingBox\n\nThe bounding rectangle of the detected face landmark.\n\nroll\n\nThe rotational angle of the face landmark around the z-axis.\n\nyaw\n\nThe rotational angle of the face landmark around the y-axis.\n\npitch\n\nThe rotational angle of the face landmark around the z-axis.\n\nSee Also\nCreating an Observation\ninit(requestRevision: Int, boundingBox: CGRect, roll: NSNumber?, yaw: NSNumber?)\nCreates an observation that contains the roll and yaw of the face.\nDeprecated"
  },
  {
    "title": "VNRequestFaceLandmarksConstellation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequestfacelandmarksconstellation",
    "html": "Topics\nTypes of Constellations\ncase constellationNotDefined\nAn undefined constellation.\ncase constellation65Points\nA constellation with 65 points.\ncase constellation76Points\nA constellation with 76 points.\nRelationships\nConforms To\nSendable\nSee Also\nLocating Face Landmarks\nvar constellation: VNRequestFaceLandmarksConstellation\nA variable that describes how a face landmarks request orders or enumerates the resulting features."
  },
  {
    "title": "keepNetworkOutput | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateopticalflowrequest/3951397-keepnetworkoutput",
    "html": "Discussion\n\nThe default is false. When you set this to true, the system ignores outputPixelFormat. Setting this for revision 1 has no effect because it’s not machine learning-based.\n\nSee Also\nConfiguring the Request\nvar computationAccuracy: VNGenerateOpticalFlowRequest.ComputationAccuracy\nThe accuracy level for computing optical flow.\nenum VNGenerateOpticalFlowRequest.ComputationAccuracy\nThe supported optical flow accuracy levels.\nvar outputPixelFormat: OSType\nThe output buffer’s pixel format."
  },
  {
    "title": "VNErrorCode.timeStampNotFound | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/timestampnotfound",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNErrorCode.unsupportedRevision | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/unsupportedrevision",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "squaredLength | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector/3548344-squaredlength",
    "html": "See Also\nInspecting a Vector\nvar length: Double\nThe length, or absolute value, of the vector.\nvar r: Double\nThe radius, absolute value, or length of the vector.\nvar theta: Double\nThe angle between the vector direction and the positive direction of the x-axis.\nvar x: Double\nA signed projection that indicates the vector’s direction on the x-axis.\nvar y: Double\nA signed projection that indicates the vector’s direction on the y-axis.\nclass func dotProduct(of: VNVector, vector: VNVector) -> Double\nCaclulates the dot product of two vectors.\nclass func unitVector(for: VNVector) -> VNVector\nCalculates a vector that’s normalized by preserving its direction, so that the vector length equals 1.0."
  },
  {
    "title": "VNErrorCode.unsupportedRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/unsupportedrequest",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNErrorCode.unsupportedComputeStage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/unsupportedcomputestage",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNErrorCode.unsupportedComputeDevice | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/unsupportedcomputedevice",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "VNErrorCode.timeout | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/timeout",
    "html": "See Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage."
  },
  {
    "title": "init(bySubtracting:from:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector/3548349-init",
    "html": "Parameters\nv1\n\nThe first vector.\n\nv2\n\nThe second vector.\n\nSee Also\nCreating a Vector\ninit(byAdding: VNVector, to: VNVector)\nCreates a new vector by adding the specified vectors.\ninit(byMultiplying: VNVector, byScalar: Double)\nCreates a new vector by multiplying the specified vector’s x-axis and y-axis projections by the scalar value.\ninit(r: Double, theta: Double)\nCreates a new vector in polar coordinate space.\ninit(vectorHead: VNPoint, tail: VNPoint)\nCreates a new vector in Cartesian coordinate space.\ninit(xComponent: Double, yComponent: Double)\nCreates a new vector in Cartesian coordinate space, based on its x-axis and y-axis projections.\nclass var zero: VNVector\nA vector object with zero length."
  },
  {
    "title": "init(byMultiplying:byScalar:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector/3548348-init",
    "html": "Parameters\nvector\n\nThe vector.\n\nscalar\n\nThe scalar value by which to multiply the x-axis and y-axis projections.\n\nSee Also\nCreating a Vector\ninit(byAdding: VNVector, to: VNVector)\nCreates a new vector by adding the specified vectors.\ninit(bySubtracting: VNVector, from: VNVector)\nCreates a new vector by subtracting the first vector from the second vector.\ninit(r: Double, theta: Double)\nCreates a new vector in polar coordinate space.\ninit(vectorHead: VNPoint, tail: VNPoint)\nCreates a new vector in Cartesian coordinate space.\ninit(xComponent: Double, yComponent: Double)\nCreates a new vector in Cartesian coordinate space, based on its x-axis and y-axis projections.\nclass var zero: VNVector\nA vector object with zero length."
  },
  {
    "title": "y | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector/3548351-y",
    "html": "See Also\nInspecting a Vector\nvar length: Double\nThe length, or absolute value, of the vector.\nvar r: Double\nThe radius, absolute value, or length of the vector.\nvar theta: Double\nThe angle between the vector direction and the positive direction of the x-axis.\nvar squaredLength: Double\nThe squared length of the vector.\nvar x: Double\nA signed projection that indicates the vector’s direction on the x-axis.\nclass func dotProduct(of: VNVector, vector: VNVector) -> Double\nCaclulates the dot product of two vectors.\nclass func unitVector(for: VNVector) -> VNVector\nCalculates a vector that’s normalized by preserving its direction, so that the vector length equals 1.0."
  },
  {
    "title": "length | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector/3548342-length",
    "html": "See Also\nInspecting a Vector\nvar r: Double\nThe radius, absolute value, or length of the vector.\nvar theta: Double\nThe angle between the vector direction and the positive direction of the x-axis.\nvar squaredLength: Double\nThe squared length of the vector.\nvar x: Double\nA signed projection that indicates the vector’s direction on the x-axis.\nvar y: Double\nA signed projection that indicates the vector’s direction on the y-axis.\nclass func dotProduct(of: VNVector, vector: VNVector) -> Double\nCaclulates the dot product of two vectors.\nclass func unitVector(for: VNVector) -> VNVector\nCalculates a vector that’s normalized by preserving its direction, so that the vector length equals 1.0."
  },
  {
    "title": "zero | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector/3548352-zero",
    "html": "See Also\nCreating a Vector\ninit(byAdding: VNVector, to: VNVector)\nCreates a new vector by adding the specified vectors.\ninit(bySubtracting: VNVector, from: VNVector)\nCreates a new vector by subtracting the first vector from the second vector.\ninit(byMultiplying: VNVector, byScalar: Double)\nCreates a new vector by multiplying the specified vector’s x-axis and y-axis projections by the scalar value.\ninit(r: Double, theta: Double)\nCreates a new vector in polar coordinate space.\ninit(vectorHead: VNPoint, tail: VNPoint)\nCreates a new vector in Cartesian coordinate space.\ninit(xComponent: Double, yComponent: Double)\nCreates a new vector in Cartesian coordinate space, based on its x-axis and y-axis projections."
  },
  {
    "title": "r | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector/3548343-r",
    "html": "See Also\nInspecting a Vector\nvar length: Double\nThe length, or absolute value, of the vector.\nvar theta: Double\nThe angle between the vector direction and the positive direction of the x-axis.\nvar squaredLength: Double\nThe squared length of the vector.\nvar x: Double\nA signed projection that indicates the vector’s direction on the x-axis.\nvar y: Double\nA signed projection that indicates the vector’s direction on the y-axis.\nclass func dotProduct(of: VNVector, vector: VNVector) -> Double\nCaclulates the dot product of two vectors.\nclass func unitVector(for: VNVector) -> VNVector\nCalculates a vector that’s normalized by preserving its direction, so that the vector length equals 1.0."
  },
  {
    "title": "theta | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector/3548345-theta",
    "html": "See Also\nInspecting a Vector\nvar length: Double\nThe length, or absolute value, of the vector.\nvar r: Double\nThe radius, absolute value, or length of the vector.\nvar squaredLength: Double\nThe squared length of the vector.\nvar x: Double\nA signed projection that indicates the vector’s direction on the x-axis.\nvar y: Double\nA signed projection that indicates the vector’s direction on the y-axis.\nclass func dotProduct(of: VNVector, vector: VNVector) -> Double\nCaclulates the dot product of two vectors.\nclass func unitVector(for: VNVector) -> VNVector\nCalculates a vector that’s normalized by preserving its direction, so that the vector length equals 1.0."
  },
  {
    "title": "x | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector/3548350-x",
    "html": "See Also\nInspecting a Vector\nvar length: Double\nThe length, or absolute value, of the vector.\nvar r: Double\nThe radius, absolute value, or length of the vector.\nvar theta: Double\nThe angle between the vector direction and the positive direction of the x-axis.\nvar squaredLength: Double\nThe squared length of the vector.\nvar y: Double\nA signed projection that indicates the vector’s direction on the y-axis.\nclass func dotProduct(of: VNVector, vector: VNVector) -> Double\nCaclulates the dot product of two vectors.\nclass func unitVector(for: VNVector) -> VNVector\nCalculates a vector that’s normalized by preserving its direction, so that the vector length equals 1.0."
  },
  {
    "title": "boundingCircle(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeometryutils/3548355-boundingcircle",
    "html": "Parameters\npoints\n\nA collection of points around which to calculate the bounding circle.\n\nReturn Value\n\nThe bounding VNCircle object.\n\nSee Also\nCalculating Bounding Circles\nclass func boundingCircle(for: VNContour) -> VNCircle\nCalculates a bounding circle for the specified contour object.\nclass func boundingCircle(forSIMDPoints: UnsafePointer<simd_float2>, pointCount: Int) -> VNCircle\nCalculates a bounding circle for the specified points."
  },
  {
    "title": "calculatePerimeter(_:for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeometryutils/3548358-calculateperimeter",
    "html": "Parameters\nperimeter\n\nThe output parameter to populate with the calculated contour perimeter.\n\ncontour\n\nThe contour object for which to calculate the perimeter.\n\nSee Also\nCalculating Area and Perimeter\nclass func calculateArea(UnsafeMutablePointer<Double>, for: VNContour, orientedArea: Bool)\nCalculates the area for the specified contour."
  },
  {
    "title": "init(center:radius:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncircle/3548317-init",
    "html": "Parameters\ncenter\n\nThe circle center.\n\nradius\n\nThe circle radius.\n\nSee Also\nCreating a Circle\ninit(center: VNPoint, diameter: Double)\nCreates a circle with the specified center and diameter.\nclass var zero: VNCircle\nA circle object centered at the origin, with a radius of zero."
  },
  {
    "title": "init(center:diameter:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncircle/3548316-init",
    "html": "Parameters\ncenter\n\nThe circle center.\n\ndiameter\n\nThe circle diameter.\n\nSee Also\nCreating a Circle\ninit(center: VNPoint, radius: Double)\nCreates a circle with the specified center and radius.\nclass var zero: VNCircle\nA circle object centered at the origin, with a radius of zero."
  },
  {
    "title": "zero | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncircle/3548319-zero",
    "html": "See Also\nCreating a Circle\ninit(center: VNPoint, radius: Double)\nCreates a circle with the specified center and radius.\ninit(center: VNPoint, diameter: Double)\nCreates a circle with the specified center and diameter."
  },
  {
    "title": "center | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncircle/3548312-center",
    "html": "See Also\nInspecting a Circle\nvar diameter: Double\nThe circle’s diameter.\nvar radius: Double\nThe circle’s radius.\nfunc contains(VNPoint) -> Bool\nDetermines if this circle, including its boundary, contains the specified point.\nfunc contains(VNPoint, inCircumferentialRingOfWidth: Double) -> Bool\nDetermines if a ring around this circle’s circumference contains the specified point."
  },
  {
    "title": "radius | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncircle/3548318-radius",
    "html": "See Also\nInspecting a Circle\nvar center: VNPoint\nThe circle’s center point.\nvar diameter: Double\nThe circle’s diameter.\nfunc contains(VNPoint) -> Bool\nDetermines if this circle, including its boundary, contains the specified point.\nfunc contains(VNPoint, inCircumferentialRingOfWidth: Double) -> Bool\nDetermines if a ring around this circle’s circumference contains the specified point."
  },
  {
    "title": "diameter | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncircle/3548315-diameter",
    "html": "See Also\nInspecting a Circle\nvar center: VNPoint\nThe circle’s center point.\nvar radius: Double\nThe circle’s radius.\nfunc contains(VNPoint) -> Bool\nDetermines if this circle, including its boundary, contains the specified point.\nfunc contains(VNPoint, inCircumferentialRingOfWidth: Double) -> Bool\nDetermines if a ring around this circle’s circumference contains the specified point."
  },
  {
    "title": "contains(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncircle/3548313-contains",
    "html": "Parameters\npoint\n\nThe point to test.\n\nReturn Value\n\ntrue if the point is contained within this circle, otherwise false.\n\nSee Also\nInspecting a Circle\nvar center: VNPoint\nThe circle’s center point.\nvar diameter: Double\nThe circle’s diameter.\nvar radius: Double\nThe circle’s radius.\nfunc contains(VNPoint, inCircumferentialRingOfWidth: Double) -> Bool\nDetermines if a ring around this circle’s circumference contains the specified point."
  },
  {
    "title": "contains(_:inCircumferentialRingOfWidth:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncircle/3548314-contains",
    "html": "Parameters\npoint\n\nThe point to test.\n\nringWidth\n\nThe width of the ring around this circle’s circumference.\n\nReturn Value\n\ntrue if the ring contains the specified point, otherwise false.\n\nSee Also\nInspecting a Circle\nvar center: VNPoint\nThe circle’s center point.\nvar diameter: Double\nThe circle’s diameter.\nvar radius: Double\nThe circle’s radius.\nfunc contains(VNPoint) -> Bool\nDetermines if this circle, including its boundary, contains the specified point."
  },
  {
    "title": "addRequest(_:processingOptions:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor/3675677-addrequest",
    "html": "Parameters\nrequest\n\nThe Vision request to add.\n\nprocessingOptions\n\nThe processing options to apply.\n\nDiscussion\n\nCall this method either before calling analyze(_:) or from within the completion handler of an already associated request.\n\nSee Also\nPerforming Requests\nclass VNVideoProcessor.RequestProcessingOptions\nAn object that defines a video processor’s configuration options.\nfunc removeRequest(VNRequest)\nRemoves a Vision request from the video processor’s request queue.\nfunc analyze(CMTimeRange)\nAnalyzes a time range of video content.\nfunc cancel()\nCancels the video processing.\nfunc add(VNRequest, withProcessingOptions: [VNVideoProcessingOption : Any])\nAdds a Vision request to perform with the specified configuration.\nDeprecated\nfunc analyze(with: CMTimeRange)\nAnalyzes the specifed time range of the video content.\nDeprecated"
  },
  {
    "title": "VNVideoProcessor.RequestProcessingOptions | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor/requestprocessingoptions",
    "html": "Topics\nConfiguring Options\nvar cadence: VNVideoProcessor.Cadence?\nThe cadence the video processor maintains to process the request.\nclass VNVideoProcessor.Cadence\nAn object that defines the cadence at which to process video.\nclass VNVideoProcessor.FrameRateCadence\nAn object that defines a frame-based cadence for processing a video stream.\nclass VNVideoProcessor.TimeIntervalCadence\nAn object that defines a time-based cadence for processing a video stream.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nSee Also\nPerforming Requests\nfunc addRequest(VNRequest, processingOptions: VNVideoProcessor.RequestProcessingOptions)\nAdds a request with processing options to the video processor.\nfunc removeRequest(VNRequest)\nRemoves a Vision request from the video processor’s request queue.\nfunc analyze(CMTimeRange)\nAnalyzes a time range of video content.\nfunc cancel()\nCancels the video processing.\nfunc add(VNRequest, withProcessingOptions: [VNVideoProcessingOption : Any])\nAdds a Vision request to perform with the specified configuration.\nDeprecated\nfunc analyze(with: CMTimeRange)\nAnalyzes the specifed time range of the video content.\nDeprecated"
  },
  {
    "title": "pointsClassification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarkregion2d/3930036-pointsclassification",
    "html": "See Also\nDescribing Region Points\nenum VNPointsClassification\nThe set of classifications that describe how to interpret the points the region provides."
  },
  {
    "title": "init(url:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor/3548386-init",
    "html": "Parameters\nassetURL\n\nThe video asset URL. The specified asset must be a video format supported by AVFoundation."
  },
  {
    "title": "init(requestRevision:topLeft:bottomLeft:bottomRight:topRight:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrectangleobservation/3172738-init",
    "html": "Deprecated\n\nUse init(requestRevision:topLeft:topRight:bottomRight:bottomLeft:) instead.\n\nParameters\nrequestRevision\n\nThe rectangle detector revision number. A higher revision indicates more recent iterations of the framework.\n\ntopLeft\n\nThe upper-left corner point.\n\nbottomLeft\n\nThe lower-left corner point.\n\nbottomRight\n\nThe lower-right corner point.\n\ntopRight\n\nThe upper-right corner point.\n\nSee Also\nCreating an Observation\ninit(requestRevision: Int, topLeft: CGPoint, topRight: CGPoint, bottomRight: CGPoint, bottomLeft: CGPoint)\nCreates a rectangle observation from its corner points."
  },
  {
    "title": "VNImageCropAndScaleOption.scaleFill | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagecropandscaleoption/scalefill",
    "html": "See Also\nCrop and Scale Options\ncase centerCrop\nAn option that scales the image to fit its shorter side within the input dimensions, while preserving its aspect ratio, and center-crops the image.\ncase scaleFit\nAn option that scales the image to fit its longer side within the input dimensions, while preserving its aspect ratio, and center-crops the image.\ncase scaleFitRotate90CCW\nAn option that rotates the image 90 degrees counterclockwise and then scales it, while preserving its aspect ratio, to fit on the long side.\ncase scaleFillRotate90CCW\nAn option that rotates the image 90 degrees counterclockwise and then scales it to fill the input dimensions."
  },
  {
    "title": "init(model:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncoremlrequest/2890152-init",
    "html": "Parameters\nmodel\n\nThe Core ML model on which to base the Vision request.\n\ncompletionHandler\n\nAn optional block of code to execute after model initialization.\n\nDiscussion\n\nInitialization can fail if the Core ML model you provide isn’t supported in Vision, such as if the model doesn’t accept an image as input.\n\nSee Also\nInitializing with a Core ML Model\ninit(model: VNCoreMLModel)\nCreates a model container to use with an image analysis request based on the model you provide.\nvar model: VNCoreMLModel\nThe model to base the image analysis request on.\nclass VNCoreMLModel\nA container for the model to use with Vision requests."
  },
  {
    "title": "VNHumanBodyPose3DObservation.HeightEstimation.reference | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/heightestimation/reference",
    "html": "Discussion\n\nIf there’s no depth data available, the framework uses a reference height of 1.8 meters.\n\nSee Also\nTechniques\ncase measured\nA technique that uses LiDAR depth data to measure body height, in meters."
  },
  {
    "title": "VNErrorCode.invalidImage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode/invalidimage",
    "html": "Discussion\n\nThis error occurs when you pass an invalid image to an operation, such as passing an image with no dimensions.\n\nSee Also\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out."
  },
  {
    "title": "featureValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncoremlfeaturevalueobservation/2890135-featurevalue",
    "html": "Discussion\n\nRefer to Core ML documentation and the model itself to learn about proper handling of the content.\n\nSee Also\nObtaining Feature Values\nvar featureName: String\nThe name used in the model description of the CoreML model that produced this observation."
  },
  {
    "title": "imageCropAndScaleOption | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncoremlrequest/2890144-imagecropandscaleoption",
    "html": "Discussion\n\nScaling an image ensures that the entire image fits into the algorithm’s input image dimensions, which may require a change in aspect ratio. Each crop-and-scale option transforms the input image in a different way.\n\nSee Also\nConfiguring Image Options\nenum VNImageCropAndScaleOption\nOptions that define how Vision crops and scales an input-image."
  },
  {
    "title": "VNImageCropAndScaleOption.centerCrop | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagecropandscaleoption/centercrop",
    "html": "See Also\nCrop and Scale Options\ncase scaleFit\nAn option that scales the image to fit its longer side within the input dimensions, while preserving its aspect ratio, and center-crops the image.\ncase scaleFill\nAn option that scales the image to fill the input dimensions, resizing it if necessary.\ncase scaleFitRotate90CCW\nAn option that rotates the image 90 degrees counterclockwise and then scales it, while preserving its aspect ratio, to fit on the long side.\ncase scaleFillRotate90CCW\nAn option that rotates the image 90 degrees counterclockwise and then scales it to fill the input dimensions."
  },
  {
    "title": "featureProvider | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncoremlmodel/3131933-featureprovider",
    "html": "Discussion\n\nThis optional object conforms to the MLFeatureProvider protocol that the model uses to predict inputs that are not supplied by Vision. Vision provides the MLModel with the image for the inputImageFeatureName via the VNRequestHandler.\n\nA feature provider is necessary for models that have more than one required input. Models with only one image input won't use the feature provider.\n\nSee Also\nProviding Features\nvar inputImageFeatureName: String\nThe name of the MLFeatureValue that Vision sets from the request handler."
  },
  {
    "title": "hasMinimumRecall(_:forPrecision:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnclassificationobservation/3152625-hasminimumrecall",
    "html": "Parameters\nminimumRecall\n\nThe minimum percentage of relevant results that the algorithm correctly classified.\n\nprecision\n\nThe percentage of classification results that are relevant.\n\nReturn Value\n\nA Boolean indicating whether or not this classification observation provides a minimum percentage of relevant results that meet the desired precision criterion.\n\nSee Also\nMeasuring Confidence and Precision\nvar hasPrecisionRecallCurve: Bool\nA Boolean variable indicating whether the observation contains precision and recall curves.\nfunc hasMinimumPrecision(Float, forRecall: Float) -> Bool\nDetermines whether the observation for a specific recall has a minimum precision value."
  },
  {
    "title": "labels | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedobjectobservation/2980942-labels",
    "html": "See Also\nClassifying a Recognized Object\nclass VNClassificationObservation\nAn object that represents classification information that an image analysis request produces."
  },
  {
    "title": "VNPointsClassification | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpointsclassification",
    "html": "Topics\nEnumeration Cases\ncase closedPath\ncase disconnected\ncase openPath\nRelationships\nConforms To\nSendable\nSee Also\nDescribing Region Points\nvar pointsClassification: VNPointsClassification\nAn enumeration that describes how to interpret the points the region provides."
  },
  {
    "title": "normalizedPoints | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarkregion2d/2928157-normalizedpoints",
    "html": "See Also\nSpecifying Region Properties\nvar precisionEstimatesPerPoint: [Float]?\nRequests an array of precision estimates for each landmark point."
  },
  {
    "title": "precisionEstimatesPerPoint | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarkregion2d/3213753-precisionestimatesperpoint",
    "html": "Discussion\n\nThis property is only populated when you configure your VNDetectFaceLandmarksRequest object with VNRequestFaceLandmarksConstellation.constellation76Points. For other constellation types, this array is set to nil.\n\nSee Also\nSpecifying Region Properties\nvar normalizedPoints: [CGPoint]\nThe array of normalized landmark points."
  },
  {
    "title": "VNFaceLandmarks2D | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks2d",
    "html": "Overview\n\nThis class represents the set of all detectable 2D face landmarks and regions, exposed as properties. The coordinates of the face landmarks are normalized to the dimensions of the face observation’s boundingBox, with the origin at the bounding box’s lower-left corner. Use the VNImagePointForFaceLandmarkPoint(_:_:_:_:) function to convert normalized face landmark points into absolute points within the image’s coordinate system.\n\nTopics\nFace Landmark Points\nvar allPoints: VNFaceLandmarkRegion2D?\nThe region containing all face landmark points.\nvar faceContour: VNFaceLandmarkRegion2D?\nThe region containing points that trace the face contour from the left cheek, over the chin, to the right cheek.\nvar leftEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the left eye.\nvar rightEye: VNFaceLandmarkRegion2D?\nThe region containing points that outline the right eye.\nvar leftEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the left eyebrow.\nvar rightEyebrow: VNFaceLandmarkRegion2D?\nThe region containing points that trace the right eyebrow.\nvar nose: VNFaceLandmarkRegion2D?\nThe region containing points that outline the nose.\nvar noseCrest: VNFaceLandmarkRegion2D?\nThe region containing points that trace the center crest of the nose.\nvar medianLine: VNFaceLandmarkRegion2D?\nThe region containing points that trace a vertical line down the center of the face.\nvar outerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the outside of the lips.\nvar innerLips: VNFaceLandmarkRegion2D?\nThe region containing points that outline the space between the lips.\nvar leftPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the left pupil is located.\nvar rightPupil: VNFaceLandmarkRegion2D?\nThe region containing the point where the right pupil is located.\nRelationships\nInherits From\nVNFaceLandmarks\nSee Also\nIdentifying Landmarks\nvar landmarks: VNFaceLandmarks2D?\nThe facial features of the detected face.\nclass VNFaceLandmarkRegion2D\n2D geometry information for a specific facial feature.\nclass VNFaceLandmarks\nThe abstract superclass for containers of face landmark information.\nclass VNFaceLandmarkRegion\nThe abstract superclass for information about a specific face landmark."
  },
  {
    "title": "init(requestRevision:boundingBox:roll:yaw:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfaceobservation/2980938-init",
    "html": "Deprecated\n\nUse init(requestRevision:boundingBox:roll:yaw:pitch:)instead.\n\nParameters\nrequestRevision\n\nThe revision of the request.\n\nboundingBox\n\nThe bounding rectangle of the detected face landmark.\n\nroll\n\nThe rotational angle of the face landmark around the z-axis.\n\nyaw\n\nThe rotational angle of the face landmark around the y-axis.\n\nSee Also\nCreating an Observation\ninit(requestRevision: Int, boundingBox: CGRect, roll: NSNumber?, yaw: NSNumber?, pitch: NSNumber?)\nCreates an observation that contains the roll, yaw, and pitch of the face."
  },
  {
    "title": "VNTrackOpticalFlowRequest.ComputationAccuracy | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackopticalflowrequest/computationaccuracy",
    "html": "Topics\nOptions\ncase low\nAn option that indicates a low level of computational accuracy.\ncase medium\nAn option that indicates a moderate level of computational accuracy.\ncase high\nAn option that indicates a high level of computational accuracy.\ncase veryHigh\nAn option that indicates a very high level of computational accuracy.\nRelationships\nConforms To\nSendable\nSee Also\nConfiguring the Request\nvar computationAccuracy: VNTrackOpticalFlowRequest.ComputationAccuracy\nThe level of accuracy to compute the optical flow.\nvar keepNetworkOutput: Bool\nA Boolean value that indicates the raw pixel buffer continues to emit from the network.\nvar outputPixelFormat: OSType\nThe pixel format type of the output value."
  },
  {
    "title": "VNFaceLandmarks | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarks",
    "html": "Overview\n\nThis class represents the set of all detectable facial landmarks and regions, exposed as properties.\n\nTopics\nDetermining Accuracy\nvar confidence: VNConfidence\nA confidence estimate for the detected landmarks.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nVNRequestRevisionProviding\nSee Also\nIdentifying Landmarks\nvar landmarks: VNFaceLandmarks2D?\nThe facial features of the detected face.\nclass VNFaceLandmarks2D\nA collection of facial features that a request detects.\nclass VNFaceLandmarkRegion2D\n2D geometry information for a specific facial feature.\nclass VNFaceLandmarkRegion\nThe abstract superclass for information about a specific face landmark."
  },
  {
    "title": "inputFaceObservations | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfaceobservationaccepting/2877424-inputfaceobservations",
    "html": "Required"
  },
  {
    "title": "VNFaceLandmarkRegion | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarkregion",
    "html": "Topics\nInstance Properties\nvar pointCount: Int\nThe number of points in the face region.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nVNRequestRevisionProviding\nSee Also\nIdentifying Landmarks\nvar landmarks: VNFaceLandmarks2D?\nThe facial features of the detected face.\nclass VNFaceLandmarks2D\nA collection of facial features that a request detects.\nclass VNFaceLandmarkRegion2D\n2D geometry information for a specific facial feature.\nclass VNFaceLandmarks\nThe abstract superclass for containers of face landmark information."
  },
  {
    "title": "VNRequestTextRecognitionLevel.fast | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequesttextrecognitionlevel/fast",
    "html": "See Also\nRecognition Levels\ncase accurate\nAccurate text recognition takes more time to produce a more comprehensive result."
  },
  {
    "title": "init(completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackopticalflowrequest/4127013-init",
    "html": "Parameters\ncompletionHandler\n\nThe callback the system invokes when it completes the request.\n\nSee Also\nCreating an Optical Flow\ninit()\nCreates a new request that tracks the optical from one image to another."
  },
  {
    "title": "VNDetectFaceLandmarksRequestRevision3 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacelandmarksrequestrevision3",
    "html": "See Also\nIdentifying Request Revisions\nclass func revision(Int, supportsConstellation: VNRequestFaceLandmarksConstellation) -> Bool\nReturns a Boolean value that indicates whether a revision supports a constellation.\nlet VNDetectFaceLandmarksRequestRevision2: Int\nA constant for specifying revision 2 of the face landmarks detection request.\nlet VNDetectFaceLandmarksRequestRevision1: Int\nA constant for specifying revision 1 of the face landmarks detection request.\nDeprecated"
  },
  {
    "title": "computationAccuracy | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackopticalflowrequest/4127011-computationaccuracy",
    "html": "Discussion\n\nThe computational time trends with accuracy level. The default value is VNTrackOpticalFlowRequest.ComputationAccuracy.medium.\n\nSee Also\nConfiguring the Request\nenum VNTrackOpticalFlowRequest.ComputationAccuracy\nComputational accuracy options.\nvar keepNetworkOutput: Bool\nA Boolean value that indicates the raw pixel buffer continues to emit from the network.\nvar outputPixelFormat: OSType\nThe pixel format type of the output value."
  },
  {
    "title": "keepNetworkOutput | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackopticalflowrequest/4127014-keepnetworkoutput",
    "html": "Discussion\n\nThe default value is false; otherwise, the request ignores outputPixelFormat.\n\nSee Also\nConfiguring the Request\nvar computationAccuracy: VNTrackOpticalFlowRequest.ComputationAccuracy\nThe level of accuracy to compute the optical flow.\nenum VNTrackOpticalFlowRequest.ComputationAccuracy\nComputational accuracy options.\nvar outputPixelFormat: OSType\nThe pixel format type of the output value."
  },
  {
    "title": "outputPixelFormat | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackopticalflowrequest/4127015-outputpixelformat",
    "html": "Discussion\n\nThe valid values are kCVPixelFormatType_TwoComponent32Float and kCVPixelFormatType_TwoComponent16Half. The default value is kCVPixelFormatType_TwoComponent32Float.\n\nSee Also\nConfiguring the Request\nvar computationAccuracy: VNTrackOpticalFlowRequest.ComputationAccuracy\nThe level of accuracy to compute the optical flow.\nenum VNTrackOpticalFlowRequest.ComputationAccuracy\nComputational accuracy options.\nvar keepNetworkOutput: Bool\nA Boolean value that indicates the raw pixel buffer continues to emit from the network."
  },
  {
    "title": "apply(_:to:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpoint/3548333-apply",
    "html": "Parameters\nvector\n\nThe vector to apply to offset the point.\n\npoint\n\nThe point to translate by the vector’s X and Y offsets.\n\nSee Also\nCreating a Point\ninit(x: Double, y: Double)\nCreates a point object with the specified coordinates.\ninit(location: CGPoint)\nCreates a point object from the specified Core Graphics point.\nclass var zero: VNPoint\nA point object that represents the origin."
  },
  {
    "title": "init(location:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpoint/3548330-init",
    "html": "Parameters\nlocation\n\nThe Core Graphics point.\n\nSee Also\nCreating a Point\ninit(x: Double, y: Double)\nCreates a point object with the specified coordinates.\nclass func apply(VNVector, to: VNPoint) -> VNPoint\nCreates a point object that’s shifted by the X and Y offsets of the specified vector.\nclass var zero: VNPoint\nA point object that represents the origin."
  },
  {
    "title": "zero | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpoint/3548336-zero",
    "html": "Discussion\n\nThe origin point is (0.0, 0.0).\n\nSee Also\nCreating a Point\ninit(x: Double, y: Double)\nCreates a point object with the specified coordinates.\ninit(location: CGPoint)\nCreates a point object from the specified Core Graphics point.\nclass func apply(VNVector, to: VNPoint) -> VNPoint\nCreates a point object that’s shifted by the X and Y offsets of the specified vector."
  },
  {
    "title": "VNGenerateOpticalFlowRequest.ComputationAccuracy | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateopticalflowrequest/computationaccuracy",
    "html": "Topics\nAccuracy Levels\nComputation time typically increases with accuracy.\ncase low\nLow accuracy.\ncase medium\nMedium accuracy.\ncase high\nHigh accuracy.\ncase veryHigh\nVery high accuracy.\nRelationships\nConforms To\nSendable\nSee Also\nConfiguring the Request\nvar computationAccuracy: VNGenerateOpticalFlowRequest.ComputationAccuracy\nThe accuracy level for computing optical flow.\nvar outputPixelFormat: OSType\nThe output buffer’s pixel format.\nvar keepNetworkOutput: Bool\nA Boolean value that indicates whether to keep the raw pixel buffer coming from the machine learning network."
  },
  {
    "title": "x | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpoint/3548334-x",
    "html": "See Also\nInspecting a Point\nvar y: Double\nThe y-coordinate.\nvar location: CGPoint\nThe Core Graphics point for this point."
  },
  {
    "title": "y | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpoint/3548335-y",
    "html": "See Also\nInspecting a Point\nvar x: Double\nThe x-coordinate.\nvar location: CGPoint\nThe Core Graphics point for this point."
  },
  {
    "title": "location | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpoint/3548332-location",
    "html": "See Also\nInspecting a Point\nvar x: Double\nThe x-coordinate.\nvar y: Double\nThe y-coordinate."
  },
  {
    "title": "computationAccuracy | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateopticalflowrequest/3672181-computationaccuracy",
    "html": "See Also\nConfiguring the Request\nenum VNGenerateOpticalFlowRequest.ComputationAccuracy\nThe supported optical flow accuracy levels.\nvar outputPixelFormat: OSType\nThe output buffer’s pixel format.\nvar keepNetworkOutput: Bool\nA Boolean value that indicates whether to keep the raw pixel buffer coming from the machine learning network."
  },
  {
    "title": "distance(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpoint/3675674-distance",
    "html": "Parameters\npoint\n\nThe point for which to calculate the distance.\n\nReturn Value\n\nThe calculated distance.\n\nSee Also\nCalculating Distance\nclass func distance(VNPoint, VNPoint) -> Double\nCalculates the distance between two points.\nDeprecated"
  },
  {
    "title": "VNGenerateOpticalFlowRequestRevision2 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateopticalflowrequestrevision2",
    "html": "See Also\nIdentifying Request Revisions\nlet VNGenerateOpticalFlowRequestRevision1: Int\nA constant for specifying revision 1 of the optical flow generation request."
  },
  {
    "title": "VNGenerateOpticalFlowRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateopticalflowrequestrevision1",
    "html": "See Also\nIdentifying Request Revisions\nlet VNGenerateOpticalFlowRequestRevision2: Int\nA constant for specifying revision 2 of the optical flow generation request."
  },
  {
    "title": "VNDetectBarcodesRequestRevision3 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectbarcodesrequestrevision3",
    "html": "See Also\nIdentifying Request Revisions\nlet VNDetectBarcodesRequestRevision2: Int\nA constant for specifying revision 2 of the barcode detection request.\nlet VNDetectBarcodesRequestRevision1: Int\nA constant for specifying revision 1 of the barcode detection request.\nDeprecated"
  },
  {
    "title": "reportCharacterBoxes | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecttextrectanglesrequest/2875410-reportcharacterboxes",
    "html": "Discussion\n\nSet the value to true to have the detector return character bounding boxes as an array of VNRectangleObservation objects."
  },
  {
    "title": "init(vectorHead:tail:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector/3548340-init",
    "html": "Parameters\nhead\n\nThe vector’s head point.\n\ntail\n\nThe vector’s tail point.\n\nSee Also\nCreating a Vector\ninit(byAdding: VNVector, to: VNVector)\nCreates a new vector by adding the specified vectors.\ninit(bySubtracting: VNVector, from: VNVector)\nCreates a new vector by subtracting the first vector from the second vector.\ninit(byMultiplying: VNVector, byScalar: Double)\nCreates a new vector by multiplying the specified vector’s x-axis and y-axis projections by the scalar value.\ninit(r: Double, theta: Double)\nCreates a new vector in polar coordinate space.\ninit(xComponent: Double, yComponent: Double)\nCreates a new vector in Cartesian coordinate space, based on its x-axis and y-axis projections.\nclass var zero: VNVector\nA vector object with zero length."
  },
  {
    "title": "unitVector(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector/3548346-unitvector",
    "html": "Parameters\nvector\n\nThe vector whose unit vector you want to calculate.\n\nReturn Value\n\nThe unit vector.\n\nSee Also\nInspecting a Vector\nvar length: Double\nThe length, or absolute value, of the vector.\nvar r: Double\nThe radius, absolute value, or length of the vector.\nvar theta: Double\nThe angle between the vector direction and the positive direction of the x-axis.\nvar squaredLength: Double\nThe squared length of the vector.\nvar x: Double\nA signed projection that indicates the vector’s direction on the x-axis.\nvar y: Double\nA signed projection that indicates the vector’s direction on the y-axis.\nclass func dotProduct(of: VNVector, vector: VNVector) -> Double\nCaclulates the dot product of two vectors."
  },
  {
    "title": "calculateArea(_:for:orientedArea:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeometryutils/3548357-calculatearea",
    "html": "Parameters\narea\n\nThe output parameter to populate with the calculated contour area.\n\ncontour\n\nThe contour object for which to calculate the area.\n\norientedArea\n\nA Boolean value that indicates whether to calculate the signed area (positive for counterclockwise-oriented contours and negative for clockwise-oriented contours). If you specify false, the returned area is always positive.\n\nDiscussion\n\nAttempting to calculate the area for a contour containing random points, or with self-crossing edges, produces undefined results.\n\nSee Also\nCalculating Area and Perimeter\nclass func calculatePerimeter(UnsafeMutablePointer<Double>, for: VNContour)\nCalculates the perimeter of a closed contour."
  },
  {
    "title": "boundingCircle(forSIMDPoints:pointCount:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeometryutils/3548356-boundingcircle",
    "html": "Parameters\npoints\n\nA collection of points around which to calculate the bounding circle.\n\npointCount\n\nThe number of points in the points argument.\n\nReturn Value\n\nThe bounding VNCircle object.\n\nSee Also\nCalculating Bounding Circles\nclass func boundingCircle(for: VNContour) -> VNCircle\nCalculates a bounding circle for the specified contour object.\nclass func boundingCircle(for: [VNPoint]) -> VNCircle\nCalculates a bounding circle for the specified array of points."
  },
  {
    "title": "Hand Landmarks | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/hand_landmarks",
    "html": "Topics\nAll Landmarks\nstatic let all: VNRecognizedPointGroupKey\nA group key identifying all landmarks.\nSee Also\nLandmarks\nBody Landmarks\nThe body landmarks that Vision detects."
  },
  {
    "title": "recognizedPoints(forGroupKey:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpoints3dobservation/4173238-recognizedpoints",
    "html": "Parameters\ngroupKey\n\nThe group key to retrieve points for.\n\nReturn Value\n\nA dictionary of labeled points for the group.\n\nSee Also\nInspecting the Observation\nvar availableKeys: [VNRecognizedPointKey]\nThe available point keys in the observation.\nvar availableGroupKeys: [VNRecognizedPointGroupKey]\nThe available point group keys in the observation.\nfunc recognizedPoint(forKey: VNRecognizedPointKey) -> VNRecognizedPoint3D\nReturns a point for a key you specify."
  },
  {
    "title": "VNRecognizeTextRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizetextrequestrevision1",
    "html": "See Also\nIdentifying Request Revisions\nlet VNRecognizeTextRequestRevision3: Int\nA constant for specifying revision 3 of the text recognition request.\nlet VNRecognizeTextRequestRevision2: Int\nA constant for specifying revision 2 of the text recognition request."
  },
  {
    "title": "angle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhorizonobservation/2867230-angle",
    "html": "Discussion\n\nUse the angle to orient the image in an upright position and make the detected horizon level.\n\nSee Also\nEvaluating the Horizon\nvar transform: CGAffineTransform\nThe transform to apply to the detected horizon.\nfunc transform(forImageWidth: Int, height: Int) -> CGAffineTransform\nCreates an affine transform for the specified image width and height."
  },
  {
    "title": "init(rectangleObservation:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackrectanglerequest/2887361-init",
    "html": "Parameters\nobservation\n\nA rectangle observation with bounding box and corner location information.\n\nSee Also\nInitializing a Rectangle Tracking Request\ninit(rectangleObservation: VNRectangleObservation, completionHandler: VNRequestCompletionHandler?)\nCreates a new rectangle tracking request with a rectangle observation."
  },
  {
    "title": "init(rectangleObservation:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackrectanglerequest/2887357-init",
    "html": "Parameters\nobservation\n\nA rectangle observation with bounding box and corner location information.\n\ncompletionHandler\n\nThe block to invoke after performing the request.\n\nSee Also\nInitializing a Rectangle Tracking Request\ninit(rectangleObservation: VNRectangleObservation)\nCreates a new rectangle tracking request with a rectangle observation."
  },
  {
    "title": "VNTrackRectangleRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackrectanglerequestrevision1",
    "html": "Discussion\n\nThe revision number is a constant that you pass on a per-request basis to indicate to the Vision framework which version of the rectangle tracker to use for that request. Each OS release in which the framework improves aspects of the algorithm (recognition speed, accuracy, number of languages supported, and so forth), the revision number increments by 1.\n\nBy default, recognition requests use the latest—the highest—revision number for the SDK that your app links against. If you don’t recompile your app against a newer SDK, your app binary uses the revision that was the default at the time you last compiled it. If you do recompile, your app uses the default of the new SDK.\n\nIf your app must support users on older OS versions that don’t have access to the latest Vision framework, you may want to specify an earlier revision. For example, your algorithm may depend on specific behavior from a Vision request, such as writing your image processing algorithm to assume the size or aspect ratio of bounding boxes from an older revision of the face detector. In such a scenario, you can support earlier versions of the algorithm by specifying lower numbers:\n\nvisionRequest.revision = VNTrackRectangleRequestRevision1\n"
  },
  {
    "title": "landmarks | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfaceobservation/2867250-landmarks",
    "html": "Discussion\n\nThis value is nil for face observations produced by a VNDetectFaceRectanglesRequest analysis. Use the VNDetectFaceLandmarksRequest class to find facial features.\n\nSee Also\nIdentifying Landmarks\nclass VNFaceLandmarks2D\nA collection of facial features that a request detects.\nclass VNFaceLandmarkRegion2D\n2D geometry information for a specific facial feature.\nclass VNFaceLandmarks\nThe abstract superclass for containers of face landmark information.\nclass VNFaceLandmarkRegion\nThe abstract superclass for information about a specific face landmark."
  },
  {
    "title": "generateMask(forInstances:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vninstancemaskobservation/4210412-generatemask",
    "html": "Parameters\ninstances\n\nThe collection of instances.\n\nReturn Value\n\nThe pixel buffer that contains the image.\n\nSee Also\nCreating a Mask\nfunc generateMaskedImage(ofInstances: IndexSet, from: VNImageRequestHandler, croppedToInstancesExtent: Bool) -> CVPixelBuffer\nCreates a high-resolution image where everything becomes transparent black, except for the instances you specify.\nfunc generateScaledMaskForImage(forInstances: IndexSet, from: VNImageRequestHandler) -> CVPixelBuffer\nCreates a high-resolution mask where everything becomes transparent black, except for the instances you specify."
  },
  {
    "title": "instanceMask | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vninstancemaskobservation/4108322-instancemask",
    "html": "Discussion\n\nA pixel can only correspond to one instance. A 0 represents the background, and all other values represent the indices of the instances.\n\nSee Also\nAccessing Instances\nvar allInstances: IndexSet\nThe collection that contains all instances, excluding the background."
  },
  {
    "title": "allInstances | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vninstancemaskobservation/4108318-allinstances",
    "html": "See Also\nAccessing Instances\nvar instanceMask: CVPixelBuffer\nThe resulting mask that represents all instances."
  },
  {
    "title": "pixelBuffer | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpixelbufferobservation/2890132-pixelbuffer",
    "html": "Discussion\n\nVNCoreMLRequest produces observations that contain images in pixel buffer format. The confidence level is always 1.0.\n\nSee Also\nParsing Observation Content\nvar featureName: String?\nA feature name that the CoreML model defines."
  },
  {
    "title": "hasMinimumPrecision(_:forRecall:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnclassificationobservation/3152624-hasminimumprecision",
    "html": "Parameters\nminimumPrecision\n\nThe minimum percentage of classification results that are relevant.\n\nrecall\n\nThe percentage of relevant results that the algorithm correctly classified.\n\nReturn Value\n\nA Boolean indicating whether or not this classification observation provides a minimum percentage of relevant results that meet the desired recall criterion.\n\nSee Also\nMeasuring Confidence and Precision\nvar hasPrecisionRecallCurve: Bool\nA Boolean variable indicating whether the observation contains precision and recall curves.\nfunc hasMinimumRecall(Float, forPrecision: Float) -> Bool\nDetermines whether the observation for a specific precision has a minimum recall value."
  },
  {
    "title": "identifier | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnclassificationobservation/2867259-identifier",
    "html": "Discussion\n\nAn example classification could be a string like cat or hotdog. The model used for the classification defines the domain of strings that may result. Usually, these strings are unlocalized technical labels not meant for direct presentation to the end user."
  },
  {
    "title": "hasPrecisionRecallCurve | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnclassificationobservation/3152626-hasprecisionrecallcurve",
    "html": "Discussion\n\nPrecision refers to the percentage of your classification results that are relevant, while recall refers to the percentage of total relevant results correctly classified.\n\nIf this property is true, then you can call precision and recall-related methods in this observation. If this property is false, then the precision and recall-related methods won't return meaningful data.\n\nSee Also\nMeasuring Confidence and Precision\nfunc hasMinimumPrecision(Float, forRecall: Float) -> Bool\nDetermines whether the observation for a specific recall has a minimum precision value.\nfunc hasMinimumRecall(Float, forPrecision: Float) -> Bool\nDetermines whether the observation for a specific precision has a minimum recall value."
  },
  {
    "title": "generateMaskedImage(ofInstances:from:croppedToInstancesExtent:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vninstancemaskobservation/4210413-generatemaskedimage",
    "html": "Parameters\ninstances\n\nThe collection of instances.\n\nrequestHandler\n\nThe image request callback.\n\ncropResult\n\nA Boolean value that indicates whether to crop the image to the smallest rectangle that contains all instances.\n\nReturn Value\n\nThe pixel buffer that contains the image.\n\nSee Also\nCreating a Mask\nfunc generateMask(forInstances: IndexSet) -> CVPixelBuffer\nCreates a low-resolution mask from the instances you specify.\nfunc generateScaledMaskForImage(forInstances: IndexSet, from: VNImageRequestHandler) -> CVPixelBuffer\nCreates a high-resolution mask where everything becomes transparent black, except for the instances you specify."
  },
  {
    "title": "VNCoreMLModel | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncoremlmodel",
    "html": "Overview\n\nA Core ML model encapsulates the information trained from a data set used to drive Vision recognition requests. See Getting a Core ML Model for instructions on training your own model. Once you train the model, use this class to initialize a VNCoreMLRequest for identification.\n\nTopics\nInitializing a Model\ninit(for: MLModel)\nCreates a model container to be used with VNCoreMLRequest.\nProviding Features\nvar featureProvider: MLFeatureProvider?\nAn optional object to support inputs outside Vision.\nvar inputImageFeatureName: String\nThe name of the MLFeatureValue that Vision sets from the request handler.\nRelationships\nInherits From\nNSObject\nSee Also\nInitializing with a Core ML Model\ninit(model: VNCoreMLModel)\nCreates a model container to use with an image analysis request based on the model you provide.\ninit(model: VNCoreMLModel, completionHandler: VNRequestCompletionHandler?)\nCreates a model container to use with an image analysis request based on the model you provide, with an optional completion handler.\nvar model: VNCoreMLModel\nThe model to base the image analysis request on."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackhomographicimageregistrationrequest/4127006-init",
    "html": "See Also\nCreating a Homographic Image\ninit(completionHandler: VNRequestCompletionHandler?)\nCreates a new request that tracks the homographic transformation of two images, with a system callback on completion."
  },
  {
    "title": "VNTranslationalImageRegistrationRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntranslationalimageregistrationrequestrevision1",
    "html": "Discussion\n\nThe revision number is a constant that you pass on a per-request basis to indicate to the Vision framework which version of the translational image registration algorithm to use for that request. Each OS release in which the framework improves aspects of the algorithm (recognition speed, accuracy, number of languages supported, and so forth), the revision number increments by 1.\n\nBy default, recognition requests use the latest—the highest—revision number for the SDK that your app links against. If you don’t recompile your app against a newer SDK, your app binary will use the revision that was the default at the time you last compiled it. If you do recompile, your app uses the default of the new SDK.\n\nIf your app must support users on older OS versions that don’t have access to the latest Vision framework, you may want to specify an earlier revision. For example, your algorithm may depend on specific behavior from a Vision request, such as writing your image processing algorithm to assume the size or aspect ratio of bounding boxes from an older revision of the face detector. In such a scenario, you can support earlier versions of the algorithm by specifying lower numbers:\n\nvisionRequest.revision = VNTranslationalImageRegistrationRequestRevision1\n"
  },
  {
    "title": "init(completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackhomographicimageregistrationrequest/4127007-init",
    "html": "Parameters\ncompletionHandler\n\nThe callback the system invokes when it completes the request.\n\nSee Also\nCreating a Homographic Image\ninit()\nCreates a new request that tracks the homographic transformation of two images."
  },
  {
    "title": "init(targetedCGImage:options:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntargetedimagerequest/2923448-init",
    "html": "Parameters\ncgImage\n\nThe targeted Core Graphics image.\n\noptions\n\nA dictionary with options specifying auxiliary information for the image.\n\ncompletionHandler\n\nThe block to invoke when the request has finished executing.\n\nSee Also\nCreating a Request\ninit(targetedCGImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image of known orientation, executing the completion handler when done.\ninit(targetedCIImage: CIImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage, executing the completion handler when done.\ninit(targetedCIImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage of known orientation, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer of known orientation, executing the completion handler when done.\ninit(targetedCMSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image in a sample buffer.\ninit(targetedCMSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image of a known orientation in a sample buffer.\ninit(targetedImageData: Data, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image as raw data, executing the completion handler when done.\ninit(targetedImageData: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a raw data image of known orientation, executing the completion handler when done.\ninit(targetedImageURL: URL, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image at the specified URL, executing the completion handler when done.\ninit(targetedImageURL: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image of known orientation, at the specified URL, executing the completion handler when done."
  },
  {
    "title": "init(targetedCIImage:options:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntargetedimagerequest/2923454-init",
    "html": "Parameters\nciImage\n\nThe CIImage encapsulating the targeted image.\n\noptions\n\nA dictionary with options specifying auxiliary information for the image.\n\ncompletionHandler\n\nThe block to invoke when the request has finished executing.\n\nSee Also\nCreating a Request\ninit(targetedCGImage: CGImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image, executing the completion handler when done.\ninit(targetedCGImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image of known orientation, executing the completion handler when done.\ninit(targetedCIImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage of known orientation, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer of known orientation, executing the completion handler when done.\ninit(targetedCMSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image in a sample buffer.\ninit(targetedCMSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image of a known orientation in a sample buffer.\ninit(targetedImageData: Data, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image as raw data, executing the completion handler when done.\ninit(targetedImageData: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a raw data image of known orientation, executing the completion handler when done.\ninit(targetedImageURL: URL, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image at the specified URL, executing the completion handler when done.\ninit(targetedImageURL: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image of known orientation, at the specified URL, executing the completion handler when done."
  },
  {
    "title": "init(targetedCGImage:orientation:options:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntargetedimagerequest/2923450-init",
    "html": "Parameters\ncgImage\n\nThe targeted Core Graphics image.\n\norientation\n\nThe orientation of the image buffer, based on EXIF specification and superseding other orientation information. The value must be an integer from 1 to 8; see kCGImagePropertyOrientation for details.\n\noptions\n\nA dictionary with options specifying auxiliary information for the image.\n\ncompletionHandler\n\nThe block to invoke when the request has finished executing.\n\nSee Also\nCreating a Request\ninit(targetedCGImage: CGImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image, executing the completion handler when done.\ninit(targetedCIImage: CIImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage, executing the completion handler when done.\ninit(targetedCIImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage of known orientation, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer of known orientation, executing the completion handler when done.\ninit(targetedCMSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image in a sample buffer.\ninit(targetedCMSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image of a known orientation in a sample buffer.\ninit(targetedImageData: Data, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image as raw data, executing the completion handler when done.\ninit(targetedImageData: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a raw data image of known orientation, executing the completion handler when done.\ninit(targetedImageURL: URL, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image at the specified URL, executing the completion handler when done.\ninit(targetedImageURL: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image of known orientation, at the specified URL, executing the completion handler when done."
  },
  {
    "title": "init(targetedCIImage:orientation:options:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntargetedimagerequest/2923451-init",
    "html": "Parameters\nciImage\n\nThe CIImage encapsulating the targeted image.\n\norientation\n\nThe orientation of the image buffer, based on EXIF specification and superseding other orientation information. The value must be an integer from 1 to 8; see kCGImagePropertyOrientation for details.\n\noptions\n\nA dictionary with options specifying auxiliary information for the image.\n\ncompletionHandler\n\nThe block to invoke when the request has finished executing.\n\nSee Also\nCreating a Request\ninit(targetedCGImage: CGImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image, executing the completion handler when done.\ninit(targetedCGImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image of known orientation, executing the completion handler when done.\ninit(targetedCIImage: CIImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer of known orientation, executing the completion handler when done.\ninit(targetedCMSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image in a sample buffer.\ninit(targetedCMSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image of a known orientation in a sample buffer.\ninit(targetedImageData: Data, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image as raw data, executing the completion handler when done.\ninit(targetedImageData: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a raw data image of known orientation, executing the completion handler when done.\ninit(targetedImageURL: URL, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image at the specified URL, executing the completion handler when done.\ninit(targetedImageURL: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image of known orientation, at the specified URL, executing the completion handler when done."
  },
  {
    "title": "revision(_:supportsConstellation:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacelandmarksrequest/3143665-revision",
    "html": "Parameters\nrequestRevision\n\nThe revision of the request.\n\nconstellation\n\nThe contellation for which to determine support.\n\nReturn Value\n\ntrue if the revision supports the constellation, otherwise false.\n\nSee Also\nIdentifying Request Revisions\nlet VNDetectFaceLandmarksRequestRevision3: Int\nA constant for specifying revision 3 of the face landmarks detection request.\nlet VNDetectFaceLandmarksRequestRevision2: Int\nA constant for specifying revision 2 of the face landmarks detection request.\nlet VNDetectFaceLandmarksRequestRevision1: Int\nA constant for specifying revision 1 of the face landmarks detection request.\nDeprecated"
  },
  {
    "title": "VNDetectFaceLandmarksRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacelandmarksrequestrevision1",
    "html": "See Also\nIdentifying Request Revisions\nclass func revision(Int, supportsConstellation: VNRequestFaceLandmarksConstellation) -> Bool\nReturns a Boolean value that indicates whether a revision supports a constellation.\nlet VNDetectFaceLandmarksRequestRevision3: Int\nA constant for specifying revision 3 of the face landmarks detection request.\nlet VNDetectFaceLandmarksRequestRevision2: Int\nA constant for specifying revision 2 of the face landmarks detection request."
  },
  {
    "title": "constellation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacelandmarksrequest/3143664-constellation",
    "html": "Discussion\n\nSet this variable to one of the supported constellation types detailed in VNRequestFaceLandmarksConstellation. The default value is VNRequestFaceLandmarksConstellation.constellationNotDefined.\n\nSee Also\nLocating Face Landmarks\nenum VNRequestFaceLandmarksConstellation\nAn enumeration of face landmarks in a constellation object."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackopticalflowrequest/4127012-init",
    "html": "See Also\nCreating an Optical Flow\ninit(completionHandler: VNRequestCompletionHandler?)\nCreates a new request that tracks the optical from one image to another, with a system callback on completion."
  },
  {
    "title": "VNDetectFaceLandmarksRequestRevision2 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacelandmarksrequestrevision2",
    "html": "See Also\nIdentifying Request Revisions\nclass func revision(Int, supportsConstellation: VNRequestFaceLandmarksConstellation) -> Bool\nReturns a Boolean value that indicates whether a revision supports a constellation.\nlet VNDetectFaceLandmarksRequestRevision3: Int\nA constant for specifying revision 3 of the face landmarks detection request.\nlet VNDetectFaceLandmarksRequestRevision1: Int\nA constant for specifying revision 1 of the face landmarks detection request.\nDeprecated"
  },
  {
    "title": "results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacelandmarksrequest/3750967-results",
    "html": "See Also\nAccessing the Results\nclass VNFaceObservation\nFace or facial-feature information that an image analysis request detects."
  },
  {
    "title": "distance(_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpoint/3548329-distance",
    "html": "Deprecated\n\nUse distance(_:) instead.\n\nParameters\npoint1\n\nThe first point.\n\npoint2\n\nThe second point.\n\nReturn Value\n\nThe calculated distance.\n\nSee Also\nCalculating Distance\nfunc distance(VNPoint) -> Double\nReturns the distance to another point."
  },
  {
    "title": "outputPixelFormat | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateopticalflowrequest/3548304-outputpixelformat",
    "html": "See Also\nConfiguring the Request\nvar computationAccuracy: VNGenerateOpticalFlowRequest.ComputationAccuracy\nThe accuracy level for computing optical flow.\nenum VNGenerateOpticalFlowRequest.ComputationAccuracy\nThe supported optical flow accuracy levels.\nvar keepNetworkOutput: Bool\nA Boolean value that indicates whether to keep the raw pixel buffer coming from the machine learning network."
  },
  {
    "title": "VNDetectBarcodesRequestRevision2 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectbarcodesrequestrevision2",
    "html": "See Also\nIdentifying Request Revisions\nlet VNDetectBarcodesRequestRevision3: Int\nA constant for specifying revision 3 of the barcode detection request.\nlet VNDetectBarcodesRequestRevision1: Int\nA constant for specifying revision 1 of the barcode detection request.\nDeprecated"
  },
  {
    "title": "maximumHandCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanhandposerequest/3571271-maximumhandcount",
    "html": "Discussion\n\nThe request orders detected hands by relative size, with only the largest ones having key points determined.\n\nThe default value is 2."
  },
  {
    "title": "contrastAdjustment | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectcontoursrequest/3548236-contrastadjustment",
    "html": "Discussion\n\nContour detection works best with high-contrast images. The default value of this property is 2.0, which doubles the image contrast to achieve the most accurate results.\n\nThis property supports a value range from 0.0 to 3.0.\n\nSee Also\nConfiguring the Request\nvar contrastPivot: NSNumber?\nThe pixel value to use as a pivot for the contrast.\nvar detectsDarkOnLight: Bool\nA Boolean value that indicates whether the request detects a dark object on a light background to aid in detection.\nvar maximumImageDimension: Int\nThe maximum image dimension to use for contour detection.\nvar detectDarkOnLight: Bool\nA Boolean value that indicates whether the request detects a dark object on a light background.\nDeprecated"
  },
  {
    "title": "availableKeys | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpoints3dobservation/4173236-availablekeys",
    "html": "See Also\nInspecting the Observation\nvar availableGroupKeys: [VNRecognizedPointGroupKey]\nThe available point group keys in the observation.\nfunc recognizedPoint(forKey: VNRecognizedPointKey) -> VNRecognizedPoint3D\nReturns a point for a key you specify.\nfunc recognizedPoints(forGroupKey: VNRecognizedPointGroupKey) -> [VNRecognizedPointKey : VNRecognizedPoint3D]\nReturns a point for a group key you specify."
  },
  {
    "title": "objectMinimumNormalizedRadius | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecttrajectoriesrequest/3675671-objectminimumnormalizedradius",
    "html": "See Also\nConfiguring the Request\nvar targetFrameTime: CMTime\nThe requested target frame time for processing trajectory detection.\nvar trajectoryLength: Int\nThe number of points to detect before calculating a trajectory.\nvar objectMaximumNormalizedRadius: Float\nThe maximum radius of the bounding circle of the object to track.\nvar minimumObjectSize: Float\nThe minimum radius of the tracked shape’s bounding circle.\nDeprecated\nvar maximumObjectSize: Float\nThe maximum radius of the tracked shape’s bounding circle.\nDeprecated"
  },
  {
    "title": "contrastPivot | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectcontoursrequest/3750961-contrastpivot",
    "html": "Discussion\n\nNumeric values range from 0.0 to 1.0. You can also specify nil to have the framework automatically detect the value according to image intensity.\n\nThe default value is 0.5, which indicates the pixel center.\n\nSee Also\nConfiguring the Request\nvar contrastAdjustment: Float\nThe amount by which to adjust the image contrast.\nvar detectsDarkOnLight: Bool\nA Boolean value that indicates whether the request detects a dark object on a light background to aid in detection.\nvar maximumImageDimension: Int\nThe maximum image dimension to use for contour detection.\nvar detectDarkOnLight: Bool\nA Boolean value that indicates whether the request detects a dark object on a light background.\nDeprecated"
  },
  {
    "title": "results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecttrajectoriesrequest/3675672-results",
    "html": "See Also\nInspecting the Results\nclass VNTrajectoryObservation\nAn observation that describes a detected trajectory."
  },
  {
    "title": "recognizedPoint(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/4173219-recognizedpoint",
    "html": "Parameters\njointName\n\nThe joint name to retrieve.\n\nReturn Value\n\nThe point for the joint name.\n\nSee Also\nAccessing Points\nvar availableJointNames: [VNAnimalBodyPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNAnimalBodyPoseObservation.JointName\nThe joint names for an animal body pose.\nvar availableJointGroupNames: [VNAnimalBodyPoseObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNAnimalBodyPoseObservation.JointsGroupName\nThe joint group names for an animal body pose.\nfunc recognizedPoints(VNAnimalBodyPoseObservation.JointsGroupName) -> [VNAnimalBodyPoseObservation.JointName : VNRecognizedPoint]\nReturns the points for a joint group name the observation recognizes."
  },
  {
    "title": "init(targetedImageURL:options:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntargetedimagerequest/2923453-init",
    "html": "Parameters\nimageURL\n\nThe URL of the targeted image.\n\noptions\n\nA dictionary with options specifying auxiliary information for the image.\n\ncompletionHandler\n\nThe block to invoke when the request has finished executing.\n\nSee Also\nCreating a Request\ninit(targetedCGImage: CGImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image, executing the completion handler when done.\ninit(targetedCGImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image of known orientation, executing the completion handler when done.\ninit(targetedCIImage: CIImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage, executing the completion handler when done.\ninit(targetedCIImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage of known orientation, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer of known orientation, executing the completion handler when done.\ninit(targetedCMSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image in a sample buffer.\ninit(targetedCMSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image of a known orientation in a sample buffer.\ninit(targetedImageData: Data, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image as raw data, executing the completion handler when done.\ninit(targetedImageData: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a raw data image of known orientation, executing the completion handler when done.\ninit(targetedImageURL: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image of known orientation, at the specified URL, executing the completion handler when done."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanbodypose3drequest/4210405-init",
    "html": "See Also\nInitializing a Request\ninit(completionHandler: VNRequestCompletionHandler?)\nCreates a new 3D body pose request with a completion handler."
  },
  {
    "title": "init(targetedImageURL:orientation:options:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntargetedimagerequest/2923457-init",
    "html": "Parameters\nimageURL\n\nThe URL of the targeted image.\n\norientation\n\nThe orientation of the image buffer, based on EXIF specification and superseding other orientation information. The value must be an integer from 1 to 8; see kCGImagePropertyOrientation for details.\n\noptions\n\nA dictionary with options specifying auxiliary information for the image.\n\ncompletionHandler\n\nThe block to invoke when the request has finished executing.\n\nSee Also\nCreating a Request\ninit(targetedCGImage: CGImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image, executing the completion handler when done.\ninit(targetedCGImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image of known orientation, executing the completion handler when done.\ninit(targetedCIImage: CIImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage, executing the completion handler when done.\ninit(targetedCIImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage of known orientation, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer of known orientation, executing the completion handler when done.\ninit(targetedCMSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image in a sample buffer.\ninit(targetedCMSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image of a known orientation in a sample buffer.\ninit(targetedImageData: Data, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image as raw data, executing the completion handler when done.\ninit(targetedImageData: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a raw data image of known orientation, executing the completion handler when done.\ninit(targetedImageURL: URL, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image at the specified URL, executing the completion handler when done."
  },
  {
    "title": "main | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncomputestage/4173285-main",
    "html": "See Also\nGet the Compute Stages\nstatic let postProcessing: VNComputeStage\nA stage that represents where the system performs additional analysis from the main compute stage."
  },
  {
    "title": "automaticallyDetectsLanguage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizetextrequest/3951398-automaticallydetectslanguage",
    "html": "See Also\nSpecifying the Language\nvar recognitionLanguages: [String]\nAn array of languages to detect, in priority order.\nvar usesLanguageCorrection: Bool\nA Boolean value that indicates whether the request applies language correction during the recognition process.\nvar customWords: [String]\nAn array of strings to supplement the recognized languages at the word-recognition stage.\nfunc supportedRecognitionLanguages() -> [String]\nReturns the identifiers of the languages that the request supports.\nclass func supportedRecognitionLanguages(for: VNRequestTextRecognitionLevel, revision: Int) -> [String]\nRequests a list of languages that the specified revision recognizes.\nDeprecated"
  },
  {
    "title": "postProcessing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncomputestage/4173286-postprocessing",
    "html": "See Also\nGet the Compute Stages\nstatic let main: VNComputeStage\nA stage that represents where the system performs the main functionality."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncomputestage/4176676-init",
    "html": "Parameters\nrawValue\n\nThe compute stage."
  },
  {
    "title": "transform(forImageWidth:height:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhorizonobservation/3958778-transform",
    "html": "Parameters\nwidth\n\nThe width of the image.\n\nheight\n\nThe height of the image.\n\nReturn Value\n\nAn affine transform.\n\nSee Also\nEvaluating the Horizon\nvar angle: CGFloat\nThe angle of the observed horizon.\nvar transform: CGAffineTransform\nThe transform to apply to the detected horizon."
  },
  {
    "title": "minimumTextHeight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizetextrequest/3152641-minimumtextheight",
    "html": "Discussion\n\nSpecify a floating-point number relative to the image height. For example, to limit recognition to text that’s half of the image height, use 0.5. Increasing the size reduces memory consumption and expedites recognition with the tradeoff of ignoring text smaller than the minimum height. The default value is 1/32, or 0.03125.\n\nSee Also\nCustomizing Recognition Constraints\nvar recognitionLevel: VNRequestTextRecognitionLevel\nA value that determines whether the request prioritizes accuracy or speed in text recognition.\nenum VNRequestTextRecognitionLevel\nConstants that identify the performance and accuracy of the text recognition."
  },
  {
    "title": "transform | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhorizonobservation/2867231-transform",
    "html": "Discussion\n\nApply the transform's inverse to orient the image in an upright position and make the detected horizon level.\n\nSee Also\nEvaluating the Horizon\nvar angle: CGFloat\nThe angle of the observed horizon.\nfunc transform(forImageWidth: Int, height: Int) -> CGAffineTransform\nCreates an affine transform for the specified image width and height."
  },
  {
    "title": "VNRequestTextRecognitionLevel | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequesttextrecognitionlevel",
    "html": "Topics\nRecognition Levels\ncase fast\nFast text recognition returns results more quickly at the expense of accuracy.\ncase accurate\nAccurate text recognition takes more time to produce a more comprehensive result.\nRelationships\nConforms To\nSendable\nSee Also\nCustomizing Recognition Constraints\nvar minimumTextHeight: Float\nThe minimum height, relative to the image height, of the text to recognize.\nvar recognitionLevel: VNRequestTextRecognitionLevel\nA value that determines whether the request prioritizes accuracy or speed in text recognition."
  },
  {
    "title": "recognitionLevel | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizetextrequest/3152643-recognitionlevel",
    "html": "Discussion\n\nThe recognition level determines which techniques the request uses during the text recognition. Set this value to VNRequestTextRecognitionLevel.fast to prioritize speed over accuracy, and to VNRequestTextRecognitionLevel.accurate for longer, more computationally intensive recognition.\n\nSee Also\nCustomizing Recognition Constraints\nvar minimumTextHeight: Float\nThe minimum height, relative to the image height, of the text to recognize.\nenum VNRequestTextRecognitionLevel\nConstants that identify the performance and accuracy of the text recognition."
  },
  {
    "title": "recognitionLanguages | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizetextrequest/3152642-recognitionlanguages",
    "html": "Discussion\n\nThe order of the languages in the array defines the order in which languages are used during language processing and text recognition.\n\nSpecify the languages as ISO language codes.\n\nSee Also\nSpecifying the Language\nvar automaticallyDetectsLanguage: Bool\nA Boolean value that indicates whether to attempt detecting the language to use the appropriate model for recognition and language correction.\nvar usesLanguageCorrection: Bool\nA Boolean value that indicates whether the request applies language correction during the recognition process.\nvar customWords: [String]\nAn array of strings to supplement the recognized languages at the word-recognition stage.\nfunc supportedRecognitionLanguages() -> [String]\nReturns the identifiers of the languages that the request supports.\nclass func supportedRecognitionLanguages(for: VNRequestTextRecognitionLevel, revision: Int) -> [String]\nRequests a list of languages that the specified revision recognizes.\nDeprecated"
  },
  {
    "title": "VNRecognizeTextRequestRevision3 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizetextrequestrevision3",
    "html": "See Also\nIdentifying Request Revisions\nlet VNRecognizeTextRequestRevision2: Int\nA constant for specifying revision 2 of the text recognition request.\nlet VNRecognizeTextRequestRevision1: Int\nA constant for specifying revision 1 of the text recognition request."
  },
  {
    "title": "usesLanguageCorrection | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizetextrequest/3166773-useslanguagecorrection",
    "html": "Discussion\n\nWhen this value is true, Vision applies language correction during the recognition process. Disabling this property returns the raw recognition results, which provides performance benefits but less accurate results.\n\nSee Also\nSpecifying the Language\nvar automaticallyDetectsLanguage: Bool\nA Boolean value that indicates whether to attempt detecting the language to use the appropriate model for recognition and language correction.\nvar recognitionLanguages: [String]\nAn array of languages to detect, in priority order.\nvar customWords: [String]\nAn array of strings to supplement the recognized languages at the word-recognition stage.\nfunc supportedRecognitionLanguages() -> [String]\nReturns the identifiers of the languages that the request supports.\nclass func supportedRecognitionLanguages(for: VNRequestTextRecognitionLevel, revision: Int) -> [String]\nRequests a list of languages that the specified revision recognizes.\nDeprecated"
  },
  {
    "title": "supportedRecognitionLanguages(for:revision:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizetextrequest/3152644-supportedrecognitionlanguages",
    "html": "Deprecated\n\nUse supportedRecognitionLanguages() instead.\n\nParameters\nrecognitionLevel\n\nThe level of recognition to prioritize. Set this level to VNRequestTextRecognitionLevel.fastto prioritize speed over accuracy, and to VNRequestTextRecognitionLevel.accurate to prioritize accuracy at the expense of speed.\n\nrequestRevision\n\nThe revision of the text recognition algorithm for the Vision framework to use.\n\nerror\n\nAn error that contains information about failed language support, or nil if no error occurred.\n\nReturn Value\n\nAn array of supported languages, listed as ISO language codes.\n\nDiscussion\n\nA language supported in one recognition level may not be available in another recognition level.\n\nSee Also\nSpecifying the Language\nvar automaticallyDetectsLanguage: Bool\nA Boolean value that indicates whether to attempt detecting the language to use the appropriate model for recognition and language correction.\nvar recognitionLanguages: [String]\nAn array of languages to detect, in priority order.\nvar usesLanguageCorrection: Bool\nA Boolean value that indicates whether the request applies language correction during the recognition process.\nvar customWords: [String]\nAn array of strings to supplement the recognized languages at the word-recognition stage.\nfunc supportedRecognitionLanguages() -> [String]\nReturns the identifiers of the languages that the request supports."
  },
  {
    "title": "customWords | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizetextrequest/3152640-customwords",
    "html": "Discussion\n\nCustom words take precedence over the standard lexicon. The request ignores this value if usesLanguageCorrection is false.\n\nSee Also\nSpecifying the Language\nvar automaticallyDetectsLanguage: Bool\nA Boolean value that indicates whether to attempt detecting the language to use the appropriate model for recognition and language correction.\nvar recognitionLanguages: [String]\nAn array of languages to detect, in priority order.\nvar usesLanguageCorrection: Bool\nA Boolean value that indicates whether the request applies language correction during the recognition process.\nfunc supportedRecognitionLanguages() -> [String]\nReturns the identifiers of the languages that the request supports.\nclass func supportedRecognitionLanguages(for: VNRequestTextRecognitionLevel, revision: Int) -> [String]\nRequests a list of languages that the specified revision recognizes.\nDeprecated"
  },
  {
    "title": "VNRecognizeTextRequestRevision2 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizetextrequestrevision2",
    "html": "See Also\nIdentifying Request Revisions\nlet VNRecognizeTextRequestRevision3: Int\nA constant for specifying revision 3 of the text recognition request.\nlet VNRecognizeTextRequestRevision1: Int\nA constant for specifying revision 1 of the text recognition request."
  },
  {
    "title": "supportedRecognitionLanguages() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizetextrequest/3751006-supportedrecognitionlanguages",
    "html": "Return Value\n\nThe language identifiers.\n\nSee Also\nSpecifying the Language\nvar automaticallyDetectsLanguage: Bool\nA Boolean value that indicates whether to attempt detecting the language to use the appropriate model for recognition and language correction.\nvar recognitionLanguages: [String]\nAn array of languages to detect, in priority order.\nvar usesLanguageCorrection: Bool\nA Boolean value that indicates whether the request applies language correction during the recognition process.\nvar customWords: [String]\nAn array of strings to supplement the recognized languages at the word-recognition stage.\nclass func supportedRecognitionLanguages(for: VNRequestTextRecognitionLevel, revision: Int) -> [String]\nRequests a list of languages that the specified revision recognizes.\nDeprecated"
  },
  {
    "title": "VNBarcodeSymbology | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodesymbology",
    "html": "Overview\n\nUse supportedSymbologies() to get the specific symbologies the request supports.\n\nTopics\nSupported Symbologies\nstatic let aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nstatic let codabar: VNBarcodeSymbology\nA constant that indicates Codabar symbology.\nstatic let code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nstatic let code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nstatic let code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nstatic let code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nstatic let code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nstatic let code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nstatic let code128: VNBarcodeSymbology\nA constant that indicates Code 128 symbology.\nstatic let dataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nstatic let ean8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nstatic let ean13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nstatic let gs1DataBar: VNBarcodeSymbology\nA constant that indicates GS1 DataBar symbology.\nstatic let gs1DataBarExpanded: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Expanded symbology.\nstatic let gs1DataBarLimited: VNBarcodeSymbology\nA constant that indicates GS1 DataBar Limited symbology.\nstatic let i2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nstatic let i2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nstatic let itf14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nstatic let microPDF417: VNBarcodeSymbology\nA constant that indicates MicroPDF417 symbology.\nstatic let microQR: VNBarcodeSymbology\nA constant that indicates MicroQR symbology.\nstatic let msiPlessey: VNBarcodeSymbology\nA constant that indicates Modified Plessey symbology.\nstatic let pdf417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nstatic let qr: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nstatic let upce: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated Symbols\nstatic var Aztec: VNBarcodeSymbology\nA constant that indicates Aztec symbology.\nDeprecated\nstatic var Code128: VNBarcodeSymbology\nDeprecated\nstatic var Code39: VNBarcodeSymbology\nA constant that indicates Code 39 symbology.\nDeprecated\nstatic var Code39Checksum: VNBarcodeSymbology\nA constant that indicates Code 39 symbology with a checksum.\nDeprecated\nstatic var Code39FullASCII: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology.\nDeprecated\nstatic var Code39FullASCIIChecksum: VNBarcodeSymbology\nA constant that indicates Code 39 Full ASCII symbology with a checksum.\nDeprecated\nstatic var Code93: VNBarcodeSymbology\nA constant that indicates Code 93 symbology.\nDeprecated\nstatic var Code93i: VNBarcodeSymbology\nA constant that indicates Code 93i symbology.\nDeprecated\nstatic var DataMatrix: VNBarcodeSymbology\nA constant that indicates Data Matrix symbology.\nDeprecated\nstatic var EAN8: VNBarcodeSymbology\nA constant that indicates EAN-8 symbology.\nDeprecated\nstatic var EAN13: VNBarcodeSymbology\nA constant that indicates EAN-13 symbology.\nDeprecated\nstatic var I2of5: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology.\nDeprecated\nstatic var I2of5Checksum: VNBarcodeSymbology\nA constant that indicates Interleaved 2 of 5 (ITF) symbology with a checksum.\nDeprecated\nstatic var ITF14: VNBarcodeSymbology\nA constant that indicates ITF-14 symbology.\nDeprecated\nstatic var PDF417: VNBarcodeSymbology\nA constant that indicates PDF417 symbology.\nDeprecated\nstatic var QR: VNBarcodeSymbology\nA constant that indicates Quick Response (QR) symbology.\nDeprecated\nstatic var UPCE: VNBarcodeSymbology\nA constant that indicates UPC-E symbology.\nDeprecated\nInitializers\ninit(rawValue: String)\nCreates a symbology with a string value.\nRelationships\nConforms To\nEquatable\nHashable\nRawRepresentable\nSendable\nSee Also\nSpecifying Symbologies\nfunc supportedSymbologies() -> [VNBarcodeSymbology]\nReturns the barcode symbologies that the request supports.\nvar symbologies: [VNBarcodeSymbology]\nThe barcode symbologies that the request detects in an image.\nvar coalesceCompositeSymbologies: Bool\nA Boolean value that indicates whether to coalesce multiple codes based on the symbology.\nclass var supportedSymbologies: [VNBarcodeSymbology]\nThe array of barcode symbologies that the request supports.\nDeprecated"
  },
  {
    "title": "supportedSymbologies() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectbarcodesrequest/3750959-supportedsymbologies",
    "html": "Return Value\n\nAn array of symbologies.\n\nSee Also\nSpecifying Symbologies\nvar symbologies: [VNBarcodeSymbology]\nThe barcode symbologies that the request detects in an image.\nvar coalesceCompositeSymbologies: Bool\nA Boolean value that indicates whether to coalesce multiple codes based on the symbology.\nstruct VNBarcodeSymbology\nThe barcode symbologies that the framework detects.\nclass var supportedSymbologies: [VNBarcodeSymbology]\nThe array of barcode symbologies that the request supports.\nDeprecated"
  },
  {
    "title": "progressHandler | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequestprogressproviding/3152652-progresshandler",
    "html": "Required\n\nDiscussion\n\nThe progress handler is an optional method that allows clients of the request to report progress to the user or to display partial results as they become available. The Vision framework may call this handler on a different dispatch queue from the thread on which you initiated the original request, so ensure that your handler can execute asynchronously, in a thread-safe manner.\n\nSee Also\nTracking Progress\nvar indeterminate: Bool\nA Boolean set to true when a request can't determine its progress in fractions completed.\n\nRequired"
  },
  {
    "title": "indeterminate | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequestprogressproviding/3152651-indeterminate",
    "html": "Required\n\nDiscussion\n\nA value of true doesn't mean that the request will run forever. Rather, it means that the nature of the request can't be broken down into identifiable fractions to report. The progressHandler will still be called at suitable intervals.\n\nSee Also\nTracking Progress\nvar progressHandler: VNRequestProgressHandler\nA block of code executed periodically during a Vision request to report progress on long-running tasks.\n\nRequired"
  },
  {
    "title": "coalesceCompositeSymbologies | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectbarcodesrequest/4173198-coalescecompositesymbologies",
    "html": "See Also\nSpecifying Symbologies\nfunc supportedSymbologies() -> [VNBarcodeSymbology]\nReturns the barcode symbologies that the request supports.\nvar symbologies: [VNBarcodeSymbology]\nThe barcode symbologies that the request detects in an image.\nstruct VNBarcodeSymbology\nThe barcode symbologies that the framework detects.\nclass var supportedSymbologies: [VNBarcodeSymbology]\nThe array of barcode symbologies that the request supports.\nDeprecated"
  },
  {
    "title": "supportedSymbologies | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectbarcodesrequest/2879281-supportedsymbologies",
    "html": "Deprecated\n\nUse supportedSymbologies() instead.\n\nDiscussion\n\nCalling this method can be an expensive operation.\n\nSee Also\nSpecifying Symbologies\nfunc supportedSymbologies() -> [VNBarcodeSymbology]\nReturns the barcode symbologies that the request supports.\nvar symbologies: [VNBarcodeSymbology]\nThe barcode symbologies that the request detects in an image.\nvar coalesceCompositeSymbologies: Bool\nA Boolean value that indicates whether to coalesce multiple codes based on the symbology.\nstruct VNBarcodeSymbology\nThe barcode symbologies that the framework detects."
  },
  {
    "title": "results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectbarcodesrequest/3750958-results",
    "html": "See Also\nAccessing the Results\nclass VNBarcodeObservation\nAn object that represents barcode information that an image analysis request detects."
  },
  {
    "title": "featureName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpixelbufferobservation/3131945-featurename",
    "html": "Discussion\n\nThis value is nil if the observation isn’t the result of a VNCoreMLRequest operation.\n\nSee Also\nParsing Observation Content\nvar pixelBuffer: CVPixelBuffer\nThe image that results from a request with image output."
  },
  {
    "title": "featureName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncoremlfeaturevalueobservation/3131944-featurename",
    "html": "See Also\nObtaining Feature Values\nvar featureValue: MLFeatureValue\nThe feature result of a VNCoreMLRequest that outputs neither a classification nor an image."
  },
  {
    "title": "generateScaledMaskForImage(forInstances:from:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vninstancemaskobservation/4210414-generatescaledmaskforimage",
    "html": "Parameters\ninstances\n\nThe collection of instances.\n\nrequestHandler\n\nThe image request callback.\n\nReturn Value\n\nThe pixel buffer that contains the image.\n\nSee Also\nCreating a Mask\nfunc generateMask(forInstances: IndexSet) -> CVPixelBuffer\nCreates a low-resolution mask from the instances you specify.\nfunc generateMaskedImage(ofInstances: IndexSet, from: VNImageRequestHandler, croppedToInstancesExtent: Bool) -> CVPixelBuffer\nCreates a high-resolution image where everything becomes transparent black, except for the instances you specify."
  },
  {
    "title": "init(completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntracktranslationalimageregistrationrequest/4127025-init",
    "html": "Parameters\ncompletionHandler\n\nThe callback the system invokes when it completes the request.\n\nSee Also\nCreating a Translational Image\ninit()\nCreates a new request that tracks the translational registration of two images."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntracktranslationalimageregistrationrequest/4127024-init",
    "html": "See Also\nCreating a Translational Image\ninit(completionHandler: VNRequestCompletionHandler?)\nCreates a new request that tracks the translational registration of two images, with a system callback on completion."
  },
  {
    "title": "init(targetedCVPixelBuffer:options:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntargetedimagerequest/2923446-init",
    "html": "Parameters\npixelBuffer\n\nThe pixel buffer containing the targeted image.\n\noptions\n\nA dictionary with options specifying auxiliary information for the image.\n\ncompletionHandler\n\nThe block to invoke when the request has finished executing.\n\nSee Also\nCreating a Request\ninit(targetedCGImage: CGImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image, executing the completion handler when done.\ninit(targetedCGImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image of known orientation, executing the completion handler when done.\ninit(targetedCIImage: CIImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage, executing the completion handler when done.\ninit(targetedCIImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage of known orientation, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer of known orientation, executing the completion handler when done.\ninit(targetedCMSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image in a sample buffer.\ninit(targetedCMSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image of a known orientation in a sample buffer.\ninit(targetedImageData: Data, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image as raw data, executing the completion handler when done.\ninit(targetedImageData: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a raw data image of known orientation, executing the completion handler when done.\ninit(targetedImageURL: URL, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image at the specified URL, executing the completion handler when done.\ninit(targetedImageURL: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image of known orientation, at the specified URL, executing the completion handler when done."
  },
  {
    "title": "quadratureTolerance | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectrectanglesrequest/2875379-quadraturetolerance",
    "html": "Discussion\n\nThe tolerance value should range from 0 to 45, inclusive. The default tolerance is 30.\n\nSee Also\nConfiguring Detection\nvar minimumAspectRatio: VNAspectRatio\nA float specifying the minimum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\nvar maximumAspectRatio: VNAspectRatio\nA float specifying the maximum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\ntypealias VNAspectRatio\nA type alias for expressing rectangle aspect ratios in Vision.\ntypealias VNDegrees\nA typealias for expressing tolerance angles in Vision.\nvar minimumSize: Float\nThe minimum size of a rectangle to detect, as a proportion of the smallest dimension.\nvar minimumConfidence: VNConfidence\nA value specifying the minimum acceptable confidence level.\ntypealias VNConfidence\nA type alias for the confidence value of an observation.\nvar maximumObservations: Int\nAn integer specifying the maximum number of rectangles Vision returns."
  },
  {
    "title": "supportedJointsGroupNames(forRevision:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanhandposerequest/3675635-supportedjointsgroupnames",
    "html": "Parameters\nrevision\n\nThe hand pose request revision.\n\nReturn Value\n\nThe array of joint group name objects for the revision.\n\nSee Also\nDetermining Supported Joints\nvar supportedJointNames: [VNHumanHandPoseObservation.JointName]\nRetrieves the supported joint names.\nclass func supportedJointNames(forRevision: Int) -> [VNHumanHandPoseObservation.JointName]\nRetrieves the supported joint names for a revision.\nDeprecated\nvar supportedJointsGroupNames: [VNHumanHandPoseObservation.JointsGroupName]\nRetrieves the supported joint group names."
  },
  {
    "title": "supportedJointNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanhandposerequest/4213146-supportedjointnames",
    "html": "See Also\nDetermining Supported Joints\nclass func supportedJointNames(forRevision: Int) -> [VNHumanHandPoseObservation.JointName]\nRetrieves the supported joint names for a revision.\nDeprecated\nvar supportedJointsGroupNames: [VNHumanHandPoseObservation.JointsGroupName]\nRetrieves the supported joint group names.\nclass func supportedJointsGroupNames(forRevision: Int) -> [VNHumanHandPoseObservation.JointsGroupName]\nRetrieves the supported joint group names for a revision.\nDeprecated"
  },
  {
    "title": "VNBarcodeObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodeobservation",
    "html": "Overview\n\nThis type of observation results from a VNDetectBarcodesRequest. It contains information about the detected barcode, including parsed payload data for supported symbologies.\n\nTopics\nParsing the Payload\nvar payloadStringValue: String?\nA string value that represents the barcode payload.\nvar payloadData: Data?\nThe raw data representation of the barcode’s payload.\nvar supplementalPayloadString: String?\nThe supplemental code decoded as a string value.\nvar supplementalPayloadData: Data?\nvar supplementalCompositeType: VNBarcodeCompositeType\nThe supplemental composite type.\nvar isGS1DataCarrier: Bool\nA Boolean value that indicates whether the barcode carries any global standards data.\nReading Barcode Descriptors\nvar barcodeDescriptor: CIBarcodeDescriptor?\nAn object that describes the low-level details about the barcode and its data.\nIdentifying Barcode Types\nvar symbology: VNBarcodeSymbology\nThe symbology of the observed barcode.\nIdentifying Barcode Colors\nvar isColorInverted: Bool\nA Boolean value that indicates whether the barcode is color inverted.\nRelationships\nInherits From\nVNRectangleObservation\nSee Also\nAccessing the Results\nvar results: [VNBarcodeObservation]?\nThe results of a barcode detection request."
  },
  {
    "title": "VNDetectBarcodesRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectbarcodesrequestrevision1",
    "html": "Deprecated\n\nUse VNDetectBarcodesRequestRevision3 instead.\n\nSee Also\nIdentifying Request Revisions\nlet VNDetectBarcodesRequestRevision3: Int\nA constant for specifying revision 3 of the barcode detection request.\nlet VNDetectBarcodesRequestRevision2: Int\nA constant for specifying revision 2 of the barcode detection request."
  },
  {
    "title": "results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectcontoursrequest/3750962-results",
    "html": "See Also\nAccessing the Results\nclass VNContoursObservation\nAn object that represents the detected contours in an image."
  },
  {
    "title": "trajectoryLength | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecttrajectoriesrequest/3675673-trajectorylength",
    "html": "See Also\nConfiguring the Request\nvar targetFrameTime: CMTime\nThe requested target frame time for processing trajectory detection.\nvar objectMinimumNormalizedRadius: Float\nThe minimum radius of the bounding circle of the object to track.\nvar objectMaximumNormalizedRadius: Float\nThe maximum radius of the bounding circle of the object to track.\nvar minimumObjectSize: Float\nThe minimum radius of the tracked shape’s bounding circle.\nDeprecated\nvar maximumObjectSize: Float\nThe maximum radius of the tracked shape’s bounding circle.\nDeprecated"
  },
  {
    "title": "objectMaximumNormalizedRadius | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecttrajectoriesrequest/3675670-objectmaximumnormalizedradius",
    "html": "See Also\nConfiguring the Request\nvar targetFrameTime: CMTime\nThe requested target frame time for processing trajectory detection.\nvar trajectoryLength: Int\nThe number of points to detect before calculating a trajectory.\nvar objectMinimumNormalizedRadius: Float\nThe minimum radius of the bounding circle of the object to track.\nvar minimumObjectSize: Float\nThe minimum radius of the tracked shape’s bounding circle.\nDeprecated\nvar maximumObjectSize: Float\nThe maximum radius of the tracked shape’s bounding circle.\nDeprecated"
  },
  {
    "title": "targetFrameTime | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecttrajectoriesrequest/3750981-targetframetime",
    "html": "Discussion\n\nUse this property value for real-time processing of frames, which requires execution within a specific amount of time. The request evaluates from frame-to-frame. If processing takes longer than the targeted time for the current frame, it attempts to decrease the overall time by reducing the accuracy (down to a set minimum) for the next frame. If a frame takes less time than the targeted time, the request increases the accuracy (up to a set maximum) of the next frame.\n\nThe default value is indefinite, which indicates that accuracy stays at the predefined maximum.\n\nSee Also\nConfiguring the Request\nvar trajectoryLength: Int\nThe number of points to detect before calculating a trajectory.\nvar objectMinimumNormalizedRadius: Float\nThe minimum radius of the bounding circle of the object to track.\nvar objectMaximumNormalizedRadius: Float\nThe maximum radius of the bounding circle of the object to track.\nvar minimumObjectSize: Float\nThe minimum radius of the tracked shape’s bounding circle.\nDeprecated\nvar maximumObjectSize: Float\nThe maximum radius of the tracked shape’s bounding circle.\nDeprecated"
  },
  {
    "title": "VNTrajectoryObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrajectoryobservation",
    "html": "Topics\nEvaluating an Observation\nvar detectedPoints: [VNPoint]\nThe centroid points of the detected contour along the trajectory.\nvar projectedPoints: [VNPoint]\nThe centroids of the calculated trajectory from the detected points.\nvar equationCoefficients: simd_float3\nThe coefficients of the parabolic equation.\nvar movingAverageRadius: CGFloat\nThe moving average radius of the object the request is tracking.\nRelationships\nInherits From\nVNObservation\nSee Also\nInspecting the Results\nvar results: [VNTrajectoryObservation]?\nThe array of detected trajectory observations."
  },
  {
    "title": "maximumObjectSize | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecttrajectoriesrequest/3600613-maximumobjectsize",
    "html": "Deprecated\n\nUse objectMaximumNormalizedRadius instead.\n\nDiscussion\n\nSet the maximum size to filter out unwanted trajectories from larger objects moving through the scene. The default value is 1.0, which means to apply no filtering.\n\nChanging this property value from frame to frame can produce erratic trajectories because objects either disappear or are added to the tracking based on this filtering.\n\nSpecify the size in normalized (0.0 to 1.0) coordinates.\n\nSee Also\nConfiguring the Request\nvar targetFrameTime: CMTime\nThe requested target frame time for processing trajectory detection.\nvar trajectoryLength: Int\nThe number of points to detect before calculating a trajectory.\nvar objectMinimumNormalizedRadius: Float\nThe minimum radius of the bounding circle of the object to track.\nvar objectMaximumNormalizedRadius: Float\nThe maximum radius of the bounding circle of the object to track.\nvar minimumObjectSize: Float\nThe minimum radius of the tracked shape’s bounding circle.\nDeprecated"
  },
  {
    "title": "minimumObjectSize | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecttrajectoriesrequest/3564820-minimumobjectsize",
    "html": "Deprecated\n\nUse objectMinimumNormalizedRadius instead.\n\nDiscussion\n\nSet the minimum size to filter out noise and small objects. The default value is 0, which means to apply no filtering.\n\nChanging this property value from frame to frame can produce erratic trajectories because objects either disappear or are added to the tracking based on this filtering.\n\nSpecify the size in normalized (0.0 to 1.0) coordinates.\n\nSee Also\nConfiguring the Request\nvar targetFrameTime: CMTime\nThe requested target frame time for processing trajectory detection.\nvar trajectoryLength: Int\nThe number of points to detect before calculating a trajectory.\nvar objectMinimumNormalizedRadius: Float\nThe minimum radius of the bounding circle of the object to track.\nvar objectMaximumNormalizedRadius: Float\nThe maximum radius of the bounding circle of the object to track.\nvar maximumObjectSize: Float\nThe maximum radius of the tracked shape’s bounding circle.\nDeprecated"
  },
  {
    "title": "VNAnimalBodyPoseObservation.JointName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointname",
    "html": "Topics\nGetting the Head Joint Names\nstatic let leftEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the left ear.\nstatic let leftEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the left ear.\nstatic let leftEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the left ear.\nstatic let leftEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the left eye.\nstatic let neck: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the neck.\nstatic let nose: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the nose.\nstatic let rightEye: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the right eye.\nstatic let rightEarTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the right ear.\nstatic let rightEarMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the right ear.\nstatic let rightEarBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the right ear.\nGetting the Leg Joint Names\nstatic let leftBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left elbow.\nstatic let leftFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left elbow.\nstatic let rightFrontElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right elbow.\nstatic let rightBackElbow: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right elbow.\nstatic let leftBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left knee.\nstatic let leftFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left knee.\nstatic let rightBackKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right knee.\nstatic let rightFrontKnee: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right knee.\nstatic let leftBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the left paw.\nstatic let leftFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the left paw.\nstatic let rightBackPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the back of the right paw.\nstatic let rightFrontPaw: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the front of the right paw.\nGetting the Tail Joint Names\nstatic let tailTop: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the top of the tail.\nstatic let tailMiddle: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the middle of the tail.\nstatic let tailBottom: VNAnimalBodyPoseObservation.JointName\nA joint name that represents the bottom of the tail.\nCreating a Joint Name\ninit(rawValue: VNRecognizedPointKey)\nCreates a joint name with the key you specify.\nRelationships\nConforms To\nHashable\nRawRepresentable\nSendable\nSee Also\nAccessing Points\nvar availableJointNames: [VNAnimalBodyPoseObservation.JointName]\nThe names of the available joints in the observation.\nvar availableJointGroupNames: [VNAnimalBodyPoseObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNAnimalBodyPoseObservation.JointsGroupName\nThe joint group names for an animal body pose.\nfunc recognizedPoint(VNAnimalBodyPoseObservation.JointName) -> VNRecognizedPoint\nReturns the point for a joint name the observation recognizes.\nfunc recognizedPoints(VNAnimalBodyPoseObservation.JointsGroupName) -> [VNAnimalBodyPoseObservation.JointName : VNRecognizedPoint]\nReturns the points for a joint group name the observation recognizes."
  },
  {
    "title": "recognizedPoints(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/4173220-recognizedpoints",
    "html": "Parameters\njointsGroupName\n\nThe joint group of the points to retrieve.\n\nReturn Value\n\nThe dictionary of points the observation associates with the group name.\n\nSee Also\nAccessing Points\nvar availableJointNames: [VNAnimalBodyPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNAnimalBodyPoseObservation.JointName\nThe joint names for an animal body pose.\nvar availableJointGroupNames: [VNAnimalBodyPoseObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNAnimalBodyPoseObservation.JointsGroupName\nThe joint group names for an animal body pose.\nfunc recognizedPoint(VNAnimalBodyPoseObservation.JointName) -> VNRecognizedPoint\nReturns the point for a joint name the observation recognizes."
  },
  {
    "title": "availableJointNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/4173218-availablejointnames",
    "html": "See Also\nAccessing Points\nstruct VNAnimalBodyPoseObservation.JointName\nThe joint names for an animal body pose.\nvar availableJointGroupNames: [VNAnimalBodyPoseObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNAnimalBodyPoseObservation.JointsGroupName\nThe joint group names for an animal body pose.\nfunc recognizedPoint(VNAnimalBodyPoseObservation.JointName) -> VNRecognizedPoint\nReturns the point for a joint name the observation recognizes.\nfunc recognizedPoints(VNAnimalBodyPoseObservation.JointsGroupName) -> [VNAnimalBodyPoseObservation.JointName : VNRecognizedPoint]\nReturns the points for a joint group name the observation recognizes."
  },
  {
    "title": "VNAnimalBodyPoseObservation.JointsGroupName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/jointsgroupname",
    "html": "Topics\nGetting the Group Names\nstatic let all: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents all joints.\nstatic let forelegs: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the forelegs.\nstatic let head: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the head.\nstatic let hindlegs: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the hindlegs.\nstatic let tail: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the tail.\nstatic let trunk: VNAnimalBodyPoseObservation.JointsGroupName\nA group name that represents the trunk.\nCreating a Group Name\ninit(rawValue: VNRecognizedPointGroupKey)\nCreates a joint name with the key you specify.\nRelationships\nConforms To\nHashable\nRawRepresentable\nSendable\nSee Also\nAccessing Points\nvar availableJointNames: [VNAnimalBodyPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNAnimalBodyPoseObservation.JointName\nThe joint names for an animal body pose.\nvar availableJointGroupNames: [VNAnimalBodyPoseObservation.JointsGroupName]\nThe available joint group names in the observation.\nfunc recognizedPoint(VNAnimalBodyPoseObservation.JointName) -> VNRecognizedPoint\nReturns the point for a joint name the observation recognizes.\nfunc recognizedPoints(VNAnimalBodyPoseObservation.JointsGroupName) -> [VNAnimalBodyPoseObservation.JointName : VNRecognizedPoint]\nReturns the points for a joint group name the observation recognizes."
  },
  {
    "title": "availableJointGroupNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation/4173217-availablejointgroupnames",
    "html": "See Also\nAccessing Points\nvar availableJointNames: [VNAnimalBodyPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNAnimalBodyPoseObservation.JointName\nThe joint names for an animal body pose.\nstruct VNAnimalBodyPoseObservation.JointsGroupName\nThe joint group names for an animal body pose.\nfunc recognizedPoint(VNAnimalBodyPoseObservation.JointName) -> VNRecognizedPoint\nReturns the point for a joint name the observation recognizes.\nfunc recognizedPoints(VNAnimalBodyPoseObservation.JointsGroupName) -> [VNAnimalBodyPoseObservation.JointName : VNRecognizedPoint]\nReturns the points for a joint group name the observation recognizes."
  },
  {
    "title": "availableGroupKeys | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpoints3dobservation/4173235-availablegroupkeys",
    "html": "See Also\nInspecting the Observation\nvar availableKeys: [VNRecognizedPointKey]\nThe available point keys in the observation.\nfunc recognizedPoint(forKey: VNRecognizedPointKey) -> VNRecognizedPoint3D\nReturns a point for a key you specify.\nfunc recognizedPoints(forGroupKey: VNRecognizedPointGroupKey) -> [VNRecognizedPointKey : VNRecognizedPoint3D]\nReturns a point for a group key you specify."
  },
  {
    "title": "recognizedPoint(forKey:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpoints3dobservation/4173237-recognizedpoint",
    "html": "Parameters\npointKey\n\nThe key of the point to retrieve.\n\nReturn Value\n\nThe point the observation associates with the key.\n\nSee Also\nInspecting the Observation\nvar availableKeys: [VNRecognizedPointKey]\nThe available point keys in the observation.\nvar availableGroupKeys: [VNRecognizedPointGroupKey]\nThe available point group keys in the observation.\nfunc recognizedPoints(forGroupKey: VNRecognizedPointGroupKey) -> [VNRecognizedPointKey : VNRecognizedPoint3D]\nReturns a point for a group key you specify."
  },
  {
    "title": "init(position:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpoint3d/4173211-init",
    "html": "Parameters\nposition\n\nThe three-dimensional position."
  },
  {
    "title": "Body Landmarks | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/body_landmarks",
    "html": "Overview\n\nThe illustration below shows the nineteen body points that you can detect with VNDetectHumanBodyPoseRequest.\n\nTopics\nHead\nstatic let bodyLandmarkKeyLeftEye: VNRecognizedPointKey\nThe left eye.\nDeprecated\nstatic let bodyLandmarkKeyRightEye: VNRecognizedPointKey\nThe right eye.\nDeprecated\nstatic let bodyLandmarkKeyLeftEar: VNRecognizedPointKey\nThe left ear.\nDeprecated\nstatic let bodyLandmarkKeyRightEar: VNRecognizedPointKey\nThe right ear.\nDeprecated\nstatic let bodyLandmarkKeyNose: VNRecognizedPointKey\nThe nose.\nDeprecated\nstatic let bodyLandmarkRegionKeyFace: VNRecognizedPointGroupKey\nA group key identifying the face, which includes the eyes, ears, and nose.\nDeprecated\nTorso\nstatic let bodyLandmarkKeyNeck: VNRecognizedPointKey\nThe center point of the neck.\nDeprecated\nstatic let bodyLandmarkKeyLeftShoulder: VNRecognizedPointKey\nThe left shoulder.\nDeprecated\nstatic let bodyLandmarkKeyRightShoulder: VNRecognizedPointKey\nThe right shoulder.\nDeprecated\nstatic let bodyLandmarkKeyLeftHip: VNRecognizedPointKey\nThe left hip.\nDeprecated\nstatic let bodyLandmarkKeyRightHip: VNRecognizedPointKey\nThe right hip.\nDeprecated\nstatic let bodyLandmarkKeyRoot: VNRecognizedPointKey\nThe center point of the waist.\nDeprecated\nstatic let bodyLandmarkRegionKeyTorso: VNRecognizedPointGroupKey\nA group key identifying the torso, which includes the neck, shoulders, hips, and root.\nDeprecated\nArms\nstatic let bodyLandmarkKeyRightWrist: VNRecognizedPointKey\nThe right wrist.\nDeprecated\nstatic let bodyLandmarkKeyRightElbow: VNRecognizedPointKey\nThe right elbow.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right arm.\nDeprecated\nstatic let bodyLandmarkKeyLeftWrist: VNRecognizedPointKey\nThe left wrist.\nDeprecated\nstatic let bodyLandmarkKeyLeftElbow: VNRecognizedPointKey\nThe left elbow.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left arm.\nDeprecated\nLegs\nstatic let bodyLandmarkKeyRightKnee: VNRecognizedPointKey\nThe right knee.\nDeprecated\nstatic let bodyLandmarkKeyRightAnkle: VNRecognizedPointKey\nThe right ankle.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right leg.\nDeprecated\nstatic let bodyLandmarkKeyLeftKnee: VNRecognizedPointKey\nThe left knee.\nDeprecated\nstatic let bodyLandmarkKeyLeftAnkle: VNRecognizedPointKey\nThe left ankle.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left leg.\nDeprecated\nAll Landmarks\nstatic let all: VNRecognizedPointGroupKey\nA group key identifying all landmarks.\nSee Also\nLandmarks\nHand Landmarks\nThe hand landmarks that Vision detects."
  },
  {
    "title": "init(targetedCMSampleBuffer:options:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntargetedimagerequest/3571275-init",
    "html": "Parameters\nsampleBuffer\n\nA sample buffer containing a valid imageBuffer.\n\noptions\n\nA dictionary with options specifying auxiliary information for the image.\n\ncompletionHandler\n\nThe callback the system invokes when the request finishes executing.\n\nSee Also\nCreating a Request\ninit(targetedCGImage: CGImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image, executing the completion handler when done.\ninit(targetedCGImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image of known orientation, executing the completion handler when done.\ninit(targetedCIImage: CIImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage, executing the completion handler when done.\ninit(targetedCIImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage of known orientation, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer of known orientation, executing the completion handler when done.\ninit(targetedCMSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image of a known orientation in a sample buffer.\ninit(targetedImageData: Data, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image as raw data, executing the completion handler when done.\ninit(targetedImageData: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a raw data image of known orientation, executing the completion handler when done.\ninit(targetedImageURL: URL, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image at the specified URL, executing the completion handler when done.\ninit(targetedImageURL: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image of known orientation, at the specified URL, executing the completion handler when done."
  },
  {
    "title": "init(targetedCMSampleBuffer:orientation:options:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntargetedimagerequest/3571277-init",
    "html": "Parameters\nsampleBuffer\n\nA sample buffer containing a valid imageBuffer.\n\norientation\n\nThe EXIF orientation of the image. See CGImagePropertyOrientation for supported orientations.\n\noptions\n\nA dictionary with options specifying auxiliary information for the image.\n\ncompletionHandler\n\nThe callback the system invokes when the request finishes executing.\n\nSee Also\nCreating a Request\ninit(targetedCGImage: CGImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image, executing the completion handler when done.\ninit(targetedCGImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image of known orientation, executing the completion handler when done.\ninit(targetedCIImage: CIImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage, executing the completion handler when done.\ninit(targetedCIImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage of known orientation, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer of known orientation, executing the completion handler when done.\ninit(targetedCMSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image in a sample buffer.\ninit(targetedImageData: Data, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image as raw data, executing the completion handler when done.\ninit(targetedImageData: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a raw data image of known orientation, executing the completion handler when done.\ninit(targetedImageURL: URL, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image at the specified URL, executing the completion handler when done.\ninit(targetedImageURL: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image of known orientation, at the specified URL, executing the completion handler when done."
  },
  {
    "title": "init(targetedImageData:options:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntargetedimagerequest/2923455-init",
    "html": "Parameters\nimageData\n\nThe data containing the targeted image.\n\noptions\n\nA dictionary with options specifying auxiliary information for the image.\n\ncompletionHandler\n\nThe block to invoke when the request has finished executing.\n\nSee Also\nCreating a Request\ninit(targetedCGImage: CGImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image, executing the completion handler when done.\ninit(targetedCGImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image of known orientation, executing the completion handler when done.\ninit(targetedCIImage: CIImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage, executing the completion handler when done.\ninit(targetedCIImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage of known orientation, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer of known orientation, executing the completion handler when done.\ninit(targetedCMSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image in a sample buffer.\ninit(targetedCMSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image of a known orientation in a sample buffer.\ninit(targetedImageData: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a raw data image of known orientation, executing the completion handler when done.\ninit(targetedImageURL: URL, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image at the specified URL, executing the completion handler when done.\ninit(targetedImageURL: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image of known orientation, at the specified URL, executing the completion handler when done."
  },
  {
    "title": "init(targetedCVPixelBuffer:orientation:options:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntargetedimagerequest/2923449-init",
    "html": "Parameters\npixelBuffer\n\nThe pixel buffer containing the targeted image.\n\norientation\n\nThe orientation of the image buffer, based on EXIF specification and superseding other orientation information. The value must be an integer from 1 to 8; see kCGImagePropertyOrientation for details.\n\noptions\n\nA dictionary with options specifying auxiliary information for the image.\n\ncompletionHandler\n\nThe block to invoke when the request has finished executing.\n\nSee Also\nCreating a Request\ninit(targetedCGImage: CGImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image, executing the completion handler when done.\ninit(targetedCGImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image of known orientation, executing the completion handler when done.\ninit(targetedCIImage: CIImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage, executing the completion handler when done.\ninit(targetedCIImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage of known orientation, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer, executing the completion handler when done.\ninit(targetedCMSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image in a sample buffer.\ninit(targetedCMSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image of a known orientation in a sample buffer.\ninit(targetedImageData: Data, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image as raw data, executing the completion handler when done.\ninit(targetedImageData: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a raw data image of known orientation, executing the completion handler when done.\ninit(targetedImageURL: URL, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image at the specified URL, executing the completion handler when done.\ninit(targetedImageURL: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image of known orientation, at the specified URL, executing the completion handler when done."
  },
  {
    "title": "init(targetedImageData:orientation:options:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntargetedimagerequest/2923443-init",
    "html": "Parameters\nimageData\n\nThe data containing the targeted image.\n\norientation\n\nThe orientation of the image buffer, based on EXIF specification and superseding other orientation information. The value must be an integer from 1 to 8; see kCGImagePropertyOrientation for details.\n\noptions\n\nA dictionary with options specifying auxiliary information for the image.\n\ncompletionHandler\n\nThe block to invoke when the request has finished executing.\n\nSee Also\nCreating a Request\ninit(targetedCGImage: CGImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image, executing the completion handler when done.\ninit(targetedCGImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image of known orientation, executing the completion handler when done.\ninit(targetedCIImage: CIImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage, executing the completion handler when done.\ninit(targetedCIImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage of known orientation, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer of known orientation, executing the completion handler when done.\ninit(targetedCMSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image in a sample buffer.\ninit(targetedCMSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image of a known orientation in a sample buffer.\ninit(targetedImageData: Data, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image as raw data, executing the completion handler when done.\ninit(targetedImageURL: URL, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image at the specified URL, executing the completion handler when done.\ninit(targetedImageURL: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image of known orientation, at the specified URL, executing the completion handler when done."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey/3626466-init",
    "html": "Parameters\nrawValue\n\nA string value that represents a body or hand landmark."
  },
  {
    "title": "bodyLandmarkRegionKeyFace | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointgroupkey/3548256-bodylandmarkregionkeyface",
    "html": "See Also\nBody Regions\nstatic let bodyLandmarkRegionKeyTorso: VNRecognizedPointGroupKey\nA group key identifying the torso, which includes the neck, shoulders, hips, and root.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right arm.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left arm.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right leg.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left leg.\nDeprecated"
  },
  {
    "title": "bodyLandmarkRegionKeyTorso | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointgroupkey/3548261-bodylandmarkregionkeytorso",
    "html": "See Also\nBody Regions\nstatic let bodyLandmarkRegionKeyFace: VNRecognizedPointGroupKey\nA group key identifying the face, which includes the eyes, ears, and nose.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right arm.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left arm.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right leg.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left leg.\nDeprecated"
  },
  {
    "title": "init(completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanbodypose3drequest/4210406-init",
    "html": "Parameters\ncompletionHandler\n\nThe block to invoke after the request finishes processing.\n\nSee Also\nInitializing a Request\ninit()\nCreates a new request with no completion handler."
  },
  {
    "title": "salientObjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsaliencyimageobservation/3114160-salientobjects",
    "html": "Discussion\n\nThe objects in this array don't follow any specific ordering. It's up to your app to iterate across the observations and apply desired ordering.\n\nRequesting this array lazily computes the bounds of salient objects within the image."
  },
  {
    "title": "maximumObservations | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectrectanglesrequest/2875373-maximumobservations",
    "html": "Discussion\n\nThe default value is 1.\n\nSetting this property to 0 allows Vision algorithms to return an unlimited number of observations.\n\nSee Also\nConfiguring Detection\nvar minimumAspectRatio: VNAspectRatio\nA float specifying the minimum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\nvar maximumAspectRatio: VNAspectRatio\nA float specifying the maximum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\ntypealias VNAspectRatio\nA type alias for expressing rectangle aspect ratios in Vision.\nvar quadratureTolerance: VNDegrees\nA float specifying the number of degrees a rectangle corner angle can deviate from 90°.\ntypealias VNDegrees\nA typealias for expressing tolerance angles in Vision.\nvar minimumSize: Float\nThe minimum size of a rectangle to detect, as a proportion of the smallest dimension.\nvar minimumConfidence: VNConfidence\nA value specifying the minimum acceptable confidence level.\ntypealias VNConfidence\nA type alias for the confidence value of an observation."
  },
  {
    "title": "minimumSize | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectrectanglesrequest/2875374-minimumsize",
    "html": "Discussion\n\nThe value should range from 0.0 to 1.0 inclusive. The default minimum size is 0.2.\n\nAny smaller rectangles that Vision may have detected aren’t returned.\n\nSee Also\nConfiguring Detection\nvar minimumAspectRatio: VNAspectRatio\nA float specifying the minimum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\nvar maximumAspectRatio: VNAspectRatio\nA float specifying the maximum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\ntypealias VNAspectRatio\nA type alias for expressing rectangle aspect ratios in Vision.\nvar quadratureTolerance: VNDegrees\nA float specifying the number of degrees a rectangle corner angle can deviate from 90°.\ntypealias VNDegrees\nA typealias for expressing tolerance angles in Vision.\nvar minimumConfidence: VNConfidence\nA value specifying the minimum acceptable confidence level.\ntypealias VNConfidence\nA type alias for the confidence value of an observation.\nvar maximumObservations: Int\nAn integer specifying the maximum number of rectangles Vision returns."
  },
  {
    "title": "bodyLandmarkRegionKeyRightArm | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointgroupkey/3548259-bodylandmarkregionkeyrightarm",
    "html": "See Also\nBody Regions\nstatic let bodyLandmarkRegionKeyFace: VNRecognizedPointGroupKey\nA group key identifying the face, which includes the eyes, ears, and nose.\nDeprecated\nstatic let bodyLandmarkRegionKeyTorso: VNRecognizedPointGroupKey\nA group key identifying the torso, which includes the neck, shoulders, hips, and root.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left arm.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right leg.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left leg.\nDeprecated"
  },
  {
    "title": "VNAspectRatio | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnaspectratio",
    "html": "Discussion\n\nThe value is a float, but limited to a range of 0.0 to 1.0, inclusive, with the default of 0.5 indicating a square image. It defines aspect ratio as the shorter dimension over the longer dimension.\n\nSee Also\nConfiguring Detection\nvar minimumAspectRatio: VNAspectRatio\nA float specifying the minimum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\nvar maximumAspectRatio: VNAspectRatio\nA float specifying the maximum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\nvar quadratureTolerance: VNDegrees\nA float specifying the number of degrees a rectangle corner angle can deviate from 90°.\ntypealias VNDegrees\nA typealias for expressing tolerance angles in Vision.\nvar minimumSize: Float\nThe minimum size of a rectangle to detect, as a proportion of the smallest dimension.\nvar minimumConfidence: VNConfidence\nA value specifying the minimum acceptable confidence level.\ntypealias VNConfidence\nA type alias for the confidence value of an observation.\nvar maximumObservations: Int\nAn integer specifying the maximum number of rectangles Vision returns."
  },
  {
    "title": "maximumAspectRatio | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectrectanglesrequest/2875376-maximumaspectratio",
    "html": "Discussion\n\nThe value should range from 0.0 to 1.0, inclusive. The default value is 0.5.\n\nSee Also\nConfiguring Detection\nvar minimumAspectRatio: VNAspectRatio\nA float specifying the minimum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\ntypealias VNAspectRatio\nA type alias for expressing rectangle aspect ratios in Vision.\nvar quadratureTolerance: VNDegrees\nA float specifying the number of degrees a rectangle corner angle can deviate from 90°.\ntypealias VNDegrees\nA typealias for expressing tolerance angles in Vision.\nvar minimumSize: Float\nThe minimum size of a rectangle to detect, as a proportion of the smallest dimension.\nvar minimumConfidence: VNConfidence\nA value specifying the minimum acceptable confidence level.\ntypealias VNConfidence\nA type alias for the confidence value of an observation.\nvar maximumObservations: Int\nAn integer specifying the maximum number of rectangles Vision returns."
  },
  {
    "title": "bodyLandmarkRegionKeyLeftArm | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointgroupkey/3548257-bodylandmarkregionkeyleftarm",
    "html": "See Also\nBody Regions\nstatic let bodyLandmarkRegionKeyFace: VNRecognizedPointGroupKey\nA group key identifying the face, which includes the eyes, ears, and nose.\nDeprecated\nstatic let bodyLandmarkRegionKeyTorso: VNRecognizedPointGroupKey\nA group key identifying the torso, which includes the neck, shoulders, hips, and root.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right arm.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right leg.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left leg.\nDeprecated"
  },
  {
    "title": "availableJointNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/3675637-availablejointnames",
    "html": "See Also\nRetrieving Points\nstruct VNHumanHandPoseObservation.JointName\nThe supported joint names for the hand pose.\nvar availableJointsGroupNames: [VNHumanHandPoseObservation.JointsGroupName]\nThe joint group names available in the observation.\nstruct VNHumanHandPoseObservation.JointsGroupName\nThe supported joint group names for the hand pose.\nfunc recognizedPoint(VNHumanHandPoseObservation.JointName) -> VNRecognizedPoint\nRetrieves the recognized point for a joint name.\nfunc recognizedPoints(VNHumanHandPoseObservation.JointsGroupName) -> [VNHumanHandPoseObservation.JointName : VNRecognizedPoint]\nRetrieves the recognized points associated with the joint group name."
  },
  {
    "title": "supportedJointNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanbodypose3drequest/4226653-supportedjointnames",
    "html": "See Also\nDetermining Supported Joints\nvar supportedJointsGroupNames: [VNHumanBodyPose3DObservation.JointsGroupName]\nReturns the joint names the request supports."
  },
  {
    "title": "VNHumanHandPoseObservation.JointName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointname",
    "html": "Topics\nThumb\nstatic let thumbTip: VNHumanHandPoseObservation.JointName\nThe tip of the thumb.\nstatic let thumbIP: VNHumanHandPoseObservation.JointName\nThe thumb’s interphalangeal (IP) joint.\nstatic let thumbMP: VNHumanHandPoseObservation.JointName\nThe thumb’s metacarpophalangeal (MP) joint.\nstatic let thumbCMC: VNHumanHandPoseObservation.JointName\nThe thumb’s carpometacarpal (CMC) joint.\nIndex\nstatic let indexTip: VNHumanHandPoseObservation.JointName\nThe tip of the index finger.\nstatic let indexDIP: VNHumanHandPoseObservation.JointName\nThe index finger’s distal interphalangeal (DIP) joint.\nstatic let indexPIP: VNHumanHandPoseObservation.JointName\nThe index finger’s proximal interphalangeal (PIP) joint.\nstatic let indexMCP: VNHumanHandPoseObservation.JointName\nThe index finger’s metacarpophalangeal (MCP) joint.\nMiddle\nstatic let middleTip: VNHumanHandPoseObservation.JointName\nThe tip of the middle finger.\nstatic let middleDIP: VNHumanHandPoseObservation.JointName\nThe middle finger’s distal interphalangeal (DIP) joint.\nstatic let middlePIP: VNHumanHandPoseObservation.JointName\nThe middle finger’s proximal interphalangeal (PIP) joint.\nstatic let middleMCP: VNHumanHandPoseObservation.JointName\nThe middle finger’s metacarpophalangeal (MCP) joint.\nRing\nstatic let ringTip: VNHumanHandPoseObservation.JointName\nThe tip of the ring finger.\nstatic let ringDIP: VNHumanHandPoseObservation.JointName\nThe ring finger’s distal interphalangeal (DIP) joint.\nstatic let ringPIP: VNHumanHandPoseObservation.JointName\nThe ring finger’s proximal interphalangeal (PIP) joint.\nstatic let ringMCP: VNHumanHandPoseObservation.JointName\nThe ring finger’s metacarpophalangeal (MCP) joint.\nLittle\nstatic let littleTip: VNHumanHandPoseObservation.JointName\nThe tip of the little finger.\nstatic let littleDIP: VNHumanHandPoseObservation.JointName\nThe little finger’s distal interphalangeal (DIP) joint.\nstatic let littlePIP: VNHumanHandPoseObservation.JointName\nThe little finger’s proximal interphalangeal (PIP) joint.\nstatic let littleMCP: VNHumanHandPoseObservation.JointName\nThe little finger’s metacarpophalangeal (MCP) joint.\nWrist\nstatic let wrist: VNHumanHandPoseObservation.JointName\nThe wrist.\nInitializers\ninit(rawValue: VNRecognizedPointKey)\nCreates a joint name with a recognized point key.\nRelationships\nConforms To\nHashable\nRawRepresentable\nSendable\nSee Also\nRetrieving Points\nvar availableJointNames: [VNHumanHandPoseObservation.JointName]\nThe names of the available joints in the observation.\nvar availableJointsGroupNames: [VNHumanHandPoseObservation.JointsGroupName]\nThe joint group names available in the observation.\nstruct VNHumanHandPoseObservation.JointsGroupName\nThe supported joint group names for the hand pose.\nfunc recognizedPoint(VNHumanHandPoseObservation.JointName) -> VNRecognizedPoint\nRetrieves the recognized point for a joint name.\nfunc recognizedPoints(VNHumanHandPoseObservation.JointsGroupName) -> [VNHumanHandPoseObservation.JointName : VNRecognizedPoint]\nRetrieves the recognized points associated with the joint group name."
  },
  {
    "title": "recognizedPoint(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/3675639-recognizedpoint",
    "html": "Parameters\njointName\n\nThe joint name of the point to retrieve.\n\nReturn Value\n\nThe point for the joint name.\n\nSee Also\nRetrieving Points\nvar availableJointNames: [VNHumanHandPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanHandPoseObservation.JointName\nThe supported joint names for the hand pose.\nvar availableJointsGroupNames: [VNHumanHandPoseObservation.JointsGroupName]\nThe joint group names available in the observation.\nstruct VNHumanHandPoseObservation.JointsGroupName\nThe supported joint group names for the hand pose.\nfunc recognizedPoints(VNHumanHandPoseObservation.JointsGroupName) -> [VNHumanHandPoseObservation.JointName : VNRecognizedPoint]\nRetrieves the recognized points associated with the joint group name."
  },
  {
    "title": "recognizedPoints(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/3675640-recognizedpoints",
    "html": "Parameters\njointsGroupName\n\nThe joint group name of the points to retrieve.\n\nReturn Value\n\nThe array of points associated with the joint group name.\n\nSee Also\nRetrieving Points\nvar availableJointNames: [VNHumanHandPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanHandPoseObservation.JointName\nThe supported joint names for the hand pose.\nvar availableJointsGroupNames: [VNHumanHandPoseObservation.JointsGroupName]\nThe joint group names available in the observation.\nstruct VNHumanHandPoseObservation.JointsGroupName\nThe supported joint group names for the hand pose.\nfunc recognizedPoint(VNHumanHandPoseObservation.JointName) -> VNRecognizedPoint\nRetrieves the recognized point for a joint name."
  },
  {
    "title": "availableJointsGroupNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/3675638-availablejointsgroupnames",
    "html": "See Also\nRetrieving Points\nvar availableJointNames: [VNHumanHandPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanHandPoseObservation.JointName\nThe supported joint names for the hand pose.\nstruct VNHumanHandPoseObservation.JointsGroupName\nThe supported joint group names for the hand pose.\nfunc recognizedPoint(VNHumanHandPoseObservation.JointName) -> VNRecognizedPoint\nRetrieves the recognized point for a joint name.\nfunc recognizedPoints(VNHumanHandPoseObservation.JointsGroupName) -> [VNHumanHandPoseObservation.JointName : VNRecognizedPoint]\nRetrieves the recognized points associated with the joint group name."
  },
  {
    "title": "VNHumanHandPoseObservation.JointsGroupName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/jointsgroupname",
    "html": "Topics\nGroup Names\nstatic let thumb: VNHumanHandPoseObservation.JointsGroupName\nThe thumb.\nstatic let indexFinger: VNHumanHandPoseObservation.JointsGroupName\nThe index finger.\nstatic let littleFinger: VNHumanHandPoseObservation.JointsGroupName\nThe little finger.\nstatic let middleFinger: VNHumanHandPoseObservation.JointsGroupName\nThe middle finger.\nstatic let ringFinger: VNHumanHandPoseObservation.JointsGroupName\nThe ring finger.\nstatic let all: VNHumanHandPoseObservation.JointsGroupName\nAll hand group names.\nInitializers\ninit(rawValue: VNRecognizedPointGroupKey)\nCreates a joint group name with a recognized point group key.\nRelationships\nConforms To\nHashable\nRawRepresentable\nSendable\nSee Also\nRetrieving Points\nvar availableJointNames: [VNHumanHandPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanHandPoseObservation.JointName\nThe supported joint names for the hand pose.\nvar availableJointsGroupNames: [VNHumanHandPoseObservation.JointsGroupName]\nThe joint group names available in the observation.\nfunc recognizedPoint(VNHumanHandPoseObservation.JointName) -> VNRecognizedPoint\nRetrieves the recognized point for a joint name.\nfunc recognizedPoints(VNHumanHandPoseObservation.JointsGroupName) -> [VNHumanHandPoseObservation.JointName : VNRecognizedPoint]\nRetrieves the recognized points associated with the joint group name."
  },
  {
    "title": "chirality | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation/3750971-chirality",
    "html": "See Also\nDetermining the Chirality\nenum VNChirality\nConstants that the define the chirality, or handedness, of a pose."
  },
  {
    "title": "availableKeys | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointsobservation/3656174-availablekeys",
    "html": "See Also\nInspecting the Observation\nvar availableGroupKeys: [VNRecognizedPointGroupKey]\nThe available point group keys in the observation.\nfunc recognizedPoint(forKey: VNRecognizedPointKey) -> VNRecognizedPoint\nRetrieves a recognized point for a key.\nfunc recognizedPoints(forGroupKey: VNRecognizedPointGroupKey) -> [VNRecognizedPointKey : VNRecognizedPoint]\nRetrieves the recognized points for a key."
  },
  {
    "title": "availableGroupKeys | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointsobservation/3618960-availablegroupkeys",
    "html": "See Also\nInspecting the Observation\nvar availableKeys: [VNRecognizedPointKey]\nThe available point keys in the observation.\nfunc recognizedPoint(forKey: VNRecognizedPointKey) -> VNRecognizedPoint\nRetrieves a recognized point for a key.\nfunc recognizedPoints(forGroupKey: VNRecognizedPointGroupKey) -> [VNRecognizedPointKey : VNRecognizedPoint]\nRetrieves the recognized points for a key."
  },
  {
    "title": "VNChirality | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnchirality",
    "html": "Topics\nChirality Values\ncase left\nIndicates a left-handed pose.\ncase right\nIndicates a right-handed pose.\ncase unknown\nIndicates that the pose chirality is unknown.\nRelationships\nConforms To\nSendable\nSee Also\nDetermining the Chirality\nvar chirality: VNChirality\nThe chirality, or handedness, of a pose."
  },
  {
    "title": "recognizedPoint(forKey:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointsobservation/3656175-recognizedpoint",
    "html": "Parameters\npointKey\n\nThe key of the point to retrieve.\n\nReturn Value\n\nThe recognized point associated with the key.\n\nSee Also\nInspecting the Observation\nvar availableKeys: [VNRecognizedPointKey]\nThe available point keys in the observation.\nvar availableGroupKeys: [VNRecognizedPointGroupKey]\nThe available point group keys in the observation.\nfunc recognizedPoints(forGroupKey: VNRecognizedPointGroupKey) -> [VNRecognizedPointKey : VNRecognizedPoint]\nRetrieves the recognized points for a key."
  },
  {
    "title": "recognizedPoints(forGroupKey:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointsobservation/3618962-recognizedpoints",
    "html": "Parameters\ngroupKey\n\nThe group key to retrieve recognized points for.\n\nReturn Value\n\nA dictionary of labeled points for the group.\n\nSee Also\nInspecting the Observation\nvar availableKeys: [VNRecognizedPointKey]\nThe available point keys in the observation.\nvar availableGroupKeys: [VNRecognizedPointGroupKey]\nThe available point group keys in the observation.\nfunc recognizedPoint(forKey: VNRecognizedPointKey) -> VNRecognizedPoint\nRetrieves a recognized point for a key."
  },
  {
    "title": "supportedJointNames(forRevision:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanhandposerequest/3675634-supportedjointnames",
    "html": "Parameters\nrevision\n\nThe hand pose request revision.\n\nReturn Value\n\nThe array of joint name objects for the specified revision.\n\nSee Also\nDetermining Supported Joints\nvar supportedJointNames: [VNHumanHandPoseObservation.JointName]\nRetrieves the supported joint names.\nvar supportedJointsGroupNames: [VNHumanHandPoseObservation.JointsGroupName]\nRetrieves the supported joint group names.\nclass func supportedJointsGroupNames(forRevision: Int) -> [VNHumanHandPoseObservation.JointsGroupName]\nRetrieves the supported joint group names for a revision.\nDeprecated"
  },
  {
    "title": "supportedJointsGroupNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanhandposerequest/4213147-supportedjointsgroupnames",
    "html": "See Also\nDetermining Supported Joints\nvar supportedJointNames: [VNHumanHandPoseObservation.JointName]\nRetrieves the supported joint names.\nclass func supportedJointNames(forRevision: Int) -> [VNHumanHandPoseObservation.JointName]\nRetrieves the supported joint names for a revision.\nDeprecated\nclass func supportedJointsGroupNames(forRevision: Int) -> [VNHumanHandPoseObservation.JointsGroupName]\nRetrieves the supported joint group names for a revision.\nDeprecated"
  },
  {
    "title": "recognizedPoint(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/3675603-recognizedpoint",
    "html": "Parameters\njointName\n\nThe joint name of the point to retrieve.\n\nReturn Value\n\nThe point for the joint name.\n\nSee Also\nAccessing Points\nvar availableJointNames: [VNHumanBodyPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanBodyPoseObservation.JointName\nThe supported joint names for the body pose.\nvar availableJointsGroupNames: [VNHumanBodyPoseObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNHumanBodyPoseObservation.JointsGroupName\nThe supported joint group names for the body pose.\nfunc recognizedPoints(VNHumanBodyPoseObservation.JointsGroupName) -> [VNHumanBodyPoseObservation.JointName : VNRecognizedPoint]\nRetrieves the recognized points associated with the joint group name."
  },
  {
    "title": "availableJointsGroupNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/3675602-availablejointsgroupnames",
    "html": "See Also\nAccessing Points\nvar availableJointNames: [VNHumanBodyPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanBodyPoseObservation.JointName\nThe supported joint names for the body pose.\nstruct VNHumanBodyPoseObservation.JointsGroupName\nThe supported joint group names for the body pose.\nfunc recognizedPoint(VNHumanBodyPoseObservation.JointName) -> VNRecognizedPoint\nRetrieves the recognized point for a joint name.\nfunc recognizedPoints(VNHumanBodyPoseObservation.JointsGroupName) -> [VNHumanBodyPoseObservation.JointName : VNRecognizedPoint]\nRetrieves the recognized points associated with the joint group name."
  },
  {
    "title": "detectsDarkOnLight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectcontoursrequest/3675596-detectsdarkonlight",
    "html": "Discussion\n\nThe default value is true.\n\nSee Also\nConfiguring the Request\nvar contrastAdjustment: Float\nThe amount by which to adjust the image contrast.\nvar contrastPivot: NSNumber?\nThe pixel value to use as a pivot for the contrast.\nvar maximumImageDimension: Int\nThe maximum image dimension to use for contour detection.\nvar detectDarkOnLight: Bool\nA Boolean value that indicates whether the request detects a dark object on a light background.\nDeprecated"
  },
  {
    "title": "VNContoursObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncontoursobservation",
    "html": "Topics\nInspecting the Observation\nvar contourCount: Int\nThe total number of detected contours.\nvar normalizedPath: CGPath\nThe detected contours as a path object in normalized coordinates.\nvar topLevelContours: [VNContour]\nAn array of contours that don’t have another contour enclosing them.\nvar topLevelContourCount: Int\nThe total number of detected top-level contours.\nfunc contour(at: Int) -> VNContour\nRetrieves the contour object at the specified index, irrespective of hierarchy.\nfunc contour(at: IndexPath) -> VNContour\nRetrieves the contour object at the specified index path.\nclass VNContour\nA class that represents a detected contour in an image.\nRelationships\nInherits From\nVNObservation\nSee Also\nAccessing the Results\nvar results: [VNContoursObservation]?\nThe results of the request to detect contours."
  },
  {
    "title": "init(frameAnalysisSpacing:trajectoryLength:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecttrajectoriesrequest/3600612-init",
    "html": "Parameters\nframeAnalysisSpacing\n\nA CMTime value that indicates the duration between analysis operations. Increase this value to reduce the number of frames analyzed on slower devices. Set this argument to zero to analyze all frames.\n\ntrajectoryLength\n\nThe number of points required to analyze to determine that a shape follows a parabolic path. This argument value must be at least 5.\n\ncompletionHandler\n\nA closure that’s invoked after the request completes its processing. The system invokes the completion handler on the same dispatch queue that the request uses to perform its processing."
  },
  {
    "title": "detectDarkOnLight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectcontoursrequest/3548237-detectdarkonlight",
    "html": "Deprecated\n\nUse detectsDarkOnLight instead.\n\nDiscussion\n\nThe default value is true.\n\nSee Also\nConfiguring the Request\nvar contrastAdjustment: Float\nThe amount by which to adjust the image contrast.\nvar contrastPivot: NSNumber?\nThe pixel value to use as a pivot for the contrast.\nvar detectsDarkOnLight: Bool\nA Boolean value that indicates whether the request detects a dark object on a light background to aid in detection.\nvar maximumImageDimension: Int\nThe maximum image dimension to use for contour detection."
  },
  {
    "title": "maximumImageDimension | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectcontoursrequest/3548238-maximumimagedimension",
    "html": "Discussion\n\nContour detection is computationally intensive. To improve performance, Vision scales the input image down, while maintaining its aspect ratio, such that its maximum dimension is the value of this property. Vision never scales the image up, so specifying the maximum value ensures that the image processes in its original size and not as a downscaled version.\n\nThis property supports values from 64 to NSUIntegerMax. The default value is 512.\n\nSee Also\nConfiguring the Request\nvar contrastAdjustment: Float\nThe amount by which to adjust the image contrast.\nvar contrastPivot: NSNumber?\nThe pixel value to use as a pivot for the contrast.\nvar detectsDarkOnLight: Bool\nA Boolean value that indicates whether the request detects a dark object on a light background to aid in detection.\nvar detectDarkOnLight: Bool\nA Boolean value that indicates whether the request detects a dark object on a light background.\nDeprecated"
  },
  {
    "title": "perform(_:onImageURL:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsequencerequesthandler/2880299-perform",
    "html": "Parameters\nrequests\n\nAn array of VNRequest requests to perform.\n\nimageURL\n\nA URL pointing to the image on which to perform the request.\n\nReturn Value\n\nReturns true if all requests were scheduled and performed. Check the error parameter if the return value is false.\n\nSee Also\nPerforming a Sequence Request\nfunc perform([VNRequest], on: CGImage)\nSchedules Vision requests to be performed on a Core Graphics image.\nfunc perform([VNRequest], on: CGImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Graphics image with known orientation.\nfunc perform([VNRequest], on: CIImage)\nSchedules one or more Vision requests to be performed on CIImage data.\nfunc perform([VNRequest], on: CIImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on CIImage data with known orientation.\nfunc perform([VNRequest], on: CVPixelBuffer)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer.\nfunc perform([VNRequest], on: CVPixelBuffer, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer with known orientation.\nfunc perform([VNRequest], on: CMSampleBuffer)\nPerforms one or more requests on an image contained within a sample buffer.\nfunc perform([VNRequest], on: CMSampleBuffer, orientation: CGImagePropertyOrientation)\nPerforms one or more requests on an image of a specified orientation contained within a sample buffer.\nfunc perform([VNRequest], onImageData: Data)\nSchedules one or more Vision requests to be performed on raw image data.\nfunc perform([VNRequest], onImageData: Data, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on raw data containing an image with known orientation.\nfunc perform([VNRequest], onImageURL: URL, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on an image with known orientation, at a specific URL."
  },
  {
    "title": "inputObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackingrequest/2887350-inputobservation",
    "html": "Discussion\n\nProviding an observation not returned from a tracker, such as a user-defined observation, begins a new tracker for the sequence. Providing an observation that was returned from a tracker continues the use of that tracker, to track the region to the next frame.\n\nIn general, unless specified in the request's documentation or header file, you must define the rectangle in normalized coordinates, with the origin at the lower-left corner.\n\nSee Also\nConfiguring a Tracking Request\nenum VNRequestTrackingLevel\nAn enumeration of tracking priorities.\nvar trackingLevel: VNRequestTrackingLevel\nA value for specifying whether to prioritize speed or location accuracy.\nvar isLastFrame: Bool\nA Boolean that indicates the last frame in a tracking sequence."
  },
  {
    "title": "perform(_:onImageData:orientation:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsequencerequesthandler/2881930-perform",
    "html": "Parameters\nrequests\n\nAn array of VNRequest requests to perform.\n\nimageData\n\nThe input image data on which to perform the request.\n\norientation\n\nThe orientation of the input image.\n\nReturn Value\n\nReturns true if all requests were scheduled and performed. Check the error parameter if the return value is false.\n\nSee Also\nPerforming a Sequence Request\nfunc perform([VNRequest], on: CGImage)\nSchedules Vision requests to be performed on a Core Graphics image.\nfunc perform([VNRequest], on: CGImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Graphics image with known orientation.\nfunc perform([VNRequest], on: CIImage)\nSchedules one or more Vision requests to be performed on CIImage data.\nfunc perform([VNRequest], on: CIImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on CIImage data with known orientation.\nfunc perform([VNRequest], on: CVPixelBuffer)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer.\nfunc perform([VNRequest], on: CVPixelBuffer, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer with known orientation.\nfunc perform([VNRequest], on: CMSampleBuffer)\nPerforms one or more requests on an image contained within a sample buffer.\nfunc perform([VNRequest], on: CMSampleBuffer, orientation: CGImagePropertyOrientation)\nPerforms one or more requests on an image of a specified orientation contained within a sample buffer.\nfunc perform([VNRequest], onImageData: Data)\nSchedules one or more Vision requests to be performed on raw image data.\nfunc perform([VNRequest], onImageURL: URL)\nSchedules one or more Vision requests to be performed on an image.\nfunc perform([VNRequest], onImageURL: URL, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on an image with known orientation, at a specific URL."
  },
  {
    "title": "perform(_:on:orientation:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsequencerequesthandler/2880308-perform",
    "html": "Parameters\nrequests\n\nAn array of VNRequest requests to perform.\n\npixelBuffer\n\nThe input CVPixelBuffer on which to perform the request.\n\norientation\n\nThe orientation of the input image.\n\nReturn Value\n\nReturns true if all requests were scheduled and performed. Check the error parameter if the return value is false.\n\nSee Also\nPerforming a Sequence Request\nfunc perform([VNRequest], on: CGImage)\nSchedules Vision requests to be performed on a Core Graphics image.\nfunc perform([VNRequest], on: CGImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Graphics image with known orientation.\nfunc perform([VNRequest], on: CIImage)\nSchedules one or more Vision requests to be performed on CIImage data.\nfunc perform([VNRequest], on: CIImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on CIImage data with known orientation.\nfunc perform([VNRequest], on: CVPixelBuffer)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer.\nfunc perform([VNRequest], on: CMSampleBuffer)\nPerforms one or more requests on an image contained within a sample buffer.\nfunc perform([VNRequest], on: CMSampleBuffer, orientation: CGImagePropertyOrientation)\nPerforms one or more requests on an image of a specified orientation contained within a sample buffer.\nfunc perform([VNRequest], onImageData: Data)\nSchedules one or more Vision requests to be performed on raw image data.\nfunc perform([VNRequest], onImageData: Data, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on raw data containing an image with known orientation.\nfunc perform([VNRequest], onImageURL: URL)\nSchedules one or more Vision requests to be performed on an image.\nfunc perform([VNRequest], onImageURL: URL, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on an image with known orientation, at a specific URL."
  },
  {
    "title": "perform(_:on:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsequencerequesthandler/2880305-perform",
    "html": "Parameters\nrequests\n\nAn array of VNRequest requests to perform.\n\nimage\n\nThe input CIImage on which to perform the request.\n\nReturn Value\n\nReturns true if all requests were scheduled and performed. Check the error parameter if the return value is false.\n\nSee Also\nPerforming a Sequence Request\nfunc perform([VNRequest], on: CGImage)\nSchedules Vision requests to be performed on a Core Graphics image.\nfunc perform([VNRequest], on: CGImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Graphics image with known orientation.\nfunc perform([VNRequest], on: CIImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on CIImage data with known orientation.\nfunc perform([VNRequest], on: CVPixelBuffer)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer.\nfunc perform([VNRequest], on: CVPixelBuffer, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer with known orientation.\nfunc perform([VNRequest], on: CMSampleBuffer)\nPerforms one or more requests on an image contained within a sample buffer.\nfunc perform([VNRequest], on: CMSampleBuffer, orientation: CGImagePropertyOrientation)\nPerforms one or more requests on an image of a specified orientation contained within a sample buffer.\nfunc perform([VNRequest], onImageData: Data)\nSchedules one or more Vision requests to be performed on raw image data.\nfunc perform([VNRequest], onImageData: Data, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on raw data containing an image with known orientation.\nfunc perform([VNRequest], onImageURL: URL)\nSchedules one or more Vision requests to be performed on an image.\nfunc perform([VNRequest], onImageURL: URL, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on an image with known orientation, at a specific URL."
  },
  {
    "title": "VNHumanBodyPoseObservation.JointName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointname",
    "html": "Topics\nHead\nstatic let leftEar: VNHumanBodyPoseObservation.JointName\nThe left ear.\nstatic let leftEye: VNHumanBodyPoseObservation.JointName\nThe left eye.\nstatic let rightEar: VNHumanBodyPoseObservation.JointName\nThe right ear.\nstatic let rightEye: VNHumanBodyPoseObservation.JointName\nThe right eye.\nstatic let neck: VNHumanBodyPoseObservation.JointName\nThe neck.\nstatic let nose: VNHumanBodyPoseObservation.JointName\nThe nose.\nArms\nstatic let leftShoulder: VNHumanBodyPoseObservation.JointName\nThe left shoulder.\nstatic let leftElbow: VNHumanBodyPoseObservation.JointName\nThe left elbow.\nstatic let leftWrist: VNHumanBodyPoseObservation.JointName\nThe left wrist.\nstatic let rightShoulder: VNHumanBodyPoseObservation.JointName\nThe right shoulder.\nstatic let rightElbow: VNHumanBodyPoseObservation.JointName\nThe right elbow.\nstatic let rightWrist: VNHumanBodyPoseObservation.JointName\nThe right wrist.\nWaist\nstatic let root: VNHumanBodyPoseObservation.JointName\nThe root (waist).\nLegs\nstatic let leftHip: VNHumanBodyPoseObservation.JointName\nThe left hip.\nstatic let leftKnee: VNHumanBodyPoseObservation.JointName\nThe left knee.\nstatic let leftAnkle: VNHumanBodyPoseObservation.JointName\nThe left ankle.\nstatic let rightHip: VNHumanBodyPoseObservation.JointName\nThe right hip.\nstatic let rightKnee: VNHumanBodyPoseObservation.JointName\nThe right knee.\nstatic let rightAnkle: VNHumanBodyPoseObservation.JointName\nThe right ankle.\nInitializers\ninit(rawValue: VNRecognizedPointKey)\nCreates a joint name with a recognized point key.\nRelationships\nConforms To\nHashable\nRawRepresentable\nSendable\nSee Also\nAccessing Points\nvar availableJointNames: [VNHumanBodyPoseObservation.JointName]\nThe names of the available joints in the observation.\nvar availableJointsGroupNames: [VNHumanBodyPoseObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNHumanBodyPoseObservation.JointsGroupName\nThe supported joint group names for the body pose.\nfunc recognizedPoint(VNHumanBodyPoseObservation.JointName) -> VNRecognizedPoint\nRetrieves the recognized point for a joint name.\nfunc recognizedPoints(VNHumanBodyPoseObservation.JointsGroupName) -> [VNHumanBodyPoseObservation.JointName : VNRecognizedPoint]\nRetrieves the recognized points associated with the joint group name."
  },
  {
    "title": "keypointsMultiArray() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointsobservation/3618961-keypointsmultiarray",
    "html": "Return Value\n\nThe key points converted to an MLMultiArray."
  },
  {
    "title": "availableJointNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/3675601-availablejointnames",
    "html": "See Also\nAccessing Points\nstruct VNHumanBodyPoseObservation.JointName\nThe supported joint names for the body pose.\nvar availableJointsGroupNames: [VNHumanBodyPoseObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNHumanBodyPoseObservation.JointsGroupName\nThe supported joint group names for the body pose.\nfunc recognizedPoint(VNHumanBodyPoseObservation.JointName) -> VNRecognizedPoint\nRetrieves the recognized point for a joint name.\nfunc recognizedPoints(VNHumanBodyPoseObservation.JointsGroupName) -> [VNHumanBodyPoseObservation.JointName : VNRecognizedPoint]\nRetrieves the recognized points associated with the joint group name."
  },
  {
    "title": "supportedJointsGroupNames(forRevision:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanbodyposerequest/3675599-supportedjointsgroupnames",
    "html": "Parameters\nrevision\n\nThe body pose request revision.\n\nReturn Value\n\nThe array of joint group name objects for the revision.\n\nSee Also\nDetermining Supported Joints\nvar supportedJointNames: [VNHumanBodyPoseObservation.JointName]\nRetrieves the supported joint names.\nclass func supportedJointNames(forRevision: Int) -> [VNHumanBodyPoseObservation.JointName]\nRetrieves the supported joint names for a revision.\nDeprecated\nvar supportedJointsGroupNames: [VNHumanBodyPoseObservation.JointsGroupName]\nRetrieves the supported joint group names."
  },
  {
    "title": "supportedJointsGroupNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanbodyposerequest/4213145-supportedjointsgroupnames",
    "html": "See Also\nDetermining Supported Joints\nvar supportedJointNames: [VNHumanBodyPoseObservation.JointName]\nRetrieves the supported joint names.\nclass func supportedJointNames(forRevision: Int) -> [VNHumanBodyPoseObservation.JointName]\nRetrieves the supported joint names for a revision.\nDeprecated\nclass func supportedJointsGroupNames(forRevision: Int) -> [VNHumanBodyPoseObservation.JointsGroupName]\nRetrieves the supported joint group names for a revision.\nDeprecated"
  },
  {
    "title": "supportedJointNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanbodyposerequest/4213144-supportedjointnames",
    "html": "See Also\nDetermining Supported Joints\nclass func supportedJointNames(forRevision: Int) -> [VNHumanBodyPoseObservation.JointName]\nRetrieves the supported joint names for a revision.\nDeprecated\nvar supportedJointsGroupNames: [VNHumanBodyPoseObservation.JointsGroupName]\nRetrieves the supported joint group names.\nclass func supportedJointsGroupNames(forRevision: Int) -> [VNHumanBodyPoseObservation.JointsGroupName]\nRetrieves the supported joint group names for a revision.\nDeprecated"
  },
  {
    "title": "supportedJointNames(forRevision:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanbodyposerequest/3675598-supportedjointnames",
    "html": "Parameters\nrevision\n\nThe body pose request revision.\n\nReturn Value\n\nThe array of joint name objects for the specified revision.\n\nSee Also\nDetermining Supported Joints\nvar supportedJointNames: [VNHumanBodyPoseObservation.JointName]\nRetrieves the supported joint names.\nvar supportedJointsGroupNames: [VNHumanBodyPoseObservation.JointsGroupName]\nRetrieves the supported joint group names.\nclass func supportedJointsGroupNames(forRevision: Int) -> [VNHumanBodyPoseObservation.JointsGroupName]\nRetrieves the supported joint group names for a revision.\nDeprecated"
  },
  {
    "title": "VNHumanBodyPoseObservation.JointsGroupName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/jointsgroupname",
    "html": "Topics\nHead\nstatic let face: VNHumanBodyPoseObservation.JointsGroupName\nThe face.\nBody\nstatic let torso: VNHumanBodyPoseObservation.JointsGroupName\nThe torso.\nArms\nstatic let leftArm: VNHumanBodyPoseObservation.JointsGroupName\nThe left arm.\nstatic let rightArm: VNHumanBodyPoseObservation.JointsGroupName\nThe right arm.\nLegs\nstatic let leftLeg: VNHumanBodyPoseObservation.JointsGroupName\nThe left leg.\nstatic let rightLeg: VNHumanBodyPoseObservation.JointsGroupName\nThe right leg.\nAll\nstatic let all: VNHumanBodyPoseObservation.JointsGroupName\nAll body point groups.\nInitializers\ninit(rawValue: VNRecognizedPointGroupKey)\nCreates a joint group name with a recognized point group key.\nRelationships\nConforms To\nHashable\nRawRepresentable\nSendable\nSee Also\nAccessing Points\nvar availableJointNames: [VNHumanBodyPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanBodyPoseObservation.JointName\nThe supported joint names for the body pose.\nvar availableJointsGroupNames: [VNHumanBodyPoseObservation.JointsGroupName]\nThe available joint group names in the observation.\nfunc recognizedPoint(VNHumanBodyPoseObservation.JointName) -> VNRecognizedPoint\nRetrieves the recognized point for a joint name.\nfunc recognizedPoints(VNHumanBodyPoseObservation.JointsGroupName) -> [VNHumanBodyPoseObservation.JointName : VNRecognizedPoint]\nRetrieves the recognized points associated with the joint group name."
  },
  {
    "title": "recognizedPoints(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation/3675604-recognizedpoints",
    "html": "Parameters\njointsGroupName\n\nThe joint group name of the points to retrieve.\n\nReturn Value\n\nThe array of points associated with the joint group name.\n\nSee Also\nAccessing Points\nvar availableJointNames: [VNHumanBodyPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanBodyPoseObservation.JointName\nThe supported joint names for the body pose.\nvar availableJointsGroupNames: [VNHumanBodyPoseObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNHumanBodyPoseObservation.JointsGroupName\nThe supported joint group names for the body pose.\nfunc recognizedPoint(VNHumanBodyPoseObservation.JointName) -> VNRecognizedPoint\nRetrieves the recognized point for a joint name."
  },
  {
    "title": "VNDetectHumanRectanglesRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanrectanglesrequestrevision1",
    "html": "See Also\nIdentifying Request Revisions\nlet VNDetectHumanRectanglesRequestRevision2: Int\nA constant for specifying revision 2 of the human rectangles detection request."
  },
  {
    "title": "VNDetectHumanRectanglesRequestRevision2 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanrectanglesrequestrevision2",
    "html": "See Also\nIdentifying Request Revisions\nlet VNDetectHumanRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the human rectangles detection request."
  },
  {
    "title": "upperBodyOnly | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanrectanglesrequest/3750977-upperbodyonly",
    "html": "Discussion\n\nThe default value of true indicates that the request requires detecting a person’s upper body only to find the bound box around it."
  },
  {
    "title": "VNDetectFaceRectanglesRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacerectanglesrequestrevision1",
    "html": "See Also\nIdentifying Request Revisions\nlet VNDetectFaceRectanglesRequestRevision3: Int\nA constant for specifying revision 3 of the face rectangles detection request.\nlet VNDetectFaceRectanglesRequestRevision2: Int\nA constant for specifying revision 2 of the face rectangles detection request."
  },
  {
    "title": "VNDetectFaceRectanglesRequestRevision3 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacerectanglesrequestrevision3",
    "html": "See Also\nIdentifying Request Revisions\nlet VNDetectFaceRectanglesRequestRevision2: Int\nA constant for specifying revision 2 of the face rectangles detection request.\nlet VNDetectFaceRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the face rectangles detection request.\nDeprecated"
  },
  {
    "title": "VNDetectFaceRectanglesRequestRevision2 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacerectanglesrequestrevision2",
    "html": "See Also\nIdentifying Request Revisions\nlet VNDetectFaceRectanglesRequestRevision3: Int\nA constant for specifying revision 3 of the face rectangles detection request.\nlet VNDetectFaceRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the face rectangles detection request.\nDeprecated"
  },
  {
    "title": "regionOfInterest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagebasedrequest/2877482-regionofinterest",
    "html": "Discussion\n\nThe rectangle is normalized to the dimensions of the processed image. Its origin is specified relative to the image's lower-left corner.\n\nThe default value is { { 0, 0 }, { 1, 1 } }.\n\nImportant\n\nThe request will fail to perform if you set this property to a rectangle outside the normalized coordinate space."
  },
  {
    "title": "VNFaceObservationAccepting | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfaceobservationaccepting",
    "html": "Overview\n\nThis protocol allows you to provide an input collection of VNFaceObservation objects as part of a request. Request objects adopt this protocol to request additional information about detected faces, such as facial landmarks.\n\nTopics\nProviding Face Observations\nvar inputFaceObservations: [VNFaceObservation]?\nAn array of VNFaceObservation objects to process as part of the request.\n\nRequired\n\nRelationships\nInherits From\nNSObjectProtocol\nConforming Types\nVNDetectFaceCaptureQualityRequest\nVNDetectFaceLandmarksRequest"
  },
  {
    "title": "faceCaptureQuality | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfaceobservation/3152627-facecapturequality",
    "html": "Discussion\n\nThe capture quality of the face allows you to compare the quality of the face in terms of its capture attributes: lighting, blur, and prime positioning. Use this value to compare the capture quality of a face against other captures of the same face in a specified set.\n\nThe value of this property value ranges from 0.0 to 1.0. Faces with quality closer to 1.0 are better lit, sharper, and more centrally positioned than faces with quality closer to 0.0."
  },
  {
    "title": "VNDetectFaceCaptureQualityRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacecapturequalityrequestrevision1",
    "html": "See Also\nIdentifying Request Revisions\nlet VNDetectFaceCaptureQualityRequestRevision2: Int\nRevision 2 of the request algorithm."
  },
  {
    "title": "VNDetectFaceCaptureQualityRequestRevision2 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacecapturequalityrequestrevision2",
    "html": "See Also\nIdentifying Request Revisions\nlet VNDetectFaceCaptureQualityRequestRevision1: Int\nA constant for specifying revision 1 of the face capture detection request."
  },
  {
    "title": "results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectrectanglesrequest/3750979-results",
    "html": "See Also\nAccessing the Results\nclass VNRectangleObservation\nAn object that represents the four vertices of a detected rectangle."
  },
  {
    "title": "results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacecapturequalityrequest/3750966-results",
    "html": "See Also\nAccessing the Results\nclass VNFaceObservation\nFace or facial-feature information that an image analysis request detects."
  },
  {
    "title": "minimumConfidence | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectrectanglesrequest/2875375-minimumconfidence",
    "html": "Discussion\n\nVision won’t return rectangles with a confidence score lower than the specified minimum.\n\nThe confidence score ranges from 0.0 to 1.0, inclusive, where 0.0 represents no confidence, and 1.0 represents full confidence.\n\nSee Also\nConfiguring Detection\nvar minimumAspectRatio: VNAspectRatio\nA float specifying the minimum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\nvar maximumAspectRatio: VNAspectRatio\nA float specifying the maximum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\ntypealias VNAspectRatio\nA type alias for expressing rectangle aspect ratios in Vision.\nvar quadratureTolerance: VNDegrees\nA float specifying the number of degrees a rectangle corner angle can deviate from 90°.\ntypealias VNDegrees\nA typealias for expressing tolerance angles in Vision.\nvar minimumSize: Float\nThe minimum size of a rectangle to detect, as a proportion of the smallest dimension.\ntypealias VNConfidence\nA type alias for the confidence value of an observation.\nvar maximumObservations: Int\nAn integer specifying the maximum number of rectangles Vision returns."
  },
  {
    "title": "VNDegrees | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndegrees",
    "html": "See Also\nConfiguring Detection\nvar minimumAspectRatio: VNAspectRatio\nA float specifying the minimum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\nvar maximumAspectRatio: VNAspectRatio\nA float specifying the maximum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\ntypealias VNAspectRatio\nA type alias for expressing rectangle aspect ratios in Vision.\nvar quadratureTolerance: VNDegrees\nA float specifying the number of degrees a rectangle corner angle can deviate from 90°.\nvar minimumSize: Float\nThe minimum size of a rectangle to detect, as a proportion of the smallest dimension.\nvar minimumConfidence: VNConfidence\nA value specifying the minimum acceptable confidence level.\ntypealias VNConfidence\nA type alias for the confidence value of an observation.\nvar maximumObservations: Int\nAn integer specifying the maximum number of rectangles Vision returns."
  },
  {
    "title": "minimumAspectRatio | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectrectanglesrequest/2875378-minimumaspectratio",
    "html": "Discussion\n\nThe value should range from 0.0 to 1.0, inclusive. The default value is 0.5.\n\nSee Also\nConfiguring Detection\nvar maximumAspectRatio: VNAspectRatio\nA float specifying the maximum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\ntypealias VNAspectRatio\nA type alias for expressing rectangle aspect ratios in Vision.\nvar quadratureTolerance: VNDegrees\nA float specifying the number of degrees a rectangle corner angle can deviate from 90°.\ntypealias VNDegrees\nA typealias for expressing tolerance angles in Vision.\nvar minimumSize: Float\nThe minimum size of a rectangle to detect, as a proportion of the smallest dimension.\nvar minimumConfidence: VNConfidence\nA value specifying the minimum acceptable confidence level.\ntypealias VNConfidence\nA type alias for the confidence value of an observation.\nvar maximumObservations: Int\nAn integer specifying the maximum number of rectangles Vision returns."
  },
  {
    "title": "VNBarcodeCompositeType.gs1TypeA | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodecompositetype/gs1typea",
    "html": "See Also\nComposite Types\ncase gs1TypeB\nA type that represents trade items by piece.\ncase gs1TypeC\nA type that represents trade items in varying quantity.\ncase linked\nA type that represents a linked composite type.\ncase none\nA type that represents no composite type."
  },
  {
    "title": "centerShoulder | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4173289-centershoulder",
    "html": "See Also\nGetting the Arm Joint Names\nstatic let leftShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left shoulder.\nstatic let rightShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right shoulder.\nstatic let leftElbow: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left elbow.\nstatic let rightElbow: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right elbow.\nstatic let leftWrist: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left wrist.\nstatic let rightWrist: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right wrist."
  },
  {
    "title": "bodyLandmarkRegionKeyLeftLeg | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointgroupkey/3548258-bodylandmarkregionkeyleftleg",
    "html": "See Also\nBody Regions\nstatic let bodyLandmarkRegionKeyFace: VNRecognizedPointGroupKey\nA group key identifying the face, which includes the eyes, ears, and nose.\nDeprecated\nstatic let bodyLandmarkRegionKeyTorso: VNRecognizedPointGroupKey\nA group key identifying the torso, which includes the neck, shoulders, hips, and root.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right arm.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left arm.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right leg.\nDeprecated"
  },
  {
    "title": "bodyLandmarkRegionKeyRightLeg | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointgroupkey/3548260-bodylandmarkregionkeyrightleg",
    "html": "See Also\nBody Regions\nstatic let bodyLandmarkRegionKeyFace: VNRecognizedPointGroupKey\nA group key identifying the face, which includes the eyes, ears, and nose.\nDeprecated\nstatic let bodyLandmarkRegionKeyTorso: VNRecognizedPointGroupKey\nA group key identifying the torso, which includes the neck, shoulders, hips, and root.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right arm.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left arm.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left leg.\nDeprecated"
  },
  {
    "title": "all | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointgroupkey/3548371-all",
    "html": "See Also\nAll Regions\nstatic let point3DGroupKeyAll: VNRecognizedPointGroupKey\nA group key identifying all three-dimensional landmarks."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointgroupkey/3626465-init",
    "html": "Parameters\nrawValue\n\nA string value that represents a body or hand region."
  },
  {
    "title": "point3DGroupKeyAll | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointgroupkey/4173233-point3dgroupkeyall",
    "html": "See Also\nAll Regions\nstatic let all: VNRecognizedPointGroupKey\nA group key identifying all landmarks."
  },
  {
    "title": "supportedJointsGroupNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanbodypose3drequest/4226654-supportedjointsgroupnames",
    "html": "See Also\nDetermining Supported Joints\nvar supportedJointNames: [VNHumanBodyPose3DObservation.JointName]\nReturns the joint group names the request supports."
  },
  {
    "title": "pitch | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfaceobservation/3750998-pitch",
    "html": "Discussion\n\nThis value indicates the rotational angle of the face around the x-axis.\n\nIf the request doesn’t calculate the angle, the value is nil.\n\nSee Also\nDetermining Facial Orientation\nvar roll: NSNumber?\nThe roll angle of a face in radians.\nvar yaw: NSNumber?\nThe yaw angle of a face in radians."
  },
  {
    "title": "init(cmSampleBuffer:orientation:options:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/3548374-init",
    "html": "Parameters\nsampleBuffer\n\nThe sample buffer that contains the image to analyze. If the sample buffer doesn’t contain an image buffer with image data, the system raises an error.\n\norientation\n\nThe EXIF orientation of the image.\n\noptions\n\nA dictionary that specifies auxiliary information about the image.\n\nDiscussion\n\nSample buffers may contain metadata, like the camera intrinsics. Vision algorithms that support this metadata use it in their analysis, unless overwritten by the options you specify.\n\nImportant\n\nUse a physical device to perform your testing. Performing requests in Simulator may produce inaccurate results due to the inability of Core Image to render certain pixel formats in this environment.\n\nSee Also\nCreating a Request Handler\ninit(cgImage: CGImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on Core Graphics images.\ninit(cgImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\ninit(ciImage: CIImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data.\ninit(ciImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\ninit(cmSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\ninit(data: Data, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image contained in an NSData object.\ninit(data: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\ninit(url: URL, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image at the specified URL.\ninit(url: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "supportedRevisions | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/2967111-supportedrevisions",
    "html": "Discussion\n\nThis method allows clients to inspect at runtime what capabilities are available for each class of VNRequest in the Vision framework.\n\nSee Also\nDetermining the Revision\nprotocol VNRequestRevisionProviding\nA protocol for specifying the revision number of Vision algorithms.\nclass var currentRevision: Int\nThe current revison supported by the request.\nclass var defaultRevision: Int\nThe revision of the latest request for the particular SDK linked with the client application."
  },
  {
    "title": "completionHandler | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/2867266-completionhandler",
    "html": "Discussion\n\nVision executes the completion handler on the same queue that it executes the request; however, this queue differs from the one where you called perform(_:).\n\nSee Also\nConfiguring a Request\nvar preferBackgroundProcessing: Bool\nA hint to minimize the resource burden of the request.\nvar results: [VNObservation]?\nThe collection of VNObservation results generated by request processing.\nvar revision: Int\nThe specific algorithm or implementation revision that’s used to perform the request.\nvar usesCPUOnly: Bool\nA Boolean signifying that the Vision request should execute exclusively on the CPU.\nDeprecated"
  },
  {
    "title": "init(requestRevision:boundingBox:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectedobjectobservation/2967105-init",
    "html": "Parameters\nrequestRevision\n\nThe revision of the request to use.\n\nboundingBox\n\nThe observation’s bounding box, in coordinates normalized to the dimensions of the processed image, with its origin at the image’s lower-left corner.\n\nSee Also\nCreating an Observation\ninit(boundingBox: CGRect)\nCreates an observation with a bounding box."
  },
  {
    "title": "VNTrackObjectRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackobjectrequestrevision1",
    "html": "See Also\nIdentifying Request Revisions\nlet VNTrackObjectRequestRevision2: Int\nA constant for specifying revision 2 of the object tracking request."
  },
  {
    "title": "knownClassifications(forRevision:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnclassifyimagerequest/3152615-knownclassifications",
    "html": "Deprecated\n\nUse supportedIdentifiers() instead.\n\nParameters\nrequestRevision\n\nThe revision of the request for which to report classifications.\n\nerror\n\nThe address of the error variable to populate if the call fails.\n\nReturn Value\n\nAn array of classifications for the revision, or nil if an error occurs.\n\nSee Also\nAccessing Results\nfunc supportedIdentifiers() -> [String]\nReturns the classification identifiers that the request supports in its current configuration.\nvar results: [VNClassificationObservation]?\nThe results of the image classification request.\nclass VNClassificationObservation\nAn object that represents classification information that an image analysis request produces."
  },
  {
    "title": "init(detectedObjectObservation:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackobjectrequest/2887295-init",
    "html": "Parameters\nobservation\n\nA detected object observation with bounding box information.\n\ncompletionHandler\n\nThe callback to invoke after performing the request.\n\nSee Also\nInitializing an Object Tracking Request\ninit(detectedObjectObservation: VNDetectedObjectObservation)\nCreates a new object tracking request with a detected object observation."
  },
  {
    "title": "supportedNumber(ofTrackersAndReturnError:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackingrequest/4127028-supportednumber",
    "html": "Parameters\nerror\n\nAn error that contains the reason why a failure occurs.\n\nReturn Value\n\nThe maximum number of trackers given a combination; or 0 if such combination doesn’t exist."
  },
  {
    "title": "isLastFrame | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackingrequest/2887356-islastframe",
    "html": "Discussion\n\nIf set to true, the current tracker will be released to the pool of available trackers when the current frame finishes processing.\n\nSee Also\nConfiguring a Tracking Request\nenum VNRequestTrackingLevel\nAn enumeration of tracking priorities.\nvar inputObservation: VNDetectedObjectObservation\nThe observation object defining a region to track.\nvar trackingLevel: VNRequestTrackingLevel\nA value for specifying whether to prioritize speed or location accuracy."
  },
  {
    "title": "trackingLevel | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackingrequest/2887353-trackinglevel",
    "html": "See Also\nConfiguring a Tracking Request\nenum VNRequestTrackingLevel\nAn enumeration of tracking priorities.\nvar inputObservation: VNDetectedObjectObservation\nThe observation object defining a region to track.\nvar isLastFrame: Bool\nA Boolean that indicates the last frame in a tracking sequence."
  },
  {
    "title": "boundingBox | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectedobjectobservation/2867227-boundingbox",
    "html": "Discussion\n\nThe system normalizes the coordinates to the dimensions of the processed image, with the origin at the lower-left corner of the image."
  },
  {
    "title": "VNRequestTrackingLevel | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequesttrackinglevel",
    "html": "Topics\nEnumeration Cases\ncase accurate\nTracking level that favors location accuracy over speed.\ncase fast\nTracking level that favors speed over location accuracy.\nRelationships\nConforms To\nSendable\nSee Also\nConfiguring a Tracking Request\nvar inputObservation: VNDetectedObjectObservation\nThe observation object defining a region to track.\nvar trackingLevel: VNRequestTrackingLevel\nA value for specifying whether to prioritize speed or location accuracy.\nvar isLastFrame: Bool\nA Boolean that indicates the last frame in a tracking sequence."
  },
  {
    "title": "perform(_:on:orientation:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsequencerequesthandler/2880298-perform",
    "html": "Parameters\nrequests\n\nAn array of VNRequest requests to perform.\n\nimage\n\nThe input CGImage on which to perform the request.\n\norientation\n\nThe orientation of the input image.\n\nReturn Value\n\nReturns true if all requests were scheduled and performed. Check the error parameter if the return value is false.\n\nSee Also\nPerforming a Sequence Request\nfunc perform([VNRequest], on: CGImage)\nSchedules Vision requests to be performed on a Core Graphics image.\nfunc perform([VNRequest], on: CIImage)\nSchedules one or more Vision requests to be performed on CIImage data.\nfunc perform([VNRequest], on: CIImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on CIImage data with known orientation.\nfunc perform([VNRequest], on: CVPixelBuffer)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer.\nfunc perform([VNRequest], on: CVPixelBuffer, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer with known orientation.\nfunc perform([VNRequest], on: CMSampleBuffer)\nPerforms one or more requests on an image contained within a sample buffer.\nfunc perform([VNRequest], on: CMSampleBuffer, orientation: CGImagePropertyOrientation)\nPerforms one or more requests on an image of a specified orientation contained within a sample buffer.\nfunc perform([VNRequest], onImageData: Data)\nSchedules one or more Vision requests to be performed on raw image data.\nfunc perform([VNRequest], onImageData: Data, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on raw data containing an image with known orientation.\nfunc perform([VNRequest], onImageURL: URL)\nSchedules one or more Vision requests to be performed on an image.\nfunc perform([VNRequest], onImageURL: URL, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on an image with known orientation, at a specific URL."
  },
  {
    "title": "perform(_:on:orientation:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsequencerequesthandler/3571273-perform",
    "html": "Parameters\nrequests\n\nThe array of requests to perform.\n\nsampleBuffer\n\nA sample buffer containing a valid imageBuffer.\n\norientation\n\nThe orientation of the image contained within the sample buffer.\n\nSee Also\nPerforming a Sequence Request\nfunc perform([VNRequest], on: CGImage)\nSchedules Vision requests to be performed on a Core Graphics image.\nfunc perform([VNRequest], on: CGImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Graphics image with known orientation.\nfunc perform([VNRequest], on: CIImage)\nSchedules one or more Vision requests to be performed on CIImage data.\nfunc perform([VNRequest], on: CIImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on CIImage data with known orientation.\nfunc perform([VNRequest], on: CVPixelBuffer)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer.\nfunc perform([VNRequest], on: CVPixelBuffer, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer with known orientation.\nfunc perform([VNRequest], on: CMSampleBuffer)\nPerforms one or more requests on an image contained within a sample buffer.\nfunc perform([VNRequest], onImageData: Data)\nSchedules one or more Vision requests to be performed on raw image data.\nfunc perform([VNRequest], onImageData: Data, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on raw data containing an image with known orientation.\nfunc perform([VNRequest], onImageURL: URL)\nSchedules one or more Vision requests to be performed on an image.\nfunc perform([VNRequest], onImageURL: URL, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on an image with known orientation, at a specific URL."
  },
  {
    "title": "perform(_:onImageURL:orientation:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsequencerequesthandler/2881927-perform",
    "html": "Parameters\nrequests\n\nAn array of VNRequest requests to perform.\n\nimageURL\n\nA URL pointing to the image on which to perform the request.\n\norientation\n\nThe orientation of the input image.\n\nReturn Value\n\nReturns true if all requests were scheduled and performed. Check the error parameter if the return value is false.\n\nSee Also\nPerforming a Sequence Request\nfunc perform([VNRequest], on: CGImage)\nSchedules Vision requests to be performed on a Core Graphics image.\nfunc perform([VNRequest], on: CGImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Graphics image with known orientation.\nfunc perform([VNRequest], on: CIImage)\nSchedules one or more Vision requests to be performed on CIImage data.\nfunc perform([VNRequest], on: CIImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on CIImage data with known orientation.\nfunc perform([VNRequest], on: CVPixelBuffer)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer.\nfunc perform([VNRequest], on: CVPixelBuffer, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer with known orientation.\nfunc perform([VNRequest], on: CMSampleBuffer)\nPerforms one or more requests on an image contained within a sample buffer.\nfunc perform([VNRequest], on: CMSampleBuffer, orientation: CGImagePropertyOrientation)\nPerforms one or more requests on an image of a specified orientation contained within a sample buffer.\nfunc perform([VNRequest], onImageData: Data)\nSchedules one or more Vision requests to be performed on raw image data.\nfunc perform([VNRequest], onImageData: Data, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on raw data containing an image with known orientation.\nfunc perform([VNRequest], onImageURL: URL)\nSchedules one or more Vision requests to be performed on an image."
  },
  {
    "title": "perform(_:on:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsequencerequesthandler/3571272-perform",
    "html": "Parameters\nrequests\n\nThe array of requests to perform.\n\nsampleBuffer\n\nA sample buffer containing a valid imageBuffer.\n\nSee Also\nPerforming a Sequence Request\nfunc perform([VNRequest], on: CGImage)\nSchedules Vision requests to be performed on a Core Graphics image.\nfunc perform([VNRequest], on: CGImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Graphics image with known orientation.\nfunc perform([VNRequest], on: CIImage)\nSchedules one or more Vision requests to be performed on CIImage data.\nfunc perform([VNRequest], on: CIImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on CIImage data with known orientation.\nfunc perform([VNRequest], on: CVPixelBuffer)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer.\nfunc perform([VNRequest], on: CVPixelBuffer, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer with known orientation.\nfunc perform([VNRequest], on: CMSampleBuffer, orientation: CGImagePropertyOrientation)\nPerforms one or more requests on an image of a specified orientation contained within a sample buffer.\nfunc perform([VNRequest], onImageData: Data)\nSchedules one or more Vision requests to be performed on raw image data.\nfunc perform([VNRequest], onImageData: Data, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on raw data containing an image with known orientation.\nfunc perform([VNRequest], onImageURL: URL)\nSchedules one or more Vision requests to be performed on an image.\nfunc perform([VNRequest], onImageURL: URL, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on an image with known orientation, at a specific URL."
  },
  {
    "title": "perform(_:on:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsequencerequesthandler/2880307-perform",
    "html": "Parameters\nrequests\n\nAn array of VNRequest requests to perform.\n\npixelBuffer\n\nThe input CVPixelBuffer on which to perform the request.\n\nReturn Value\n\nReturns true if all requests were scheduled and performed. Check the error parameter if the return value is false.\n\nSee Also\nPerforming a Sequence Request\nfunc perform([VNRequest], on: CGImage)\nSchedules Vision requests to be performed on a Core Graphics image.\nfunc perform([VNRequest], on: CGImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Graphics image with known orientation.\nfunc perform([VNRequest], on: CIImage)\nSchedules one or more Vision requests to be performed on CIImage data.\nfunc perform([VNRequest], on: CIImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on CIImage data with known orientation.\nfunc perform([VNRequest], on: CVPixelBuffer, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer with known orientation.\nfunc perform([VNRequest], on: CMSampleBuffer)\nPerforms one or more requests on an image contained within a sample buffer.\nfunc perform([VNRequest], on: CMSampleBuffer, orientation: CGImagePropertyOrientation)\nPerforms one or more requests on an image of a specified orientation contained within a sample buffer.\nfunc perform([VNRequest], onImageData: Data)\nSchedules one or more Vision requests to be performed on raw image data.\nfunc perform([VNRequest], onImageData: Data, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on raw data containing an image with known orientation.\nfunc perform([VNRequest], onImageURL: URL)\nSchedules one or more Vision requests to be performed on an image.\nfunc perform([VNRequest], onImageURL: URL, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on an image with known orientation, at a specific URL."
  },
  {
    "title": "perform(_:onImageData:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsequencerequesthandler/2880302-perform",
    "html": "Parameters\nrequests\n\nAn array of VNRequest requests to perform.\n\nimageData\n\nThe input image data on which to perform the request.\n\nReturn Value\n\nReturns true if all requests were scheduled and performed. Check the error parameter if the return value is false.\n\nSee Also\nPerforming a Sequence Request\nfunc perform([VNRequest], on: CGImage)\nSchedules Vision requests to be performed on a Core Graphics image.\nfunc perform([VNRequest], on: CGImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Graphics image with known orientation.\nfunc perform([VNRequest], on: CIImage)\nSchedules one or more Vision requests to be performed on CIImage data.\nfunc perform([VNRequest], on: CIImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on CIImage data with known orientation.\nfunc perform([VNRequest], on: CVPixelBuffer)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer.\nfunc perform([VNRequest], on: CVPixelBuffer, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer with known orientation.\nfunc perform([VNRequest], on: CMSampleBuffer)\nPerforms one or more requests on an image contained within a sample buffer.\nfunc perform([VNRequest], on: CMSampleBuffer, orientation: CGImagePropertyOrientation)\nPerforms one or more requests on an image of a specified orientation contained within a sample buffer.\nfunc perform([VNRequest], onImageData: Data, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on raw data containing an image with known orientation.\nfunc perform([VNRequest], onImageURL: URL)\nSchedules one or more Vision requests to be performed on an image.\nfunc perform([VNRequest], onImageURL: URL, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on an image with known orientation, at a specific URL."
  },
  {
    "title": "perform(_:on:orientation:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsequencerequesthandler/2880301-perform",
    "html": "Parameters\nrequests\n\nAn array of VNRequest requests to perform.\n\nimage\n\nThe input CIImage on which to perform the request.\n\norientation\n\nThe orientation of the input image.\n\nReturn Value\n\nReturns true if all requests were scheduled and performed. Check the error parameter if the return value is false.\n\nSee Also\nPerforming a Sequence Request\nfunc perform([VNRequest], on: CGImage)\nSchedules Vision requests to be performed on a Core Graphics image.\nfunc perform([VNRequest], on: CGImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Graphics image with known orientation.\nfunc perform([VNRequest], on: CIImage)\nSchedules one or more Vision requests to be performed on CIImage data.\nfunc perform([VNRequest], on: CVPixelBuffer)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer.\nfunc perform([VNRequest], on: CVPixelBuffer, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer with known orientation.\nfunc perform([VNRequest], on: CMSampleBuffer)\nPerforms one or more requests on an image contained within a sample buffer.\nfunc perform([VNRequest], on: CMSampleBuffer, orientation: CGImagePropertyOrientation)\nPerforms one or more requests on an image of a specified orientation contained within a sample buffer.\nfunc perform([VNRequest], onImageData: Data)\nSchedules one or more Vision requests to be performed on raw image data.\nfunc perform([VNRequest], onImageData: Data, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on raw data containing an image with known orientation.\nfunc perform([VNRequest], onImageURL: URL)\nSchedules one or more Vision requests to be performed on an image.\nfunc perform([VNRequest], onImageURL: URL, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on an image with known orientation, at a specific URL."
  },
  {
    "title": "perform(_:on:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsequencerequesthandler/2880300-perform",
    "html": "Parameters\nrequests\n\nA nonempty array of VNRequest requests to perform.\n\nimage\n\nThe input CGImage on which to perform the request.\n\nReturn Value\n\nReturns true if all requests were scheduled and performed. Check the error parameter if the return value is false.\n\nSee Also\nPerforming a Sequence Request\nfunc perform([VNRequest], on: CGImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Graphics image with known orientation.\nfunc perform([VNRequest], on: CIImage)\nSchedules one or more Vision requests to be performed on CIImage data.\nfunc perform([VNRequest], on: CIImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on CIImage data with known orientation.\nfunc perform([VNRequest], on: CVPixelBuffer)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer.\nfunc perform([VNRequest], on: CVPixelBuffer, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer with known orientation.\nfunc perform([VNRequest], on: CMSampleBuffer)\nPerforms one or more requests on an image contained within a sample buffer.\nfunc perform([VNRequest], on: CMSampleBuffer, orientation: CGImagePropertyOrientation)\nPerforms one or more requests on an image of a specified orientation contained within a sample buffer.\nfunc perform([VNRequest], onImageData: Data)\nSchedules one or more Vision requests to be performed on raw image data.\nfunc perform([VNRequest], onImageData: Data, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on raw data containing an image with known orientation.\nfunc perform([VNRequest], onImageURL: URL)\nSchedules one or more Vision requests to be performed on an image.\nfunc perform([VNRequest], onImageURL: URL, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on an image with known orientation, at a specific URL."
  },
  {
    "title": "parentJointName(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/4173230-parentjointname",
    "html": "Parameters\njointName\n\nThe name of the body joint to return the parent of.\n\nReturn Value\n\nThe name of the parent joint."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsequencerequesthandler/2880304-init",
    "html": "Discussion\n\nUnlike the VNImageRequestHandler, this initializer accepts no input because image data and auxiliary parameters may change from frame to frame, and are handled dynamically in the request-handling perform methods."
  },
  {
    "title": "results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectdocumentsegmentationrequest/3750964-results",
    "html": "See Also\nAccessing the Results\nclass VNRectangleObservation\nAn object that represents the four vertices of a detected rectangle."
  },
  {
    "title": "VNGeneratePersonSegmentationRequest.QualityLevel | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeneratepersonsegmentationrequest/qualitylevel",
    "html": "Topics\nQuality Levels\ncase accurate\nPrefers image quality over performance.\ncase balanced\nPrefers processing that balances image quality and performance.\ncase fast\nPrefers performance over image quality.\nRelationships\nConforms To\nSendable\nSee Also\nConfiguring the Request\nvar outputPixelFormat: OSType\nThe pixel format of the output image.\nvar qualityLevel: VNGeneratePersonSegmentationRequest.QualityLevel\nA value that indicates how the request balances accuracy and performance."
  },
  {
    "title": "results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeneratepersonsegmentationrequest/3750990-results",
    "html": "See Also\nAccessing the Results\nclass VNPixelBufferObservation\nAn object that represents an image that an image analysis request produces."
  },
  {
    "title": "results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacerectanglesrequest/3750968-results",
    "html": "See Also\nAccessing the Results\nclass VNFaceObservation\nFace or facial-feature information that an image analysis request detects."
  },
  {
    "title": "symbologies | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectbarcodesrequest/2875397-symbologies",
    "html": "Discussion\n\nBy default, a request scans for all symbologies. Specify a subset of symbologies to limit the request’s detection range.\n\nNote\n\nSetting a revision on the request resets the symbologies to all symbologies for the specified revision.\n\nSee Also\nSpecifying Symbologies\nfunc supportedSymbologies() -> [VNBarcodeSymbology]\nReturns the barcode symbologies that the request supports.\nvar coalesceCompositeSymbologies: Bool\nA Boolean value that indicates whether to coalesce multiple codes based on the symbology.\nstruct VNBarcodeSymbology\nThe barcode symbologies that the framework detects.\nclass var supportedSymbologies: [VNBarcodeSymbology]\nThe array of barcode symbologies that the request supports.\nDeprecated"
  },
  {
    "title": "characterBoxes | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntextobservation/2867213-characterboxes",
    "html": "Discussion\n\nIf the associated VNDetectTextRectanglesRequest request indicates interest in character boxes by setting the option reportCharacterBoxes to true, this property is non-nil. If no characters are found, it remains empty."
  },
  {
    "title": "VNBarcodeCompositeType.linked | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodecompositetype/linked",
    "html": "See Also\nComposite Types\ncase gs1TypeA\nA type that represents trade items in bulk.\ncase gs1TypeB\nA type that represents trade items by piece.\ncase gs1TypeC\nA type that represents trade items in varying quantity.\ncase none\nA type that represents no composite type."
  },
  {
    "title": "VNRecognizedText | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedtext",
    "html": "Overview\n\nA single VNRecognizedTextObservation can contain multiple recognized text objects—one for each candidate.\n\nTopics\nDetermining Recognized Text\nvar string: String\nThe top candidate for recognized text.\nvar confidence: VNConfidence\nA normalized confidence score for the text recognition result.\nLocating Recognized Text\nfunc boundingBox(for: Range<String.Index>) -> VNRectangleObservation?\nCalculates the bounding box around the characters in the range of a string.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nVNRequestRevisionProviding\nSee Also\nObtaining Recognized Text\nfunc topCandidates(Int) -> [VNRecognizedText]\nRequests the n top candidates for a recognized text string."
  },
  {
    "title": "init(cvPixelBuffer:orientation:options:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2880303-init",
    "html": "Parameters\npixelBuffer\n\nA pixel buffer that contains the image to use for performing the requests. The contents can’t change for the lifetime of the request handler.\n\norientation\n\nThe orientation of the input image.\n\noptions\n\nA dictionary that specifies auxiliary information about the image.\n\nSee Also\nCreating a Request Handler\ninit(cgImage: CGImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on Core Graphics images.\ninit(cgImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\ninit(ciImage: CIImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data.\ninit(ciImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\ninit(cmSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\ninit(data: Data, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image contained in an NSData object.\ninit(data: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\ninit(url: URL, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image at the specified URL.\ninit(url: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "rightShoulder | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4173302-rightshoulder",
    "html": "See Also\nGetting the Arm Joint Names\nstatic let centerShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the point between the shoulders.\nstatic let leftShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left shoulder.\nstatic let leftElbow: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left elbow.\nstatic let rightElbow: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right elbow.\nstatic let leftWrist: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left wrist.\nstatic let rightWrist: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right wrist."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4176677-init",
    "html": "Parameters\nrawValue\n\nThe point key."
  },
  {
    "title": "yaw | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfaceobservation/2980940-yaw",
    "html": "Discussion\n\nThis value indicates the rotational angle of the face around the y-axis.\n\nIf the request doesn’t calculate the angle, the value is nil.\n\nSee Also\nDetermining Facial Orientation\nvar roll: NSNumber?\nThe roll angle of a face in radians.\nvar pitch: NSNumber?\nThe pitch angle of a face in radians."
  },
  {
    "title": "head | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointsgroupname/4173307-head",
    "html": "See Also\nGetting the Group Names\nstatic let all: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents all joints.\nstatic let leftArm: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the left arm joints.\nstatic let leftLeg: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the left leg joints.\nstatic let rightArm: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the right arm joints.\nstatic let rightLeg: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the right leg joints.\nstatic let torso: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the torso joints."
  },
  {
    "title": "roll | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfaceobservation/2980939-roll",
    "html": "Discussion\n\nThis value indicates the rotational angle of the face around the z-axis.\n\nIf the request doesn’t calculate the angle, the value is nil.\n\nSee Also\nDetermining Facial Orientation\nvar yaw: NSNumber?\nThe yaw angle of a face in radians.\nvar pitch: NSNumber?\nThe pitch angle of a face in radians."
  },
  {
    "title": "init(url:orientation:options:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2869645-init",
    "html": "Parameters\nimageURL\n\nA URL pointing to the image to be used for performing the requests. The image must be in a format supported by Image I/O. Image content is immutable.\n\norientation\n\nThe orientation of the input image.\n\noptions\n\nAn optional dictionary containing VNImageOption keys to auxiliary image data.\n\nSee Also\nCreating a Request Handler\ninit(cgImage: CGImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on Core Graphics images.\ninit(cgImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\ninit(ciImage: CIImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data.\ninit(ciImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\ninit(cmSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\ninit(data: Data, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image contained in an NSData object.\ninit(data: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\ninit(url: URL, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image at the specified URL."
  },
  {
    "title": "VNImageOption | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimageoption",
    "html": "Overview\n\nPass an option key into the VNImageRequestHandler instance on creation or request. Option keys are used to describe specific properties of an image or specify how an image needs to be handled.\n\nTopics\nInitializers\ninit(rawValue: String)\nInitializes an option key using its string name.\nOptions Dictionary Keys\nstatic let properties: VNImageOption\nThe dictionary from CGImageSourceCopyPropertiesAtIndex(_:_:_:) containing metadata for algorithms like horizon detection.\nstatic let cameraIntrinsics: VNImageOption\nAn option to specify the camera intrinstics as an NSData or CFData object representing a matrix_float3x3.\nstatic let ciContext: VNImageOption\nAn option key to specify the CIContext to be used in the handler's Core Image operations.\nRelationships\nConforms To\nEquatable\nHashable\nRawRepresentable\nSendable"
  },
  {
    "title": "timeRange | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnobservation/3548370-timerange",
    "html": "Discussion\n\nWhen evaluating a sequence of image buffers, use this property to determine each observation’s start time and duration. If a request doesn’t support time ranges, or the time range is unknown, the value of this property is zero.\n\nSee Also\nEvaluating Observations\nvar confidence: VNConfidence\nThe level of confidence in the observation’s accuracy.\ntypealias VNConfidence\nA type alias for the confidence value of an observation."
  },
  {
    "title": "VNConfidence | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnconfidence",
    "html": "Discussion\n\nThe Vision framework normalizes this value to [0.0, 1.0] under most circumstances. A value of 0.0 indicates no confidence. A value of 1.0 indicates highest confidence, or the observation doesn’t support or assign meaning to confidence.\n\nNote\n\nWhen the results come from a VNCoreMLRequest, Vision forwards confidence values as-is and doesn’t normalize them.\n\nSee Also\nEvaluating Observations\nvar timeRange: CMTimeRange\nThe time range of the reported observation.\nvar confidence: VNConfidence\nThe level of confidence in the observation’s accuracy."
  },
  {
    "title": "confidence | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnobservation/2867220-confidence",
    "html": "Discussion\n\nThe Vision framework normalizes this value to [0.0, 1.0] under most circumstances. A value of 0.0 indicates no confidence. A value of 1.0 indicates highest confidence, or the observation doesn’t support or assign meaning to confidence.\n\nNote\n\nWhen the results come from a VNCoreMLRequest, Vision forwards confidence values as-is and doesn’t normalize them.\n\nSee Also\nEvaluating Observations\nvar timeRange: CMTimeRange\nThe time range of the reported observation.\ntypealias VNConfidence\nA type alias for the confidence value of an observation."
  },
  {
    "title": "init(cmSampleBuffer:depthData:orientation:options:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/4144382-init",
    "html": "Parameters\nsampleBuffer\n\nThe sample buffer that contains the image to analyze. If the sample buffer doesn’t contain an image buffer with image data, the system raises an error.\n\ndepthData\n\nThe depth data to associate with the image.\n\norientation\n\nThe EXIF orientation of the image.\n\noptions\n\nA dictionary that specifies auxiliary information about the image.\n\nDiscussion\n\nSample buffers may contain metadata, like the camera intrinsics. Vision algorithms that support this metadata use it in their analysis, unless overwritten by the options you specify.\n\nImportant\n\nUse a physical device to perform your testing. Performing requests in Simulator may produce inaccurate results due to the inability of Core Image to render certain pixel formats in this environment.\n\nSee Also\nCreating a Request Handler\ninit(cgImage: CGImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on Core Graphics images.\ninit(cgImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\ninit(ciImage: CIImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data.\ninit(ciImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\ninit(cmSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\ninit(data: Data, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image contained in an NSData object.\ninit(data: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\ninit(url: URL, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image at the specified URL.\ninit(url: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "init(data:options:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2866551-init",
    "html": "Parameters\nimageData\n\nData containing the image to be used for performing the requests. Image content is immutable.\n\noptions\n\nAn optional dictionary containing VNImageOption keys to auxiliary image data.\n\nDiscussion\n\nThe intended use cases of this type of initializer include compressed images and network downloads, where a client may receive a JPEG from a website or the cloud.\n\nSee Also\nCreating a Request Handler\ninit(cgImage: CGImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on Core Graphics images.\ninit(cgImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\ninit(ciImage: CIImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data.\ninit(ciImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\ninit(cmSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\ninit(data: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\ninit(url: URL, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image at the specified URL.\ninit(url: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "init(cmSampleBuffer:options:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/3548373-init",
    "html": "Parameters\nsampleBuffer\n\nThe sample buffer that contains the image to analyze. If the sample buffer doesn’t contain an image buffer with image data, the system raises an error.\n\noptions\n\nA dictionary that specifies auxiliary information about the image.\n\nDiscussion\n\nSample buffers may contain metadata, like the camera intrinsics. Vision algorithms that support this metadata use it in their analysis, unless overwritten by the options you specify.\n\nImportant\n\nUse a physical device to perform your testing. Performing requests in Simulator may produce inaccurate results due to the inability of Core Image to render certain pixel formats in this environment.\n\nSee Also\nCreating a Request Handler\ninit(cgImage: CGImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on Core Graphics images.\ninit(cgImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\ninit(ciImage: CIImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data.\ninit(ciImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\ninit(cmSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\ninit(data: Data, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image contained in an NSData object.\ninit(data: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\ninit(url: URL, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image at the specified URL.\ninit(url: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "currentRevision | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/2967108-currentrevision",
    "html": "See Also\nDetermining the Revision\nprotocol VNRequestRevisionProviding\nA protocol for specifying the revision number of Vision algorithms.\nclass var defaultRevision: Int\nThe revision of the latest request for the particular SDK linked with the client application.\nclass var supportedRevisions: IndexSet\nThe collection of currently-supported algorithm versions for the class of request."
  },
  {
    "title": "init(cvPixelBuffer:depthData:orientation:options:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/4144383-init",
    "html": "See Also\nCreating a Request Handler\ninit(cgImage: CGImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on Core Graphics images.\ninit(cgImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\ninit(ciImage: CIImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data.\ninit(ciImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\ninit(cmSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\ninit(data: Data, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image contained in an NSData object.\ninit(data: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\ninit(url: URL, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image at the specified URL.\ninit(url: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "init(cvPixelBuffer:options:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2880309-init",
    "html": "Parameters\npixelBuffer\n\nA pixel buffer that contains the image to use for performing the requests. The contents can’t change for the lifetime of the request handler.\n\noptions\n\nA dictionary that specifies auxiliary information about the image.\n\nSee Also\nCreating a Request Handler\ninit(cgImage: CGImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on Core Graphics images.\ninit(cgImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\ninit(ciImage: CIImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data.\ninit(ciImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\ninit(cmSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\ninit(data: Data, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image contained in an NSData object.\ninit(data: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\ninit(url: URL, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image at the specified URL.\ninit(url: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "init(ciImage:options:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2866549-init",
    "html": "Parameters\nimage\n\nA CIImage containing the image to be used for performing the requests. Image content is immutable.\n\noptions\n\nAn optional dictionary containing properties keys to auxiliary image data.\n\nSee Also\nCreating a Request Handler\ninit(cgImage: CGImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on Core Graphics images.\ninit(cgImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\ninit(ciImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\ninit(cmSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\ninit(data: Data, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image contained in an NSData object.\ninit(data: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\ninit(url: URL, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image at the specified URL.\ninit(url: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "init(cgImage:orientation:options:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2869629-init",
    "html": "Parameters\nimage\n\nA CGImage containing the image to be used for performing the requests. Image content is immutable.\n\norientation\n\nThe orientation of the input image.\n\noptions\n\nAn optional dictionary containing VNImageOption keys to auxiliary image data.\n\nSee Also\nCreating a Request Handler\ninit(cgImage: CGImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on Core Graphics images.\ninit(ciImage: CIImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data.\ninit(ciImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\ninit(cmSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\ninit(data: Data, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image contained in an NSData object.\ninit(data: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\ninit(url: URL, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image at the specified URL.\ninit(url: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "defaultRevision | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/2967109-defaultrevision",
    "html": "See Also\nDetermining the Revision\nprotocol VNRequestRevisionProviding\nA protocol for specifying the revision number of Vision algorithms.\nclass var currentRevision: Int\nThe current revison supported by the request.\nclass var supportedRevisions: IndexSet\nThe collection of currently-supported algorithm versions for the class of request."
  },
  {
    "title": "init(cgImage:options:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2866541-init",
    "html": "Parameters\nimage\n\nA CGImage containing the image to be used for performing the requests. Image content is immutable.\n\noptions\n\nAn optional dictionary containing VNImageOption keys to auxiliary image data.\n\nSee Also\nCreating a Request Handler\ninit(cgImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\ninit(ciImage: CIImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data.\ninit(ciImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\ninit(cmSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\ninit(data: Data, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image contained in an NSData object.\ninit(data: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\ninit(url: URL, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image at the specified URL.\ninit(url: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "init(completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/2875416-init",
    "html": "Parameters\ncompletionHandler\n\nThe block to invoke after the request finishes processing.\n\nDiscussion\n\nVision executes the completion handler on the same queue that it executes the request; however, this queue differs from the one where you called perform(_:).\n\nSee Also\nInitializing a Request\ninit()\nCreates a new Vision request with no completion handler."
  },
  {
    "title": "init() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/2875423-init",
    "html": "See Also\nInitializing a Request\ninit(completionHandler: VNRequestCompletionHandler?)\nCreates a new Vision request with an optional completion handler."
  },
  {
    "title": "init(detectedObjectObservation:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackobjectrequest/2887297-init",
    "html": "Parameters\nobservation\n\nA detected object observation with bounding box information.\n\nSee Also\nInitializing an Object Tracking Request\ninit(detectedObjectObservation: VNDetectedObjectObservation, completionHandler: VNRequestCompletionHandler?)\nCreates a new object tracking request with a detected object observation."
  },
  {
    "title": "VNClassifyImageRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnclassifyimagerequestrevision1",
    "html": "Discussion\n\nThe revision number is a constant that you pass on a per-request basis to indicate to the Vision framework which version of the image classifier to use for that request. Each OS release in which the framework improves aspects of the algorithm (recognition speed, accuracy, number of languages supported, and so forth), the revision number increments by 1.\n\nBy default, recognition requests use the latest—the highest—revision number for the SDK that your app links against. If you don’t recompile your app against a newer SDK, your app binary uses the revision that was the default at the time you last compiled it. If you do recompile, your app uses the default of the new SDK.\n\nIf your app must support users on older OS versions that don’t have access to the latest Vision framework, you may want to specify an earlier revision. For example, your algorithm may depend on specific behavior from a Vision request, such as writing your image processing algorithm to assume the size or aspect ratio of bounding boxes from an older revision of the face detector. In such a scenario, you can support earlier versions of the algorithm by specifying lower numbers:\n\nvisionRequest.revision = VNClassifyImageRequestRevision1\n"
  },
  {
    "title": "VNTrackObjectRequestRevision2 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackobjectrequestrevision2",
    "html": "See Also\nIdentifying Request Revisions\nlet VNTrackObjectRequestRevision1: Int\nA constant for specifying revision 1 of the object tracking request."
  },
  {
    "title": "init(boundingBox:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectedobjectobservation/2879297-init",
    "html": "Parameters\nboundingBox\n\nThe observation’s bounding box, in coordinates normalized to the dimensions of the processed image, with its origin at the image’s lower-left corner.\n\nSee Also\nCreating an Observation\ninit(requestRevision: Int, boundingBox: CGRect)\nCreates an observation with a revision number and bounding box."
  },
  {
    "title": "VNGenerateImageFeaturePrintRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateimagefeatureprintrequestrevision1",
    "html": "Discussion\n\nThe revision number is a constant that you pass on a per-request basis to indicate to the Vision framework which version of the feature print algorithm to use for that request. Each OS release in which the framework improves aspects of the algorithm (recognition speed, accuracy, number of languages supported, and so forth), the revision number increments by 1.\n\nBy default, recognition requests use the latest—the highest—revision number for the SDK that your app links against. If you don’t recompile your app against a newer SDK, your app binary uses the revision that was the default at the time you last compiled it. If you do recompile, your app uses the default of the new SDK.\n\nIf your app must support users on older OS versions that don’t have access to the latest Vision framework, you may want to specify an earlier revision. For example, your algorithm may depend on specific behavior from a Vision request, such as writing your image processing algorithm to assume the size or aspect ratio of bounding boxes from an older revision of the face detector. In such a scenario, you can support earlier versions of the algorithm by specifying lower numbers:\n\nvisionRequest.revision = VNGenerateImageFeaturePrintRequestRevision1\n"
  },
  {
    "title": "supportedIdentifiers() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnclassifyimagerequest/3750957-supportedidentifiers",
    "html": "Parameters\nerror\n\nA pointer to an error object.\n\nIf an error occurs, the system sets the pointer an NSError instance that contains the error details. Specify nil for this parameter if you don’t want error details.\n\nReturn Value\n\nAn array of supported identifiers.\n\nSee Also\nAccessing Results\nvar results: [VNClassificationObservation]?\nThe results of the image classification request.\nclass VNClassificationObservation\nAn object that represents classification information that an image analysis request produces.\nclass func knownClassifications(forRevision: Int) -> [VNClassificationObservation]\nRequests the collection of classifications that the Vision framework recognizes.\nDeprecated"
  },
  {
    "title": "results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnclassifyimagerequest/3750956-results",
    "html": "See Also\nAccessing Results\nfunc supportedIdentifiers() -> [String]\nReturns the classification identifiers that the request supports in its current configuration.\nclass VNClassificationObservation\nAn object that represents classification information that an image analysis request produces.\nclass func knownClassifications(forRevision: Int) -> [VNClassificationObservation]\nRequests the collection of classifications that the Vision framework recognizes.\nDeprecated"
  },
  {
    "title": "VNFeaturePrintObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfeatureprintobservation",
    "html": "Topics\nFetching Feature Print Data\nvar data: Data\nThe feature print data.\nvar elementCount: Int\nThe total number of elements in the data.\nDetermining Types of Feature Prints\nvar elementType: VNElementType\nThe type of each element in the data.\nenum VNElementType\nAn enumeration of the type of element in feature print data.\nfunc VNElementTypeSize(VNElementType) -> Int\nReturns the size of a feature print element.\nComputing Distance Between Features\nfunc computeDistance(UnsafeMutablePointer<Float>, to: VNFeaturePrintObservation)\nComputes the distance between two feature print observations.\nRelationships\nInherits From\nVNObservation\nSee Also\nAccessing the Results\nvar results: [VNFeaturePrintObservation]?\nThe results of the feature print request."
  },
  {
    "title": "results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateimagefeatureprintrequest/3750984-results",
    "html": "See Also\nAccessing the Results\nclass VNFeaturePrintObservation\nAn observation that provides the recognized feature print."
  },
  {
    "title": "init(completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeneratepersonsegmentationrequest/3783572-init",
    "html": "Parameters\ncompletionHandler\n\nA completion handler that processes the resuts of the request."
  },
  {
    "title": "VNAnimalIdentifier | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalidentifier",
    "html": "Topics\nAnimals\nstatic let cat: VNAnimalIdentifier\nAn animal identifier for cats.\nstatic let dog: VNAnimalIdentifier\nAn animal identifier for dogs.\nInitializers\ninit(rawValue: String)\nCreates an identifier with a string.\nRelationships\nConforms To\nHashable\nRawRepresentable\nSendable\nSee Also\nIdentifying Animals\nfunc supportedIdentifiers() -> [VNAnimalIdentifier]\nReturns the identifiers of the animals that the request detects.\nclass func knownAnimalIdentifiers(forRevision: Int) -> [VNAnimalIdentifier]\nReturns a list of animal identifiers the recognition algorithm supports for the specified revision.\nDeprecated"
  },
  {
    "title": "VNRecognizeAnimalsRequestRevision2 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizeanimalsrequestrevision2",
    "html": "See Also\nIdentifying Request Revisions\nlet VNRecognizeAnimalsRequestRevision1: Int\nA constant for specifying revision 1 of the animal recognition request."
  },
  {
    "title": "VNDetectHumanBodyPose3DRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanbodypose3drequestrevision1",
    "html": "See Also\nVision Framework Version and Revision Numbers\nvar VNVisionVersionNumber: Double\nThe current version number of the Vision framework.\nlet VNDetectAnimalBodyPoseRequestRevision1: Int\nA value that indicates the first revision for an animal body pose request.\nlet VNTrackHomographicImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a homographic image registration request.\nlet VNTrackTranslationalImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a translational image registration request.\nlet VNTrackOpticalFlowRequestRevision1: Int\nA value that indicates the first revision for an optial flow request.\nlet VNClassifyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an attention saliency image request.\nlet VNGenerateImageFeaturePrintRequestRevision2: Int\nA value that indicates the second revision for a feature print request.\nlet VNDetectFaceCaptureQualityRequestRevision3: Int\nA value that indicates the third revision for a face capture quality request.\nlet VNDetectBarcodesRequestRevision4: Int\nA value that indicates the fourth revision for a barcode request."
  },
  {
    "title": "VNImageCropAndScaleOption | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagecropandscaleoption",
    "html": "Overview\n\nScaling an image ensures that it fits within the algorithm’s input image dimensions, which may require a change in aspect ratio. The figure below shows how each crop-and-scale option transforms the input image:\n\nTopics\nCrop and Scale Options\ncase centerCrop\nAn option that scales the image to fit its shorter side within the input dimensions, while preserving its aspect ratio, and center-crops the image.\ncase scaleFit\nAn option that scales the image to fit its longer side within the input dimensions, while preserving its aspect ratio, and center-crops the image.\ncase scaleFill\nAn option that scales the image to fill the input dimensions, resizing it if necessary.\ncase scaleFitRotate90CCW\nAn option that rotates the image 90 degrees counterclockwise and then scales it, while preserving its aspect ratio, to fit on the long side.\ncase scaleFillRotate90CCW\nAn option that rotates the image 90 degrees counterclockwise and then scales it to fill the input dimensions.\nRelationships\nConforms To\nSendable\nSee Also\nConfiguring Image Options\nvar imageCropAndScaleOption: VNImageCropAndScaleOption\nAn optional setting that tells the Vision algorithm how to scale an input image."
  },
  {
    "title": "payloadStringValue | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodeobservation/2923485-payloadstringvalue",
    "html": "Discussion\n\nDepending on the symbology or the payload data itself, a string representation of the payload may not be available.\n\nSee Also\nParsing the Payload\nvar payloadData: Data?\nThe raw data representation of the barcode’s payload.\nvar supplementalPayloadString: String?\nThe supplemental code decoded as a string value.\nvar supplementalPayloadData: Data?\nvar supplementalCompositeType: VNBarcodeCompositeType\nThe supplemental composite type.\nvar isGS1DataCarrier: Bool\nA Boolean value that indicates whether the barcode carries any global standards data."
  },
  {
    "title": "frameAnalysisSpacing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnstatefulrequest/3675676-frameanalysisspacing",
    "html": "See Also\nConfiguring the Request\nvar minimumLatencyFrameCount: Int\nThe minimum number of frames a request processes before reporting an observation."
  },
  {
    "title": "VNVideoProcessor | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvideoprocessor",
    "html": "Topics\nCreating a Video Processor\ninit(url: URL)\nCreates a video processor to perform Vision requests against the specified video asset.\nPerforming Requests\nfunc addRequest(VNRequest, processingOptions: VNVideoProcessor.RequestProcessingOptions)\nAdds a request with processing options to the video processor.\nclass VNVideoProcessor.RequestProcessingOptions\nAn object that defines a video processor’s configuration options.\nfunc removeRequest(VNRequest)\nRemoves a Vision request from the video processor’s request queue.\nfunc analyze(CMTimeRange)\nAnalyzes a time range of video content.\nfunc cancel()\nCancels the video processing.\nfunc add(VNRequest, withProcessingOptions: [VNVideoProcessingOption : Any])\nAdds a Vision request to perform with the specified configuration.\nDeprecated\nfunc analyze(with: CMTimeRange)\nAnalyzes the specifed time range of the video content.\nDeprecated\nRelationships\nInherits From\nNSObject\nSee Also\nUtilities\nstruct VNComputeStage\nTypes that represent the compute stage.\nclass VNGeometryUtils\nUtility methods to determine the geometries of various Vision types."
  },
  {
    "title": "imageCropAndScaleOption | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateimagefeatureprintrequest/3152620-imagecropandscaleoption",
    "html": "Discussion\n\nScaling is applied before generating the feature print. The default value is VNImageCropAndScaleOption.scaleFill.\n\nScaling an image ensures that the entire image fits into the algorithm's input image dimensions, which may require a change in aspect ratio. Each crop and scale option transforms the input image in a different way:\n\nSee Also\nScaling and Cropping Images\nenum VNImageCropAndScaleOption\nOptions that define how Vision crops and scales an input-image."
  },
  {
    "title": "VNBarcodeCompositeType.gs1TypeB | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodecompositetype/gs1typeb",
    "html": "See Also\nComposite Types\ncase gs1TypeA\nA type that represents trade items in bulk.\ncase gs1TypeC\nA type that represents trade items in varying quantity.\ncase linked\nA type that represents a linked composite type.\ncase none\nA type that represents no composite type."
  },
  {
    "title": "init(ciImage:orientation:options:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2869641-init",
    "html": "Parameters\nimage\n\nA CIImage containing the image to be used for performing the requests. Image content is immutable.\n\norientation\n\nThe orientation of the input image.\n\noptions\n\nAn optional dictionary containing VNImageOption keys to auxiliary image data.\n\nSee Also\nCreating a Request Handler\ninit(cgImage: CGImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on Core Graphics images.\ninit(cgImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\ninit(ciImage: CIImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data.\ninit(cvPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\ninit(cmSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\ninit(data: Data, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image contained in an NSData object.\ninit(data: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\ninit(url: URL, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image at the specified URL.\ninit(url: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "init(data:orientation:options:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2869635-init",
    "html": "Parameters\nimageData\n\nData containing the image to be used for performing the requests. Image content is immutable.\n\norientation\n\nThe orientation of the input image.\n\noptions\n\nAn optional dictionary containing VNImageOption keys to auxiliary image data.\n\nDiscussion\n\nThe intended use cases of this type of initializer include compressed images and network downloads, where a client may receive a JPEG from a website or the cloud.\n\nSee Also\nCreating a Request Handler\ninit(cgImage: CGImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on Core Graphics images.\ninit(cgImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\ninit(ciImage: CIImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data.\ninit(ciImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\ninit(cmSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\ninit(data: Data, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image contained in an NSData object.\ninit(url: URL, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image at the specified URL.\ninit(url: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL."
  },
  {
    "title": "VNRectangleObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrectangleobservation",
    "html": "Topics\nCreating an Observation\ninit(requestRevision: Int, topLeft: CGPoint, topRight: CGPoint, bottomRight: CGPoint, bottomLeft: CGPoint)\nCreates a rectangle observation from its corner points.\ninit(requestRevision: Int, topLeft: CGPoint, bottomLeft: CGPoint, bottomRight: CGPoint, topRight: CGPoint)\nCreates a rectangle observation from its corner points.\nDeprecated\nAccessing the Coordinates\nvar bottomLeft: CGPoint\nThe coordinates of the lower-left corner of the observation bounding box.\nvar bottomRight: CGPoint\nThe coordinates of the lower-right corner of the observation bounding box.\nvar topLeft: CGPoint\nThe coordinates of the upper-left corner of the observation bounding box.\nvar topRight: CGPoint\nThe coordinates of the upper-right corner of the observation bounding box.\nRelationships\nInherits From\nVNDetectedObjectObservation\nSee Also\nAccessing the Results\nvar results: [VNRectangleObservation]?\nThe results of the request to detect rectangles."
  },
  {
    "title": "topCandidates(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedtextobservation/3152637-topcandidates",
    "html": "Parameters\nmaxCandidateCount\n\nThe maximum number of candidates to return. This can't exceed 10.\n\nReturn Value\n\nAn array of the n top candidates, sorted by decreasing confidence score.\n\nDiscussion\n\nThis function returns no more than n candidates, but it may return fewer than n candidates.\n\nSee Also\nObtaining Recognized Text\nclass VNRecognizedText\nText recognized in an image through a text recognition request."
  },
  {
    "title": "centerHead | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4173288-centerhead",
    "html": "See Also\nGetting the Head Joint Names\nstatic let topHead: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the top of the head."
  },
  {
    "title": "topHead | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4173304-tophead",
    "html": "See Also\nGetting the Head Joint Names\nstatic let centerHead: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the center of the head."
  },
  {
    "title": "VNBarcodeCompositeType.gs1TypeC | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodecompositetype/gs1typec",
    "html": "See Also\nComposite Types\ncase gs1TypeA\nA type that represents trade items in bulk.\ncase gs1TypeB\nA type that represents trade items by piece.\ncase linked\nA type that represents a linked composite type.\ncase none\nA type that represents no composite type."
  },
  {
    "title": "leftShoulder | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4173296-leftshoulder",
    "html": "See Also\nGetting the Arm Joint Names\nstatic let centerShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the point between the shoulders.\nstatic let rightShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right shoulder.\nstatic let leftElbow: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left elbow.\nstatic let rightElbow: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right elbow.\nstatic let leftWrist: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left wrist.\nstatic let rightWrist: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right wrist."
  },
  {
    "title": "VNBarcodeCompositeType.none | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodecompositetype/none",
    "html": "See Also\nComposite Types\ncase gs1TypeA\nA type that represents trade items in bulk.\ncase gs1TypeB\nA type that represents trade items by piece.\ncase gs1TypeC\nA type that represents trade items in varying quantity.\ncase linked\nA type that represents a linked composite type."
  },
  {
    "title": "leftElbow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4173291-leftelbow",
    "html": "See Also\nGetting the Arm Joint Names\nstatic let centerShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the point between the shoulders.\nstatic let leftShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left shoulder.\nstatic let rightShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right shoulder.\nstatic let rightElbow: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right elbow.\nstatic let leftWrist: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left wrist.\nstatic let rightWrist: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right wrist."
  },
  {
    "title": "rightKnee | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4173301-rightknee",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftHip: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left hip.\nstatic let rightHip: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right hip.\nstatic let leftKnee: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left knee.\nstatic let leftAnkle: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left ankle.\nstatic let rightAnkle: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right ankle."
  },
  {
    "title": "leftHip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4173294-lefthip",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let rightHip: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right hip.\nstatic let leftKnee: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left knee.\nstatic let rightKnee: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right knee.\nstatic let leftAnkle: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left ankle.\nstatic let rightAnkle: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right ankle."
  },
  {
    "title": "rightElbow | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4173297-rightelbow",
    "html": "See Also\nGetting the Arm Joint Names\nstatic let centerShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the point between the shoulders.\nstatic let leftShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left shoulder.\nstatic let rightShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right shoulder.\nstatic let leftElbow: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left elbow.\nstatic let leftWrist: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left wrist.\nstatic let rightWrist: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right wrist."
  },
  {
    "title": "leftWrist | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4210416-leftwrist",
    "html": "See Also\nGetting the Arm Joint Names\nstatic let centerShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the point between the shoulders.\nstatic let leftShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left shoulder.\nstatic let rightShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right shoulder.\nstatic let leftElbow: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left elbow.\nstatic let rightElbow: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right elbow.\nstatic let rightWrist: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right wrist."
  },
  {
    "title": "availableJointsGroupNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/4173226-availablejointsgroupnames",
    "html": "See Also\nAccessing Points\nvar availableJointNames: [VNHumanBodyPose3DObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose.\nfunc recognizedPoint(VNHumanBodyPose3DObservation.JointName) -> VNHumanBodyRecognizedPoint3D\nReturns the point for a joint name that the observation recognizes.\nfunc recognizedPoints(VNHumanBodyPose3DObservation.JointsGroupName) -> [VNHumanBodyPose3DObservation.JointName : VNHumanBodyRecognizedPoint3D]\nReturns a collection of points for the group name you specify."
  },
  {
    "title": "leftAnkle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4210415-leftankle",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftHip: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left hip.\nstatic let rightHip: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right hip.\nstatic let leftKnee: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left knee.\nstatic let rightKnee: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right knee.\nstatic let rightAnkle: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right ankle."
  },
  {
    "title": "rightHip | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4173300-righthip",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftHip: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left hip.\nstatic let leftKnee: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left knee.\nstatic let rightKnee: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right knee.\nstatic let leftAnkle: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left ankle.\nstatic let rightAnkle: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right ankle."
  },
  {
    "title": "rightAnkle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4210417-rightankle",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftHip: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left hip.\nstatic let rightHip: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right hip.\nstatic let leftKnee: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left knee.\nstatic let rightKnee: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right knee.\nstatic let leftAnkle: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left ankle."
  },
  {
    "title": "availableJointNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/4173225-availablejointnames",
    "html": "See Also\nAccessing Points\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nvar availableJointsGroupNames: [VNHumanBodyPose3DObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose.\nfunc recognizedPoint(VNHumanBodyPose3DObservation.JointName) -> VNHumanBodyRecognizedPoint3D\nReturns the point for a joint name that the observation recognizes.\nfunc recognizedPoints(VNHumanBodyPose3DObservation.JointsGroupName) -> [VNHumanBodyPose3DObservation.JointName : VNHumanBodyRecognizedPoint3D]\nReturns a collection of points for the group name you specify."
  },
  {
    "title": "all | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointsgroupname/4173306-all",
    "html": "See Also\nGetting the Group Names\nstatic let head: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the head joints.\nstatic let leftArm: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the left arm joints.\nstatic let leftLeg: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the left leg joints.\nstatic let rightArm: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the right arm joints.\nstatic let rightLeg: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the right leg joints.\nstatic let torso: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the torso joints."
  },
  {
    "title": "leftArm | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointsgroupname/4173308-leftarm",
    "html": "See Also\nGetting the Group Names\nstatic let all: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents all joints.\nstatic let head: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the head joints.\nstatic let leftLeg: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the left leg joints.\nstatic let rightArm: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the right arm joints.\nstatic let rightLeg: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the right leg joints.\nstatic let torso: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the torso joints."
  },
  {
    "title": "leftLeg | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointsgroupname/4173309-leftleg",
    "html": "See Also\nGetting the Group Names\nstatic let all: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents all joints.\nstatic let head: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the head joints.\nstatic let leftArm: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the left arm joints.\nstatic let rightArm: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the right arm joints.\nstatic let rightLeg: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the right leg joints.\nstatic let torso: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the torso joints."
  },
  {
    "title": "rightArm | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointsgroupname/4173310-rightarm",
    "html": "See Also\nGetting the Group Names\nstatic let all: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents all joints.\nstatic let head: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the head joints.\nstatic let leftArm: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the left arm joints.\nstatic let leftLeg: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the left leg joints.\nstatic let rightLeg: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the right leg joints.\nstatic let torso: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the torso joints."
  },
  {
    "title": "qualityLevel | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeneratepersonsegmentationrequest/3750989-qualitylevel",
    "html": "See Also\nConfiguring the Request\nvar outputPixelFormat: OSType\nThe pixel format of the output image.\nenum VNGeneratePersonSegmentationRequest.QualityLevel\nConstants that define the levels of quality for a person segmentation request."
  },
  {
    "title": "VNRequestRevisionProviding | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequestrevisionproviding",
    "html": "Overview\n\nSubclasses of VNRequest should adopt this protocol to specify which revision of an algorithm the Vision framework uses to generate requests.\n\nTopics\nSpecifying Revision Number\nvar requestRevision: Int\nThe revision of the VNRequest subclass used to generate the implementing object.\n\nRequired\n\nDetermining Revision Type\nlet VNRequestRevisionUnspecified: Int\nA constant for specifying an unspecified request revision.\nlet VNDetectRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the rectangle detection request.\nlet VNTrackRectangleRequestRevision1: Int\nA constant for specifying revision 1 of the rectangling tracking request.\nlet VNTrackObjectRequestRevision1: Int\nA constant for specifying revision 1 of the object tracking request.\nlet VNDetectFaceRectanglesRequestRevision2: Int\nA constant for specifying revision 2 of the face rectangles detection request.\nlet VNDetectFaceRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the face rectangles detection request.\nDeprecated\nlet VNDetectFaceLandmarksRequestRevision3: Int\nA constant for specifying revision 3 of the face landmarks detection request.\nlet VNDetectFaceLandmarksRequestRevision2: Int\nA constant for specifying revision 2 of the face landmarks detection request.\nlet VNDetectFaceLandmarksRequestRevision1: Int\nA constant for specifying revision 1 of the face landmarks detection request.\nDeprecated\nlet VNRecognizeTextRequestRevision1: Int\nA constant for specifying revision 1 of the text recognition request.\nlet VNDetectTextRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the text rectangles detection request.\nlet VNDetectBarcodesRequestRevision1: Int\nA constant for specifying revision 1 of the barcode detection request.\nDeprecated\nlet VNDetectHorizonRequestRevision1: Int\nA constant for specifying revision 1 of the horizon detection request.\nlet VNTranslationalImageRegistrationRequestRevision1: Int\nA constant for specifying revision 1 of the translational image registration request.\nlet VNHomographicImageRegistrationRequestRevision1: Int\nA constant for specifying revision 1 of the homographic image registration request.\nlet VNCoreMLRequestRevision1: Int\nA constant for specifying revision 1 of a Core ML request.\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision1: Int\nA constant for specifying revision 1 of the image saliency request.\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision1: Int\nA constant for specifying revision 1 of the image saliency request.\nlet VNClassifyImageRequestRevision1: Int\nA constant for specifying revision 1 of the image classification request.\nlet VNGenerateImageFeaturePrintRequestRevision1: Int\nA constant for specifying revision 1 of the feature print request.\nlet VNDetectFaceCaptureQualityRequestRevision1: Int\nA constant for specifying revision 1 of the face capture detection request.\nlet VNDetectHumanRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the human rectangles detection request.\nRelationships\nConforming Types\nVNContour\nVNFaceLandmarkRegion\nVNFaceLandmarks\nVNObservation\nVNRecognizedText\nSee Also\nDetermining the Revision\nclass var currentRevision: Int\nThe current revison supported by the request.\nclass var defaultRevision: Int\nThe revision of the latest request for the particular SDK linked with the client application.\nclass var supportedRevisions: IndexSet\nThe collection of currently-supported algorithm versions for the class of request."
  },
  {
    "title": "VNRequestCompletionHandler | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequestcompletionhandler",
    "html": "Discussion\n\nVision executes the completion handler on the same queue that it executes the request; however, this queue differs from the one where you called perform(_:)."
  },
  {
    "title": "computeDevice(for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/4284410-computedevice",
    "html": "Parameters\ncomputeStage\n\nThe compute stage to inspect.\n\nReturn Value\n\nThe current compute device; otherwise, nil if one isn’t assigned.\n\nSee Also\nConfiguring the Compute Device\nfunc setComputeDevice(MLComputeDevice?, for: VNComputeStage)\nAssigns a compute device for a compute stage.\nvar supportedComputeStageDevices: [VNComputeStage : [MLComputeDevice]]\nThe collection of compute devices per stage that a request supports."
  },
  {
    "title": "supportedComputeStageDevices | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/4284412-supportedcomputestagedevices",
    "html": "Discussion\n\nA dictionary of per-stage compute devices; otherwise, nil if an error occurs.\n\nSee Also\nConfiguring the Compute Device\nfunc setComputeDevice(MLComputeDevice?, for: VNComputeStage)\nAssigns a compute device for a compute stage.\nfunc computeDevice(for: VNComputeStage) -> MLComputeDevice?\nReturns the compute device for a compute stage."
  },
  {
    "title": "revision | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/2967110-revision",
    "html": "See Also\nConfiguring a Request\nvar completionHandler: VNRequestCompletionHandler?\nThe completion handler the system invokes after the request finishes processing.\nvar preferBackgroundProcessing: Bool\nA hint to minimize the resource burden of the request.\nvar results: [VNObservation]?\nThe collection of VNObservation results generated by request processing.\nvar usesCPUOnly: Bool\nA Boolean signifying that the Vision request should execute exclusively on the CPU.\nDeprecated"
  },
  {
    "title": "setComputeDevice(_:for:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/4284411-setcomputedevice",
    "html": "Parameters\ncomputeDevice\n\nThe compute device to assign to the compute stage.\n\ncomputeStage\n\nThe compute stage.\n\nDiscussion\n\nIf the parameter computeDevice is nil, the framework removes any explicit compute device assignment and allows the framework to select the device.\n\nConfigure any compute device for a given compute stage. When performing a request, the system makes a validity check. Call supportedComputeStageDevices() to get valid compute devices for a request’s compute stages.\n\nSee Also\nConfiguring the Compute Device\nfunc computeDevice(for: VNComputeStage) -> MLComputeDevice?\nReturns the compute device for a compute stage.\nvar supportedComputeStageDevices: [VNComputeStage : [MLComputeDevice]]\nThe collection of compute devices per stage that a request supports."
  },
  {
    "title": "preferBackgroundProcessing | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/2875404-preferbackgroundprocessing",
    "html": "Discussion\n\nIf set to true, this property reduces the request’s memory footprint, processing footprint, and CPU/GPU contention at the potential cost of longer execution time.\n\nSetting this value can help ensure that Vision processing doesn’t block UI updates and other rendering on the main thread.\n\nSee Also\nConfiguring a Request\nvar completionHandler: VNRequestCompletionHandler?\nThe completion handler the system invokes after the request finishes processing.\nvar results: [VNObservation]?\nThe collection of VNObservation results generated by request processing.\nvar revision: Int\nThe specific algorithm or implementation revision that’s used to perform the request.\nvar usesCPUOnly: Bool\nA Boolean signifying that the Vision request should execute exclusively on the CPU.\nDeprecated"
  },
  {
    "title": "usesCPUOnly | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/2923480-usescpuonly",
    "html": "Discussion\n\nThis value defaults to false to signify that the Vision request is free to leverage the GPU to accelerate its processing.\n\nSee Also\nConfiguring a Request\nvar completionHandler: VNRequestCompletionHandler?\nThe completion handler the system invokes after the request finishes processing.\nvar preferBackgroundProcessing: Bool\nA hint to minimize the resource burden of the request.\nvar results: [VNObservation]?\nThe collection of VNObservation results generated by request processing.\nvar revision: Int\nThe specific algorithm or implementation revision that’s used to perform the request."
  },
  {
    "title": "VNNormalizedPointForImagePoint(_:_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/3564831-vnnormalizedpointforimagepoint",
    "html": "Parameters\nimagePoint\n\nThe input point in image coordinate space.\n\nimageWidth\n\nThe image width.\n\nimageHeight\n\nThe image height.\n\nReturn Value\n\nThe input point projected into normalized coordinates.\n\nDiscussion\n\nThe resulting point in image coordinate space may have nonintegral (floating-point) coordinates.\n\nSee Also\nCoordinate Conversion\nfunc VNImagePointForNormalizedPoint(CGPoint, Int, Int) -> CGPoint\nProjects a point in normalized coordinates into image coordinates.\nfunc VNImagePointForNormalizedPointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the image coordinates space into normalized coordinates.\nlet VNNormalizedIdentityRect: CGRect\nA normalized identity rectangle with an origin of zero and unit length and width.\nfunc VNNormalizedRectIsIdentityRect(CGRect) -> Bool\nReturns a Boolean value that indicates whether the rectangle has an origin of zero and unit length and width.\nfunc VNImagePointForFaceLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the image coordinates of a specified face landmark point.\nfunc VNNormalizedFaceBoundingBoxPointForLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the coordinates of a specified face landmark point, in bounding box coordinates."
  },
  {
    "title": "VNImagePointForNormalizedPoint(_:_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/2908997-vnimagepointfornormalizedpoint",
    "html": "Parameters\nnormalizedPoint\n\nThe input point in normalized coordinate space.\n\nimageWidth\n\nThe width of the image into whose coordinate space you’re projecting the input point.\n\nimageHeight\n\nThe height of the image into whose coordinate space you’re projecting the input point.\n\nReturn Value\n\nThe input point projected into image coordinates.\n\nDiscussion\n\nThe resulting point in image coordinate space may have nonintegral (floating-point) coordinates.\n\nSee Also\nCoordinate Conversion\nfunc VNNormalizedPointForImagePoint(CGPoint, Int, Int) -> CGPoint\nProjects a point from image coordinates into normalized coordinates.\nfunc VNImagePointForNormalizedPointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the image coordinates space into normalized coordinates.\nlet VNNormalizedIdentityRect: CGRect\nA normalized identity rectangle with an origin of zero and unit length and width.\nfunc VNNormalizedRectIsIdentityRect(CGRect) -> Bool\nReturns a Boolean value that indicates whether the rectangle has an origin of zero and unit length and width.\nfunc VNImagePointForFaceLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the image coordinates of a specified face landmark point.\nfunc VNNormalizedFaceBoundingBoxPointForLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the coordinates of a specified face landmark point, in bounding box coordinates."
  },
  {
    "title": "VNCoreMLRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncoremlrequest",
    "html": "Overview\n\nThe results array of a Core ML-based image analysis request contains a different observation type, depending on the kind of MLModel object you use:\n\nIf the model predicts a single feature, the model’s modelDescription object has a non-nil value for predictedFeatureName and Vision treats the model as a classifier. The results are VNClassificationObservation objects.\n\nIf the model’s outputs include at least one output with a feature type of MLFeatureType.image, Vision treats that model as an image-to-image model. The results are VNPixelBufferObservation objects.\n\nOtherwise, Vision treats the model as a general predictor model. The results are VNCoreMLFeatureValueObservation objects.\n\nNote\n\nVision forwards all confidence values from Core ML models as-is and doesn’t normalize them to [0, 1].\n\nTopics\nInitializing with a Core ML Model\ninit(model: VNCoreMLModel)\nCreates a model container to use with an image analysis request based on the model you provide.\ninit(model: VNCoreMLModel, completionHandler: VNRequestCompletionHandler?)\nCreates a model container to use with an image analysis request based on the model you provide, with an optional completion handler.\nvar model: VNCoreMLModel\nThe model to base the image analysis request on.\nclass VNCoreMLModel\nA container for the model to use with Vision requests.\nConfiguring Image Options\nvar imageCropAndScaleOption: VNImageCropAndScaleOption\nAn optional setting that tells the Vision algorithm how to scale an input image.\nenum VNImageCropAndScaleOption\nOptions that define how Vision crops and scales an input-image.\nIdentifying Request Revisions\nlet VNCoreMLRequestRevision1: Int\nA constant for specifying revision 1 of a Core ML request.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nMachine Learning Image Analysis\nClassifying Images with Vision and Core ML\nCrop and scale photos using the Vision framework and classify them with a Core ML model.\nTraining a Create ML Model to Classify Flowers\nTrain a flower classifier using Create ML in Swift Playgrounds, and apply the resulting model to real-time image classification using Vision.\nclass VNClassificationObservation\nAn object that represents classification information that an image analysis request produces.\nclass VNPixelBufferObservation\nAn object that represents an image that an image analysis request produces.\nclass VNCoreMLFeatureValueObservation\nAn object that represents a collection of key-value information that a Core ML image analysis request produces."
  },
  {
    "title": "Analyzing Image Similarity with Feature Print | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/analyzing_image_similarity_with_feature_print",
    "html": "Overview\n\nNote\n\nFor more information about this sample code project, see WWDC 2019 Session 222: Understanding Images in Vision Framework.\n\nSee Also\nStill Image Analysis\nDetecting Objects in Still Images\nLocate and demarcate rectangles, faces, barcodes, and text in images using the Vision framework.\nClassifying Images for Categorization and Search\nAnalyze and label images using a Vision classification request.\nclass VNRequest\nThe abstract superclass for analysis requests.\nclass VNImageBasedRequest\nThe abstract superclass for image analysis requests that focus on a specific part of an image.\nclass VNClassifyImageRequest\nA request to classify an image.\nclass VNGenerateImageFeaturePrintRequest\nAn image-based request to generate feature prints from an image.\nclass VNImageRequestHandler\nAn object that processes one or more image analysis requests pertaining to a single image.\nclass VNObservation\nThe abstract superclass for analysis results."
  },
  {
    "title": "recognizedPoint(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/4173232-recognizedpoint",
    "html": "Parameters\njointName\n\nThe joint name to retrieve.\n\nReturn Value\n\nThe point for the joint name.\n\nSee Also\nAccessing Points\nvar availableJointNames: [VNHumanBodyPose3DObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nvar availableJointsGroupNames: [VNHumanBodyPose3DObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose.\nfunc recognizedPoints(VNHumanBodyPose3DObservation.JointsGroupName) -> [VNHumanBodyPose3DObservation.JointName : VNHumanBodyRecognizedPoint3D]\nReturns a collection of points for the group name you specify."
  },
  {
    "title": "rightLeg | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointsgroupname/4173311-rightleg",
    "html": "See Also\nGetting the Group Names\nstatic let all: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents all joints.\nstatic let head: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the head joints.\nstatic let leftArm: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the left arm joints.\nstatic let leftLeg: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the left leg joints.\nstatic let rightArm: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the right arm joints.\nstatic let torso: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the torso joints."
  },
  {
    "title": "pointInImage(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/4200087-pointinimage",
    "html": "Parameters\njointName\n\nThe name of the human body joint.\n\nReturn Value\n\nA projection of the 3D position onto the original 2D image in normalized, lower left origin coordinates."
  },
  {
    "title": "VNHumanBodyPose3DObservation.HeightEstimation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/heightestimation",
    "html": "Topics\nTechniques\ncase measured\nA technique that uses LiDAR depth data to measure body height, in meters.\ncase reference\nA technique that uses a reference height.\nRelationships\nConforms To\nSendable\nSee Also\nGetting the Body Height\nvar heightEstimation: VNHumanBodyPose3DObservation.HeightEstimation\nThe technique the framework uses to estimate body height.\nvar bodyHeight: Float\nThe estimated human body height, in meters."
  },
  {
    "title": "cameraRelativePosition(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/4226655-camerarelativeposition",
    "html": "Parameters\njointName\n\nThe name of the human body joint.\n\nReturn Value\n\nThe joint position, in meters.\n\nSee Also\nGetting the Camera Position\nvar cameraOriginMatrix: simd_float4x4\nA transform from the skeleton hip to the camera."
  },
  {
    "title": "bodyHeight | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/4210410-bodyheight",
    "html": "Discussion\n\nThe framework returns an accurate height if heightEstimation is VNHumanBodyPose3DObservation.HeightEstimation.measured; otherwise, it returns a VNHumanBodyPose3DObservation.HeightEstimation.reference height.\n\nSee Also\nGetting the Body Height\nvar heightEstimation: VNHumanBodyPose3DObservation.HeightEstimation\nThe technique the framework uses to estimate body height.\nenum VNHumanBodyPose3DObservation.HeightEstimation\nConstants that identify body height estimation techniques."
  },
  {
    "title": "knownAnimalIdentifiers(forRevision:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizeanimalsrequest/3366120-knownanimalidentifiers",
    "html": "Deprecated\n\nUse supportedIdentifiers() instead.\n\nParameters\nrequestRevision\n\nThe revision of the animal recognition request.\n\nReturn Value\n\nThe animal identifiers.\n\nSee Also\nIdentifying Animals\nfunc supportedIdentifiers() -> [VNAnimalIdentifier]\nReturns the identifiers of the animals that the request detects.\nstruct VNAnimalIdentifier\nAn animal identifier string."
  },
  {
    "title": "VNDetectFaceCaptureQualityRequestRevision3 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacecapturequalityrequestrevision3",
    "html": "See Also\nVision Framework Version and Revision Numbers\nvar VNVisionVersionNumber: Double\nThe current version number of the Vision framework.\nlet VNDetectAnimalBodyPoseRequestRevision1: Int\nA value that indicates the first revision for an animal body pose request.\nlet VNDetectHumanBodyPose3DRequestRevision1: Int\nA value that indicates the first revision for a human three-dimensional body pose request.\nlet VNTrackHomographicImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a homographic image registration request.\nlet VNTrackTranslationalImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a translational image registration request.\nlet VNTrackOpticalFlowRequestRevision1: Int\nA value that indicates the first revision for an optial flow request.\nlet VNClassifyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an attention saliency image request.\nlet VNGenerateImageFeaturePrintRequestRevision2: Int\nA value that indicates the second revision for a feature print request.\nlet VNDetectBarcodesRequestRevision4: Int\nA value that indicates the fourth revision for a barcode request."
  },
  {
    "title": "VNGenerateAttentionBasedSaliencyImageRequestRevision2 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateattentionbasedsaliencyimagerequestrevision2",
    "html": "See Also\nVision Framework Version and Revision Numbers\nvar VNVisionVersionNumber: Double\nThe current version number of the Vision framework.\nlet VNDetectAnimalBodyPoseRequestRevision1: Int\nA value that indicates the first revision for an animal body pose request.\nlet VNDetectHumanBodyPose3DRequestRevision1: Int\nA value that indicates the first revision for a human three-dimensional body pose request.\nlet VNTrackHomographicImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a homographic image registration request.\nlet VNTrackTranslationalImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a translational image registration request.\nlet VNTrackOpticalFlowRequestRevision1: Int\nA value that indicates the first revision for an optial flow request.\nlet VNClassifyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateImageFeaturePrintRequestRevision2: Int\nA value that indicates the second revision for a feature print request.\nlet VNDetectFaceCaptureQualityRequestRevision3: Int\nA value that indicates the third revision for a face capture quality request.\nlet VNDetectBarcodesRequestRevision4: Int\nA value that indicates the fourth revision for a barcode request."
  },
  {
    "title": "VNGenerateObjectnessBasedSaliencyImageRequestRevision2 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateobjectnessbasedsaliencyimagerequestrevision2",
    "html": "See Also\nVision Framework Version and Revision Numbers\nvar VNVisionVersionNumber: Double\nThe current version number of the Vision framework.\nlet VNDetectAnimalBodyPoseRequestRevision1: Int\nA value that indicates the first revision for an animal body pose request.\nlet VNDetectHumanBodyPose3DRequestRevision1: Int\nA value that indicates the first revision for a human three-dimensional body pose request.\nlet VNTrackHomographicImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a homographic image registration request.\nlet VNTrackTranslationalImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a translational image registration request.\nlet VNTrackOpticalFlowRequestRevision1: Int\nA value that indicates the first revision for an optial flow request.\nlet VNClassifyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an attention saliency image request.\nlet VNGenerateImageFeaturePrintRequestRevision2: Int\nA value that indicates the second revision for a feature print request.\nlet VNDetectFaceCaptureQualityRequestRevision3: Int\nA value that indicates the third revision for a face capture quality request.\nlet VNDetectBarcodesRequestRevision4: Int\nA value that indicates the fourth revision for a barcode request."
  },
  {
    "title": "VNDetectBarcodesRequestRevision4 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectbarcodesrequestrevision4",
    "html": "See Also\nVision Framework Version and Revision Numbers\nvar VNVisionVersionNumber: Double\nThe current version number of the Vision framework.\nlet VNDetectAnimalBodyPoseRequestRevision1: Int\nA value that indicates the first revision for an animal body pose request.\nlet VNDetectHumanBodyPose3DRequestRevision1: Int\nA value that indicates the first revision for a human three-dimensional body pose request.\nlet VNTrackHomographicImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a homographic image registration request.\nlet VNTrackTranslationalImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a translational image registration request.\nlet VNTrackOpticalFlowRequestRevision1: Int\nA value that indicates the first revision for an optial flow request.\nlet VNClassifyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an attention saliency image request.\nlet VNGenerateImageFeaturePrintRequestRevision2: Int\nA value that indicates the second revision for a feature print request.\nlet VNDetectFaceCaptureQualityRequestRevision3: Int\nA value that indicates the third revision for a face capture quality request."
  },
  {
    "title": "VNGenerateImageFeaturePrintRequestRevision2 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateimagefeatureprintrequestrevision2",
    "html": "See Also\nVision Framework Version and Revision Numbers\nvar VNVisionVersionNumber: Double\nThe current version number of the Vision framework.\nlet VNDetectAnimalBodyPoseRequestRevision1: Int\nA value that indicates the first revision for an animal body pose request.\nlet VNDetectHumanBodyPose3DRequestRevision1: Int\nA value that indicates the first revision for a human three-dimensional body pose request.\nlet VNTrackHomographicImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a homographic image registration request.\nlet VNTrackTranslationalImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a translational image registration request.\nlet VNTrackOpticalFlowRequestRevision1: Int\nA value that indicates the first revision for an optial flow request.\nlet VNClassifyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an attention saliency image request.\nlet VNDetectFaceCaptureQualityRequestRevision3: Int\nA value that indicates the third revision for a face capture quality request.\nlet VNDetectBarcodesRequestRevision4: Int\nA value that indicates the fourth revision for a barcode request."
  },
  {
    "title": "minimumLatencyFrameCount | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnstatefulrequest/3564829-minimumlatencyframecount",
    "html": "Discussion\n\nVideo-based requests often need a minimum number of frames before they can report an observation. For example, a movement detection request requires a minimum of five frames before it can generate an observation. This value indicates how responsive a request is at processing incoming data.\n\nSee Also\nConfiguring the Request\nvar frameAnalysisSpacing: CMTime\nA time value that indicates the interval between analysis operations."
  },
  {
    "title": "init(frameAnalysisSpacing:completionHandler:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnstatefulrequest/3564828-init",
    "html": "Parameters\nframeAnalysisSpacing\n\nA CMTime value that indicates the duration between analysis operations. Increase this value to reduce the number of frames analyzed on slower devices. Set this argument to zero to analyze all frames.\n\ncompletionHandler\n\nA closure that’s invoked after the request has completed its processing. The system invokes the completion handler on the same dispatch queue as the request performs its processing."
  },
  {
    "title": "VNClassifyImageRequestRevision2 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnclassifyimagerequestrevision2",
    "html": "See Also\nVision Framework Version and Revision Numbers\nvar VNVisionVersionNumber: Double\nThe current version number of the Vision framework.\nlet VNDetectAnimalBodyPoseRequestRevision1: Int\nA value that indicates the first revision for an animal body pose request.\nlet VNDetectHumanBodyPose3DRequestRevision1: Int\nA value that indicates the first revision for a human three-dimensional body pose request.\nlet VNTrackHomographicImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a homographic image registration request.\nlet VNTrackTranslationalImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a translational image registration request.\nlet VNTrackOpticalFlowRequestRevision1: Int\nA value that indicates the first revision for an optial flow request.\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an attention saliency image request.\nlet VNGenerateImageFeaturePrintRequestRevision2: Int\nA value that indicates the second revision for a feature print request.\nlet VNDetectFaceCaptureQualityRequestRevision3: Int\nA value that indicates the third revision for a face capture quality request.\nlet VNDetectBarcodesRequestRevision4: Int\nA value that indicates the fourth revision for a barcode request."
  },
  {
    "title": "VNTrackOpticalFlowRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackopticalflowrequestrevision1",
    "html": "See Also\nVision Framework Version and Revision Numbers\nvar VNVisionVersionNumber: Double\nThe current version number of the Vision framework.\nlet VNDetectAnimalBodyPoseRequestRevision1: Int\nA value that indicates the first revision for an animal body pose request.\nlet VNDetectHumanBodyPose3DRequestRevision1: Int\nA value that indicates the first revision for a human three-dimensional body pose request.\nlet VNTrackHomographicImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a homographic image registration request.\nlet VNTrackTranslationalImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a translational image registration request.\nlet VNClassifyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an attention saliency image request.\nlet VNGenerateImageFeaturePrintRequestRevision2: Int\nA value that indicates the second revision for a feature print request.\nlet VNDetectFaceCaptureQualityRequestRevision3: Int\nA value that indicates the third revision for a face capture quality request.\nlet VNDetectBarcodesRequestRevision4: Int\nA value that indicates the fourth revision for a barcode request."
  },
  {
    "title": "VNTrackTranslationalImageRegistrationRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntracktranslationalimageregistrationrequestrevision1",
    "html": "See Also\nVision Framework Version and Revision Numbers\nvar VNVisionVersionNumber: Double\nThe current version number of the Vision framework.\nlet VNDetectAnimalBodyPoseRequestRevision1: Int\nA value that indicates the first revision for an animal body pose request.\nlet VNDetectHumanBodyPose3DRequestRevision1: Int\nA value that indicates the first revision for a human three-dimensional body pose request.\nlet VNTrackHomographicImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a homographic image registration request.\nlet VNTrackOpticalFlowRequestRevision1: Int\nA value that indicates the first revision for an optial flow request.\nlet VNClassifyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an attention saliency image request.\nlet VNGenerateImageFeaturePrintRequestRevision2: Int\nA value that indicates the second revision for a feature print request.\nlet VNDetectFaceCaptureQualityRequestRevision3: Int\nA value that indicates the third revision for a face capture quality request.\nlet VNDetectBarcodesRequestRevision4: Int\nA value that indicates the fourth revision for a barcode request."
  },
  {
    "title": "VNTrackHomographicImageRegistrationRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackhomographicimageregistrationrequestrevision1",
    "html": "See Also\nVision Framework Version and Revision Numbers\nvar VNVisionVersionNumber: Double\nThe current version number of the Vision framework.\nlet VNDetectAnimalBodyPoseRequestRevision1: Int\nA value that indicates the first revision for an animal body pose request.\nlet VNDetectHumanBodyPose3DRequestRevision1: Int\nA value that indicates the first revision for a human three-dimensional body pose request.\nlet VNTrackTranslationalImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a translational image registration request.\nlet VNTrackOpticalFlowRequestRevision1: Int\nA value that indicates the first revision for an optial flow request.\nlet VNClassifyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an attention saliency image request.\nlet VNGenerateImageFeaturePrintRequestRevision2: Int\nA value that indicates the second revision for a feature print request.\nlet VNDetectFaceCaptureQualityRequestRevision3: Int\nA value that indicates the third revision for a face capture quality request.\nlet VNDetectBarcodesRequestRevision4: Int\nA value that indicates the fourth revision for a barcode request."
  },
  {
    "title": "VNErrorDomain | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrordomain",
    "html": "See Also\nErrors\nenum VNErrorCode\nConstants that identify errors from the framework."
  },
  {
    "title": "VNErrorCode | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnerrorcode",
    "html": "Topics\nError Codes\ncase turiCoreErrorCode\nAn error occurred during Create ML training due to an invalid transformation or image.\ncase OK\nThe operation finished without error.\ncase dataUnavailable\nThe data isn’t available.\ncase internalError\nAn internal error occurred within the framework.\ncase invalidArgument\nAn app passed an invalid parameter to a request.\ncase invalidFormat\nThe format of the image is invalid.\ncase invalidImage\nThe image is invalid.\ncase invalidModel\nThe Core ML model is incompatible with the request.\ncase invalidOperation\nAn app requested an unsupported operation.\ncase invalidOption\nAn app specified an invalid option on a request.\ncase ioError\nAn I/O error for an image, image sequence, or Core ML model.\ncase missingOption\nA request is missing a required option.\ncase notImplemented\nThe method isn’t implemented in the underlying model.\ncase operationFailed\nThe requested operation failed.\ncase outOfBoundsError\nAn app attempted to access data that’s out-of-bounds.\ncase outOfMemory\nThe system doesn’t have enough memory to complete the request.\ncase requestCancelled\nAn app canceled the request.\ncase timeStampNotFound\nThe system can’t find a timestamp.\ncase unknownError\nAn unidentified error occurred.\ncase unsupportedRevision\nAn app specified an unsupported request revision.\ncase unsupportedRequest\nAn app attempted an unsupported request.\ncase unsupportedComputeDevice\nAn app requested an unsupported compute device.\ncase unsupportedComputeStage\nAn app requested an unsupported compute stage.\ncase timeout\nThe requested operation timed out.\nRelationships\nConforms To\nSendable\nSee Also\nErrors\nlet VNErrorDomain: String\nThe domain of errors that the framework generates."
  },
  {
    "title": "VNDetectAnimalBodyPoseRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectanimalbodyposerequestrevision1",
    "html": "See Also\nVision Framework Version and Revision Numbers\nvar VNVisionVersionNumber: Double\nThe current version number of the Vision framework.\nlet VNDetectHumanBodyPose3DRequestRevision1: Int\nA value that indicates the first revision for a human three-dimensional body pose request.\nlet VNTrackHomographicImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a homographic image registration request.\nlet VNTrackTranslationalImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a translational image registration request.\nlet VNTrackOpticalFlowRequestRevision1: Int\nA value that indicates the first revision for an optial flow request.\nlet VNClassifyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an attention saliency image request.\nlet VNGenerateImageFeaturePrintRequestRevision2: Int\nA value that indicates the second revision for a feature print request.\nlet VNDetectFaceCaptureQualityRequestRevision3: Int\nA value that indicates the third revision for a face capture quality request.\nlet VNDetectBarcodesRequestRevision4: Int\nA value that indicates the fourth revision for a barcode request."
  },
  {
    "title": "VNVector | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvector",
    "html": "Topics\nCreating a Vector\ninit(byAdding: VNVector, to: VNVector)\nCreates a new vector by adding the specified vectors.\ninit(bySubtracting: VNVector, from: VNVector)\nCreates a new vector by subtracting the first vector from the second vector.\ninit(byMultiplying: VNVector, byScalar: Double)\nCreates a new vector by multiplying the specified vector’s x-axis and y-axis projections by the scalar value.\ninit(r: Double, theta: Double)\nCreates a new vector in polar coordinate space.\ninit(vectorHead: VNPoint, tail: VNPoint)\nCreates a new vector in Cartesian coordinate space.\ninit(xComponent: Double, yComponent: Double)\nCreates a new vector in Cartesian coordinate space, based on its x-axis and y-axis projections.\nclass var zero: VNVector\nA vector object with zero length.\nInspecting a Vector\nvar length: Double\nThe length, or absolute value, of the vector.\nvar r: Double\nThe radius, absolute value, or length of the vector.\nvar theta: Double\nThe angle between the vector direction and the positive direction of the x-axis.\nvar squaredLength: Double\nThe squared length of the vector.\nvar x: Double\nA signed projection that indicates the vector’s direction on the x-axis.\nvar y: Double\nA signed projection that indicates the vector’s direction on the y-axis.\nclass func dotProduct(of: VNVector, vector: VNVector) -> Double\nCaclulates the dot product of two vectors.\nclass func unitVector(for: VNVector) -> VNVector\nCalculates a vector that’s normalized by preserving its direction, so that the vector length equals 1.0.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nSee Also\nCommon Data Types\nclass VNCircle\nAn immutable, two-dimensional circle represented by its center point and radius."
  },
  {
    "title": "VNVisionVersionNumber | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnvisionversionnumber",
    "html": "See Also\nVision Framework Version and Revision Numbers\nlet VNDetectAnimalBodyPoseRequestRevision1: Int\nA value that indicates the first revision for an animal body pose request.\nlet VNDetectHumanBodyPose3DRequestRevision1: Int\nA value that indicates the first revision for a human three-dimensional body pose request.\nlet VNTrackHomographicImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a homographic image registration request.\nlet VNTrackTranslationalImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a translational image registration request.\nlet VNTrackOpticalFlowRequestRevision1: Int\nA value that indicates the first revision for an optial flow request.\nlet VNClassifyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an attention saliency image request.\nlet VNGenerateImageFeaturePrintRequestRevision2: Int\nA value that indicates the second revision for a feature print request.\nlet VNDetectFaceCaptureQualityRequestRevision3: Int\nA value that indicates the third revision for a face capture quality request.\nlet VNDetectBarcodesRequestRevision4: Int\nA value that indicates the fourth revision for a barcode request."
  },
  {
    "title": "VNGeometryUtils | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeometryutils",
    "html": "Topics\nCalculating Bounding Circles\nclass func boundingCircle(for: VNContour) -> VNCircle\nCalculates a bounding circle for the specified contour object.\nclass func boundingCircle(for: [VNPoint]) -> VNCircle\nCalculates a bounding circle for the specified array of points.\nclass func boundingCircle(forSIMDPoints: UnsafePointer<simd_float2>, pointCount: Int) -> VNCircle\nCalculates a bounding circle for the specified points.\nCalculating Area and Perimeter\nclass func calculateArea(UnsafeMutablePointer<Double>, for: VNContour, orientedArea: Bool)\nCalculates the area for the specified contour.\nclass func calculatePerimeter(UnsafeMutablePointer<Double>, for: VNContour)\nCalculates the perimeter of a closed contour.\nRelationships\nInherits From\nNSObject\nSee Also\nUtilities\nstruct VNComputeStage\nTypes that represent the compute stage.\nclass VNVideoProcessor\nAn object that performs offline analysis of video content."
  },
  {
    "title": "VNCircle | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncircle",
    "html": "Topics\nCreating a Circle\ninit(center: VNPoint, radius: Double)\nCreates a circle with the specified center and radius.\ninit(center: VNPoint, diameter: Double)\nCreates a circle with the specified center and diameter.\nclass var zero: VNCircle\nA circle object centered at the origin, with a radius of zero.\nInspecting a Circle\nvar center: VNPoint\nThe circle’s center point.\nvar diameter: Double\nThe circle’s diameter.\nvar radius: Double\nThe circle’s radius.\nfunc contains(VNPoint) -> Bool\nDetermines if this circle, including its boundary, contains the specified point.\nfunc contains(VNPoint, inCircumferentialRingOfWidth: Double) -> Bool\nDetermines if a ring around this circle’s circumference contains the specified point.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nSee Also\nCommon Data Types\nclass VNVector\nAn immutable, two-dimensional vector represented by its x-axis and y-axis projections."
  },
  {
    "title": "VNNormalizedFaceBoundingBoxPointForLandmarkPoint(_:_:_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/2908994-vnnormalizedfaceboundingboxpoint",
    "html": "Parameters\nfaceLandmarkPoint\n\nThe location of the face landmark, as returned from a VNFaceLandmarkRegion2D instance.\n\nfaceBoundingBox\n\nThe normalized bounding box rect around the face, as obtained from a VNFaceObservation instance.\n\nimageWidth\n\nThe width of the image from which the VNFaceObservation instance was generated.\n\nimageHeight\n\nThe height of the image from which the VNFaceObservation instance was generated.\n\nReturn Value\n\nThe input point projected into normalized bounding box coordinates.\n\nSee Also\nCoordinate Conversion\nfunc VNImagePointForNormalizedPoint(CGPoint, Int, Int) -> CGPoint\nProjects a point in normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePoint(CGPoint, Int, Int) -> CGPoint\nProjects a point from image coordinates into normalized coordinates.\nfunc VNImagePointForNormalizedPointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the image coordinates space into normalized coordinates.\nlet VNNormalizedIdentityRect: CGRect\nA normalized identity rectangle with an origin of zero and unit length and width.\nfunc VNNormalizedRectIsIdentityRect(CGRect) -> Bool\nReturns a Boolean value that indicates whether the rectangle has an origin of zero and unit length and width.\nfunc VNImagePointForFaceLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the image coordinates of a specified face landmark point."
  },
  {
    "title": "VNImagePointForFaceLandmarkPoint(_:_:_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/2908996-vnimagepointforfacelandmarkpoint",
    "html": "Parameters\nfaceLandmarkPoint\n\nThe location of the face landmark, as returned from a VNFaceLandmarkRegion2D instance.\n\nfaceBoundingBox\n\nThe normalized bounding box rect around the face, as obtained from a VNFaceObservation.\n\nimageWidth\n\nThe width of the image from which the VNFaceObservation was generated.\n\nimageHeight\n\nThe height of the image from which the VNFaceObservation was generated.\n\nReturn Value\n\nThe input face landmark point, projected into image coordinates.\n\nDiscussion\n\nThe resulting point in image coordinate space may have nonintegral (floating-point) coordinates.\n\nSee Also\nCoordinate Conversion\nfunc VNImagePointForNormalizedPoint(CGPoint, Int, Int) -> CGPoint\nProjects a point in normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePoint(CGPoint, Int, Int) -> CGPoint\nProjects a point from image coordinates into normalized coordinates.\nfunc VNImagePointForNormalizedPointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the image coordinates space into normalized coordinates.\nlet VNNormalizedIdentityRect: CGRect\nA normalized identity rectangle with an origin of zero and unit length and width.\nfunc VNNormalizedRectIsIdentityRect(CGRect) -> Bool\nReturns a Boolean value that indicates whether the rectangle has an origin of zero and unit length and width.\nfunc VNNormalizedFaceBoundingBoxPointForLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the coordinates of a specified face landmark point, in bounding box coordinates."
  },
  {
    "title": "VNNormalizedIdentityRect | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnnormalizedidentityrect",
    "html": "See Also\nCoordinate Conversion\nfunc VNImagePointForNormalizedPoint(CGPoint, Int, Int) -> CGPoint\nProjects a point in normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePoint(CGPoint, Int, Int) -> CGPoint\nProjects a point from image coordinates into normalized coordinates.\nfunc VNImagePointForNormalizedPointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the image coordinates space into normalized coordinates.\nfunc VNNormalizedRectIsIdentityRect(CGRect) -> Bool\nReturns a Boolean value that indicates whether the rectangle has an origin of zero and unit length and width.\nfunc VNImagePointForFaceLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the image coordinates of a specified face landmark point.\nfunc VNNormalizedFaceBoundingBoxPointForLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the coordinates of a specified face landmark point, in bounding box coordinates."
  },
  {
    "title": "VNImageRectForNormalizedRectUsingRegionOfInterest(_:_:_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/3751017-vnimagerectfornormalizedrectusin",
    "html": "Parameters\nnormalizedRect\n\nThe rectangle in normalized coordinates.\n\nimageWidth\n\nThe width of the image.\n\nimageHeight\n\nThe height of the image.\n\nroi\n\nThe region of interest within the normalized-coordinate space.\n\nReturn Value\n\nA rectangle in the image-coordinate space.\n\nSee Also\nCoordinate Conversion\nfunc VNImagePointForNormalizedPoint(CGPoint, Int, Int) -> CGPoint\nProjects a point in normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePoint(CGPoint, Int, Int) -> CGPoint\nProjects a point from image coordinates into normalized coordinates.\nfunc VNImagePointForNormalizedPointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from image coordinates into normalized coordinates.\nfunc VNNormalizedRectForImageRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the image coordinates space into normalized coordinates.\nlet VNNormalizedIdentityRect: CGRect\nA normalized identity rectangle with an origin of zero and unit length and width.\nfunc VNNormalizedRectIsIdentityRect(CGRect) -> Bool\nReturns a Boolean value that indicates whether the rectangle has an origin of zero and unit length and width.\nfunc VNImagePointForFaceLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the image coordinates of a specified face landmark point.\nfunc VNNormalizedFaceBoundingBoxPointForLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the coordinates of a specified face landmark point, in bounding box coordinates."
  },
  {
    "title": "VNNormalizedRectIsIdentityRect(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/2908998-vnnormalizedrectisidentityrect",
    "html": "Parameters\nnormalizedRect\n\nNormalized input rect to test for identity.\n\nReturn Value\n\ntrue if the rectangle is the identity rectangle, otherwise false.\n\nSee Also\nCoordinate Conversion\nfunc VNImagePointForNormalizedPoint(CGPoint, Int, Int) -> CGPoint\nProjects a point in normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePoint(CGPoint, Int, Int) -> CGPoint\nProjects a point from image coordinates into normalized coordinates.\nfunc VNImagePointForNormalizedPointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the image coordinates space into normalized coordinates.\nlet VNNormalizedIdentityRect: CGRect\nA normalized identity rectangle with an origin of zero and unit length and width.\nfunc VNImagePointForFaceLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the image coordinates of a specified face landmark point.\nfunc VNNormalizedFaceBoundingBoxPointForLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the coordinates of a specified face landmark point, in bounding box coordinates."
  },
  {
    "title": "VNNormalizedRectForImageRectUsingRegionOfInterest(_:_:_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/3751019-vnnormalizedrectforimagerectusin",
    "html": "Parameters\nimageRect\n\nThe rectangle in image coordinates.\n\nimageWidth\n\nThe width of the image.\n\nimageHeight\n\nThe height of the image.\n\nroi\n\nThe region of interest within the image-coordinate space.\n\nReturn Value\n\nA rectangle in the normalized-coordinate space.\n\nSee Also\nCoordinate Conversion\nfunc VNImagePointForNormalizedPoint(CGPoint, Int, Int) -> CGPoint\nProjects a point in normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePoint(CGPoint, Int, Int) -> CGPoint\nProjects a point from image coordinates into normalized coordinates.\nfunc VNImagePointForNormalizedPointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the normalized coordinates into image coordinates.\nlet VNNormalizedIdentityRect: CGRect\nA normalized identity rectangle with an origin of zero and unit length and width.\nfunc VNNormalizedRectIsIdentityRect(CGRect) -> Bool\nReturns a Boolean value that indicates whether the rectangle has an origin of zero and unit length and width.\nfunc VNImagePointForFaceLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the image coordinates of a specified face landmark point.\nfunc VNNormalizedFaceBoundingBoxPointForLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the coordinates of a specified face landmark point, in bounding box coordinates."
  },
  {
    "title": "VNDetectHorizonRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthorizonrequest",
    "html": "Topics\nAccessing the Results\nvar results: [VNHorizonObservation]?\nThe results of the horizon detection request.\nIdentifying Request Revisions\nlet VNDetectHorizonRequestRevision1: Int\nA constant for specifying revision 1 of the horizon detection request.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nHorizon Detection\nclass VNHorizonObservation\nThe horizon angle information that an image analysis request detects."
  },
  {
    "title": "VNNormalizedRectForImageRect(_:_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/2908992-vnnormalizedrectforimagerect",
    "html": "Parameters\nimageRect\n\nThe input rectangle in image coordinate space.\n\nimageWidth\n\nThe width of the image in whose coordinates the input rect resides.\n\nimageHeight\n\nThe height of the image in whose coordinates the input rect resides.\n\nReturn Value\n\nThe input rectangle projected into normalized coordinates.\n\nSee Also\nCoordinate Conversion\nfunc VNImagePointForNormalizedPoint(CGPoint, Int, Int) -> CGPoint\nProjects a point in normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePoint(CGPoint, Int, Int) -> CGPoint\nProjects a point from image coordinates into normalized coordinates.\nfunc VNImagePointForNormalizedPointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from normalized coordinates into image coordinates.\nfunc VNImageRectForNormalizedRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the image coordinates space into normalized coordinates.\nlet VNNormalizedIdentityRect: CGRect\nA normalized identity rectangle with an origin of zero and unit length and width.\nfunc VNNormalizedRectIsIdentityRect(CGRect) -> Bool\nReturns a Boolean value that indicates whether the rectangle has an origin of zero and unit length and width.\nfunc VNImagePointForFaceLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the image coordinates of a specified face landmark point.\nfunc VNNormalizedFaceBoundingBoxPointForLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the coordinates of a specified face landmark point, in bounding box coordinates."
  },
  {
    "title": "Structuring Recognized Text on a Document | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/structuring_recognized_text_on_a_document",
    "html": "Overview\n\nNote\n\nFor more information about this sample code project, see WWDC 2019 Session 234: Text Recognition in Vision Framework.\n\nSee Also\nText Recognition\nRecognizing Text in Images\nAdd text-recognition features to your app using the Vision framework.\nExtracting phone numbers from text in images\nAnalyze and filter phone numbers from text in live capture by using Vision.\nLocating and Displaying Recognized Text\nConfigure and perform text recognition on images to identify their textual content.\nclass VNRecognizeTextRequest\nAn image analysis request that finds and recognizes text in an image.\nclass VNRecognizedTextObservation\nA request that detects and recognizes regions of text in an image."
  },
  {
    "title": "VNImageRectForNormalizedRect(_:_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/2908993-vnimagerectfornormalizedrect",
    "html": "Parameters\nnormalizedRect\n\nThe input rectangle in normalized coordinate space.\n\nimageWidth\n\nThe width of the image into whose coordinate space you’re projecting the input rect.\n\nimageHeight\n\nThe height of the image into whose coordinate space you’re projecting the input rect.\n\nReturn Value\n\nThe input rect projected into image (pixel) coordinates.\n\nSee Also\nCoordinate Conversion\nfunc VNImagePointForNormalizedPoint(CGPoint, Int, Int) -> CGPoint\nProjects a point in normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePoint(CGPoint, Int, Int) -> CGPoint\nProjects a point from image coordinates into normalized coordinates.\nfunc VNImagePointForNormalizedPointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the image coordinates into normalized coordinates.\nfunc VNNormalizedRectForImageRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the image coordinates space into normalized coordinates.\nlet VNNormalizedIdentityRect: CGRect\nA normalized identity rectangle with an origin of zero and unit length and width.\nfunc VNNormalizedRectIsIdentityRect(CGRect) -> Bool\nReturns a Boolean value that indicates whether the rectangle has an origin of zero and unit length and width.\nfunc VNImagePointForFaceLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the image coordinates of a specified face landmark point.\nfunc VNNormalizedFaceBoundingBoxPointForLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the coordinates of a specified face landmark point, in bounding box coordinates."
  },
  {
    "title": "VNFaceLandmarkRegion2D | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfacelandmarkregion2d",
    "html": "Overview\n\nThis class represents the set of all facial landmark regions in 2D, exposed as properties.\n\nTopics\nDescribing Region Points\nvar pointsClassification: VNPointsClassification\nAn enumeration that describes how to interpret the points the region provides.\nenum VNPointsClassification\nThe set of classifications that describe how to interpret the points the region provides.\nSpecifying Region Properties\nvar normalizedPoints: [CGPoint]\nThe array of normalized landmark points.\nvar precisionEstimatesPerPoint: [Float]?\nRequests an array of precision estimates for each landmark point.\nComputing Feature Points\nfunc pointsInImage(imageSize: CGSize) -> [CGPoint]\nReturns an array containing landmark points in the coordinate space of the specified image size.\nRelationships\nInherits From\nVNFaceLandmarkRegion\nSee Also\nIdentifying Landmarks\nvar landmarks: VNFaceLandmarks2D?\nThe facial features of the detected face.\nclass VNFaceLandmarks2D\nA collection of facial features that a request detects.\nclass VNFaceLandmarks\nThe abstract superclass for containers of face landmark information.\nclass VNFaceLandmarkRegion\nThe abstract superclass for information about a specific face landmark."
  },
  {
    "title": "VNFaceObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnfaceobservation",
    "html": "Overview\n\nThis type of observation results from a VNDetectFaceRectanglesRequest. It contains information about facial landmarks and regions it finds in the image.\n\nTopics\nCreating an Observation\ninit(requestRevision: Int, boundingBox: CGRect, roll: NSNumber?, yaw: NSNumber?, pitch: NSNumber?)\nCreates an observation that contains the roll, yaw, and pitch of the face.\ninit(requestRevision: Int, boundingBox: CGRect, roll: NSNumber?, yaw: NSNumber?)\nCreates an observation that contains the roll and yaw of the face.\nDeprecated\nIdentifying Landmarks\nvar landmarks: VNFaceLandmarks2D?\nThe facial features of the detected face.\nclass VNFaceLandmarks2D\nA collection of facial features that a request detects.\nclass VNFaceLandmarkRegion2D\n2D geometry information for a specific facial feature.\nclass VNFaceLandmarks\nThe abstract superclass for containers of face landmark information.\nclass VNFaceLandmarkRegion\nThe abstract superclass for information about a specific face landmark.\nDetermining Facial Orientation\nvar roll: NSNumber?\nThe roll angle of a face in radians.\nvar yaw: NSNumber?\nThe yaw angle of a face in radians.\nvar pitch: NSNumber?\nThe pitch angle of a face in radians.\nDetermining Capture Quality\nvar faceCaptureQuality: Float?\nA value that indicates the quality of the face capture.\nRelationships\nInherits From\nVNDetectedObjectObservation\nSee Also\nAccessing the Results\nvar results: [VNFaceObservation]?\nThe results of the face landmarks request."
  },
  {
    "title": "results | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest/2867238-results",
    "html": "Discussion\n\nIf the request fails, this property is nil. Otherwise, it contains an array of VNObservation subclasses specific to the VNRequest subclass.\n\nDon’t access this property until the request has finished processing.\n\nSee Also\nConfiguring a Request\nvar completionHandler: VNRequestCompletionHandler?\nThe completion handler the system invokes after the request finishes processing.\nvar preferBackgroundProcessing: Bool\nA hint to minimize the resource burden of the request.\nvar revision: Int\nThe specific algorithm or implementation revision that’s used to perform the request.\nvar usesCPUOnly: Bool\nA Boolean signifying that the Vision request should execute exclusively on the CPU.\nDeprecated"
  },
  {
    "title": "perform(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler/2880297-perform",
    "html": "Parameters\nrequests\n\nAn array of Vision requests to perform.\n\nDiscussion\n\nThe function returns after all requests have either completed or failed. Check individual requests and errors for their respective successes and failures."
  },
  {
    "title": "leftKnee | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4173295-leftknee",
    "html": "See Also\nGetting the Leg Joint Names\nstatic let leftHip: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left hip.\nstatic let rightHip: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right hip.\nstatic let rightKnee: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right knee.\nstatic let leftAnkle: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left ankle.\nstatic let rightAnkle: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right ankle."
  },
  {
    "title": "rightWrist | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname/4210418-rightwrist",
    "html": "See Also\nGetting the Arm Joint Names\nstatic let centerShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the point between the shoulders.\nstatic let leftShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left shoulder.\nstatic let rightShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right shoulder.\nstatic let leftElbow: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left elbow.\nstatic let rightElbow: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right elbow.\nstatic let leftWrist: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left wrist."
  },
  {
    "title": "Recognizing Text in Images | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/recognizing_text_in_images",
    "html": "Overview\n\nOne of Vision’s many powerful features is its ability to detect and recognize multilanguage text in images. You can use this functionality in your own apps to handle both real-time and offline use cases. In all cases, all of Vision’s processing happens on the user’s device to enhance performance and user privacy.\n\nVision’s text-recognition capabilities operate using one of these paths:\n\nFast\n\nThe fast path uses the framework’s character-detection capabilities to find individual characters, and then uses a small machine learning model to recognize individual characters and words. This approach is similar to traditional optical character recognition (OCR).\n\nFor example code using the fast path, see Extracting phone numbers from text in images.\n\nAccurate\n\nThe accurate path uses a neural network to find text in terms of strings and lines, and then performs further analysis to find individual words and sentences. This approach is much more in line with how humans read text.\n\nFor example code using the accurate path, see Structuring Recognized Text on a Document.\n\nUsing either path, you may optionally apply a language-correction phase based on Natural Language Processing (NLP) to minimize the potential for misreadings.\n\nNote\n\nUsing Vision’s text-recognition features is similar to performing other Vision operations, where you perform computer vision requests on an image and retrieve the resulting observations. If you’re new to the Vision framework, see Detecting Objects in Still Images.\n\nPerform a Text-Recognition Request\n\nVision provides its text-recognition capabilities through VNRecognizeTextRequest, an image-based request type that finds and extracts text in images. The following example shows how to use VNImageRequestHandler to perform a VNRecognizeTextRequest for recognizing text in the specified CGImage.\n\n// Get the CGImage on which to perform requests.\nguard let cgImage = UIImage(named: \"snapshot\")?.cgImage else { return }\n\n\n// Create a new image-request handler.\nlet requestHandler = VNImageRequestHandler(cgImage: cgImage)\n\n\n// Create a new request to recognize text.\nlet request = VNRecognizeTextRequest(completionHandler: recognizeTextHandler)\n\n\ndo {\n    // Perform the text-recognition request.\n    try requestHandler.perform([request])\n} catch {\n    print(\"Unable to perform the requests: \\(error).\")\n}\n\n\nNote\n\nVNRecognizeTextRequest uses the accurate path by default. To select the fast path, set the request’s recognitionLevel property to VNRequestTextRecognitionLevel.fast.\n\nProcess the Results\n\nAfter the request handler processes the request, it calls the request’s completion closure, passing it the request and any errors that occurred. Retrieve the observations by querying the request object for its results, which it returns as an array of VNRecognizedTextObservation objects. Each observation provides the recognized text string, along with a confidence score that indicates the confidence in the accuracy of the recognition.\n\nfunc recognizeTextHandler(request: VNRequest, error: Error?) {\n    guard let observations =\n            request.results as? [VNRecognizedTextObservation] else {\n        return\n    }\n    let recognizedStrings = observations.compactMap { observation in\n        // Return the string of the top VNRecognizedText instance.\n        return observation.topCandidates(1).first?.string\n    }\n    \n    // Process the recognized strings.\n    processResults(recognizedStrings)\n}\n\n\nIf you’d like to render the bounding rectangles around recognized text in your user interface, you can also retrieve that information from the observation. The rectangles it provides are in normalized coordinates. To render them correctly in your user interface, convert CGRect instances from normalized coordinates to image coordinates by using the VNImageRectForNormalizedRect(_:_:_:) function as shown below.\n\nlet boundingRects: [CGRect] = observations.compactMap { observation in\n\n\n    // Find the top observation.\n    guard let candidate = observation.topCandidates(1).first else { return .zero }\n    \n    // Find the bounding-box observation for the string range.\n    let stringRange = candidate.string.startIndex..<candidate.string.endIndex\n    let boxObservation = try? candidate.boundingBox(for: stringRange)\n    \n    // Get the normalized CGRect value.\n    let boundingBox = boxObservation?.boundingBox ?? .zero\n    \n    // Convert the rectangle from normalized coordinates to image coordinates.\n    return VNImageRectForNormalizedRect(boundingBox,\n                                        Int(image.size.width),\n                                        Int(image.size.height))\n}\n\n\nThe resulting bounding box differs depending on the path you choose. The fast path calculates the recognized text’s bounding rectangle based on its individual characters. The accurate path tokenizes on whitespace, which means when working with Chinese text, the resulting bounding boxes will likely contain lines or line fragments instead of complete text.\n\nOptimize Language Settings\n\nYour choice of fast or accurate path, along with your use of a particular API revision, determines the language support the text-recognition algorithms provide. To determine which languages a particular path and revision support, call the request’s supportedRecognitionLanguages(for:revision:) class method.\n\nIf not otherwise specified, Vision biases its results toward English. To alter its default behavior, provide an array of supported languages in the request’s recognitionLanguages property. The order in which you provide the languages dictates their relative importance. To recognize traditional and simplified Chinese, specify zh-Hant and zh-Hans as the first elements in the request’s recognitionLanguages property. English is the only other language that you can pair with Chinese.\n\nEnabling language correction on the request helps minimize common recognition errors. If the text you’re recognizing uses domain-specific jargon, such as medical or technical terms, you can tailor the language correction’s behavior by setting the request’s customWords property. Language correction gives precedence to the custom words when performing its processing. The request ignores the customWords property if language correction isn’t enabled.\n\nNote\n\nVision doesn’t support language correction, or its related customWords array, for Chinese.\n\nSee Also\nText Recognition\nStructuring Recognized Text on a Document\nDetect, recognize, and structure text on a business card or receipt using Vision and VisionKit.\nExtracting phone numbers from text in images\nAnalyze and filter phone numbers from text in live capture by using Vision.\nLocating and Displaying Recognized Text\nConfigure and perform text recognition on images to identify their textual content.\nclass VNRecognizeTextRequest\nAn image analysis request that finds and recognizes text in an image.\nclass VNRecognizedTextObservation\nA request that detects and recognizes regions of text in an image."
  },
  {
    "title": "VNDetectFaceLandmarksRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacelandmarksrequest",
    "html": "Overview\n\nBy default, a face landmarks request first locates all faces in the input image, then analyzes each to detect facial features.\n\nIf you’ve already located all the faces in an image, or want to detect landmarks in only a subset of the faces in the image, set the inputFaceObservations property to an array of VNFaceObservation objects representing the faces you want to analyze. You can either use face observations output by a VNDetectFaceRectanglesRequest or manually create VNFaceObservation instances with the bounding boxes of the faces you want to analyze.\n\nTopics\nConfiguring a Face Landmarks Request\nprotocol VNFaceObservationAccepting\nAn image analysis request that operates on face observations.\nAccessing the Results\nvar results: [VNFaceObservation]?\nThe results of the face landmarks request.\nclass VNFaceObservation\nFace or facial-feature information that an image analysis request detects.\nLocating Face Landmarks\nvar constellation: VNRequestFaceLandmarksConstellation\nA variable that describes how a face landmarks request orders or enumerates the resulting features.\nenum VNRequestFaceLandmarksConstellation\nAn enumeration of face landmarks in a constellation object.\nIdentifying Request Revisions\nclass func revision(Int, supportsConstellation: VNRequestFaceLandmarksConstellation) -> Bool\nReturns a Boolean value that indicates whether a revision supports a constellation.\nlet VNDetectFaceLandmarksRequestRevision3: Int\nA constant for specifying revision 3 of the face landmarks detection request.\nlet VNDetectFaceLandmarksRequestRevision2: Int\nA constant for specifying revision 2 of the face landmarks detection request.\nlet VNDetectFaceLandmarksRequestRevision1: Int\nA constant for specifying revision 1 of the face landmarks detection request.\nDeprecated\nRelationships\nInherits From\nVNImageBasedRequest\nConforms To\nVNFaceObservationAccepting\nSee Also\nFace and Body Detection\nSelecting a selfie based on capture quality\nCompare face-capture quality in a set of images by using Vision.\nclass VNDetectFaceCaptureQualityRequest\nA request that produces a floating-point number that represents the capture quality of a face in a photo.\nclass VNDetectFaceRectanglesRequest\nA request that finds faces within an image.\nclass VNDetectHumanRectanglesRequest\nA request that finds rectangular regions that contain people in an image.\nclass VNHumanObservation\nAn object that represents a person that the request detects."
  },
  {
    "title": "VNTrackOpticalFlowRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackopticalflowrequest",
    "html": "Overview\n\nThis request works at the pixel level, so both images must have the same dimensions to successfully perform the request.\n\nSetting a region of interest isolates where to perform the change determination.\n\nImportant\n\nOptical flow requests are very resource intensive, so perform only one request at a time. Release memory immediately after generating an optical flow.\n\nTopics\nCreating an Optical Flow\ninit()\nCreates a new request that tracks the optical from one image to another.\ninit(completionHandler: VNRequestCompletionHandler?)\nCreates a new request that tracks the optical from one image to another, with a system callback on completion.\nConfiguring the Request\nvar computationAccuracy: VNTrackOpticalFlowRequest.ComputationAccuracy\nThe level of accuracy to compute the optical flow.\nenum VNTrackOpticalFlowRequest.ComputationAccuracy\nComputational accuracy options.\nvar keepNetworkOutput: Bool\nA Boolean value that indicates the raw pixel buffer continues to emit from the network.\nvar outputPixelFormat: OSType\nThe pixel format type of the output value.\nAccessing the Results\nvar results: [VNPixelBufferObservation]?\nThe optical flow results the request observes.\nRelationships\nInherits From\nVNStatefulRequest\nSee Also\nOptical Flow\nclass VNGenerateOpticalFlowRequest\nAn object that generates directional change vectors for each pixel in the targeted image."
  },
  {
    "title": "Detecting human body poses in 3D with Vision | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/detecting_human_body_poses_in_3d_with_vision",
    "html": "Overview\n\nNote\n\nThis sample code project is associated with WWDC23 session 111241: Explore 3D body pose and person segmentation in Vision.\n\nConfigure the sample code project\n\nBefore you run the sample code project in Xcode, ensure you’re using an iOS device with an A12 chip or later. The input image should have all limbs of the subject visible.\n\nNote\n\nDue to a behavior change with cameraOriginMatrix API, if this sample project is run on a device on a build earlier than beta 3, camera position will be rotated 180 degrees.\n\nSee Also\n3D Body Pose Detection\nIdentifying 3D human body poses in images\nDetect three-dimensional human body poses using the Vision framework.\nclass VNDetectHumanBodyPose3DRequest\nA request that detects points on human bodies in three-dimensional space, relative to the camera.\nclass VNHumanBodyPose3DObservation\nAn observation that provides the three-dimensional body points the request recognizes.\nclass VNRecognizedPoints3DObservation\nAn observation that provides the three-dimensional points for a request.\nclass VNHumanBodyRecognizedPoint3D\nA recognized three-dimensional point that includes a parent joint.\nclass VNPoint3D\nAn object that represents a three-dimensional point in an image.\nclass VNRecognizedPoint3D\nA three-dimensional point that includes an identifier to the point.\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose."
  },
  {
    "title": "VNPoint | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpoint",
    "html": "Topics\nCreating a Point\ninit(x: Double, y: Double)\nCreates a point object with the specified coordinates.\ninit(location: CGPoint)\nCreates a point object from the specified Core Graphics point.\nclass func apply(VNVector, to: VNPoint) -> VNPoint\nCreates a point object that’s shifted by the X and Y offsets of the specified vector.\nclass var zero: VNPoint\nA point object that represents the origin.\nInspecting a Point\nvar x: Double\nThe x-coordinate.\nvar y: Double\nThe y-coordinate.\nvar location: CGPoint\nThe Core Graphics point for this point.\nCalculating Distance\nfunc distance(VNPoint) -> Double\nReturns the distance to another point.\nclass func distance(VNPoint, VNPoint) -> Double\nCalculates the distance between two points.\nDeprecated\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nSee Also\nBody and Hand Pose Detection\nDetecting Human Body Poses in Images\nAdd the capability to detect human body poses to your app using the Vision framework.\nDetecting Hand Poses with Vision\nCreate a virtual drawing app by using Vision’s capability to detect hand poses.\nclass VNDetectHumanBodyPoseRequest\nA request that detects a human body pose.\nclass VNDetectHumanHandPoseRequest\nA request that detects a human hand pose.\nclass VNRecognizedPointsObservation\nAn observation that provides the points the analysis recognized.\nclass VNHumanBodyPoseObservation\nAn observation that provides the body points the analysis recognized.\nclass VNHumanHandPoseObservation\nAn observation that provides the hand points the analysis recognized.\nclass VNDetectedPoint\nAn object that represents a normalized point in an image, along with a confidence value.\nclass VNRecognizedPoint\nAn object that represents a normalized point in an image, along with an identifier label and a confidence value.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys.\nstruct VNRecognizedPointGroupKey\nThe data type for all recognized point group keys."
  },
  {
    "title": "VNGenerateOpticalFlowRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateopticalflowrequest",
    "html": "Overview\n\nThis request operates at a pixel level, so both images need to have the same dimensions to successfully perform the analysis. Setting a region of interest limits the region in which the analysis occurs. However, the system reports the resulting observation at full resolution.\n\nOptical flow requests are resource-intensive, so create only one request at a time, and release it immediately after generating optical flows.\n\nTopics\nConfiguring the Request\nvar computationAccuracy: VNGenerateOpticalFlowRequest.ComputationAccuracy\nThe accuracy level for computing optical flow.\nenum VNGenerateOpticalFlowRequest.ComputationAccuracy\nThe supported optical flow accuracy levels.\nvar outputPixelFormat: OSType\nThe output buffer’s pixel format.\nvar keepNetworkOutput: Bool\nA Boolean value that indicates whether to keep the raw pixel buffer coming from the machine learning network.\nAccessing the Results\nvar results: [VNPixelBufferObservation]?\nThe results of the request to generate optical flow.\nIdentifying Request Revisions\nlet VNGenerateOpticalFlowRequestRevision2: Int\nA constant for specifying revision 2 of the optical flow generation request.\nlet VNGenerateOpticalFlowRequestRevision1: Int\nA constant for specifying revision 1 of the optical flow generation request.\nRelationships\nInherits From\nVNTargetedImageRequest\nSee Also\nOptical Flow\nclass VNTrackOpticalFlowRequest\nAn object that determines the direction change of vectors for each pixel from a previous to current image."
  },
  {
    "title": "Detecting Hand Poses with Vision | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/detecting_hand_poses_with_vision",
    "html": "Overview\n\nNote\n\nThis sample code project is associated with WWDC20 session 10653: Detect Body and Hand Pose with Vision.\n\nSee Also\nBody and Hand Pose Detection\nDetecting Human Body Poses in Images\nAdd the capability to detect human body poses to your app using the Vision framework.\nclass VNDetectHumanBodyPoseRequest\nA request that detects a human body pose.\nclass VNDetectHumanHandPoseRequest\nA request that detects a human hand pose.\nclass VNRecognizedPointsObservation\nAn observation that provides the points the analysis recognized.\nclass VNHumanBodyPoseObservation\nAn observation that provides the body points the analysis recognized.\nclass VNHumanHandPoseObservation\nAn observation that provides the hand points the analysis recognized.\nclass VNPoint\nAn immutable object that represents a single, two-dimensional point in an image.\nclass VNDetectedPoint\nAn object that represents a normalized point in an image, along with a confidence value.\nclass VNRecognizedPoint\nAn object that represents a normalized point in an image, along with an identifier label and a confidence value.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys.\nstruct VNRecognizedPointGroupKey\nThe data type for all recognized point group keys."
  },
  {
    "title": "VNTrackRectangleRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackrectanglerequest",
    "html": "Overview\n\nUse this type of request to track the bounding boxes of rectangles throughout a sequence of images. Vision returns locations for rectangles found in all orientations and sizes.\n\nTopics\nInitializing a Rectangle Tracking Request\ninit(rectangleObservation: VNRectangleObservation)\nCreates a new rectangle tracking request with a rectangle observation.\ninit(rectangleObservation: VNRectangleObservation, completionHandler: VNRequestCompletionHandler?)\nCreates a new rectangle tracking request with a rectangle observation.\nIdentifying Request Revisions\nlet VNTrackRectangleRequestRevision1: Int\nA constant for specifying revision 1 of the rectangling tracking request.\nRelationships\nInherits From\nVNTrackingRequest\nSee Also\nObject Tracking\nTracking the User’s Face in Real Time\nDetect and track faces from the selfie cam feed in real time.\nTracking Multiple Objects or Rectangles in Video\nApply Vision algorithms to track objects or rectangles throughout a video.\nclass VNTrackingRequest\nThe abstract superclass for image analysis requests that track unique features across multiple images or video frames.\nclass VNTrackObjectRequest\nAn image analysis request that tracks the movement of a previously identified object across multiple images or video frames.\nclass VNDetectedObjectObservation\nAn observation that provides the position and extent of an image feature that an image analysis request detects."
  },
  {
    "title": "Tracking the User’s Face in Real Time | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/tracking_the_user_s_face_in_real_time",
    "html": "Overview\n\nThe Vision framework can detect and track rectangles, faces, and other salient objects across a sequence of images.\n\nThis sample shows how to create requests to track human faces and interpret the results of those requests. In order to visualize the geometry of observed facial features, the code draws paths around the primary detected face and its most prominent features.\n\nThe sample app applies computer vision algorithms to find a face in the provided image. Once it finds a face, it attempts to track that face across subsequent frames of the video. Finally, it draws a green box around the observed face, as well as yellow paths outlining facial features, on Core Animation layers.\n\nTo see this sample app in action, build and run the project on iOS 11. Grant the app permission to use the camera.\n\nConfigure the Camera to Capture Video\n\nThis section shows how to set up a camera capture session using delegates to prepare images for Vision. Configuring the camera involves the following steps.\n\nFirst, create a new AVCaptureSession to represent video capture. Channel its output through AVCaptureSession.\n\nQuery the user’s input device and configure it for video data output by specifying its resolution and camera.\n\nNext, create a serial dispatch queue. This queue ensures that video frames, received asynchronously through delegate callback methods, are delivered in order. Establish a capture session with AVMediaType video and set its device and resolution.\n\nFinally, designate the video’s preview layer and add it to your view hierarchy, so the camera knows where to display video frames as they are captured.\n\nMost of this code is boilerplate setup that enables you to handle video input properly. Tweak the values only if you choose a different camera arrangement.\n\nParse Face Detection Results\n\nYou can provide a completion handler for a Vision request handler to execute when it finishes. The completion handler indicates whether the request succeeded or resulted in an error. If the request succeeded, its results property contains data specific to the type of request that you can use to identify the object’s location and bounding box.\n\nFor face rectangle requests, the VNFaceObservation provided via callback includes a bounding box for each detected face. The sample uses this bounding box to draw paths around each of the detected face landmarks on top of the preview image.\n\nlet faceDetectionRequest = VNDetectFaceRectanglesRequest(completionHandler: { (request, error) in\n    \n    if error != nil {\n        print(\"FaceDetection error: \\(String(describing: error)).\")\n    }\n    \n    guard let faceDetectionRequest = request as? VNDetectFaceRectanglesRequest,\n        let results = faceDetectionRequest.results as? [VNFaceObservation] else {\n            return\n    }\n    DispatchQueue.main.async {\n        // Add the observations to the tracking list\n        for observation in results {\n            let faceTrackingRequest = VNTrackObjectRequest(detectedObjectObservation: observation)\n            requests.append(faceTrackingRequest)\n        }\n        self.trackingRequests = requests\n    }\n})\n\n\nIn addition to drawing paths on CALayer to visualize the feature, you can access specific facial-feature data such as eye, pupil, nose, and lip classifications in the face observation’s landmarks property. Your app can leverage this information to track the user’s face and apply custom effects. For a face landmarks request, the face rectangle detector will also run implicitly.\n\nPreprocess Images\n\nPerform any image preprocessing in the delegate method fileOutput(_:didOutputSampleBuffer:from:). In this delegate method, create a pixel buffer to hold image contents, determine the device’s orientation, and check whether you have a face to track.\n\nBefore the Vision framework can track an object, it must first know which object to track. Determine which face to track by creating a VNImageRequestHandler and passing it a still image frame. In the case of video, submit individual frames to the request handler as they arrive in the delegate method fileOutput(_:didOutputSampleBuffer:from:).\n\nlet imageRequestHandler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer,\n                                                orientation: exifOrientation,\n                                                options: requestHandlerOptions)\n\n\ndo {\n    guard let detectRequests = self.detectionRequests else {\n        return\n    }\n    try imageRequestHandler.perform(detectRequests)\n} catch let error as NSError {\n    NSLog(\"Failed to perform FaceRectangleRequest: %@\", error)\n}\n\n\nThe VNImageRequestHandler handles detection of faces and objects in still images, but it doesn’t carry information from one frame to the next. For tracking an object, create a VNSequenceRequestHandler, which can handle VNTrackObjectRequest.\n\nTrack the Detected Face\n\nOnce you have an observation from the image request handler’s face detection, input it to the sequence request handler.\n\ntry self.sequenceRequestHandler.perform(requests,\n                                         on: pixelBuffer,\n                                         orientation: exifOrientation)\n\n\nIf the detector hasn’t found a face, create an image request handler to detect a face. Once that detection succeeds, and you have a face observation, track it by creating a VNTrackObjectRequest.\n\n// Setup the next round of tracking.\nvar newTrackingRequests = [VNTrackObjectRequest]()\nfor trackingRequest in requests {\n    \n    guard let results = trackingRequest.results else {\n        return\n    }\n    \n    guard let observation = results[0] as? VNDetectedObjectObservation else {\n        return\n    }\n    \n    if !trackingRequest.isLastFrame {\n        if observation.confidence > 0.3 {\n            trackingRequest.inputObservation = observation\n        } else {\n            trackingRequest.isLastFrame = true\n        }\n        newTrackingRequests.append(trackingRequest)\n    }\n}\n\n\nThen call the sequence handler’s perform(_:) function. This method runs synchronously, so use a background queue to avoid blocking the main queue as it executes, and call back to the main queue only if you need to perform UI updates such as path drawing.\n\nSee Also\nObject Tracking\nTracking Multiple Objects or Rectangles in Video\nApply Vision algorithms to track objects or rectangles throughout a video.\nclass VNTrackingRequest\nThe abstract superclass for image analysis requests that track unique features across multiple images or video frames.\nclass VNTrackRectangleRequest\nAn image analysis request that tracks movement of a previously identified rectangular object across multiple images or video frames.\nclass VNTrackObjectRequest\nAn image analysis request that tracks the movement of a previously identified object across multiple images or video frames.\nclass VNDetectedObjectObservation\nAn observation that provides the position and extent of an image feature that an image analysis request detects."
  },
  {
    "title": "VNImageBasedRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagebasedrequest",
    "html": "Overview\n\nOther Vision request handlers that operate on still images inherit from this abstract base class. Don’t use it directly.\n\nTopics\nConfiguring a Request\nvar regionOfInterest: CGRect\nThe region of the image in which Vision will perform the request.\nRelationships\nInherits From\nVNRequest\nSee Also\nStill Image Analysis\nDetecting Objects in Still Images\nLocate and demarcate rectangles, faces, barcodes, and text in images using the Vision framework.\nClassifying Images for Categorization and Search\nAnalyze and label images using a Vision classification request.\nAnalyzing Image Similarity with Feature Print\nGenerate a feature print to compute distance between images.\nclass VNRequest\nThe abstract superclass for analysis requests.\nclass VNClassifyImageRequest\nA request to classify an image.\nclass VNGenerateImageFeaturePrintRequest\nAn image-based request to generate feature prints from an image.\nclass VNImageRequestHandler\nAn object that processes one or more image analysis requests pertaining to a single image.\nclass VNObservation\nThe abstract superclass for analysis results."
  },
  {
    "title": "VNNormalizedPointForImagePointUsingRegionOfInterest(_:_:_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/3751018-vnnormalizedpointforimagepointus",
    "html": "Parameters\nimagePoint\n\nThe input point in image coordinates.\n\nimageWidth\n\nThe width of the image.\n\nimageHeight\n\nThe height of the image.\n\nroi\n\nThe region of interest within the image-coordinate space.\n\nReturn Value\n\nA point in the normalized-coordinate space.\n\nSee Also\nCoordinate Conversion\nfunc VNImagePointForNormalizedPoint(CGPoint, Int, Int) -> CGPoint\nProjects a point in normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePoint(CGPoint, Int, Int) -> CGPoint\nProjects a point from image coordinates into normalized coordinates.\nfunc VNImagePointForNormalizedPointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the normalized coordinates into image coordinates.\nfunc VNImageRectForNormalizedRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the image coordinates space into normalized coordinates.\nlet VNNormalizedIdentityRect: CGRect\nA normalized identity rectangle with an origin of zero and unit length and width.\nfunc VNNormalizedRectIsIdentityRect(CGRect) -> Bool\nReturns a Boolean value that indicates whether the rectangle has an origin of zero and unit length and width.\nfunc VNImagePointForFaceLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the image coordinates of a specified face landmark point.\nfunc VNNormalizedFaceBoundingBoxPointForLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the coordinates of a specified face landmark point, in bounding box coordinates."
  },
  {
    "title": "VNImagePointForNormalizedPointUsingRegionOfInterest(_:_:_:_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/3751016-vnimagepointfornormalizedpointus",
    "html": "Parameters\nnormalizedPoint\n\nThe input point in normalized coordinates.\n\nimageWidth\n\nThe width of the image.\n\nimageHeight\n\nThe height of the image.\n\nroi\n\nThe region of interest within the normalized-coordinate space.\n\nReturn Value\n\nA point in the image-coordinate space.\n\nSee Also\nCoordinate Conversion\nfunc VNImagePointForNormalizedPoint(CGPoint, Int, Int) -> CGPoint\nProjects a point in normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePoint(CGPoint, Int, Int) -> CGPoint\nProjects a point from image coordinates into normalized coordinates.\nfunc VNNormalizedPointForImagePointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the image coordinates space into normalized coordinates.\nlet VNNormalizedIdentityRect: CGRect\nA normalized identity rectangle with an origin of zero and unit length and width.\nfunc VNNormalizedRectIsIdentityRect(CGRect) -> Bool\nReturns a Boolean value that indicates whether the rectangle has an origin of zero and unit length and width.\nfunc VNImagePointForFaceLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the image coordinates of a specified face landmark point.\nfunc VNNormalizedFaceBoundingBoxPointForLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the coordinates of a specified face landmark point, in bounding box coordinates."
  },
  {
    "title": "Building a feature-rich app for sports analysis | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/building_a_feature-rich_app_for_sports_analysis",
    "html": "Overview\n\nThe Action & Vision sample app leverages several capabilities available in Vision and Core ML in iOS 14 and later. The app provides an example of how you can use these technologies together to help players improve their bean-bag tossing performance.\n\nNote\n\nThis sample code project is associated with the WWDC20 session 10099: Explore the Action & Vision App.\n\nThe app analyzes player performance in a game of bean bag toss. In this easy-to-learn game, you throw bean bags at a 4 x 2-foot board that has a 6-inch hole in one end. You stand 25 feet away from the center hole of the board, and score points if you toss the bean bag onto the board, and score additional points if it goes through the hole. To make gameplay more interesting, the app adds a style dimension to the scoring. It analyzes your throw and adds additional points based on your throw style, including overhand, underhand, or underleg.\n\nAfter each throw, the app also provides real-time feedback that indicates your throw’s type, speed, and trajectory, along with your score.\n\nConfigure the project and prepare your environment\n\nYou need to run the sample app on a physical device with an A12 processor or later, running iOS 14.\n\nSome Vision algorithms require a stable scene with a fixed camera position and stationary background. To analyze your own gameplay, mount your iOS device to a tripod and keep it fixed on the field of play. Alternatively, try the app by downloading and analyzing a prerecorded video of a bean bag toss player in action.\n\nAnalyze the field of play\n\nBefore gameplay can begin, the app analyzes the scene to find the location of the gameboard. It detects its location by using an instance of VNCoreMLRequest to run inference on a custom object-detection model created using Create ML. The app has a model trained using photos of gameboards, taken from various angles and distances, in both indoor and outdoor environments. To further improve the model’s accuracy, the training included images with people in the frame, and bean bags on and around the board. The app performs the request and retrieves its top observation to find the gameboard’s bounding rectangle.\n\nNext, the app performs a VNDetectContoursRequest to detect the contours of the gameboard’s edges and uses them to determine the normalized pixel dimensions of the board, and the location of the hole. Because the app previously calculated the bounding rectangle of the detected board, it sets the bounding rectangle as the request’s region of interest, which helps the request significantly reduce noise and improve performance. Because the app knows the real-world size of the gameboard, it accurately determines the pixel dimensions of the field of play.\n\nTo learn more about about contour detection, see the WWDC20 session 10673: Explore Computer Vision APIs.\n\nDetermine scene stability\n\nSome Vision algorithms require a stable scene to produce accurate results. To determine scene stability, the app uses VNTranslationalImageRegistrationRequest, a request that compares two images to calculate the x-axis and y-axis shift between them. The app analyzes video frames until the shift reaches a minimum difference threshold, at which point the app considers the scene stable.\n\nIdentify the player\n\nWith the field of play established, the app is ready to start the game when a player enters the scene. To detect a player entering the frame, the app performs VNDetectHumanBodyPoseRequest, a request available in iOS 14, that detects key body points of the face, torso, arms, and legs. The app performs this request on each video frame, and uses the data points it produces to draw a bounding box around the player. The app also uses the detected body points to determine the throw type.\n\nFor more information about using human body-pose detection, see Detecting Human Body Poses in Images and the WWDC20 session 10653: Detect Body and Hand Pose with Vision.\n\nDetect the player’s throw trajectory\n\nThe app uses Vision’s VNDetectTrajectoriesRequest to analyze the player’s throw. This request follows objects moving on a parabolic path. Detecting a parabola requires more than a single data point, so this is a stateful request that builds evidence of the object’s movement over time. The app creates a single instance of the request and performs it multiple times over a sequence of video frames. After the request collects enough data points to recognize the trajectory, it calls its completion handler, and passes it the observations that contain the trajectory coordinates and timestamps. The app provides a data visualization by progressively drawing a line that traces the bag’s trajectory. When the request stops producing observations for more than 20 frames, the app considers the throw complete, and calculates the points that the player scored.\n\nThe app knows the pixel dimensions of the gameboard, and uses them as a reference to determine the distance of the player’s throw. Because the request also provides the detected trajectory’s duration, the app uses the time and distance to measure the throw’s speed.\n\nTo learn more about using VNDetectTrajectoriesRequest, see Identifying Trajectories in Video.\n\nNote\n\nVNDetectTrajectoriesRequest detects multiple, concurrent trajectories, so an app needs to apply its own business logic to filter the trajectories to only those of interest. For example, the sample app knows the location of the gameboard, and it discards any trajectories that move in the opposite direction.\n\nDetermine the throw type\n\nTo determine the throw type, the app uses another custom Core ML model, one trained using the Action Classification template available in Create ML. The app’s model training used video clips of people performing overhand, underhand, and underleg throws, and also included a negative training set with players performing actions other than throwing.\n\nAs discussed previously, the app uses VNDetectHumanBodyPoseRequest to detect the player’s location. In each of the observations the request returns, the request also provides the normalized coordinates for various detected body points. The app retrieves these points from the request as a Core ML MLMultiArray and passes them as inputs to the Action Classifier. The model runs its predictions and returns the labeled results, which indicate the throw type, and a confidence value in the accuracy of the prediction.\n\nTo learn more about Action Classification, see Creating an Action Classifier Model."
  },
  {
    "title": "VNCoreMLFeatureValueObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncoremlfeaturevalueobservation",
    "html": "Overview\n\nThis type of observation results from performing a VNCoreMLRequest image analysis with a Core ML model whose role is prediction rather than classification or image-to-image processing.\n\nVision infers that an MLModel object is a predictor model if that model predicts multiple features. You can tell that a model predicts multiple features when its modelDescription object has a nil value for its predictedFeatureName property, or when it inserts its output in an outputDescriptionsByName dictionary.\n\nTopics\nObtaining Feature Values\nvar featureValue: MLFeatureValue\nThe feature result of a VNCoreMLRequest that outputs neither a classification nor an image.\nvar featureName: String\nThe name used in the model description of the CoreML model that produced this observation.\nRelationships\nInherits From\nVNObservation\nSee Also\nMachine Learning Image Analysis\nClassifying Images with Vision and Core ML\nCrop and scale photos using the Vision framework and classify them with a Core ML model.\nTraining a Create ML Model to Classify Flowers\nTrain a flower classifier using Create ML in Swift Playgrounds, and apply the resulting model to real-time image classification using Vision.\nclass VNCoreMLRequest\nAn image analysis request that uses a Core ML model to process images.\nclass VNClassificationObservation\nAn object that represents classification information that an image analysis request produces.\nclass VNPixelBufferObservation\nAn object that represents an image that an image analysis request produces."
  },
  {
    "title": "VNPixelBufferObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpixelbufferobservation",
    "html": "Overview\n\nThis type of observation results from performing a VNCoreMLRequest image analysis with a Core ML model that has an image-to-image processing role. For example, this observation might result from a model that analyzes the style of one image and then transfers that style to a different image.\n\nVision infers that an MLModel object is an image-to-image model if that model includes an image. Its modelDescription object includes an image-typed feature description in its outputDescriptionsByName dictionary.\n\nTopics\nParsing Observation Content\nvar pixelBuffer: CVPixelBuffer\nThe image that results from a request with image output.\nvar featureName: String?\nA feature name that the CoreML model defines.\nRelationships\nInherits From\nVNObservation\nSee Also\nMachine Learning Image Analysis\nClassifying Images with Vision and Core ML\nCrop and scale photos using the Vision framework and classify them with a Core ML model.\nTraining a Create ML Model to Classify Flowers\nTrain a flower classifier using Create ML in Swift Playgrounds, and apply the resulting model to real-time image classification using Vision.\nclass VNCoreMLRequest\nAn image analysis request that uses a Core ML model to process images.\nclass VNClassificationObservation\nAn object that represents classification information that an image analysis request produces.\nclass VNCoreMLFeatureValueObservation\nAn object that represents a collection of key-value information that a Core ML image analysis request produces."
  },
  {
    "title": "Training a Create ML Model to Classify Flowers | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/training_a_create_ml_model_to_classify_flowers",
    "html": "Overview\n\nTo classify images in real time, you need a classification model with the categories you’d like identified, and a way to capture images to feed to the classifier.\n\nThis sample code project contains two components: a Create ML model you train in Swift Playgrounds, and the iOS app, FlowerShop, which you use to classify different flower types. This project uses the same code as the robot shop demo in the WWDC 2018 session Vision with Core ML.\n\nGetting Started\n\nTo see Create ML in action, run the Swift Playground Training/ImageClassifierPlayground.playground in Xcode 10 on a Mac running macOS 10.14 or later. This Swift Playground performs classification on the training set and generates the Create ML model ImageClassifier.mlmodel.\n\nThe sample app, FlowerShop, requires the following:\n\nXcode 10.\n\nAn iOS device running iOS 12 or later.\n\nBecause Xcode can’t access the camera, FlowerShop won’t work in Simulator.\n\nTrain a Custom Classifier on an Organized Image Set\n\nThe sample images that Create ML uses to train the custom classifier were taken with a set of categories in mind. For your own app, decide on a set of classification labels before preparing images. Take the following measures to improve your data set:\n\nAim for a minimum of 10 images per category—the more, the better.\n\nAvoid highly unbalanced datasets by preparing a roughly equal number between categories.\n\nMake your model more robust by enabling the Create ML UI’s Augmentation options: Crop, Rotate, Blur, Expose, Noise, and Flip.\n\nInclude redundancy in your training set: Take lots of images at different angles, on different backgrounds, and in different lighting conditions. Simulate real-world camera capture, including noise and motion blur.\n\nPhotograph sample objects in your hand to simulate real-world users that try to classify objects in their hands.\n\nRemove other objects, especially ones that you’d like to classify differently, from view.\n\nIf you’re using photos taken on an iOS device to train your model, you can use the macOS utility, Image Capture, to import the images onto your computer, and do the following:\n\nIn Xcode, open ImageClassifierPlayground.playground and display the Assistant Editor.\n\nClick Run on the last line of the Swift Playground; this opens the Create ML training environment.\n\nPlace the training images you’d like to use into named folders (such as Agapanthus).\n\nDrag the set of folders into the Assistant Editor to perform image training.\n\nFor more information about configuring the resultant model, as well as screenshots of the Create ML UI, see Creating an Image Classifier Model.\n\nCreate ML exports its trained results as a .mlmodel file, which you can import into your app in Xcode. After importing the model, you can examine the prototypical image size by opening the model file in Xcode’s navigation menu. For example, a parameter such as “Color 299 x 299” indicates the size of the training image. You can also confirm the size of the model.\n\nBuild an iOS App Around the Classifier\n\nThe app leverages the trained model and uses Vision for both registration and classification:\n\nIt performs registration on subsequent video frame buffers to deem when the user is still enough for image capture.\n\nWhen the user is holding the camera sufficiently still, it performs image classification on the frame, attempting to identify the focused object as one of the categories in the Create ML classifier.\n\nIf the confidence score associated with a classifier exceeds a high confidence threshold of 0.9, the app shows its most confident classification through an overlay.\n\nUse Registration for Scene Stability\n\nRegistration takes and aligns two images to determine the relative difference. Vision’s registration operation uses an inexpensive, fast algorithm that tells the app if the subject is still and stable. Theoretically, the app could make a classification request on every frame buffer, but classification is a computationally expensive operation—so attempting to classify every frame could result in delays and poor performance with the UI. Classify the scene in a frame only if the registration algorithm determines that the scene and camera are still, indicating the user’s intent to classify an object.\n\nThe FlowerShop app uses VNSequenceRequestHandler with VNTranslationalImageRegistrationRequest objects to compare consecutive frames, keeping a history of 15 frames. This amount of history amounts to half a second of capture at 30 frames per second and carries no special significance beyond empirical tuning. It takes the result of a request as alignmentObservation.alignmentTransform to determine if the scene is stable enough to perform classification. Check for scene stability by performing a request on the sequence request handler:\n\nlet registrationRequest = VNTranslationalImageRegistrationRequest(targetedCVPixelBuffer: pixelBuffer)\n\n\nThis algorithm deems a scene to be stable if the Manhattan distance between frames is less than 20:\n\nfileprivate func sceneStabilityAchieved() -> Bool {\n    // Determine if we have enough evidence of stability.\n    if transpositionHistoryPoints.count == maximumHistoryLength {\n        // Calculate the moving average.\n        var movingAverage: CGPoint = CGPoint.zero\n        for currentPoint in transpositionHistoryPoints {\n            movingAverage.x += currentPoint.x\n            movingAverage.y += currentPoint.y\n        }\n        let distance = abs(movingAverage.x) + abs(movingAverage.y)\n        if distance < 20 {\n            return true\n        }\n    }\n    return false\n}\n\n\nAfter registration has determined that the scene is longer varying, the app sends the stable frame to Vision for Core ML classification:\n\nif self.sceneStabilityAchieved() {\n    showDetectionOverlay(true)\n    if currentlyAnalyzedPixelBuffer == nil {\n        // Retain the image buffer for Vision processing.\n        currentlyAnalyzedPixelBuffer = pixelBuffer\n        analyzeCurrentImage()\n    }\n} else {\n    showDetectionOverlay(false)\n}\n\nPerform Image Classification\n\nThe sample app wraps two request objects—a barcode detection request and an image classification request—in a single request execution so Vision can perform them together. Performing the combined request is faster than performing separate requests, since Vision can share the same visual data between both.\n\nClassification contains a setup stage and a performance stage. The setup stage involves initializing requests for the types of objects you’d like Vision to detect and defining completion handlers to tell the app how to handle detection results after the requests finish their work.\n\nThe sample code sets up both a classification request and a barcode detection request. FlowerShop uses barcode identification to label an object—fertilizer—for which it has no training data. For example, the curator of a museum exhibit or owner of a flower shop can place the barcode beside or in place of an actual item, so that scanning the barcode classifies the item.\n\nBy using it as a proxy for the actual item, the app can still provide a confident classification even if the user doesn’t scan the actual item. This kind of proxy works particularly well for items that Create ML may have trouble training through images, such as fertilizer, gasoline, transparent gases, and clear liquids. Set up this kind of barcode detection using a VNDetectBarcodesRequest object:\n\nlet barcodeDetection = VNDetectBarcodesRequest(completionHandler: { (request, error) in\n    if let results = request.results as? [VNBarcodeObservation] {\n        if let mainBarcode = results.first {\n            if let payloadString = mainBarcode.payloadStringValue {\n                self.showProductInfo(payloadString)\n            }\n        }\n    }\n})\nself.analysisRequests = ([barcodeDetection])\n\n\n// Setup a classification request.\nguard let modelURL = Bundle.main.url(forResource: \"FlowerShop\", withExtension: \"mlmodelc\") else {\n    return NSError(domain: \"VisionViewController\", code: -1, userInfo: [NSLocalizedDescriptionKey: \"The model file is missing.\"])\n}\nguard let objectRecognition = createClassificationRequest(modelURL: modelURL) else {\n    return NSError(domain: \"VisionViewController\", code: -1, userInfo: [NSLocalizedDescriptionKey: \"The classification request failed.\"])\n}\nself.analysisRequests.append(objectRecognition)\n\n\nThe sample appends the normal model-based classification request to the same array. You can create both requests at once, but the sample code staggers the classification request to guard against failure to load the Core ML model. The classification request loads the Core ML classifier into a VNCoreMLRequest object:\n\nlet objectClassifier = try VNCoreMLModel(for: MLModel(contentsOf: modelURL))\nlet classificationRequest = VNCoreMLRequest(model: objectClassifier, completionHandler: { (request, error) in\n\n\nDefining the requests and completion handlers concludes the setup stage; the second stage performs identification in real time. The sample sends the stable frame to the classifier and tells Vision to perform classification by calling perform(_:):\n\nprivate func analyzeCurrentImage() {\n    // Most computer vision tasks are not rotation-agnostic, so it is important to pass in the orientation of the image with respect to device.\n    let orientation = exifOrientationFromDeviceOrientation()\n    \n    let requestHandler = VNImageRequestHandler(cvPixelBuffer: currentlyAnalyzedPixelBuffer!, orientation: orientation)\n    visionQueue.async {\n        do {\n            // Release the pixel buffer when done, allowing the next buffer to be processed.\n            defer { self.currentlyAnalyzedPixelBuffer = nil }\n            try requestHandler.perform(self.analysisRequests)\n        } catch {\n            print(\"Error: Vision request failed with error \\\"\\(error)\\\"\")\n        }\n    }\n}\n\n\nPerform tasks asynchronously on a background queue, so the camera and user interface can keep running unhindered. Don’t continuously queue up every buffer that the camera provides; instead, drop buffers to keep the pipeline moving. The app works with a queue of one buffer, skipping subsequent frames so long as it is still processing that buffer. When one request finishes, it queues the next buffer and submits a classification request.\n\nprivate let visionQueue = DispatchQueue(label: \"com.example.apple-samplecode.FlowerShop.serialVisionQueue\")\n\n\nEven if captured frames don’t match the size of the image under which you trained the Create ML model (299 × 299), the Vision framework crops and scales down its input images to match the model’s expected size on your behalf.\n\nInterpret Classification Results\n\nCheck the results in the request’s completion handler. When you create and pass in a request, you handle results and errors and show the classification results in your app’s UI.\n\nif let results = request.results as? [VNClassificationObservation] {\n    print(\"\\(results.first!.identifier) : \\(results.first!.confidence)\")\n    if results.first!.confidence > 0.9 {\n        self.showProductInfo(results.first!.identifier)\n    }\n}\n\n\nThe sample app sets a confidence threshold of 0.9—empirically tuned—to filter out false classifications. A score of 1.0 means only that the photo submitted for request satisfies the algorithm and trained classifier. The algorithm could output a score of 1.0 even when the classfication is wrong. When tuning your application for the optimal confidence threshold, use the output streamed to Xcode’s debugger window to gauge typical confidence values, making sure to note how far the confidence spikes on typical correct classifications. A white background with no object can still yield a confidence score of 0.6.\n\nThe sample shows the top result, but in a search app, you can rank the labels by confidence, from most confident classification to least. The array of confidence scores and classifications is available, so use more than the top result if it fits your app’s context. Try different thresholds to determine the best balance of reducing false positives and surfacing real-world results when they are correct; a result can be correct at a lower confidence score, like 0.8. Even though this app’s threshold is 0.9, the ideal threshold may vary from model to model.\n\nRelease Your Buffers\n\nAfter processing your buffers, be sure to release them to prevent them from queuing up. Because the input is a capture device that is constantly streaming frames, your app will run out of memory quickly if you don’t discard extra frames. The sample app limits the number of queued frame buffers to only one, which prevents overflow from happening and clears the buffer by setting it to nil:\n\nself.currentlyAnalyzedPixelBuffer = nil\n\nSee Also\nMachine Learning Image Analysis\nClassifying Images with Vision and Core ML\nCrop and scale photos using the Vision framework and classify them with a Core ML model.\nclass VNCoreMLRequest\nAn image analysis request that uses a Core ML model to process images.\nclass VNClassificationObservation\nAn object that represents classification information that an image analysis request produces.\nclass VNPixelBufferObservation\nAn object that represents an image that an image analysis request produces.\nclass VNCoreMLFeatureValueObservation\nAn object that represents a collection of key-value information that a Core ML image analysis request produces."
  },
  {
    "title": "VNClassificationObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnclassificationobservation",
    "html": "Overview\n\nThis type of observation results from performing a VNCoreMLRequest image analysis with a Core ML model whose role is classification (rather than prediction or image-to-image processing). Vision infers that an MLModel object is a classifier model if that model predicts a single feature. That is, the model’s modelDescription object has a non-nil value for its predictedFeatureName property.\n\nTopics\nDetermining Classification\nvar identifier: String\nClassification label identifying the type of observation.\nMeasuring Confidence and Precision\nvar hasPrecisionRecallCurve: Bool\nA Boolean variable indicating whether the observation contains precision and recall curves.\nfunc hasMinimumPrecision(Float, forRecall: Float) -> Bool\nDetermines whether the observation for a specific recall has a minimum precision value.\nfunc hasMinimumRecall(Float, forPrecision: Float) -> Bool\nDetermines whether the observation for a specific precision has a minimum recall value.\nRelationships\nInherits From\nVNObservation\nSee Also\nMachine Learning Image Analysis\nClassifying Images with Vision and Core ML\nCrop and scale photos using the Vision framework and classify them with a Core ML model.\nTraining a Create ML Model to Classify Flowers\nTrain a flower classifier using Create ML in Swift Playgrounds, and apply the resulting model to real-time image classification using Vision.\nclass VNCoreMLRequest\nAn image analysis request that uses a Core ML model to process images.\nclass VNPixelBufferObservation\nAn object that represents an image that an image analysis request produces.\nclass VNCoreMLFeatureValueObservation\nAn object that represents a collection of key-value information that a Core ML image analysis request produces."
  },
  {
    "title": "Classifying Images with Vision and Core ML | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/classifying_images_with_vision_and_core_ml",
    "html": "Overview\n\nThe app in this sample identifies the most prominent object in an image by using MobileNet, an open source image classifier model that recognizes around 1,000 different categories.\n\nEach time a user selects a photo from the library or takes a photo with a camera, the app passes it to a Vision image classification request. Vision resizes and crops the photo to meet the MobileNet model’s constraints for its image input, and then passes the photo to the model using the Core ML framework behind the scenes. Once the model generates a prediction, Vision relays it back to the app, which presents the results to the user.\n\nThe sample uses MobileNet as an example of how to use a third-party Core ML model. You can download open source models — including a newer version of MobileNet — on the Core ML model gallery.\n\nBefore you integrate a third-party model to solve a problem — which may increase the size of your app — consider using an API in the SDK. For example, the Vision framework’s VNClassifyImageRequest class offers the same functionality as MobileNet, but with potentially better performance and without increasing the size of your app (see Classifying Images for Categorization and Search).\n\nNote\n\nYou can make a custom image classifier that identifies your choice of object types with Create ML. See Creating an Image Classifier Model to learn how to create a custom image classifier that can replace the MobileNet model in this sample.\n\nConfigure the Sample Code Project\n\nThe sample targets iOS 14 or later, but the MobileNet model in the project works with:\n\niOS 11 or later\n\nmacOS 10.13 or later\n\nTo take photos within the app, run the sample on a device with a camera. Otherwise, you can select photos from the library in Simulator.\n\nNote\n\nAdd your own photos to the photo library in Simulator by dragging photos onto its window.\n\nCreate an Image Classifier Instance\n\nAt launch, the ImagePredictor class creates an image classifier singleton by calling its createImageClassifier() type method.\n\n/// - Tag: name\nstatic func createImageClassifier() -> VNCoreMLModel {\n    // Use a default model configuration.\n    let defaultConfig = MLModelConfiguration()\n\n\n    // Create an instance of the image classifier's wrapper class.\n    let imageClassifierWrapper = try? MobileNet(configuration: defaultConfig)\n\n\n    guard let imageClassifier = imageClassifierWrapper else {\n        fatalError(\"App failed to create an image classifier model instance.\")\n    }\n\n\n    // Get the underlying model instance.\n    let imageClassifierModel = imageClassifier.model\n\n\n    // Create a Vision instance using the image classifier's model instance.\n    guard let imageClassifierVisionModel = try? VNCoreMLModel(for: imageClassifierModel) else {\n        fatalError(\"App failed to create a `VNCoreMLModel` instance.\")\n    }\n\n\n    return imageClassifierVisionModel\n}\n\n\nThe method creates a Core ML model instance for Vision by:\n\nCreating an instance of the model’s wrapper class that Xcode auto-generates at compile time\n\nRetrieving the wrapper class instance’s underlying MLModel property\n\nPassing the model instance to a VNCoreMLModel initializer\n\nThe Image Predictor class minimizes runtime by only creating a single instance it shares across the app.\n\nNote\n\nShare a single VNCoreMLModel instance for each Core ML model in your project.\n\nCreate an Image Classification Request\n\nThe Image Predictor class creates an image classification request — a VNCoreMLRequest instance — by passing the shared image classifier model instance and a request handler to its initializer.\n\n// Create an image classification request with an image classifier model.\n\n\nlet imageClassificationRequest = VNCoreMLRequest(model: ImagePredictor.imageClassifier,\n                                                 completionHandler: visionRequestHandler)\n\n\nimageClassificationRequest.imageCropAndScaleOption = .centerCrop\n\n\nThe method tells Vision how to adjust images that don’t meet the model’s image input constraints by setting the request’s imageCropAndScaleOption property to VNImageCropAndScaleOption.centerCrop.\n\nCreate a Request Handler\n\nThe Image Predictor’s makePredictions(for photo, ...) method creates a VNImageRequestHandler for each image by passing the image and its orientation to the initializer.\n\nlet handler = VNImageRequestHandler(cgImage: photoImage, orientation: orientation)\n\n\nVision rotates the image based on orientation — a CGImagePropertyOrientation instance — before sending the image to the model.\n\nIf the image you want to classify has a URL, create a Vision image request handler with one of these initializers:\n\ninit(url:options:)\n\ninit(url:orientation:options:)\n\nStart the Request\n\nThe makePredictions(for photo, ...) method starts the request by adding it into a VNRequest array and passes it to the handler’s perform(_:) method.\n\nlet requests: [VNRequest] = [imageClassificationRequest]\n\n\n// Start the image classification request.\ntry handler.perform(requests)\n\n\nNote\n\nYou can perform multiple Vision requests on the same image by adding each request to the array you pass to the perform(_:) method’s requests parameter.\n\nRetrieve the Request’s Results\n\nWhen the image classification request is finished, Vision notifies the Image Predictor by calling the request’s completion handler, visionRequestHandler(_:error:). The method retrieves the request’s results by:\n\nChecking the error parameter\n\nCasting results to a VNClassificationObservation array\n\n// Cast the request's results as an `VNClassificationObservation` array.\nguard let observations = request.results as? [VNClassificationObservation] else {\n    // Image classifiers, like MobileNet, only produce classification observations.\n    // However, other Core ML model types can produce other observations.\n    // For example, a style transfer model produces `VNPixelBufferObservation` instances.\n    print(\"VNRequest produced the wrong result type: \\(type(of: request.results)).\")\n    return\n}\n\n\n// Create a prediction array from the observations.\npredictions = observations.map { observation in\n    // Convert each observation into an `ImagePredictor.Prediction` instance.\n    Prediction(classification: observation.identifier,\n               confidencePercentage: observation.confidencePercentageString)\n}\n\n\nThe Image Predictor converts each result to Prediction instances, a simple structure with two string properties.\n\nThe method sends the predictions array to the Image Predictor’s client — the main view controller — by calling the client’s completion handler.\n\n// Send the predictions back to the client.\npredictionHandler(predictions)\n\nFormat and Present the Predictions\n\nThe main view controller’s imagePredictionHandler(_:) method formats the individual predictions into a single string and updates a label in the app’s UI using helper methods.\n\nprivate func imagePredictionHandler(_ predictions: [ImagePredictor.Prediction]?) {\n    guard let predictions = predictions else {\n        updatePredictionLabel(\"No predictions. (Check console log.)\")\n        return\n    }\n\n\n    let formattedPredictions = formatPredictions(predictions)\n\n\n    let predictionString = formattedPredictions.joined(separator: \"\\n\")\n    updatePredictionLabel(predictionString)\n}\n\n\nThe updatePredictionLabel(_:) helper method safely updates the UI by updating the label’s text on the main dispatch queue.\n\nfunc updatePredictionLabel(_ message: String) {\n    DispatchQueue.main.async {\n        self.predictionLabel.text = message\n    }\n\n\nImportant\n\nKeep your app’s UI responsive by making predictions with Core ML models off of the main thread.\n\nSee Also\nMachine Learning Image Analysis\nTraining a Create ML Model to Classify Flowers\nTrain a flower classifier using Create ML in Swift Playgrounds, and apply the resulting model to real-time image classification using Vision.\nclass VNCoreMLRequest\nAn image analysis request that uses a Core ML model to process images.\nclass VNClassificationObservation\nAn object that represents classification information that an image analysis request produces.\nclass VNPixelBufferObservation\nAn object that represents an image that an image analysis request produces.\nclass VNCoreMLFeatureValueObservation\nAn object that represents a collection of key-value information that a Core ML image analysis request produces."
  },
  {
    "title": "VNInstanceMaskObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vninstancemaskobservation",
    "html": "Topics\nAccessing Instances\nvar allInstances: IndexSet\nThe collection that contains all instances, excluding the background.\nvar instanceMask: CVPixelBuffer\nThe resulting mask that represents all instances.\nCreating a Mask\nfunc generateMask(forInstances: IndexSet) -> CVPixelBuffer\nCreates a low-resolution mask from the instances you specify.\nfunc generateMaskedImage(ofInstances: IndexSet, from: VNImageRequestHandler, croppedToInstancesExtent: Bool) -> CVPixelBuffer\nCreates a high-resolution image where everything becomes transparent black, except for the instances you specify.\nfunc generateScaledMaskForImage(forInstances: IndexSet, from: VNImageRequestHandler) -> CVPixelBuffer\nCreates a high-resolution mask where everything becomes transparent black, except for the instances you specify.\nRelationships\nInherits From\nVNObservation\nSee Also\nImage Background Removal\nApplying visual effects to foreground subjects\nSegment the foreground subjects of an image and composite them to a new background with visual effects.\nclass VNGenerateForegroundInstanceMaskRequest\nA request that generates an instance mask of noticable objects to separate from the background.\nlet VNGenerateForegroundInstanceMaskRequestRevision1: Int\nA constant for specifying revision 1 of the foreground instance mask request."
  },
  {
    "title": "Understanding a Dice Roll with Vision and Object Detection | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/understanding_a_dice_roll_with_vision_and_object_detection",
    "html": "Overview\n\nThis sample app uses an object detection model trained with to recognize the tops of dice and their values when the dice roll onto a flat surface.\n\nAfter you run the object detection model on camera frames through Vision, the model interprets the result to identify when a roll has ended and what values the dice show.\n\nNote\n\nThis sample code project is associated with WWDC 2019 session 228: Creating Great Apps Using Core ML and ARKit.\n\nConfigure the Sample Code Project\n\nBefore you run the sample code project in Xcode, note the following:\n\nYou must run this sample code project on a physical device that uses iOS 13 or later. The project doesn’t work with Simulator.\n\nThe model works best on white dice with black pips. It may perform differently on dice that use other colors.\n\nAdd Inputs to the Request\n\nIn Vision, beginning in iOS 13, you can provide inputs other than images to a model by attaching an MLFeatureProvider object to your model. This is useful in the case of object detection when you want to specify different thresholds than the defaults.\n\nAs shown below, a feature provider can provide values for the iouThreshold and confidenceThreshold inputs to your object detection model.\n\nclass ThresholdProvider: MLFeatureProvider {\n    /// The actual values to provide as input\n    ///\n    /// Create ML Defaults are 0.45 for IOU and 0.25 for confidence.\n    /// Here the IOU threshold is relaxed a little bit because there are\n    /// sometimes multiple overlapping boxes per die.\n    /// Technically, relaxing the IOU threshold means\n    /// non-maximum-suppression (NMS) becomes stricter (fewer boxes are shown).\n    /// The confidence threshold can also be relaxed slightly because\n    /// objects look very consistent and are easily detected on a homogeneous\n    /// background.\n    open var values = [\n        \"iouThreshold\": MLFeatureValue(double: 0.3),\n        \"confidenceThreshold\": MLFeatureValue(double: 0.2)\n    ]\n\n\n    /// The feature names the provider has, per the MLFeatureProvider protocol\n    var featureNames: Set<String> {\n        return Set(values.keys)\n    }\n\n\n    /// The actual values for the features the provider can provide\n    func featureValue(for featureName: String) -> MLFeatureValue? {\n        return values[featureName]\n    }\n}\n\n\nTo use this threshold provider with your VNCoreMLModel, assign it to the featureProvider property of your VNCoreMLModel as seen in the following example.\n\nSet Up a Vision Request to Handle Camera Frames\n\nFor simplicity, you can use camera frames coming from an ARSession.\n\nTo run your detector on these frames, first set up a VNCoreMLRequest request with your model, as shown in the example below.\n\nguard let mlModel = try? DiceDetector(configuration: .init()).model,\n      let detector = try? VNCoreMLModel(for: mlModel) else {\n    print(\"Failed to load detector!\")\n    return\n}\n\n\n// Use a threshold provider to specify custom thresholds for the object detector.\ndetector.featureProvider = ThresholdProvider()\n\n\ndiceDetectionRequest = VNCoreMLRequest(model: detector) { [weak self] request, error in\n    self?.detectionRequestHandler(request: request, error: error)\n}\n// .scaleFill results in a slight skew but the model was trained accordingly\n// see https://developer.apple.com/documentation/vision/vnimagecropandscaleoption for more information\ndiceDetectionRequest.imageCropAndScaleOption = .scaleFill\n\n\nPass Camera Frames to the Object Detector to Predict Dice Locations\n\nPass the frames from the camera to the VNCoreMLRequest so it can make predictions using a VNImageRequestHandler object. The VNImageRequestHandler object handles image resizing and preprocessing as well as post-processing of your model’s outputs for every prediction.\n\nTo pass camera frames to your model, you first need to find the image orientation that corresponds to your device’s physical orientation. If the device’s orientation changes, the aspect ratio of the images can also change. Because you need to scale the bounding boxes for the detected objects back to your original image, you need to keep track of its size.\n\n// The frame is always oriented based on the camera sensor,\n// so in most cases Vision needs to rotate it for the model to work as expected.\nlet orientation = UIDevice.current.orientation\n\n\n// The image captured by the camera\nlet image = frame.capturedImage\n\n\nlet imageOrientation: CGImagePropertyOrientation\nswitch orientation {\ncase .portrait:\n    imageOrientation = .right\ncase .portraitUpsideDown:\n    imageOrientation = .left\ncase .landscapeLeft:\n    imageOrientation = .up\ncase .landscapeRight:\n    imageOrientation = .down\ncase .unknown:\n    print(\"The device orientation is unknown, the predictions may be affected\")\n    fallthrough\ndefault:\n    // By default keep the last orientation\n    // This applies for faceUp and faceDown\n    imageOrientation = self.lastOrientation\n}\n\n\n// For object detection, keeping track of the image buffer size\n// to know how to draw bounding boxes based on relative values.\nif self.bufferSize == nil || self.lastOrientation != imageOrientation {\n    self.lastOrientation = imageOrientation\n    let pixelBufferWidth = CVPixelBufferGetWidth(image)\n    let pixelBufferHeight = CVPixelBufferGetHeight(image)\n    if [.up, .down].contains(imageOrientation) {\n        self.bufferSize = CGSize(width: pixelBufferWidth,\n                                 height: pixelBufferHeight)\n    } else {\n        self.bufferSize = CGSize(width: pixelBufferHeight,\n                                 height: pixelBufferWidth)\n    }\n}\n\n\n\n\nFinally, you invoke the VNImageRequestHandler with the image from the camera and information about the current orientation to make a prediction using your object detector.\n\n\n\n// Invoke a VNRequestHandler with that image\nlet handler = VNImageRequestHandler(cvPixelBuffer: image, orientation: imageOrientation, options: [:])\n\n\ndo {\n    try handler.perform([self.diceDetectionRequest])\n} catch {\n    print(\"CoreML request failed with error: \\(error.localizedDescription)\")\n}\n\n\nNow that the app handles providing input data to your model, it’s time to interpret your model’s output.\n\nDraw Bounding Boxes to Understand Your Model’s Behavior\n\nYou can get a better understanding of how well your detector performs by drawing bounding boxes around each object and its text label. The dice detection model detects the tops of dice and labels them according to the number of pips shown on each die’s top side.\n\nTo draw bounding boxes, see Recognizing Objects in Live Capture.\n\nDetermine When a Roll Has Ended\n\nWhen playing a dice game, users want to know the result of a roll. The app determines that the roll has ended by waiting for the dice’s positions and values to stabilize.\n\nYou can define the requirements of an ended roll as a comparison between two consecutive camera frames with the following conditions:\n\nThe number of detected dice must be the same.\n\nFor each detected die:\n\nThe bounding box must have not moved.\n\nThe identified class must match.\n\nBased on these constraints, you can make a function that tells the app whether a roll has ended based on the current and the previous VNRecognizedObjectObservation objects.\n\n/// Determines if a roll has ended with the current dice values O(n^2)\n///\n/// - parameter observations: The object detection observations from the model\n/// - returns: True if the roll has ended\nfunc hasRollEnded(observations: [VNRecognizedObjectObservation]) -> Bool {\n    // First check if same number of dice were detected\n    if lastObservations.count != observations.count {\n        lastObservations = observations\n        return false\n    }\n    var matches = 0\n    for newObservation in observations {\n        for oldObservation in lastObservations {\n            // If the labels don't match, skip it\n            // Or if the IOU is less than 85%, consider this box different\n            // Either it's a different die or the same die has moved\n            if newObservation.labels.first?.identifier == oldObservation.labels.first?.identifier &&\n                intersectionOverUnion(oldObservation.boundingBox, newObservation.boundingBox) > 0.85 {\n                matches += 1\n            }\n        }\n    }\n    lastObservations = observations\n    return matches == observations.count\n}\n\n\nNow for every prediction (meaning every new camera frame) you can check whether the roll has ended.\n\nDisplay the Dice Values\n\nOnce the roll has ended, you can display the information on the screen or trigger some other behavior in the setting of a game.\n\nThis sample app shows the list of recognized values on screen, sorted from left-most to right-most VNRecognizedObjectObservation. It sorts the values based on where the dice are on the surface according to each observation’s bounding box coordinates. The app does this by sorting the observations by their bounding box’s centerX property in ascending order.\n\nvar sortableDiceValues = [(value: Int, xPosition: CGFloat)]()\n\n\nfor observation in observations {\n    // Select only the label with the highest confidence.\n    guard let topLabelObservation = observation.labels.first else {\n        print(\"Object observation has no labels\")\n        continue\n    }\n\n\n    if let intValue = Int(topLabelObservation.identifier) {\n        sortableDiceValues.append((value: intValue, xPosition: observation.boundingBox.midX))\n    }\n}\n\n\nlet diceValues = sortableDiceValues.sorted { $0.xPosition < $1.xPosition }.map { $0.value }\n\n\nSee Also\nObject Recognition\nRecognizing Objects in Live Capture\nApply Vision algorithms to identify objects in real-time video.\nclass VNRecognizedObjectObservation\nA detected object observation with an array of classification labels that classify the recognized object."
  },
  {
    "title": "Applying visual effects to foreground subjects | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/applying_visual_effects_to_foreground_subjects",
    "html": "Overview\n\nNote\n\nThis sample code project is associated with WWDC23 session 10176: Lift subjects from images in your app.\n\nConfigure the sample code project\n\nBefore you run the sample code project in Xcode, set the run destination to an iOS 17 device.\n\nSee Also\nImage Background Removal\nclass VNInstanceMaskObservation\nAn observation that contains an instance mask that labels instances in the mask.\nclass VNGenerateForegroundInstanceMaskRequest\nA request that generates an instance mask of noticable objects to separate from the background.\nlet VNGenerateForegroundInstanceMaskRequestRevision1: Int\nA constant for specifying revision 1 of the foreground instance mask request."
  },
  {
    "title": "Recognizing Objects in Live Capture | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture",
    "html": "Overview\n\nWith the Vision framework, you can recognize objects in live capture. Starting in iOS 12, macOS 10.14, and tvOS 12, Vision requests made with a Core ML model return results as VNRecognizedObjectObservation objects, which identify objects found in the captured scene.\n\nThis sample app shows you how to set up your camera for live capture, incorporate a Core ML model into Vision, and parse results as classified objects.\n\nSet Up Live Capture\n\nAlthough implementing AV live capture is similar from one capture app to another, configuring the camera to work best with Vision algorithms involves some subtle differences.\n\nConfigure the camera to use for capture. This sample app feeds camera output from AVFoundation into the main view controller. Start by configuring an AVCaptureSession:\n\nprivate let session = AVCaptureSession()\n\n\nSet your device and session resolution. It’s important to choose the right resolution for your app. Don’t simply select the highest resolution available if your app doesn’t require it. It’s better to select a lower resolution so Vision can process results more efficiently. Check the model parameters in Xcode to find out if your app requires a resolution smaller than 640 x 480 pixels.\n\nSet the camera resolution to the nearest resolution that is greater than or equal to the resolution of images used in the model:\n\nlet videoDevice = AVCaptureDevice.DiscoverySession(deviceTypes: [.builtInWideAngleCamera], mediaType: .video, position: .back).devices.first\ndo {\n    deviceInput = try AVCaptureDeviceInput(device: videoDevice!)\n} catch {\n    print(\"Could not create video device input: \\(error)\")\n    return\n}\n\n\nsession.beginConfiguration()\nsession.sessionPreset = .vga640x480 // Model image size is smaller.\n\n\nVision will perform the remaining scaling.\n\nAdd video input to your session by adding the camera as a device:\n\nguard session.canAddInput(deviceInput) else {\n    print(\"Could not add video device input to the session\")\n    session.commitConfiguration()\n    return\n}\nsession.addInput(deviceInput)\n\n\nAdd video output to your session, being sure to specify the pixel format:\n\nif session.canAddOutput(videoDataOutput) {\n    session.addOutput(videoDataOutput)\n    // Add a video data output\n    videoDataOutput.alwaysDiscardsLateVideoFrames = true\n    videoDataOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: Int(kCVPixelFormatType_420YpCbCr8BiPlanarFullRange)]\n    videoDataOutput.setSampleBufferDelegate(self, queue: videoDataOutputQueue)\n} else {\n    print(\"Could not add video data output to the session\")\n    session.commitConfiguration()\n    return\n}\n\n\nProcess every frame, but don’t hold on to more than one Vision request at a time. The camera will stop working if the buffer queue overflows available memory. To simplify buffer management, in the capture output, Vision blocks the call for as long as the previous request requires. As a result, AVFoundation may drop frames, if necessary. The sample app keeps a queue size of 1; if a Vision request is already queued up for processing when another becomes available, skip it instead of holding on to extras.\n\nlet captureConnection = videoDataOutput.connection(with: .video)\n// Always process the frames\ncaptureConnection?.isEnabled = true\ndo {\n    try  videoDevice!.lockForConfiguration()\n    let dimensions = CMVideoFormatDescriptionGetDimensions((videoDevice?.activeFormat.formatDescription)!)\n    bufferSize.width = CGFloat(dimensions.width)\n    bufferSize.height = CGFloat(dimensions.height)\n    videoDevice!.unlockForConfiguration()\n} catch {\n    print(error)\n}\n\n\nCommit the session configuration:\n\nsession.commitConfiguration()\n\n\nSet up a preview layer on your view controller, so the camera can feed its frames into your app’s UI:\n\npreviewLayer = AVCaptureVideoPreviewLayer(session: session)\npreviewLayer.videoGravity = AVLayerVideoGravity.resizeAspectFill\nrootLayer = previewView.layer\npreviewLayer.frame = rootLayer.bounds\nrootLayer.addSublayer(previewLayer)\n\nSpecify Device Orientation\n\nYou must input the camera’s orientation properly using the device orientation. Vision algorithms aren’t orientation-agnostic, so when you make a request, use an orientation that’s relative to that of the capture device.\n\nlet curDeviceOrientation = UIDevice.current.orientation\nlet exifOrientation: CGImagePropertyOrientation\n\n\nswitch curDeviceOrientation {\ncase UIDeviceOrientation.portraitUpsideDown:  // Device oriented vertically, home button on the top\n    exifOrientation = .left\ncase UIDeviceOrientation.landscapeLeft:       // Device oriented horizontally, home button on the right\n    exifOrientation = .upMirrored\ncase UIDeviceOrientation.landscapeRight:      // Device oriented horizontally, home button on the left\n    exifOrientation = .down\ncase UIDeviceOrientation.portrait:            // Device oriented vertically, home button on the bottom\n    exifOrientation = .up\ndefault:\n    exifOrientation = .up\n}\n\nDesignate Labels Using a Core ML Classifier\n\nThe Core ML model you include in your app determines which labels are used in Vision’s object identifiers. The model in this sample app was trained in Turi Create 4.3.2 using Darknet YOLO (You Only Look Once). See Object Detection to learn how to generate your own models using Turi Create. Vision analyzes these models and returns observations as VNRecognizedObjectObservation objects.\n\nLoad the model using a VNCoreMLModel:\n\nlet visionModel = try VNCoreMLModel(for: MLModel(contentsOf: modelURL))\n\n\nCreate a VNCoreMLRequest with that model:\n\nlet objectRecognition = VNCoreMLRequest(model: visionModel, completionHandler: { (request, error) in\n    DispatchQueue.main.async(execute: {\n        // perform all the UI updates on the main queue\n        if let results = request.results {\n            self.drawVisionRequestResults(results)\n        }\n    })\n})\n\n\nThe completion handler could execute on a background queue, so perform UI updates on the main queue to provide immediate visual feedback.\n\nAccess results in the request’s completion handler, or through the requests property.\n\nParse Recognized Object Observations\n\nThe results property is an array of observations, each with a set of labels and bounding boxes. Parse those observations by iterating through the array, as follows:\n\nfor observation in results where observation is VNRecognizedObjectObservation {\n    guard let objectObservation = observation as? VNRecognizedObjectObservation else {\n        continue\n    }\n    // Select only the label with the highest confidence.\n    let topLabelObservation = objectObservation.labels[0]\n    let objectBounds = VNImageRectForNormalizedRect(objectObservation.boundingBox, Int(bufferSize.width), Int(bufferSize.height))\n    \n    let shapeLayer = self.createRoundedRectLayerWithBounds(objectBounds)\n    \n    let textLayer = self.createTextSubLayerInBounds(objectBounds,\n                                                    identifier: topLabelObservation.identifier,\n                                                    confidence: topLabelObservation.confidence)\n    shapeLayer.addSublayer(textLayer)\n    detectionOverlay.addSublayer(shapeLayer)\n}\n\n\nThe labels array lists each classification identifier along with its confidence value, ordered from highest confidence to lowest. The sample app notes only the classification with the highest confidence score, at element 0. It then displays this classification and confidence in a textual overlay.\n\nThe bounding box tells where the object was observed. The sample uses this location to draw a bounding box around the object.\n\nThis sample simplifies classification by returning only the top classification; the array is ordered in decreasing order of confidence score. However, your app could analyze the confidence score and show multiple classifications, either to further describe your detected objects, or to show competing classifications.\n\nYou can also use the VNRecognizedObjectObservation resulting from object recognition to initialize an object tracker such as VNTrackObjectRequest. For more information about tracking, see the article on object tracking: Tracking Multiple Objects or Rectangles in Video.\n\nSee Also\nObject Recognition\nclass VNRecognizedObjectObservation\nA detected object observation with an array of classification labels that classify the recognized object.\nUnderstanding a Dice Roll with Vision and Object Detection\nDetect dice position and values shown in a camera frame, and determine the end of a roll by leveraging a dice detection model."
  },
  {
    "title": "VNImageTranslationAlignmentObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagetranslationalignmentobservation",
    "html": "Overview\n\nThis type of observation results from a VNTranslationalImageRegistrationRequest, informing the alignmentTransform performed to align the input images.\n\nTopics\nDetermining Alignment\nvar alignmentTransform: CGAffineTransform\nThe alignment transform to align the floating image with the reference image.\nIdentifying Request Revisions\nlet VNTranslationalImageRegistrationRequestRevision1: Int\nA constant for specifying revision 1 of the translational image registration request.\nRelationships\nInherits From\nVNImageAlignmentObservation\nSee Also\nImage Alignment\nAligning Similar Images\nConstruct a composite image from images that capture the same scene.\nclass VNTargetedImageRequest\nThe abstract superclass for image analysis requests that operate on both the processed image and a secondary image.\nclass VNImageRegistrationRequest\nThe abstract superclass for image analysis requests that align images according to their content.\nclass VNTranslationalImageRegistrationRequest\nAn image analysis request that determines the affine transform necessary to align the content of two images.\nclass VNTrackTranslationalImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the affine transform necessary to align the content of two images.\nclass VNHomographicImageRegistrationRequest\nAn image analysis request that determines the perspective warp matrix necessary to align the content of two images.\nclass VNTrackHomographicImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the perspective warp matrix necessary to align the content of two images.\nclass VNImageAlignmentObservation\nThe abstract superclass for image analysis results that describe the relative alignment of two images.\nclass VNImageHomographicAlignmentObservation\nAn object that represents a perspective warp transformation."
  },
  {
    "title": "VNTrackHomographicImageRegistrationRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackhomographicimageregistrationrequest",
    "html": "Overview\n\nThis request is similar to VNHomographicImageRegistrationRequest. However, as a VNStatefulRequest, it automatically computes the registration against the previous frame.\n\nTopics\nCreating a Homographic Image\ninit()\nCreates a new request that tracks the homographic transformation of two images.\ninit(completionHandler: VNRequestCompletionHandler?)\nCreates a new request that tracks the homographic transformation of two images, with a system callback on completion.\nAccessing the Results\nvar results: [VNImageHomographicAlignmentObservation]?\nThe observed homographic image alignment request.\nRelationships\nInherits From\nVNStatefulRequest\nSee Also\nImage Alignment\nAligning Similar Images\nConstruct a composite image from images that capture the same scene.\nclass VNTargetedImageRequest\nThe abstract superclass for image analysis requests that operate on both the processed image and a secondary image.\nclass VNImageRegistrationRequest\nThe abstract superclass for image analysis requests that align images according to their content.\nclass VNTranslationalImageRegistrationRequest\nAn image analysis request that determines the affine transform necessary to align the content of two images.\nclass VNTrackTranslationalImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the affine transform necessary to align the content of two images.\nclass VNHomographicImageRegistrationRequest\nAn image analysis request that determines the perspective warp matrix necessary to align the content of two images.\nclass VNImageAlignmentObservation\nThe abstract superclass for image analysis results that describe the relative alignment of two images.\nclass VNImageTranslationAlignmentObservation\nAffine transform information that an image alignment request produces.\nclass VNImageHomographicAlignmentObservation\nAn object that represents a perspective warp transformation."
  },
  {
    "title": "VNRecognizedObjectObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedobjectobservation",
    "html": "Overview\n\nThe confidence of the classifications sum up to 1.0. Multiply the classification confidence with the confidence of this observation.\n\nTopics\nClassifying a Recognized Object\nvar labels: [VNClassificationObservation]\nAn array of observations that classify the recognized object.\nclass VNClassificationObservation\nAn object that represents classification information that an image analysis request produces.\nRelationships\nInherits From\nVNDetectedObjectObservation\nSee Also\nObject Recognition\nRecognizing Objects in Live Capture\nApply Vision algorithms to identify objects in real-time video.\nUnderstanding a Dice Roll with Vision and Object Detection\nDetect dice position and values shown in a camera frame, and determine the end of a roll by leveraging a dice detection model."
  },
  {
    "title": "VNGenerateForegroundInstanceMaskRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateforegroundinstancemaskrequestrevision1",
    "html": "See Also\nImage Background Removal\nApplying visual effects to foreground subjects\nSegment the foreground subjects of an image and composite them to a new background with visual effects.\nclass VNInstanceMaskObservation\nAn observation that contains an instance mask that labels instances in the mask.\nclass VNGenerateForegroundInstanceMaskRequest\nA request that generates an instance mask of noticable objects to separate from the background."
  },
  {
    "title": "VNImageAlignmentObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagealignmentobservation",
    "html": "Overview\n\nThis abstract superclass forms the basis of image alignment or registration output. You receive its subclasses, such as VNImageTranslationAlignmentObservation and VNImageHomographicAlignmentObservation, by performing specific registration requests. Don’t create one of these classes yourself.\n\nRelationships\nInherits From\nVNObservation\nSee Also\nImage Alignment\nAligning Similar Images\nConstruct a composite image from images that capture the same scene.\nclass VNTargetedImageRequest\nThe abstract superclass for image analysis requests that operate on both the processed image and a secondary image.\nclass VNImageRegistrationRequest\nThe abstract superclass for image analysis requests that align images according to their content.\nclass VNTranslationalImageRegistrationRequest\nAn image analysis request that determines the affine transform necessary to align the content of two images.\nclass VNTrackTranslationalImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the affine transform necessary to align the content of two images.\nclass VNHomographicImageRegistrationRequest\nAn image analysis request that determines the perspective warp matrix necessary to align the content of two images.\nclass VNTrackHomographicImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the perspective warp matrix necessary to align the content of two images.\nclass VNImageTranslationAlignmentObservation\nAffine transform information that an image alignment request produces.\nclass VNImageHomographicAlignmentObservation\nAn object that represents a perspective warp transformation."
  },
  {
    "title": "VNGenerateForegroundInstanceMaskRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateforegroundinstancemaskrequest",
    "html": "Topics\nAccessing the Results\nvar results: [VNInstanceMaskObservation]?\nThe instance masks the request observes.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nImage Background Removal\nApplying visual effects to foreground subjects\nSegment the foreground subjects of an image and composite them to a new background with visual effects.\nclass VNInstanceMaskObservation\nAn observation that contains an instance mask that labels instances in the mask.\nlet VNGenerateForegroundInstanceMaskRequestRevision1: Int\nA constant for specifying revision 1 of the foreground instance mask request."
  },
  {
    "title": "VNHomographicImageRegistrationRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhomographicimageregistrationrequest",
    "html": "Overview\n\nCreate and perform a homographic image registration request to align content in two images through a homography. A homography is an isomorphism of projected spaces, a bijection that maps lines to lines.\n\nTopics\nAccessing the Results\nvar results: [VNImageHomographicAlignmentObservation]?\nThe results of the image registration request.\nIdentifying Request Revisions\nlet VNHomographicImageRegistrationRequestRevision1: Int\nA constant for specifying revision 1 of the homographic image registration request.\nRelationships\nInherits From\nVNImageRegistrationRequest\nSee Also\nImage Alignment\nAligning Similar Images\nConstruct a composite image from images that capture the same scene.\nclass VNTargetedImageRequest\nThe abstract superclass for image analysis requests that operate on both the processed image and a secondary image.\nclass VNImageRegistrationRequest\nThe abstract superclass for image analysis requests that align images according to their content.\nclass VNTranslationalImageRegistrationRequest\nAn image analysis request that determines the affine transform necessary to align the content of two images.\nclass VNTrackTranslationalImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the affine transform necessary to align the content of two images.\nclass VNTrackHomographicImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the perspective warp matrix necessary to align the content of two images.\nclass VNImageAlignmentObservation\nThe abstract superclass for image analysis results that describe the relative alignment of two images.\nclass VNImageTranslationAlignmentObservation\nAffine transform information that an image alignment request produces.\nclass VNImageHomographicAlignmentObservation\nAn object that represents a perspective warp transformation."
  },
  {
    "title": "VNTrackTranslationalImageRegistrationRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntracktranslationalimageregistrationrequest",
    "html": "Overview\n\nThis request is similar to VNTranslationalImageRegistrationRequest. However, as a VNStatefulRequest, it automatically computes the registration against the previous frame.\n\nTopics\nCreating a Translational Image\ninit()\nCreates a new request that tracks the translational registration of two images.\ninit(completionHandler: VNRequestCompletionHandler?)\nCreates a new request that tracks the translational registration of two images, with a system callback on completion.\nAccessing the Results\nvar results: [VNImageTranslationAlignmentObservation]?\nThe observed translational image alignment request.\nRelationships\nInherits From\nVNStatefulRequest\nSee Also\nImage Alignment\nAligning Similar Images\nConstruct a composite image from images that capture the same scene.\nclass VNTargetedImageRequest\nThe abstract superclass for image analysis requests that operate on both the processed image and a secondary image.\nclass VNImageRegistrationRequest\nThe abstract superclass for image analysis requests that align images according to their content.\nclass VNTranslationalImageRegistrationRequest\nAn image analysis request that determines the affine transform necessary to align the content of two images.\nclass VNHomographicImageRegistrationRequest\nAn image analysis request that determines the perspective warp matrix necessary to align the content of two images.\nclass VNTrackHomographicImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the perspective warp matrix necessary to align the content of two images.\nclass VNImageAlignmentObservation\nThe abstract superclass for image analysis results that describe the relative alignment of two images.\nclass VNImageTranslationAlignmentObservation\nAffine transform information that an image alignment request produces.\nclass VNImageHomographicAlignmentObservation\nAn object that represents a perspective warp transformation."
  },
  {
    "title": "VNImageRegistrationRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimageregistrationrequest",
    "html": "Overview\n\nThis abstract superclass forms the basis of image alignment or registration requests. Make specific requests through one of its subclasses, VNTranslationalImageRegistrationRequest or VNHomographicImageRegistrationRequest. Don’t create an instance of this superclass yourself.\n\nRelationships\nInherits From\nVNTargetedImageRequest\nSee Also\nImage Alignment\nAligning Similar Images\nConstruct a composite image from images that capture the same scene.\nclass VNTargetedImageRequest\nThe abstract superclass for image analysis requests that operate on both the processed image and a secondary image.\nclass VNTranslationalImageRegistrationRequest\nAn image analysis request that determines the affine transform necessary to align the content of two images.\nclass VNTrackTranslationalImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the affine transform necessary to align the content of two images.\nclass VNHomographicImageRegistrationRequest\nAn image analysis request that determines the perspective warp matrix necessary to align the content of two images.\nclass VNTrackHomographicImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the perspective warp matrix necessary to align the content of two images.\nclass VNImageAlignmentObservation\nThe abstract superclass for image analysis results that describe the relative alignment of two images.\nclass VNImageTranslationAlignmentObservation\nAffine transform information that an image alignment request produces.\nclass VNImageHomographicAlignmentObservation\nAn object that represents a perspective warp transformation."
  },
  {
    "title": "VNTargetedImageRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntargetedimagerequest",
    "html": "Overview\n\nOther Vision request handlers that operate on both the processed image and a secondary image inherit from this abstract base class. Instantiate one of its subclasses to perform image analysis, and pass in auxiliary image data by filling in the options dictionary at initialization.\n\nTopics\nCreating a Request\ninit(targetedCGImage: CGImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image, executing the completion handler when done.\ninit(targetedCGImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a Core Graphics image of known orientation, executing the completion handler when done.\ninit(targetedCIImage: CIImage, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage, executing the completion handler when done.\ninit(targetedCIImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a CIImage of known orientation, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer, executing the completion handler when done.\ninit(targetedCVPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image in a CVPixelBuffer of known orientation, executing the completion handler when done.\ninit(targetedCMSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image in a sample buffer.\ninit(targetedCMSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request with a completion handler that targets an image of a known orientation in a sample buffer.\ninit(targetedImageData: Data, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image as raw data, executing the completion handler when done.\ninit(targetedImageData: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting a raw data image of known orientation, executing the completion handler when done.\ninit(targetedImageURL: URL, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image at the specified URL, executing the completion handler when done.\ninit(targetedImageURL: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any], completionHandler: VNRequestCompletionHandler?)\nCreates a new request targeting an image of known orientation, at the specified URL, executing the completion handler when done.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nImage Alignment\nAligning Similar Images\nConstruct a composite image from images that capture the same scene.\nclass VNImageRegistrationRequest\nThe abstract superclass for image analysis requests that align images according to their content.\nclass VNTranslationalImageRegistrationRequest\nAn image analysis request that determines the affine transform necessary to align the content of two images.\nclass VNTrackTranslationalImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the affine transform necessary to align the content of two images.\nclass VNHomographicImageRegistrationRequest\nAn image analysis request that determines the perspective warp matrix necessary to align the content of two images.\nclass VNTrackHomographicImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the perspective warp matrix necessary to align the content of two images.\nclass VNImageAlignmentObservation\nThe abstract superclass for image analysis results that describe the relative alignment of two images.\nclass VNImageTranslationAlignmentObservation\nAffine transform information that an image alignment request produces.\nclass VNImageHomographicAlignmentObservation\nAn object that represents a perspective warp transformation."
  },
  {
    "title": "VNTranslationalImageRegistrationRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntranslationalimageregistrationrequest",
    "html": "Overview\n\nCreate and perform a translational image registration request to align content in two images through translation.\n\nTopics\nAccessing the Results\nvar results: [VNImageTranslationAlignmentObservation]?\nThe results of a translational image alignment request.\nRelationships\nInherits From\nVNImageRegistrationRequest\nSee Also\nImage Alignment\nAligning Similar Images\nConstruct a composite image from images that capture the same scene.\nclass VNTargetedImageRequest\nThe abstract superclass for image analysis requests that operate on both the processed image and a secondary image.\nclass VNImageRegistrationRequest\nThe abstract superclass for image analysis requests that align images according to their content.\nclass VNTrackTranslationalImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the affine transform necessary to align the content of two images.\nclass VNHomographicImageRegistrationRequest\nAn image analysis request that determines the perspective warp matrix necessary to align the content of two images.\nclass VNTrackHomographicImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the perspective warp matrix necessary to align the content of two images.\nclass VNImageAlignmentObservation\nThe abstract superclass for image analysis results that describe the relative alignment of two images.\nclass VNImageTranslationAlignmentObservation\nAffine transform information that an image alignment request produces.\nclass VNImageHomographicAlignmentObservation\nAn object that represents a perspective warp transformation."
  },
  {
    "title": "supportedIdentifiers() | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizeanimalsrequest/3751003-supportedidentifiers",
    "html": "Return Value\n\nThe animal identifiers.\n\nSee Also\nIdentifying Animals\nstruct VNAnimalIdentifier\nAn animal identifier string.\nclass func knownAnimalIdentifiers(forRevision: Int) -> [VNAnimalIdentifier]\nReturns a list of animal identifiers the recognition algorithm supports for the specified revision.\nDeprecated"
  },
  {
    "title": "supportedJointsGroupNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectanimalbodyposerequest/4213143-supportedjointsgroupnames",
    "html": "See Also\nDetermining Supported Joints\nvar supportedJointNames: [VNAnimalBodyPoseObservation.JointName]\nRetrieves the joint names the request supports."
  },
  {
    "title": "supportedJointNames | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectanimalbodyposerequest/4213142-supportedjointnames",
    "html": "See Also\nDetermining Supported Joints\nvar supportedJointsGroupNames: [VNAnimalBodyPoseObservation.JointsGroupName]\nRetrieves the joint group names the request supports."
  },
  {
    "title": "heightEstimation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/4245795-heightestimation",
    "html": "See Also\nGetting the Body Height\nenum VNHumanBodyPose3DObservation.HeightEstimation\nConstants that identify body height estimation techniques.\nvar bodyHeight: Float\nThe estimated human body height, in meters."
  },
  {
    "title": "cameraOriginMatrix | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/4173227-cameraoriginmatrix",
    "html": "See Also\nGetting the Camera Position\nfunc cameraRelativePosition(VNHumanBodyPose3DObservation.JointName) -> simd_float4x4\nReturns a position relative to the camera for the body joint you specify."
  },
  {
    "title": "torso | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointsgroupname/4173312-torso",
    "html": "See Also\nGetting the Group Names\nstatic let all: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents all joints.\nstatic let head: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the head joints.\nstatic let leftArm: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the left arm joints.\nstatic let leftLeg: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the left leg joints.\nstatic let rightArm: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the right arm joints.\nstatic let rightLeg: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the right leg joints."
  },
  {
    "title": "recognizedPoints(_:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/4200088-recognizedpoints",
    "html": "Parameters\njointsGroupName\n\nThe name of the human body joints group.\n\nReturn Value\n\nA collection of points; otherwise, nil if an error occurs.\n\nSee Also\nAccessing Points\nvar availableJointNames: [VNHumanBodyPose3DObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nvar availableJointsGroupNames: [VNHumanBodyPose3DObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose.\nfunc recognizedPoint(VNHumanBodyPose3DObservation.JointName) -> VNHumanBodyRecognizedPoint3D\nReturns the point for a joint name that the observation recognizes."
  },
  {
    "title": "init(rawValue:) | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointsgroupname/4176678-init",
    "html": "Parameters\nrawValue\n\nThe point group key."
  },
  {
    "title": "VNRecognizeAnimalsRequestRevision1 | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizeanimalsrequestrevision1",
    "html": "See Also\nIdentifying Request Revisions\nlet VNRecognizeAnimalsRequestRevision2: Int\nA constant for specifying revision 2 of the animal recognition request."
  },
  {
    "title": "VNComputeStage | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vncomputestage",
    "html": "Topics\nGet the Compute Stages\nstatic let main: VNComputeStage\nA stage that represents where the system performs the main functionality.\nstatic let postProcessing: VNComputeStage\nA stage that represents where the system performs additional analysis from the main compute stage.\nCreate a Compute Stage\ninit(rawValue: String)\nCreates a compute stage with the value you specify.\nRelationships\nConforms To\nHashable\nRawRepresentable\nSendable\nSee Also\nUtilities\nclass VNGeometryUtils\nUtility methods to determine the geometries of various Vision types.\nclass VNVideoProcessor\nAn object that performs offline analysis of video content."
  },
  {
    "title": "Aligning Similar Images | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/aligning_similar_images",
    "html": "Overview\n\nThis sample app uses image registration requests from the Vision framework to calculate an alignment transform between two images of the same scene that are slightly different. The sample app uses the alignment transform to construct a single, composite image that contains the aligned content of both images.\n\nProvide Input Images\n\nImage registration requests require two images, a reference image and a floating image. The best alignment occurs when the content of the input images is very similar. In this sample app, the input images simulate an image capture of the same scene from a slightly different perspective. The two images are nearly identical — one has a slight warp and is offset from the other. The sample app displays the input images next to the aligned composite image for visual comparison in the registrationImages view.\n\nSelect a Registration Mechanism\n\nThe Vision framework provides a translational image registration mechanism, VNTranslationalImageRegistrationRequest, and a homographic image registration mechanism, VNHomographicImageRegistrationRequest. The sample app provides a toggle between these two image registration mechanisms to visually demonstrate the differences in their image alignment observations.\n\nApply the Alignment Observation\n\nThis project applies the alignment observation for each image registration mechanism in the register function, and returns an image that contains the composited result. The register function uses the makeAlignedImage function to transform the floating image. The sample uses the transformed(by:) method to apply the translational transform for the translational image registration mechanism, and uses the CIPerspectiveTransform filter to apply the pespective transform for the homographic image registration mechanism.\n\nSee Also\nImage Alignment\nclass VNTargetedImageRequest\nThe abstract superclass for image analysis requests that operate on both the processed image and a secondary image.\nclass VNImageRegistrationRequest\nThe abstract superclass for image analysis requests that align images according to their content.\nclass VNTranslationalImageRegistrationRequest\nAn image analysis request that determines the affine transform necessary to align the content of two images.\nclass VNTrackTranslationalImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the affine transform necessary to align the content of two images.\nclass VNHomographicImageRegistrationRequest\nAn image analysis request that determines the perspective warp matrix necessary to align the content of two images.\nclass VNTrackHomographicImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the perspective warp matrix necessary to align the content of two images.\nclass VNImageAlignmentObservation\nThe abstract superclass for image analysis results that describe the relative alignment of two images.\nclass VNImageTranslationAlignmentObservation\nAffine transform information that an image alignment request produces.\nclass VNImageHomographicAlignmentObservation\nAn object that represents a perspective warp transformation."
  },
  {
    "title": "VNHorizonObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhorizonobservation",
    "html": "Overview\n\nInstances of this class result from invoking a VNDetectHorizonRequest, and report the angle and transform of the horizon in an image.\n\nTopics\nEvaluating the Horizon\nvar angle: CGFloat\nThe angle of the observed horizon.\nvar transform: CGAffineTransform\nThe transform to apply to the detected horizon.\nfunc transform(forImageWidth: Int, height: Int) -> CGAffineTransform\nCreates an affine transform for the specified image width and height.\nRelationships\nInherits From\nVNObservation\nSee Also\nHorizon Detection\nclass VNDetectHorizonRequest\nAn image analysis request that determines the horizon angle in an image."
  },
  {
    "title": "VNRecognizeTextRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizetextrequest",
    "html": "Overview\n\nBy default, a text recognition request first locates all possible glyphs or characters in the input image, and then analyzes each string. To specify or limit the languages to find in the request, set the recognitionLanguages property to an array that contains the names of the languages of text you want to recognize. Vision returns the result of this request in a VNRecognizedTextObservation object.\n\nTopics\nCustomizing Recognition Constraints\nvar minimumTextHeight: Float\nThe minimum height, relative to the image height, of the text to recognize.\nvar recognitionLevel: VNRequestTextRecognitionLevel\nA value that determines whether the request prioritizes accuracy or speed in text recognition.\nenum VNRequestTextRecognitionLevel\nConstants that identify the performance and accuracy of the text recognition.\nAccessing the Results\nvar results: [VNRecognizedTextObservation]?\nThe results of the text recognition request.\nSpecifying the Language\nvar automaticallyDetectsLanguage: Bool\nA Boolean value that indicates whether to attempt detecting the language to use the appropriate model for recognition and language correction.\nvar recognitionLanguages: [String]\nAn array of languages to detect, in priority order.\nvar usesLanguageCorrection: Bool\nA Boolean value that indicates whether the request applies language correction during the recognition process.\nvar customWords: [String]\nAn array of strings to supplement the recognized languages at the word-recognition stage.\nfunc supportedRecognitionLanguages() -> [String]\nReturns the identifiers of the languages that the request supports.\nclass func supportedRecognitionLanguages(for: VNRequestTextRecognitionLevel, revision: Int) -> [String]\nRequests a list of languages that the specified revision recognizes.\nDeprecated\nIdentifying Request Revisions\nlet VNRecognizeTextRequestRevision3: Int\nA constant for specifying revision 3 of the text recognition request.\nlet VNRecognizeTextRequestRevision2: Int\nA constant for specifying revision 2 of the text recognition request.\nlet VNRecognizeTextRequestRevision1: Int\nA constant for specifying revision 1 of the text recognition request.\nRelationships\nInherits From\nVNImageBasedRequest\nConforms To\nVNRequestProgressProviding\nSee Also\nText Recognition\nRecognizing Text in Images\nAdd text-recognition features to your app using the Vision framework.\nStructuring Recognized Text on a Document\nDetect, recognize, and structure text on a business card or receipt using Vision and VisionKit.\nExtracting phone numbers from text in images\nAnalyze and filter phone numbers from text in live capture by using Vision.\nLocating and Displaying Recognized Text\nConfigure and perform text recognition on images to identify their textual content.\nclass VNRecognizedTextObservation\nA request that detects and recognizes regions of text in an image."
  },
  {
    "title": "VNRequestProgressHandler | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequestprogresshandler",
    "html": "Parameters\nrequest\n\nThe completed Vision request. The results of the request, if no error occurred, reside in the results array of the request.\n\nfractionCompleted\n\nThe fraction of the request that is completed, reported between 0.0 and 1.0. If the indeterminate property is set, this value is undefined.\n\nerror\n\nThe error that caused the request to fail, or nil if the request completed successfully.\n\nDiscussion\n\nVision may populate the results array in the request with partial data, before all results are ready.\n\nNote\n\nThe Vision framework may call the progress handler on a different dispatch queue from the thread on which you initiated the original request, so ensure that your handler can execute asynchronously, in a thread-safe manner.\n\nSee Also\nRequest Progress Tracking\nprotocol VNRequestProgressProviding\nA protocol for providing progress information on long-running tasks in Vision."
  },
  {
    "title": "VNRequestProgressProviding | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequestprogressproviding",
    "html": "Overview\n\nAdopt this protocol for potentially long-running Vision requests to provide information about progress throughout processing. For example, you can use the optional progressHandler to update the user interface, provide a percentage of completion, or process partial results.\n\nNote\n\nThe Vision framework may call the progress handler on a different dispatch queue from the thread on which you initiated the original request, so ensure that your handler can execute asynchronously, in a thread-safe manner.\n\nTopics\nTracking Progress\nvar progressHandler: VNRequestProgressHandler\nA block of code executed periodically during a Vision request to report progress on long-running tasks.\n\nRequired\n\nvar indeterminate: Bool\nA Boolean set to true when a request can't determine its progress in fractions completed.\n\nRequired\n\nRelationships\nInherits From\nNSObjectProtocol\nConforming Types\nVNRecognizeTextRequest\nSee Also\nRequest Progress Tracking\ntypealias VNRequestProgressHandler\nA block executed at intervals during the processing of a Vision request."
  },
  {
    "title": "Locating and Displaying Recognized Text | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/locating_and_displaying_recognized_text",
    "html": "Overview\n\nNote\n\nThis sample code project is associated with WWDC21 session 10041: Extract Document Data Using Vision.\n\nSee Also\nText Recognition\nRecognizing Text in Images\nAdd text-recognition features to your app using the Vision framework.\nStructuring Recognized Text on a Document\nDetect, recognize, and structure text on a business card or receipt using Vision and VisionKit.\nExtracting phone numbers from text in images\nAnalyze and filter phone numbers from text in live capture by using Vision.\nclass VNRecognizeTextRequest\nAn image analysis request that finds and recognizes text in an image.\nclass VNRecognizedTextObservation\nA request that detects and recognizes regions of text in an image."
  },
  {
    "title": "Extracting phone numbers from text in images | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/extracting_phone_numbers_from_text_in_images",
    "html": "Overview\n\nNew in iOS 13, the Vision framework adds improved text-recognition capabilites that enable you to more easily extract text data from images. The sample app shows you how to use these capabilities to detect and display phone numbers scanned by the camera. It highlights the use of the fast recognition level and a reduced scope to provide responsive real-time text recognition.\n\nThe VNRecognizeTextRequest encapsulates all the necessary logic to recognize text in an image. There are two recognition levels you can choose from, to prioritize accuracy or speed of recognition. Additional configuration properties enable you to specify the languages to recognize, use language correction during recognition, specify the area of the live video to scan, and more. With all of these capabilities, there are tradeoffs between convenience, accuracy, and performance.\n\nFirst the app creates a request to recognize text:\n\n// Set up the Vision request before letting ViewController set up the camera\n// so it exists when the first buffer is received.\nrequest = VNRecognizeTextRequest(completionHandler: recognizeTextHandler)\n\n\nNext, in the delegate method that Vision calls when it writes a sample buffer, the app configures the request to use fast recognition, disable language correction, and set a small rectangular region of interest. Then, the app performs the request:\n\n// Configure for running in real time.\nrequest.recognitionLevel = .fast\n         // Language correction doesn't help in recognizing phone numbers and also\n         // slows recognition.\nrequest.usesLanguageCorrection = false\n// Only run on the region of interest for maximum speed.\nrequest.regionOfInterest = regionOfInterest\n\n\nlet requestHandler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, orientation: textOrientation, options: [:])\ndo {\n    try requestHandler.perform([request])\n} catch {\n    print(error)\n}\n\n\nDisabling language correction improves performance but produces less accurate results. The app includes extensions on Character and String to perform correction optimized for phone-number recognition. See StringUtils.swift to review this code.\n\nConfigure the sample code project\n\nTo run this sample app, you need the following:\n\nXcode 11 or later\n\niPhone with iOS 13 or later\n\nConnect the iPhone to the Mac over USB. The first time you run this sample app, the system prompts you to grant the app access to the camera. You must allow the sample app to access the camera for it to function correctly.\n\nNote\n\nThis sample code project is associated with WWDC19 session 234: Text Recognition in Vision Framework.\n\nSee Also\nText Recognition\nRecognizing Text in Images\nAdd text-recognition features to your app using the Vision framework.\nStructuring Recognized Text on a Document\nDetect, recognize, and structure text on a business card or receipt using Vision and VisionKit.\nLocating and Displaying Recognized Text\nConfigure and perform text recognition on images to identify their textual content.\nclass VNRecognizeTextRequest\nAn image analysis request that finds and recognizes text in an image.\nclass VNRecognizedTextObservation\nA request that detects and recognizes regions of text in an image."
  },
  {
    "title": "VNTextObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntextobservation",
    "html": "Overview\n\nThis type of observation results from a VNDetectTextRectanglesRequest. It expresses the location of each detected character by its bounding box.\n\nTopics\nFinding Individual Characters\nvar characterBoxes: [VNRectangleObservation]?\nAn array of detected individual character bounding boxes.\nRelationships\nInherits From\nVNRectangleObservation\nSee Also\nText Detection\nclass VNDetectTextRectanglesRequest\nAn image analysis request that finds regions of visible text in an image."
  },
  {
    "title": "VNDetectBarcodesRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectbarcodesrequest",
    "html": "Overview\n\nThis request returns an array of VNBarcodeObservation objects, one for each barcode it detects.\n\nTopics\nSpecifying Symbologies\nfunc supportedSymbologies() -> [VNBarcodeSymbology]\nReturns the barcode symbologies that the request supports.\nvar symbologies: [VNBarcodeSymbology]\nThe barcode symbologies that the request detects in an image.\nvar coalesceCompositeSymbologies: Bool\nA Boolean value that indicates whether to coalesce multiple codes based on the symbology.\nstruct VNBarcodeSymbology\nThe barcode symbologies that the framework detects.\nclass var supportedSymbologies: [VNBarcodeSymbology]\nThe array of barcode symbologies that the request supports.\nDeprecated\nAccessing the Results\nvar results: [VNBarcodeObservation]?\nThe results of a barcode detection request.\nclass VNBarcodeObservation\nAn object that represents barcode information that an image analysis request detects.\nIdentifying Request Revisions\nlet VNDetectBarcodesRequestRevision3: Int\nA constant for specifying revision 3 of the barcode detection request.\nlet VNDetectBarcodesRequestRevision2: Int\nA constant for specifying revision 2 of the barcode detection request.\nlet VNDetectBarcodesRequestRevision1: Int\nA constant for specifying revision 1 of the barcode detection request.\nDeprecated\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nBarcode Detection\nenum VNBarcodeCompositeType\nComposite types for barcode requests."
  },
  {
    "title": "VNDetectTextRectanglesRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecttextrectanglesrequest",
    "html": "Overview\n\nThis request returns detected text characters as rectangular bounding boxes with origin and size.\n\nTopics\nConfiguring a Request\nvar reportCharacterBoxes: Bool\nA Boolean value that indicates whether the request detects character bounding boxes.\nAccessing the Results\nvar results: [VNTextObservation]?\nThe results of the request to detect text rectangles.\nIdentifying Request Revisions\nlet VNDetectTextRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the text rectangles detection request.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nText Detection\nclass VNTextObservation\nInformation about regions of text that an image analysis request detects."
  },
  {
    "title": "VNDetectContoursRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectcontoursrequest",
    "html": "Topics\nConfiguring the Request\nvar contrastAdjustment: Float\nThe amount by which to adjust the image contrast.\nvar contrastPivot: NSNumber?\nThe pixel value to use as a pivot for the contrast.\nvar detectsDarkOnLight: Bool\nA Boolean value that indicates whether the request detects a dark object on a light background to aid in detection.\nvar maximumImageDimension: Int\nThe maximum image dimension to use for contour detection.\nvar detectDarkOnLight: Bool\nA Boolean value that indicates whether the request detects a dark object on a light background.\nDeprecated\nAccessing the Results\nvar results: [VNContoursObservation]?\nThe results of the request to detect contours.\nclass VNContoursObservation\nAn object that represents the detected contours in an image.\nIdentifying Request Revisions\nlet VNDetectContourRequestRevision1: Int\nA constant for specifying revision 1 of the contours detection request.\nRelationships\nInherits From\nVNImageBasedRequest"
  },
  {
    "title": "VNDetectTrajectoriesRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecttrajectoriesrequest",
    "html": "Overview\n\nAfter the request detects a trajectory, it produces an observation that contains the shape’s detected points and an equation describing the parabola.\n\nTopics\nCreating a Request\ninit(frameAnalysisSpacing: CMTime, trajectoryLength: Int, completionHandler: VNRequestCompletionHandler?)\nCreates a new request to detect trajectories.\nConfiguring the Request\nvar targetFrameTime: CMTime\nThe requested target frame time for processing trajectory detection.\nvar trajectoryLength: Int\nThe number of points to detect before calculating a trajectory.\nvar objectMinimumNormalizedRadius: Float\nThe minimum radius of the bounding circle of the object to track.\nvar objectMaximumNormalizedRadius: Float\nThe maximum radius of the bounding circle of the object to track.\nvar minimumObjectSize: Float\nThe minimum radius of the tracked shape’s bounding circle.\nDeprecated\nvar maximumObjectSize: Float\nThe maximum radius of the tracked shape’s bounding circle.\nDeprecated\nInspecting the Results\nvar results: [VNTrajectoryObservation]?\nThe array of detected trajectory observations.\nclass VNTrajectoryObservation\nAn observation that describes a detected trajectory.\nIdentifying Request Revisions\nlet VNDetectTrajectoriesRequestRevision1: Int\nA constant for specifying revision 1 of the trajectories detection request.\nRelationships\nInherits From\nVNStatefulRequest\nSee Also\nTrajectory Detection\nIdentifying Trajectories in Video\nGain new insights into your video data by using Vision to detect trajectories.\nDetecting moving objects in a video\nIdentify the trajectory of a thrown object by using Vision."
  },
  {
    "title": "Detecting moving objects in a video | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/detecting_moving_objects_in_a_video",
    "html": "Overview\n\nThe Vision framework provides the ability to detect trajectories of objects in a video. The algorithm looks at the differences between frames of a video and identifies objects that travel along a parabolic path. A single object — like a bouncing ball — may produce multiple trajectories.\n\nThe sample code project shows you how to configure a trajectory detection request to analyze sample buffers. It explores how to set up a capture session to analyze a live-capture feed, and an asset reader to analyze a prerecorded video. When the sample app detects a trajectory, it illustrates it by displaying the detected path on the screen.\n\nFor more information about identifying trajectories, see Identifying Trajectories in Video.\n\nConfigure the project and prepare your environment\n\nYou must run the sample code project on a physical device with an A12 processor or later.\n\nThe sample’s current configuration looks for a small object moving left to right in a video. Try the sample app by downloading and analyzing a prerecorded video of a person tossing a bean bag. For live capture, the sample app requires a stable scene with a fixed camera position and stationary background, so mount your iOS device to a tripod and keep it fixed on the field of view. You need to modify the trajectory request’s configuration based on your conditions when you use your own video.\n\nCreate a trajectory request\n\nBefore the sample app creates a trajectory request, it gets sample buffers based on the selected source — live capture or a prerecorded video — in CameraViewController. After the AVCaptureSession or AVAssetReader retrieves a sample buffer, it passes to the ContentAnalysisViewController. The sample app creates a VNImageRequestHandler to perform the trajectory request with.\n\nlet visionHandler = VNImageRequestHandler(cmSampleBuffer: buffer,\n                                          orientation: orientation,\n                                          options: [:])\n\n\nThe sample app sets up a VNDetectTrajectoriesRequest object to define what the sample app looks for. It looks for trajectories after 1/60 second of video, and returns trajectories with a length of 6 or greater. Generally, developers use a shorter length for real-time apps, and longer lengths to observe finer and longer curves.\n\ndetectTrajectoryRequest = VNDetectTrajectoriesRequest(frameAnalysisSpacing: CMTime(value: 10, timescale: 600),\n                                                      trajectoryLength: 6) { [weak self] (request: VNRequest, error: Error?) -> Void in\n    \n    guard let results = request.results as? [VNTrajectoryObservation] else {\n        return\n    }\n    \n    DispatchQueue.main.async {\n        self?.processTrajectoryObservation(results: results)\n    }\n    \n}\n\n\nAfter the sample app creates the VNDetectTrajectoriesRequest, it configures additional options that describe the radius of the object it’s looking for. To improve the efficiency of the analysis, it also specifies the region of interest.\n\n// Following optional bounds by checking for the moving average radius\n// of the trajectories the app is looking for.\ndetectTrajectoryRequest.objectMinimumNormalizedRadius = 10.0 / Float(1920.0)\ndetectTrajectoryRequest.objectMaximumNormalizedRadius = 30.0 / Float(1920.0)\n\n\n// Help manage the real-time use case to improve the precision versus delay tradeoff.\ndetectTrajectoryRequest.targetFrameTime = CMTimeMake(value: 1, timescale: 60)\n\n\n// The region of interest where the object is moving in the normalized image space.\ndetectTrajectoryRequest.regionOfInterest = normalizedFrame\n\n\nAfter the sample app configures the trajectory request for the buffer, it processes the list of VNTrajectoryObservation results.\n\nProcess the trajectory observation results\n\nThe sample app configuration targets the prerecorded video from the configuration section. The VNDetectTrajectoriesRequest follows objects moving on a parabolic path, and requires more than a single data point (trajectory length). After a request gathers enough data points to recognize the trajectory — a length of at least 5 — it passes the observation results that contain the trajectory information.\n\nThe first step in processing the results involves filtering the VNTrajectoryObservation based on the following conditions:\n\nThe trajectory moves from left to right.\n\nThe trajectory starts in the first half of the region of interest.\n\nThe trajectory length increases to 8, which indicates a throw instead of smaller movements.\n\nThe trajectory contains a parabolic equation constant a, less than or equal to 0, and implies there are either straight lines or downward-facing lines.\n\nThe trajectory confidence is greater than 0.9.\n\nWhen the results meet the above conditions, the sample app deems the observation a valid trajectory. The sample app confirms the trajectory and makes any necessary correction to the path. If a left-to-right moving trajectory begins too far from a fixed region, the sample extrapolates it back to the region by using the available quadratic equation coefficients.\n\nfor trajectory in results {\n    // Filter the trajectory.\n    if filterParabola(trajectory: trajectory) {\n        // Verify and correct an incomplete path.\n        trajectoryView.points = correctTrajectoryPath(trajectoryToCorrect: trajectory)\n        \n        // Display a transition.\n        trajectoryView.performTransition(.fadeIn, duration: 0.05)\n        \n        // Determine the size of the moving object that the app tracks.\n        print(\"The object's moving average radius: \\(trajectory.movingAverageRadius)\")\n    }\n}\n\n\nThe sample app displays valid trajectories on the screen with particle effects by using SpriteKit.\n\nSee Also\nTrajectory Detection\nIdentifying Trajectories in Video\nGain new insights into your video data by using Vision to detect trajectories.\nclass VNDetectTrajectoriesRequest\nA request that detects the trajectories of shapes moving along a parabolic path."
  },
  {
    "title": "Identifying Trajectories in Video | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/identifying_trajectories_in_video",
    "html": "Overview\n\nStarting in iOS 14, tvOS 14, and macOS 11, Vision provides the ability to detect the trajectories of objects in a video sequence. It detects multiple, simultaneous trajectories in a scene, following the path of objects, including those that are only a few pixels in size. This feature has wide-ranging uses, but can be of particular use in sports and fitness apps, where you commonly want to follow the trajectories of balls, pucks, and so on.\n\nVision’s trajectory-detection algorithm requires a stable scene, meaning the camera is mounted on a tripod, and the camera and background remain stationary. The algorithm looks at the frame differentials in the video to detect any objects traveling along a parabolic path. Any camera movement introduces noise and motion blur, which reduces the accuracy of the detection.\n\nA single object may produce multiple trajectories. For example, a bouncing ball forms a new trajectory with each bounce. Similarly, if a trajectory goes offscreen and comes back at a later time, Vision creates a new trajectory for the object.\n\nPerform a Request to Detect Trajectories\n\nTo detect trajectories in video, use an instance of VNDetectTrajectoriesRequest. This is a new stateful request type that you use to build evidence over time. Most Vision requests are stateless, and because they require limited resources to create, you typically make new instances as needed. Because a request to detect trajectories maintains state, you instead create a single instance of it and perform the request multiple times.\n\nWhen you create a VNDetectTrajectoriesRequest, you pass it the following arguments:\n\nA frame analysis spacing, which lets you control the rate at which the request performs its analysis. You provide a CMTime value that defines a millisecond interval value to wait between analysis runs. Setting this argument to zero processes all frames (if the device can keep up). Increasing the value reduces processor consumption, but may produce less accurate results.\n\nA trajectory length, which indicates the number of points you require to recognize a parabola. The minimum number is 5, but you can increase it to fit your needs. The value you set changes the number of points given in each observation. If you know you’re looking for a long throw, you could increase the length to filter out small, spurious movements.\n\nA completion handler to process the results.\n\nThe following example shows how to create and perform a request in an app thatʼs capturing live video from the camera. Note that VNDetectTrajectoriesRequest requires using CMSampleBuffer objects that contain timestamps so it can correctly calculate the trajectory observationʼs time range.\n\n// Lazily create a single instance of VNDetectTrajectoriesRequest.\nprivate lazy var request: VNDetectTrajectoriesRequest = {\n    return VNDetectTrajectoriesRequest(frameAnalysisSpacing: .zero,\n                                       trajectoryLength: 15,\n                                       completionHandler: completionHandler)\n}()\n\n\n// AVCaptureVideoDataOutputSampleBufferDelegate callback.\nfunc captureOutput(_ output: AVCaptureOutput,\n                   didOutput sampleBuffer: CMSampleBuffer,\n                   from connection: AVCaptureConnection) {\n    do {\n        let requestHandler = VNImageRequestHandler(cmSampleBuffer: sampleBuffer)\n        try requestHandler.perform([request])\n    } catch {\n        // Handle the error.\n    }\n}\n\n\nfunc completionHandler(request: VNRequest, error: Error?) {\n    // Process the results.\n}\n\n\n\n\nYou can improve the performance and accuracy of the request by applying filtering criteria to it. If you know the approximate size of the objects you want to follow, you can set a minimum and maximum object size. You specify an object’s size as its normalized pixel diameters. Set the minimum size to filter out small, spurious movements like a bird flying through the scene. Likewise, set the maximum size to filter out larger objects. Like with all Vision requests, you can also set a region of interest (ROI) if you want to detect only those trajectories occurring in a particular region of the frame.\n\n// Set the normalized (0.0 to 1.0) minimum and maximum object sizes.\nrequest.minimumObjectSize = smallDiameter / videoWidth\nrequest.maximumObjectSize = largeDiameter / videoWidth\n\n\n// Set the ROI to the left half of the image.\nrequest.regionOfInterest = CGRect(x: 0, y: 0, width: 0.5, height: 1.0)\n\n\nTip\n\nThe minimum and maximum object sizes aren’t pixel accurate, but instead provide general guidelines to the algorithm. Setting a size that’s slightly smaller than your target minimumObjectSize, and slightly larger than your maximumObjectSize may produce better results.\n\nProcess the Results\n\nAfter the handler performs the request, it calls the request’s completion closure, passing it the request and any errors that occurred. Retrieve the observations by querying the request object for its results, which it returns as an array of VNTrajectoryObservation objects.\n\nfunc requestHandler(request: VNRequest, error: Error?) {\n    guard let observations =\n            request.results as? [VNTrajectoryObservation] else { return }\n    \n    // Process the observations.\n    \n}\n\n\nBecause the request builds evidence over time before it produces trajectory observations, the handler is called, but with no observations until the result has at least one trajectory with a high confidence score and a trajectory length matching the requested length.\n\nThe key pieces of data that a VNTrajectoryObservation provides are the detected and projected points the object travels on the parabolic path. The detected points follow the centroids of the object in motion, which may not follow the parabolic path exactly, whereas the projected points represent the path precisely. You can retrieve the equation coefficients for the quadratic equation, f(x) = ax2 + bx + c, from the observation, which it provides as a simd_float3 value.\n\nEach trajectory provides a uuid value that you can use to track it over time, which is useful when performing ongoing calculations on the data or drawing visualizations of it. For an example of how you can visualize trajectory data, see the Building a feature-rich app for sports analysis sample app.\n\nApply Your Business Logic\n\nVision gives you the tools to detect trajectories and retrieve the relevant data about them. How you use the data is unique to your app, and requires you to apply your own business logic. Some key things to keep in mind as you develop that logic:\n\nFilter out trajectories whose confidence scores don’t meet your criteria.\n\nFilter out irrelevant directional movement. If you only care about objects traveling in a certain direction, evaluate the direction of the path and eliminate irrelevant trajectories.\n\nKnow where to look. If you know the specific region of the image you want to analyze, set the request’s region of interest. Doing so also reduces noise and enhances performance.\n\nKnow the size of what you’re looking for. Whenever possible, set a minimum and maximum object size. Doing so also reduces noise and enhances performance.\n\nPick a camera resolution that’s appropriate for the size of object you want to track. For small objects, like a tennis or cricket ball, you may need 1080p resolution to achieve the results you want. Tracking larger objects, like a soccer ball, may require only VGA resolution for optimal results. Vision downscales the movie as needed, based on the minimum object size, to improve performance.\n\nSee Also\nTrajectory Detection\nDetecting moving objects in a video\nIdentify the trajectory of a thrown object by using Vision.\nclass VNDetectTrajectoriesRequest\nA request that detects the trajectories of shapes moving along a parabolic path."
  },
  {
    "title": "VNAnimalBodyPoseObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnanimalbodyposeobservation",
    "html": "Topics\nAccessing Points\nvar availableJointNames: [VNAnimalBodyPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNAnimalBodyPoseObservation.JointName\nThe joint names for an animal body pose.\nvar availableJointGroupNames: [VNAnimalBodyPoseObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNAnimalBodyPoseObservation.JointsGroupName\nThe joint group names for an animal body pose.\nfunc recognizedPoint(VNAnimalBodyPoseObservation.JointName) -> VNRecognizedPoint\nReturns the point for a joint name the observation recognizes.\nfunc recognizedPoints(VNAnimalBodyPoseObservation.JointsGroupName) -> [VNAnimalBodyPoseObservation.JointName : VNRecognizedPoint]\nReturns the points for a joint group name the observation recognizes.\nRelationships\nInherits From\nVNRecognizedPointsObservation\nSee Also\nAnimal Body Pose Detection\nDetecting animal body poses with Vision\nDraw the skeleton of an animal by using Vision’s capability to detect animal body poses.\nclass VNDetectAnimalBodyPoseRequest\nA request that detects an animal body pose."
  },
  {
    "title": "VNRecognizedPoints3DObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpoints3dobservation",
    "html": "Topics\nInspecting the Observation\nvar availableKeys: [VNRecognizedPointKey]\nThe available point keys in the observation.\nvar availableGroupKeys: [VNRecognizedPointGroupKey]\nThe available point group keys in the observation.\nfunc recognizedPoint(forKey: VNRecognizedPointKey) -> VNRecognizedPoint3D\nReturns a point for a key you specify.\nfunc recognizedPoints(forGroupKey: VNRecognizedPointGroupKey) -> [VNRecognizedPointKey : VNRecognizedPoint3D]\nReturns a point for a group key you specify.\nRelationships\nInherits From\nVNObservation\nSee Also\n3D Body Pose Detection\nIdentifying 3D human body poses in images\nDetect three-dimensional human body poses using the Vision framework.\nDetecting human body poses in 3D with Vision\nRender skeletons of 3D body pose points in a scene overlaying the input image.\nclass VNDetectHumanBodyPose3DRequest\nA request that detects points on human bodies in three-dimensional space, relative to the camera.\nclass VNHumanBodyPose3DObservation\nAn observation that provides the three-dimensional body points the request recognizes.\nclass VNHumanBodyRecognizedPoint3D\nA recognized three-dimensional point that includes a parent joint.\nclass VNPoint3D\nAn object that represents a three-dimensional point in an image.\nclass VNRecognizedPoint3D\nA three-dimensional point that includes an identifier to the point.\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose."
  },
  {
    "title": "VNRecognizedPoint3D | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpoint3d",
    "html": "Topics\nGetting the Identifier\nvar identifier: VNRecognizedPointKey\nThe identifier that provides context about what kind of point the request recognizes.\nRelationships\nInherits From\nVNPoint3D\nSee Also\n3D Body Pose Detection\nIdentifying 3D human body poses in images\nDetect three-dimensional human body poses using the Vision framework.\nDetecting human body poses in 3D with Vision\nRender skeletons of 3D body pose points in a scene overlaying the input image.\nclass VNDetectHumanBodyPose3DRequest\nA request that detects points on human bodies in three-dimensional space, relative to the camera.\nclass VNHumanBodyPose3DObservation\nAn observation that provides the three-dimensional body points the request recognizes.\nclass VNRecognizedPoints3DObservation\nAn observation that provides the three-dimensional points for a request.\nclass VNHumanBodyRecognizedPoint3D\nA recognized three-dimensional point that includes a parent joint.\nclass VNPoint3D\nAn object that represents a three-dimensional point in an image.\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose."
  },
  {
    "title": "VNHumanBodyRecognizedPoint3D | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyrecognizedpoint3d",
    "html": "Topics\nGetting the Position\nvar localPosition: simd_float4x4\nThe three-dimensional position.\nGetting the Parent Joint\nvar parentJoint: VNHumanBodyPose3DObservation.JointName\nThe parent joint in the observation.\nRelationships\nInherits From\nVNRecognizedPoint3D\nSee Also\n3D Body Pose Detection\nIdentifying 3D human body poses in images\nDetect three-dimensional human body poses using the Vision framework.\nDetecting human body poses in 3D with Vision\nRender skeletons of 3D body pose points in a scene overlaying the input image.\nclass VNDetectHumanBodyPose3DRequest\nA request that detects points on human bodies in three-dimensional space, relative to the camera.\nclass VNHumanBodyPose3DObservation\nAn observation that provides the three-dimensional body points the request recognizes.\nclass VNRecognizedPoints3DObservation\nAn observation that provides the three-dimensional points for a request.\nclass VNPoint3D\nAn object that represents a three-dimensional point in an image.\nclass VNRecognizedPoint3D\nA three-dimensional point that includes an identifier to the point.\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose."
  },
  {
    "title": "VNPoint3D | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnpoint3d",
    "html": "Topics\nCreating a Point\ninit?(position: simd_float4x4)\nCreates a point object with the position you specify.\nGetting the Position\nvar position: simd_float4x4\nThe three-dimensional position.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nSee Also\n3D Body Pose Detection\nIdentifying 3D human body poses in images\nDetect three-dimensional human body poses using the Vision framework.\nDetecting human body poses in 3D with Vision\nRender skeletons of 3D body pose points in a scene overlaying the input image.\nclass VNDetectHumanBodyPose3DRequest\nA request that detects points on human bodies in three-dimensional space, relative to the camera.\nclass VNHumanBodyPose3DObservation\nAn observation that provides the three-dimensional body points the request recognizes.\nclass VNRecognizedPoints3DObservation\nAn observation that provides the three-dimensional points for a request.\nclass VNHumanBodyRecognizedPoint3D\nA recognized three-dimensional point that includes a parent joint.\nclass VNRecognizedPoint3D\nA three-dimensional point that includes an identifier to the point.\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose."
  },
  {
    "title": "VNHumanBodyPose3DObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation",
    "html": "Topics\nAccessing Points\nvar availableJointNames: [VNHumanBodyPose3DObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nvar availableJointsGroupNames: [VNHumanBodyPose3DObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose.\nfunc recognizedPoint(VNHumanBodyPose3DObservation.JointName) -> VNHumanBodyRecognizedPoint3D\nReturns the point for a joint name that the observation recognizes.\nfunc recognizedPoints(VNHumanBodyPose3DObservation.JointsGroupName) -> [VNHumanBodyPose3DObservation.JointName : VNHumanBodyRecognizedPoint3D]\nReturns a collection of points for the group name you specify.\nGetting the Joint Position\nfunc pointInImage(VNHumanBodyPose3DObservation.JointName) -> VNPoint\nReturns a 2D point for the joint name you specify, relative to the input image.\nGetting the Parent Joint Name\nfunc parentJointName(VNHumanBodyPose3DObservation.JointName) -> VNHumanBodyPose3DObservation.JointName?\nReturns the parent joint of the joint name you specify.\nGetting the Body Height\nvar heightEstimation: VNHumanBodyPose3DObservation.HeightEstimation\nThe technique the framework uses to estimate body height.\nenum VNHumanBodyPose3DObservation.HeightEstimation\nConstants that identify body height estimation techniques.\nvar bodyHeight: Float\nThe estimated human body height, in meters.\nGetting the Camera Position\nvar cameraOriginMatrix: simd_float4x4\nA transform from the skeleton hip to the camera.\nfunc cameraRelativePosition(VNHumanBodyPose3DObservation.JointName) -> simd_float4x4\nReturns a position relative to the camera for the body joint you specify.\nRelationships\nInherits From\nVNRecognizedPoints3DObservation\nSee Also\n3D Body Pose Detection\nIdentifying 3D human body poses in images\nDetect three-dimensional human body poses using the Vision framework.\nDetecting human body poses in 3D with Vision\nRender skeletons of 3D body pose points in a scene overlaying the input image.\nclass VNDetectHumanBodyPose3DRequest\nA request that detects points on human bodies in three-dimensional space, relative to the camera.\nclass VNRecognizedPoints3DObservation\nAn observation that provides the three-dimensional points for a request.\nclass VNHumanBodyRecognizedPoint3D\nA recognized three-dimensional point that includes a parent joint.\nclass VNPoint3D\nAn object that represents a three-dimensional point in an image.\nclass VNRecognizedPoint3D\nA three-dimensional point that includes an identifier to the point.\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose."
  },
  {
    "title": "VNRecognizedPointKey | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointkey",
    "html": "Topics\nLandmarks\nBody Landmarks\nThe body landmarks that Vision detects.\nHand Landmarks\nThe hand landmarks that Vision detects.\nInitializers\ninit(rawValue: String)\nCreates a recognized point key with a string value.\nRelationships\nConforms To\nEquatable\nHashable\nRawRepresentable\nSendable\nSee Also\nBody and Hand Pose Detection\nDetecting Human Body Poses in Images\nAdd the capability to detect human body poses to your app using the Vision framework.\nDetecting Hand Poses with Vision\nCreate a virtual drawing app by using Vision’s capability to detect hand poses.\nclass VNDetectHumanBodyPoseRequest\nA request that detects a human body pose.\nclass VNDetectHumanHandPoseRequest\nA request that detects a human hand pose.\nclass VNRecognizedPointsObservation\nAn observation that provides the points the analysis recognized.\nclass VNHumanBodyPoseObservation\nAn observation that provides the body points the analysis recognized.\nclass VNHumanHandPoseObservation\nAn observation that provides the hand points the analysis recognized.\nclass VNPoint\nAn immutable object that represents a single, two-dimensional point in an image.\nclass VNDetectedPoint\nAn object that represents a normalized point in an image, along with a confidence value.\nclass VNRecognizedPoint\nAn object that represents a normalized point in an image, along with an identifier label and a confidence value.\nstruct VNRecognizedPointGroupKey\nThe data type for all recognized point group keys."
  },
  {
    "title": "VNDetectHumanBodyPose3DRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanbodypose3drequest",
    "html": "Overview\n\nThis request generates a collection of VNHumanBodyPose3DObservation objects that describe the position of each body the request detects. If the system allows it, the request uses AVDepthData information to improve the accuracy.\n\nTopics\nInitializing a Request\ninit()\nCreates a new request with no completion handler.\ninit(completionHandler: VNRequestCompletionHandler?)\nCreates a new 3D body pose request with a completion handler.\nDetermining Supported Joints\nvar supportedJointsGroupNames: [VNHumanBodyPose3DObservation.JointsGroupName]\nReturns the joint names the request supports.\nvar supportedJointNames: [VNHumanBodyPose3DObservation.JointName]\nReturns the joint group names the request supports.\nAccessing the Results\nvar results: [VNHumanBodyPose3DObservation]?\nThe 3D body pose the request observes.\nRelationships\nInherits From\nVNStatefulRequest\nSee Also\n3D Body Pose Detection\nIdentifying 3D human body poses in images\nDetect three-dimensional human body poses using the Vision framework.\nDetecting human body poses in 3D with Vision\nRender skeletons of 3D body pose points in a scene overlaying the input image.\nclass VNHumanBodyPose3DObservation\nAn observation that provides the three-dimensional body points the request recognizes.\nclass VNRecognizedPoints3DObservation\nAn observation that provides the three-dimensional points for a request.\nclass VNHumanBodyRecognizedPoint3D\nA recognized three-dimensional point that includes a parent joint.\nclass VNPoint3D\nAn object that represents a three-dimensional point in an image.\nclass VNRecognizedPoint3D\nA three-dimensional point that includes an identifier to the point.\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose."
  },
  {
    "title": "VNRecognizedPointGroupKey | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointgroupkey",
    "html": "Topics\nBody Regions\nstatic let bodyLandmarkRegionKeyFace: VNRecognizedPointGroupKey\nA group key identifying the face, which includes the eyes, ears, and nose.\nDeprecated\nstatic let bodyLandmarkRegionKeyTorso: VNRecognizedPointGroupKey\nA group key identifying the torso, which includes the neck, shoulders, hips, and root.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right arm.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftArm: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left arm.\nDeprecated\nstatic let bodyLandmarkRegionKeyRightLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the right leg.\nDeprecated\nstatic let bodyLandmarkRegionKeyLeftLeg: VNRecognizedPointGroupKey\nA group key identifying the landmarks of the left leg.\nDeprecated\nAll Regions\nstatic let all: VNRecognizedPointGroupKey\nA group key identifying all landmarks.\nstatic let point3DGroupKeyAll: VNRecognizedPointGroupKey\nA group key identifying all three-dimensional landmarks.\nInitializers\ninit(rawValue: String)\nCreates a recognized point key with a string value.\nRelationships\nConforms To\nEquatable\nHashable\nRawRepresentable\nSendable\nSee Also\nBody and Hand Pose Detection\nDetecting Human Body Poses in Images\nAdd the capability to detect human body poses to your app using the Vision framework.\nDetecting Hand Poses with Vision\nCreate a virtual drawing app by using Vision’s capability to detect hand poses.\nclass VNDetectHumanBodyPoseRequest\nA request that detects a human body pose.\nclass VNDetectHumanHandPoseRequest\nA request that detects a human hand pose.\nclass VNRecognizedPointsObservation\nAn observation that provides the points the analysis recognized.\nclass VNHumanBodyPoseObservation\nAn observation that provides the body points the analysis recognized.\nclass VNHumanHandPoseObservation\nAn observation that provides the hand points the analysis recognized.\nclass VNPoint\nAn immutable object that represents a single, two-dimensional point in an image.\nclass VNDetectedPoint\nAn object that represents a normalized point in an image, along with a confidence value.\nclass VNRecognizedPoint\nAn object that represents a normalized point in an image, along with an identifier label and a confidence value.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys."
  },
  {
    "title": "Identifying 3D human body poses in images | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/identifying_3d_human_body_poses_in_images",
    "html": "Overview\n\nVision allows you to take your app’s body pose detection into the third dimension. All photos — with people in them — are a 2D representation of people in a 3D world. Starting in iOS 17 and macOS 14, Vision detects human body poses and measures 17 individual joint locations in 3D space. You access a joint location using the joint name itself, or with a joint group name that returns a collection of joints. The following illustration of a 3D model identifies the 17 joint locations that Vision detects.\n\nImages you capture in Portrait mode using Camera — or using AVFoundation — contain depth data that helps to detect distance for each pixel. If your content contains depth data, Vision fetches it automatically. With Vision, you can build an app that tracks a person performing an exercise in 3D space, follows the arm movement during a golf swing, or captures character animations for a video game.\n\nFor more information about recognizing a body pose in 2D, see Detecting Human Body Poses in Images.\n\nPerform a body pose request\n\nDetecting the position of the joints on a human body in 3D space begins with VNDetectHumanBodyPose3DRequest. The request only supports the detection and results for the most prominent person in the frame, so it generates a single observation. For example, performing a request on an image with three people — with one person being closer to the camera — returns an observation that detects the person closest to the camera.\n\nThe observation contains a collection of 17 VNHumanBodyRecognizedPoint3D objects that contain the 3D position of a joint you specify, and the parent joint it connects to. The framework doesn’t return a partial list of joints, so you get all 17 joints or none.\n\nThe request doesn’t require images with depth data to run. However, providing depth data improves detection accuracy.\n\nimport Vision\n\n\n// Get an image from the project bundle. \nguard let filePath = Bundle.main.path(forResource: \"bodypose\", ofType: \"heic\") else { \n    return \n} \nlet fileUrl = URL(fileURLWithPath: filePath) \n\n\n// Create an object to process the request.  \nlet requestHandler = VNImageRequestHandler(url: fileUrl) \n\n\n// Create a request to detect a body pose in 3D space. \nlet request = VNDetectHumanBodyPose3DRequest() \n\n\ndo {    \n    // Perform the body pose request.    \n    try requestHandler.perform([request])    \n\n\n    // Get the observation.    \n    if let observation = request.results?.first {        \n        // Handle the observation.    \n    }\n} catch {\n    print(\"Unable to perform the request: \\(error).\") \n}\n\nHandle the resulting observation\n\nA VNHumanBodyPose3DObservation provides body pose information in 3D space, in meters. The framework normalizes 2D points — from other framework requests — to a lower-left origin. Points that a 3D body pose request returns are relative to the scene in the real world, with an origin at the root joint, located between the leftHip and rightHip.\n\nTo get a list of the available joint names, call supportedJointNames. Joints are also grouped by their location on the body. For example, the left arm group provides the left shoulder, elbow and wrist joints. To get a list of the group names, call supportedJointsGroupNames.\n\n// Get a recognized joint by using a joint name. \nlet leftShoulder = try observation.recognizedPoint(.leftShoulder) \nlet leftElbow = try observation.recognizedPoint(.leftElbow) \nlet leftWrist = try observation.recognizedPoint(.leftWrist) \n\n\n// Get a collection of joints by using a joint group name. \nlet leftArm = try observation.recognizedPoints(.leftArm)\n\n\nIf there’s enough depth metadata, bodyHeight provides an estimated height of the subject, in meters; otherwise, bodyHeight returns a reference height of 1.8 meters. The framework provides a measured height only when configuring an AVCaptureSession to use the LiDAR camera. For more information about configuring your session, see Capturing depth using the LiDAR camera.\n\nUse cameraRelativePosition(_:) to get an estimate of how far the person was away from a camera. To get an accurate understanding of where the camera was when capturing the image, use cameraOriginMatrix.\n\nWork with positions in 3D space\n\nThe Vision frameworks represents a 3D position as a simd_float4x4 matrix. A VNHumanBodyRecognizedPoint3D object contains the position of the joint, in meters, along with an identifier that corresponds to the joint name. It also contains the localPosition of the joint, which describes the position relative to a parentJoint.\n\nImportant\n\nA joint’s position is always relative to the root joint.\n\nUse pointInImage(_:) to project a 3D joint coordinate back to a 2D input image for the body joint you specify. For instance, if you want to align the 3D body pose with the 2D input image, use pointInImage(_:) to get the root joint position in the input image.\n\nA localPosition is useful if your app is only working with one area of the body, and simplifies determining the angle between a child and parent joint.\n\nimport simd \n\n\nvar angleVector: simd_float3 = simd_float3() \n\n\n// Get the position relative to the parent shoulder joint. \nlet childPosition = leftElbow.localPosition \nlet translationChild = simd_make_float3(childPosition.columns.3[0], \n                                        childPosition.columns.3[1], \n                                        childPosition.columns.3[2]) \n\n\n// The rotation around the x-axis. \nlet pitch = (Float.pi / 2) \n\n\n// The rotation around the y-axis. \nlet yaw = acos(translationChild.z / simd_length(translationChild)) \n\n\n// The rotation around the z-axis. \nlet roll = atan2((translationChild.y), (translationChild.x)) \n\n\n// The angle between the elbow and shoulder joint. \nangleVector = simd_float3(pitch, yaw, roll)\n\n\nFor more information about matrices, see Working with Matrices.\n\nUse depth data as input\n\nDepth data contains the information the system needs to reconstruct a 3D scene, and includes camera calibration data. AVDepthData serves as the container class for interfacing with depth metadata. Starting in iOS 17 and macOS 14, provide depth data when you initialize a VNImageRequestHandler with sample or pixel buffers.\n\nImages you capture using Photo mode in Camera store depth as disparity maps — a 2D map reduced from 3D space — with calibration data. You can also configure your AVCaptureSession to use LiDAR to get depth data. For more information about working with depth, see Capturing Photos with Depth and Enhancing Live Video by Leveraging TrueDepth Camera Data.\n\nSee Also\n3D Body Pose Detection\nDetecting human body poses in 3D with Vision\nRender skeletons of 3D body pose points in a scene overlaying the input image.\nclass VNDetectHumanBodyPose3DRequest\nA request that detects points on human bodies in three-dimensional space, relative to the camera.\nclass VNHumanBodyPose3DObservation\nAn observation that provides the three-dimensional body points the request recognizes.\nclass VNRecognizedPoints3DObservation\nAn observation that provides the three-dimensional points for a request.\nclass VNHumanBodyRecognizedPoint3D\nA recognized three-dimensional point that includes a parent joint.\nclass VNPoint3D\nAn object that represents a three-dimensional point in an image.\nclass VNRecognizedPoint3D\nA three-dimensional point that includes an identifier to the point.\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose."
  },
  {
    "title": "VNRecognizedPoint | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpoint",
    "html": "Topics\nInspecting a Point\nvar identifier: VNRecognizedPointKey\nThe point’s identifier label.\nRelationships\nInherits From\nVNDetectedPoint\nSee Also\nBody and Hand Pose Detection\nDetecting Human Body Poses in Images\nAdd the capability to detect human body poses to your app using the Vision framework.\nDetecting Hand Poses with Vision\nCreate a virtual drawing app by using Vision’s capability to detect hand poses.\nclass VNDetectHumanBodyPoseRequest\nA request that detects a human body pose.\nclass VNDetectHumanHandPoseRequest\nA request that detects a human hand pose.\nclass VNRecognizedPointsObservation\nAn observation that provides the points the analysis recognized.\nclass VNHumanBodyPoseObservation\nAn observation that provides the body points the analysis recognized.\nclass VNHumanHandPoseObservation\nAn observation that provides the hand points the analysis recognized.\nclass VNPoint\nAn immutable object that represents a single, two-dimensional point in an image.\nclass VNDetectedPoint\nAn object that represents a normalized point in an image, along with a confidence value.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys.\nstruct VNRecognizedPointGroupKey\nThe data type for all recognized point group keys."
  },
  {
    "title": "VNDetectedPoint | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectedpoint",
    "html": "Topics\nInspecting a Point\nvar confidence: VNConfidence\nA confidence score that indicates the detected point’s accuracy.\nRelationships\nInherits From\nVNPoint\nSee Also\nBody and Hand Pose Detection\nDetecting Human Body Poses in Images\nAdd the capability to detect human body poses to your app using the Vision framework.\nDetecting Hand Poses with Vision\nCreate a virtual drawing app by using Vision’s capability to detect hand poses.\nclass VNDetectHumanBodyPoseRequest\nA request that detects a human body pose.\nclass VNDetectHumanHandPoseRequest\nA request that detects a human hand pose.\nclass VNRecognizedPointsObservation\nAn observation that provides the points the analysis recognized.\nclass VNHumanBodyPoseObservation\nAn observation that provides the body points the analysis recognized.\nclass VNHumanHandPoseObservation\nAn observation that provides the hand points the analysis recognized.\nclass VNPoint\nAn immutable object that represents a single, two-dimensional point in an image.\nclass VNRecognizedPoint\nAn object that represents a normalized point in an image, along with an identifier label and a confidence value.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys.\nstruct VNRecognizedPointGroupKey\nThe data type for all recognized point group keys."
  },
  {
    "title": "VNHumanHandPoseObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanhandposeobservation",
    "html": "Topics\nRetrieving Points\nvar availableJointNames: [VNHumanHandPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanHandPoseObservation.JointName\nThe supported joint names for the hand pose.\nvar availableJointsGroupNames: [VNHumanHandPoseObservation.JointsGroupName]\nThe joint group names available in the observation.\nstruct VNHumanHandPoseObservation.JointsGroupName\nThe supported joint group names for the hand pose.\nfunc recognizedPoint(VNHumanHandPoseObservation.JointName) -> VNRecognizedPoint\nRetrieves the recognized point for a joint name.\nfunc recognizedPoints(VNHumanHandPoseObservation.JointsGroupName) -> [VNHumanHandPoseObservation.JointName : VNRecognizedPoint]\nRetrieves the recognized points associated with the joint group name.\nDetermining the Chirality\nvar chirality: VNChirality\nThe chirality, or handedness, of a pose.\nenum VNChirality\nConstants that the define the chirality, or handedness, of a pose.\nRelationships\nInherits From\nVNRecognizedPointsObservation\nSee Also\nBody and Hand Pose Detection\nDetecting Human Body Poses in Images\nAdd the capability to detect human body poses to your app using the Vision framework.\nDetecting Hand Poses with Vision\nCreate a virtual drawing app by using Vision’s capability to detect hand poses.\nclass VNDetectHumanBodyPoseRequest\nA request that detects a human body pose.\nclass VNDetectHumanHandPoseRequest\nA request that detects a human hand pose.\nclass VNRecognizedPointsObservation\nAn observation that provides the points the analysis recognized.\nclass VNHumanBodyPoseObservation\nAn observation that provides the body points the analysis recognized.\nclass VNPoint\nAn immutable object that represents a single, two-dimensional point in an image.\nclass VNDetectedPoint\nAn object that represents a normalized point in an image, along with a confidence value.\nclass VNRecognizedPoint\nAn object that represents a normalized point in an image, along with an identifier label and a confidence value.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys.\nstruct VNRecognizedPointGroupKey\nThe data type for all recognized point group keys."
  },
  {
    "title": "VNRecognizedPointsObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedpointsobservation",
    "html": "Topics\nInspecting the Observation\nvar availableKeys: [VNRecognizedPointKey]\nThe available point keys in the observation.\nvar availableGroupKeys: [VNRecognizedPointGroupKey]\nThe available point group keys in the observation.\nfunc recognizedPoint(forKey: VNRecognizedPointKey) -> VNRecognizedPoint\nRetrieves a recognized point for a key.\nfunc recognizedPoints(forGroupKey: VNRecognizedPointGroupKey) -> [VNRecognizedPointKey : VNRecognizedPoint]\nRetrieves the recognized points for a key.\nConverting Points for Core ML\nfunc keypointsMultiArray() -> MLMultiArray\nRetrieves the grouping of normalized point coordinates and confidence scores in a format compatible with Core ML.\nRelationships\nInherits From\nVNObservation\nSee Also\nBody and Hand Pose Detection\nDetecting Human Body Poses in Images\nAdd the capability to detect human body poses to your app using the Vision framework.\nDetecting Hand Poses with Vision\nCreate a virtual drawing app by using Vision’s capability to detect hand poses.\nclass VNDetectHumanBodyPoseRequest\nA request that detects a human body pose.\nclass VNDetectHumanHandPoseRequest\nA request that detects a human hand pose.\nclass VNHumanBodyPoseObservation\nAn observation that provides the body points the analysis recognized.\nclass VNHumanHandPoseObservation\nAn observation that provides the hand points the analysis recognized.\nclass VNPoint\nAn immutable object that represents a single, two-dimensional point in an image.\nclass VNDetectedPoint\nAn object that represents a normalized point in an image, along with a confidence value.\nclass VNRecognizedPoint\nAn object that represents a normalized point in an image, along with an identifier label and a confidence value.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys.\nstruct VNRecognizedPointGroupKey\nThe data type for all recognized point group keys."
  },
  {
    "title": "VNDetectHumanHandPoseRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanhandposerequest",
    "html": "Overview\n\nThe framework provides the detected hand pose as a VNIdentifiedPointsObservation.\n\nTopics\nConfiguring the Request\nvar maximumHandCount: Int\nThe maximum number of hands to detect in an image.\nDetermining Supported Joints\nvar supportedJointNames: [VNHumanHandPoseObservation.JointName]\nRetrieves the supported joint names.\nclass func supportedJointNames(forRevision: Int) -> [VNHumanHandPoseObservation.JointName]\nRetrieves the supported joint names for a revision.\nDeprecated\nvar supportedJointsGroupNames: [VNHumanHandPoseObservation.JointsGroupName]\nRetrieves the supported joint group names.\nclass func supportedJointsGroupNames(forRevision: Int) -> [VNHumanHandPoseObservation.JointsGroupName]\nRetrieves the supported joint group names for a revision.\nDeprecated\nAccessing the Results\nvar results: [VNHumanHandPoseObservation]?\nThe observed hand poses.\nIdentifying Hand Pose Revisions\nlet VNDetectHumanHandPoseRequestRevision1: Int\nA constant for specifying revision 1 of the hand pose detection request.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nBody and Hand Pose Detection\nDetecting Human Body Poses in Images\nAdd the capability to detect human body poses to your app using the Vision framework.\nDetecting Hand Poses with Vision\nCreate a virtual drawing app by using Vision’s capability to detect hand poses.\nclass VNDetectHumanBodyPoseRequest\nA request that detects a human body pose.\nclass VNRecognizedPointsObservation\nAn observation that provides the points the analysis recognized.\nclass VNHumanBodyPoseObservation\nAn observation that provides the body points the analysis recognized.\nclass VNHumanHandPoseObservation\nAn observation that provides the hand points the analysis recognized.\nclass VNPoint\nAn immutable object that represents a single, two-dimensional point in an image.\nclass VNDetectedPoint\nAn object that represents a normalized point in an image, along with a confidence value.\nclass VNRecognizedPoint\nAn object that represents a normalized point in an image, along with an identifier label and a confidence value.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys.\nstruct VNRecognizedPointGroupKey\nThe data type for all recognized point group keys."
  },
  {
    "title": "VNHumanBodyPoseObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodyposeobservation",
    "html": "Topics\nAccessing Points\nvar availableJointNames: [VNHumanBodyPoseObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanBodyPoseObservation.JointName\nThe supported joint names for the body pose.\nvar availableJointsGroupNames: [VNHumanBodyPoseObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNHumanBodyPoseObservation.JointsGroupName\nThe supported joint group names for the body pose.\nfunc recognizedPoint(VNHumanBodyPoseObservation.JointName) -> VNRecognizedPoint\nRetrieves the recognized point for a joint name.\nfunc recognizedPoints(VNHumanBodyPoseObservation.JointsGroupName) -> [VNHumanBodyPoseObservation.JointName : VNRecognizedPoint]\nRetrieves the recognized points associated with the joint group name.\nRelationships\nInherits From\nVNRecognizedPointsObservation\nSee Also\nBody and Hand Pose Detection\nDetecting Human Body Poses in Images\nAdd the capability to detect human body poses to your app using the Vision framework.\nDetecting Hand Poses with Vision\nCreate a virtual drawing app by using Vision’s capability to detect hand poses.\nclass VNDetectHumanBodyPoseRequest\nA request that detects a human body pose.\nclass VNDetectHumanHandPoseRequest\nA request that detects a human hand pose.\nclass VNRecognizedPointsObservation\nAn observation that provides the points the analysis recognized.\nclass VNHumanHandPoseObservation\nAn observation that provides the hand points the analysis recognized.\nclass VNPoint\nAn immutable object that represents a single, two-dimensional point in an image.\nclass VNDetectedPoint\nAn object that represents a normalized point in an image, along with a confidence value.\nclass VNRecognizedPoint\nAn object that represents a normalized point in an image, along with an identifier label and a confidence value.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys.\nstruct VNRecognizedPointGroupKey\nThe data type for all recognized point group keys."
  },
  {
    "title": "VNDetectHumanBodyPoseRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanbodyposerequest",
    "html": "Overview\n\nThe framework provides the detected body pose as a VNHumanBodyPoseObservation.\n\nTopics\nDetermining Supported Joints\nvar supportedJointNames: [VNHumanBodyPoseObservation.JointName]\nRetrieves the supported joint names.\nclass func supportedJointNames(forRevision: Int) -> [VNHumanBodyPoseObservation.JointName]\nRetrieves the supported joint names for a revision.\nDeprecated\nvar supportedJointsGroupNames: [VNHumanBodyPoseObservation.JointsGroupName]\nRetrieves the supported joint group names.\nclass func supportedJointsGroupNames(forRevision: Int) -> [VNHumanBodyPoseObservation.JointsGroupName]\nRetrieves the supported joint group names for a revision.\nDeprecated\nAccessing the Results\nvar results: [VNHumanBodyPoseObservation]?\nThe observed body poses.\nIdentifying Body Pose Revisions\nlet VNDetectHumanBodyPoseRequestRevision1: Int\nA constant for specifying revision 1 of the body pose detection request.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nBody and Hand Pose Detection\nDetecting Human Body Poses in Images\nAdd the capability to detect human body poses to your app using the Vision framework.\nDetecting Hand Poses with Vision\nCreate a virtual drawing app by using Vision’s capability to detect hand poses.\nclass VNDetectHumanHandPoseRequest\nA request that detects a human hand pose.\nclass VNRecognizedPointsObservation\nAn observation that provides the points the analysis recognized.\nclass VNHumanBodyPoseObservation\nAn observation that provides the body points the analysis recognized.\nclass VNHumanHandPoseObservation\nAn observation that provides the hand points the analysis recognized.\nclass VNPoint\nAn immutable object that represents a single, two-dimensional point in an image.\nclass VNDetectedPoint\nAn object that represents a normalized point in an image, along with a confidence value.\nclass VNRecognizedPoint\nAn object that represents a normalized point in an image, along with an identifier label and a confidence value.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys.\nstruct VNRecognizedPointGroupKey\nThe data type for all recognized point group keys."
  },
  {
    "title": "Detecting Human Body Poses in Images | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/detecting_human_body_poses_in_images",
    "html": "Overview\n\nA primary goal of Vision is to provide you with tools to help you better identify and understand people in your visual data. Starting in iOS 14 and macOS 11, Vision adds the powerful new ability to identify human body poses. It does so by detecting up to 19 unique body points, as shown in the figure below.\n\nYou can use Vision’s capability for detecting human body poses on its own or with Core ML. Combining Vision with the power of machine learning enables a wide variety of feature possibilities. For example, a safety-training app could help employees use correct ergonomics, a fitness app could automatically track the exercise a user performs, and a media-editing app could find photos or videos based on pose similarity.\n\nPerform a Body Pose Request\n\nVision provides its body pose-detection capabilities through VNDetectHumanBodyPoseRequest, an image-based request type that detects key body points. The following example shows how to use VNImageRequestHandler to perform a VNDetectHumanBodyPoseRequest for detecting body points in the specified CGImage.\n\n// Get the CGImage on which to perform requests.\nguard let cgImage = UIImage(named: \"bodypose\")?.cgImage else { return }\n\n\n// Create a new image-request handler.\nlet requestHandler = VNImageRequestHandler(cgImage: cgImage)\n\n\n// Create a new request to recognize a human body pose.\nlet request = VNDetectHumanBodyPoseRequest(completionHandler: bodyPoseHandler)\n\n\ndo {\n    // Perform the body pose-detection request.\n    try requestHandler.perform([request])\n} catch {\n    print(\"Unable to perform the request: \\(error).\")\n}\n\n\nNote\n\nIf you have a known region of interest (ROI) in an image, you can specify it using the request’s regionOfInterest property. Setting an ROI reduces the region in the image where the request performs its analysis, which generally results in more accurate pose estimation.\n\nProcess the Results\n\nAfter the request handler processes the request, it calls the request’s completion closure, passing it the request and any errors that occurred. Retrieve the observations by querying the request object for its results, which it returns as an array of VNHumanBodyPoseObservation objects. The request returns a unique observation for each detected human body pose, with each containing the recognized points and a confidence score indicating the accuracy of the observation.\n\nfunc bodyPoseHandler(request: VNRequest, error: Error?) {\n    guard let observations =\n            request.results as? [VNHumanBodyPoseObservation] else { \n        return \n    }\n    \n    // Process each observation to find the recognized body pose points.\n    observations.forEach { processObservation($0) }\n}\n\n\nRetrieve the Points\n\nRetrieve the points of interest from the observation by calling its recognizedPoints(_:) method. The argument you pass to this method is a key that identifies all of the points for a particular body region (see VNHumanBodyPoseObservation.JointsGroupName for supported values). This method returns the recognized points for the region as a dictionary of VNRecognizedPoint objects keyed by joint name. Each instance of VNRecognizedPoint provides the X and Y coordinates, in normalized space, and a confidence score for the point. Ignore any recognized points with a confidence value of 0, because they’re invalid.\n\nThe following code example retrieves all of the recognized points for the torso and maps them to an array of CGPoint objects. The example first retrieves the recognized points of the torso by calling recognizedPoints(_:)with the torso key. It then iterates over the specific point keys of the torso and retrieves their associated VNRecognizedPoint object. Finally, if a point’s confidence score is greater than 0, it extracts the point’s coordinates as a CGPoint.\n\nfunc processObservation(_ observation: VNHumanBodyPoseObservation) {\n    \n    // Retrieve all torso points.\n    guard let recognizedPoints =\n            try? observation.recognizedPoints(.torso) else { return }\n    \n    // Torso joint names in a clockwise ordering.\n    let torsoJointNames: [VNHumanBodyPoseObservation.JointName] = [\n        .neck,\n        .rightShoulder,\n        .rightHip,\n        .root,\n        .leftHip,\n        .leftShoulder\n    ]\n    \n    // Retrieve the CGPoints containing the normalized X and Y coordinates.\n    let imagePoints: [CGPoint] = torsoJointNames.compactMap {\n        guard let point = recognizedPoints[$0], point.confidence > 0 else { return nil }\n        \n        // Translate the point from normalized-coordinates to image coordinates.\n        return VNImagePointForNormalizedPoint(point.location,\n                                              Int(imageSize.width),\n                                              Int(imageSize.height))\n    }\n    \n    // Draw the points onscreen.\n    draw(points: imagePoints)\n}\n\n\nNote\n\nThe points the recognizedPoints(_:) method returns are in normalized coordinates (0.0 to 1.0), with the origin at the bottom-left. Use the VNImagePointForNormalizedPoint(_:_:_:) function to translate the normalized points to the input image coordinates.\n\nFor an example of how you can use and visualize recognized body points, see the Building a feature-rich app for sports analysis sample app.\n\nImprove Pose-Detection Accuracy\n\nTo achieve the most accurate results from Vision’s human body pose-detection capabilities, consider the following points:\n\nThe subject’s height should ideally be at least a third of the overall image height.\n\nA large portion of the subject’s key body regions and points should be present in the image.\n\nA subject wearing flowing or robe-like clothing reduces the detection accuracy.\n\nAttempting to detect body poses in dense crowd scenes is likely to produce inaccurate results.\n\nSee Also\nBody and Hand Pose Detection\nDetecting Hand Poses with Vision\nCreate a virtual drawing app by using Vision’s capability to detect hand poses.\nclass VNDetectHumanBodyPoseRequest\nA request that detects a human body pose.\nclass VNDetectHumanHandPoseRequest\nA request that detects a human hand pose.\nclass VNRecognizedPointsObservation\nAn observation that provides the points the analysis recognized.\nclass VNHumanBodyPoseObservation\nAn observation that provides the body points the analysis recognized.\nclass VNHumanHandPoseObservation\nAn observation that provides the hand points the analysis recognized.\nclass VNPoint\nAn immutable object that represents a single, two-dimensional point in an image.\nclass VNDetectedPoint\nAn object that represents a normalized point in an image, along with a confidence value.\nclass VNRecognizedPoint\nAn object that represents a normalized point in an image, along with an identifier label and a confidence value.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys.\nstruct VNRecognizedPointGroupKey\nThe data type for all recognized point group keys."
  },
  {
    "title": "VNDetectHumanRectanglesRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetecthumanrectanglesrequest",
    "html": "Topics\nConfiguring the Request\nvar upperBodyOnly: Bool\nA Boolean value that indicates whether the request requires detecting a full body or upper body only to produce a result.\nAccessing the Results\nvar results: [VNHumanObservation]?\nThe results of the request to find rectangular regions that contain people in an image.\nIdentifying Request Revisions\nlet VNDetectHumanRectanglesRequestRevision2: Int\nA constant for specifying revision 2 of the human rectangles detection request.\nlet VNDetectHumanRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the human rectangles detection request.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nFace and Body Detection\nSelecting a selfie based on capture quality\nCompare face-capture quality in a set of images by using Vision.\nclass VNDetectFaceCaptureQualityRequest\nA request that produces a floating-point number that represents the capture quality of a face in a photo.\nclass VNDetectFaceLandmarksRequest\nAn image analysis request that finds facial features like eyes and mouth in an image.\nclass VNDetectFaceRectanglesRequest\nA request that finds faces within an image.\nclass VNHumanObservation\nAn object that represents a person that the request detects."
  },
  {
    "title": "VNDetectFaceRectanglesRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacerectanglesrequest",
    "html": "Overview\n\nThis request returns faces as rectangular bounding boxes with origin and size.\n\nTopics\nAccessing the Results\nvar results: [VNFaceObservation]?\nThe results of the face detection request.\nclass VNFaceObservation\nFace or facial-feature information that an image analysis request detects.\nIdentifying Request Revisions\nlet VNDetectFaceRectanglesRequestRevision3: Int\nA constant for specifying revision 3 of the face rectangles detection request.\nlet VNDetectFaceRectanglesRequestRevision2: Int\nA constant for specifying revision 2 of the face rectangles detection request.\nlet VNDetectFaceRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the face rectangles detection request.\nDeprecated\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nFace and Body Detection\nSelecting a selfie based on capture quality\nCompare face-capture quality in a set of images by using Vision.\nclass VNDetectFaceCaptureQualityRequest\nA request that produces a floating-point number that represents the capture quality of a face in a photo.\nclass VNDetectFaceLandmarksRequest\nAn image analysis request that finds facial features like eyes and mouth in an image.\nclass VNDetectHumanRectanglesRequest\nA request that finds rectangular regions that contain people in an image.\nclass VNHumanObservation\nAn object that represents a person that the request detects."
  },
  {
    "title": "VNHumanObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanobservation",
    "html": "Topics\nInspecting the Observation\nvar upperBodyOnly: Bool\nA Boolean value that indicates whether the observation represents an upper-body or full-body rectangle.\nRelationships\nInherits From\nVNDetectedObjectObservation\nSee Also\nFace and Body Detection\nSelecting a selfie based on capture quality\nCompare face-capture quality in a set of images by using Vision.\nclass VNDetectFaceCaptureQualityRequest\nA request that produces a floating-point number that represents the capture quality of a face in a photo.\nclass VNDetectFaceLandmarksRequest\nAn image analysis request that finds facial features like eyes and mouth in an image.\nclass VNDetectFaceRectanglesRequest\nA request that finds faces within an image.\nclass VNDetectHumanRectanglesRequest\nA request that finds rectangular regions that contain people in an image."
  },
  {
    "title": "VNDetectFaceCaptureQualityRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectfacecapturequalityrequest",
    "html": "Overview\n\nThis request produces or updates a VNFaceObservation object’s property faceCaptureQuality with a floating-point value. The value ranges from 0 to 1. Faces with quality closer to 1 are better lit, sharper, and more centrally positioned than faces with quality closer to 0.\n\nIf you don’t execute the request, or the request fails, the property faceCaptureQuality is nil.\n\nTopics\nAccessing the Results\nvar results: [VNFaceObservation]?\nThe results of the face-capture quality request.\nclass VNFaceObservation\nFace or facial-feature information that an image analysis request detects.\nIdentifying Request Revisions\nlet VNDetectFaceCaptureQualityRequestRevision2: Int\nRevision 2 of the request algorithm.\nlet VNDetectFaceCaptureQualityRequestRevision1: Int\nA constant for specifying revision 1 of the face capture detection request.\nRelationships\nInherits From\nVNImageBasedRequest\nConforms To\nVNFaceObservationAccepting\nSee Also\nFace and Body Detection\nSelecting a selfie based on capture quality\nCompare face-capture quality in a set of images by using Vision.\nclass VNDetectFaceLandmarksRequest\nAn image analysis request that finds facial features like eyes and mouth in an image.\nclass VNDetectFaceRectanglesRequest\nA request that finds faces within an image.\nclass VNDetectHumanRectanglesRequest\nA request that finds rectangular regions that contain people in an image.\nclass VNHumanObservation\nAn object that represents a person that the request detects."
  },
  {
    "title": "Selecting a selfie based on capture quality | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/selecting_a_selfie_based_on_capture_quality",
    "html": "Overview\n\nNew in iOS 13, the Vision framework adds the Face Capture Quality metric to represent the capture quality of a given face in a photo. The sample app shows you how to use this metric to evaluate a collection of images of the same person and identify which one has the best capture quality.\n\nFace Capture Quality is a holistic measure that considers scene lighting, blur, occlusion, expression, pose, focus, and more. It provides a score you can use to rank multiple captures of the same person. The pre-trained underlying models score a capture lower if, for example, the image contains low light or bad focus, or if the person has a negative expression. These scores are floating-point numbers normalized between 0.0 and 1.0.\n\nFirst the app creates and performs a VNDetectFaceCaptureQualityRequest and obtains face observations from the results:\n\nlet faceDetectionRequest = VNDetectFaceCaptureQualityRequest()\ndo {\n    try handler.perform([faceDetectionRequest])\n    guard let faceObservations = faceDetectionRequest.results as? [VNFaceObservation] else {\n        return\n    }\n    displayFaceObservations(faceObservations)\n    if isCapturingFaces {\n        saveFaceObservations(faceObservations, in: pixelBuffer)\n    }\n} catch {\n    print(\"Vision error: \\(error.localizedDescription)\")\n}\n\n\nThen the app passes the face observations to saveFaceObservations(_:in:), where it retrieves the faceCaptureQuality score for each capture. When the user presses down on the capture button, the app saves each capture’s image data along with its quality score:\n\nlet faceDetectionRequest = VNDetectFaceCaptureQualityRequest()\ndo {\n    try handler.perform([faceDetectionRequest])\n    guard let faceObservations = faceDetectionRequest.results as? [VNFaceObservation] else {\n        return\n    }\n    displayFaceObservations(faceObservations)\n    if isCapturingFaces {\n        saveFaceObservations(faceObservations, in: pixelBuffer)\n    }\n} catch {\n    print(\"Vision error: \\(error.localizedDescription)\")\n}\n\n\nNext, the app sorts the captures based on quality score:\n\n// Sort faces in descending quality-score order.\nsavedFaces.sort { $0.qualityScore < $1.qualityScore }\n\n\nFinally, the app displays the saved faces with their quality scores:\n\nlet savedFace = savedFaces[indexPath.item]\nlet faceImage = UIImage(contentsOfFile: savedFace.url.path)\ncell.imageView.image = faceImage\ncell.label.text = \"\\(savedFace.qualityScore)\"\n\nConfigure the sample code project\n\nTo run this sample app, you need the following:\n\nXcode 11 or later\n\niPhone with iOS 13 or later\n\nConnect the iPhone to the Mac over USB. The first time you run this sample app, the system prompts you to grant the app access to the camera. You must allow the sample app to access the camera for it to function correctly.\n\nNote\n\nThis sample code project is associated with WWDC19 session 222: Understanding Images in Vision Framework.\n\nSee Also\nFace and Body Detection\nclass VNDetectFaceCaptureQualityRequest\nA request that produces a floating-point number that represents the capture quality of a face in a photo.\nclass VNDetectFaceLandmarksRequest\nAn image analysis request that finds facial features like eyes and mouth in an image.\nclass VNDetectFaceRectanglesRequest\nA request that finds faces within an image.\nclass VNDetectHumanRectanglesRequest\nA request that finds rectangular regions that contain people in an image.\nclass VNHumanObservation\nAn object that represents a person that the request detects."
  },
  {
    "title": "VNDetectRectanglesRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectrectanglesrequest",
    "html": "Overview\n\nA rectangle detection request locates regions of an image with rectangular shape, like credit cards, business cards, documents, and signs. The request returns its observations in the form of VNRectangleObservation objects, which contain normalized coordinates of bounding boxes containing the rectangle.\n\nUse this type of request to find the bounding boxes of rectangles in an image. Vision returns observations for rectangles found in all orientations and sizes, along with a confidence level to indicate how likely it’s that the observation contains an actual rectangle.\n\nTo further configure or restrict the types of rectangles found, set properties on the request specifying a range of aspect ratios, sizes, and quadrature tolerance.\n\nTopics\nConfiguring Detection\nvar minimumAspectRatio: VNAspectRatio\nA float specifying the minimum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\nvar maximumAspectRatio: VNAspectRatio\nA float specifying the maximum aspect ratio of the rectangle to detect, defined as the shorter dimension over the longer dimension.\ntypealias VNAspectRatio\nA type alias for expressing rectangle aspect ratios in Vision.\nvar quadratureTolerance: VNDegrees\nA float specifying the number of degrees a rectangle corner angle can deviate from 90°.\ntypealias VNDegrees\nA typealias for expressing tolerance angles in Vision.\nvar minimumSize: Float\nThe minimum size of a rectangle to detect, as a proportion of the smallest dimension.\nvar minimumConfidence: VNConfidence\nA value specifying the minimum acceptable confidence level.\ntypealias VNConfidence\nA type alias for the confidence value of an observation.\nvar maximumObservations: Int\nAn integer specifying the maximum number of rectangles Vision returns.\nAccessing the Results\nvar results: [VNRectangleObservation]?\nThe results of the request to detect rectangles.\nclass VNRectangleObservation\nAn object that represents the four vertices of a detected rectangle.\nIdentifying Request Revisions\nlet VNDetectRectanglesRequestRevision1: Int\nA constant for specifying revision 1 of the rectangle detection request.\nRelationships\nInherits From\nVNImageBasedRequest"
  },
  {
    "title": "VNDetectedObjectObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectedobjectobservation",
    "html": "Overview\n\nThis class is the observation type that VNTrackObjectRequest generates. It represents an object that the Vision request detects and tracks.\n\nTopics\nCreating an Observation\ninit(boundingBox: CGRect)\nCreates an observation with a bounding box.\ninit(requestRevision: Int, boundingBox: CGRect)\nCreates an observation with a revision number and bounding box.\nLocating a Detected Object\nvar boundingBox: CGRect\nThe bounding box of the object that the request detects.\nAccessing an Image Mask\nvar globalSegmentationMask: VNPixelBufferObservation?\nA resulting pixel buffer from a request to generate a segmentation mask for an image.\nRelationships\nInherits From\nVNObservation\nSee Also\nObject Tracking\nTracking the User’s Face in Real Time\nDetect and track faces from the selfie cam feed in real time.\nTracking Multiple Objects or Rectangles in Video\nApply Vision algorithms to track objects or rectangles throughout a video.\nclass VNTrackingRequest\nThe abstract superclass for image analysis requests that track unique features across multiple images or video frames.\nclass VNTrackRectangleRequest\nAn image analysis request that tracks movement of a previously identified rectangular object across multiple images or video frames.\nclass VNTrackObjectRequest\nAn image analysis request that tracks the movement of a previously identified object across multiple images or video frames."
  },
  {
    "title": "VNTrackObjectRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackobjectrequest",
    "html": "Overview\n\nUse this type of request to track the bounding boxes around objects previously identified in an image. Vision attempts to locate the same object from the input observation throughout the sequence.\n\nTopics\nInitializing an Object Tracking Request\ninit(detectedObjectObservation: VNDetectedObjectObservation)\nCreates a new object tracking request with a detected object observation.\ninit(detectedObjectObservation: VNDetectedObjectObservation, completionHandler: VNRequestCompletionHandler?)\nCreates a new object tracking request with a detected object observation.\nIdentifying Request Revisions\nlet VNTrackObjectRequestRevision2: Int\nA constant for specifying revision 2 of the object tracking request.\nlet VNTrackObjectRequestRevision1: Int\nA constant for specifying revision 1 of the object tracking request.\nRelationships\nInherits From\nVNTrackingRequest\nSee Also\nObject Tracking\nTracking the User’s Face in Real Time\nDetect and track faces from the selfie cam feed in real time.\nTracking Multiple Objects or Rectangles in Video\nApply Vision algorithms to track objects or rectangles throughout a video.\nclass VNTrackingRequest\nThe abstract superclass for image analysis requests that track unique features across multiple images or video frames.\nclass VNTrackRectangleRequest\nAn image analysis request that tracks movement of a previously identified rectangular object across multiple images or video frames.\nclass VNDetectedObjectObservation\nAn observation that provides the position and extent of an image feature that an image analysis request detects."
  },
  {
    "title": "VNTrackingRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vntrackingrequest",
    "html": "Overview\n\nInstantiate a tracking request subclass to perform object tracking across multiple frames of an image. After initialization, configure the degree of accuracy by setting trackingLevel, and provide observations you’d like to track by setting the inputObservation initial bounding box.\n\nTopics\nConfiguring a Tracking Request\nenum VNRequestTrackingLevel\nAn enumeration of tracking priorities.\nvar inputObservation: VNDetectedObjectObservation\nThe observation object defining a region to track.\nvar trackingLevel: VNRequestTrackingLevel\nA value for specifying whether to prioritize speed or location accuracy.\nvar isLastFrame: Bool\nA Boolean that indicates the last frame in a tracking sequence.\nGetting the Number of Trackers\nfunc supportedNumber(ofTrackersAndReturnError: NSErrorPointer) -> Int\nReturns the maximum number of simultaneous trackers for the request.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nObject Tracking\nTracking the User’s Face in Real Time\nDetect and track faces from the selfie cam feed in real time.\nTracking Multiple Objects or Rectangles in Video\nApply Vision algorithms to track objects or rectangles throughout a video.\nclass VNTrackRectangleRequest\nAn image analysis request that tracks movement of a previously identified rectangular object across multiple images or video frames.\nclass VNTrackObjectRequest\nAn image analysis request that tracks the movement of a previously identified object across multiple images or video frames.\nclass VNDetectedObjectObservation\nAn observation that provides the position and extent of an image feature that an image analysis request detects."
  },
  {
    "title": "Tracking Multiple Objects or Rectangles in Video | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/tracking_multiple_objects_or_rectangles_in_video",
    "html": "Overview\n\nWith the Vision framework, you can detect and track objects or rectangles through a sequence of frames coming from video, live capture, or other sources.\n\nThis sample app shows you how to pick an initial object to track, how to create Vision tracking requests to follow that object, and how to parse results from the object or rectangle tracker.\n\nPreview the Sample App\n\nTo see this sample app in action, build and run the project in Xcode, then choose a video from your photo library. Once the video is loaded from your photo library, choose to track either objects or rectangles.\n\nNominate Objects or Rectangles to Track\n\nTo track rectangles, select Rectangles. The app runs the rectangle detector and shows rectangles it finds in a preview of the scene.\n\nOtherwise, to track objects, select Objects. Then nominate objects to track by touching them in the preview and dragging boxes around them. You can select multiple objects; the app identifies them by their UUID, using differently colored rectangles.\n\nThe VNTrackObjectRequest class requires a detected object observation to initialize. The sample provides this observation by running VNDetectRectanglesRequest, or by creating one from the bounding box you drew in the preview. It tracks multiple objects by iterating through each observation and creating a VNDetectedObjectObservation from its bounding box.\n\nvar inputObservations = [UUID: VNDetectedObjectObservation]()\nvar trackedObjects = [UUID: TrackedPolyRect]()\nswitch type {\ncase .object:\n    for rect in self.objectsToTrack {\n        let inputObservation = VNDetectedObjectObservation(boundingBox: rect.boundingBox)\n        inputObservations[inputObservation.uuid] = inputObservation\n        trackedObjects[inputObservation.uuid] = rect\n    }\ncase .rectangle:\n    for rectangleObservation in initialRectObservations {\n        inputObservations[rectangleObservation.uuid] = rectangleObservation\n        let rectColor = TrackedObjectsPalette.color(atIndex: trackedObjects.count)\n        trackedObjects[rectangleObservation.uuid] = TrackedPolyRect(observation: rectangleObservation, color: rectColor)\n    }\n}\n\n\nIn your own app, if you prefer to nominate objects programmatically, you can use observations returned from Vision’s own object detection algorithms. For example, the Vision framework’s VNImageRequestHandler accepts face detection, text detection, and barcode detection requests, and those requests return their results in subclasses of VNObservation. You can pass these observations directly into VNTrackObjectRequest.\n\nSelecting a salient object heavily influences the performance of the tracking algorithm; provide the best initial bounding box segmentation of your object that you can.\n\nTrack Objects or Rectangles with a Request Handler\n\nThe Vision framework handles tracking requests through a VNSequenceRequestHandler. Whereas the VNImageRequestHandler handles object detection requests on a still image, the VNSequenceRequestHandler handles tracking requests.\n\nCreate a tracking request for each rectangle or object you’d like to track. Seed each tracking request with the observation created during nomination.\n\nlet request: VNTrackingRequest!\nswitch type {\ncase .object:\n    request = VNTrackObjectRequest(detectedObjectObservation: inputObservation.value)\ncase .rectangle:\n    guard let rectObservation = inputObservation.value as? VNRectangleObservation else {\n        continue\n    }\n    request = VNTrackRectangleRequest(rectangleObservation: rectObservation)\n}\nrequest.trackingLevel = trackingLevel\n\n\ntrackingRequests.append(request)\n\n\nFor each such request, call the sequence request handler’s perform(_:on:orientation:) method, making sure to pass in the video reader’s orientation to ensure upright tracking. This method runs synchronously; use a background queue, such as workQueue in the sample code, so that the main queue isn’t blocked while your requests execute.\n\ntry requestHandler.perform(trackingRequests, on: frame, orientation: videoReader.orientation)\n\n\nBy iterating through each selected object or rectangle, creating a tracking request from it, and calling perform on the request handler, Vision follows the object or rectangle over the image sequence and returns results through its results property.\n\nInterpret Tracking Results\n\nAccess tracking results through the request’s results property or its completion handler. A single tracking request represents a single tracked object in a one-to-one relationship. If a tracking request succeeds, its results property contains VNDetectedObjectObservation objects describing the tracked object’s new location in the frame.\n\nguard let results = processedRequest.results as? [VNObservation] else {\n    continue\n}\nguard let observation = results.first as? VNDetectedObjectObservation else {\n    continue\n}\n// Assume threshold = 0.5f\nlet rectStyle: TrackedPolyRectStyle = observation.confidence > 0.5 ? .solid : .dashed\nlet knownRect = trackedObjects[observation.uuid]!\nswitch type {\ncase .object:\n    rects.append(TrackedPolyRect(observation: observation, color: knownRect.color, style: rectStyle))\ncase .rectangle:\n    guard let rectObservation = observation as? VNRectangleObservation else {\n        break\n    }\n    rects.append(TrackedPolyRect(observation: rectObservation, color: knownRect.color, style: rectStyle))\n}\n\n\nUse the observation’s boundingBox to determine its location, so you can update your app or UI with the tracked object’s new location. Also use it to seed the next round of tracking.\n\ninputObservations[observation.uuid] = observation\n\n\nIn practice, you should periodically run a new tracking request with an updated set of input observations to capture objects that weren’t present in your initial nomination frame. For instance, you could create a new VNTrackObjectRequest every ten frames.\n\nSee Also\nObject Tracking\nTracking the User’s Face in Real Time\nDetect and track faces from the selfie cam feed in real time.\nclass VNTrackingRequest\nThe abstract superclass for image analysis requests that track unique features across multiple images or video frames.\nclass VNTrackRectangleRequest\nAn image analysis request that tracks movement of a previously identified rectangular object across multiple images or video frames.\nclass VNTrackObjectRequest\nAn image analysis request that tracks the movement of a previously identified object across multiple images or video frames.\nclass VNDetectedObjectObservation\nAn observation that provides the position and extent of an image feature that an image analysis request detects."
  },
  {
    "title": "VNGenerateObjectnessBasedSaliencyImageRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateobjectnessbasedsaliencyimagerequest",
    "html": "Overview\n\nThe resulting observation, VNSaliencyImageObservation, encodes this data as a heat map, which you can use to highlight regions of interest.\n\nTopics\nAccessing the Results\nvar results: [VNSaliencyImageObservation]?\nThe results of the image saliency request.\nIdentifying Request Revisions\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision1: Int\nA constant for specifying revision 1 of the image saliency request.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nSaliency Analysis\nCropping Images Using Saliency\nIsolate regions in an image that are most likely to draw people's attention.\nHighlighting Areas of Interest in an Image Using Saliency\nQuantify and visualize where people are likely to look in an image.\nclass VNGenerateAttentionBasedSaliencyImageRequest\nAn object that produces a heat map that identifies the parts of an image most likely to draw attention.\nclass VNSaliencyImageObservation\nAn observation that contains a grayscale heat map of important areas across an image."
  },
  {
    "title": "VNSaliencyImageObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsaliencyimageobservation",
    "html": "Overview\n\nThe heat map is a CVPixelBuffer in a one-component floating-point pixel format. Its dimensions are 64 x 64 when fetched in real time, or 68 x 68 when requested in its deferred form.\n\nTopics\nLocating Salient Regions\nvar salientObjects: [VNRectangleObservation]?\nA collection of objects describing the distinct areas of the saliency heat map.\nRelationships\nInherits From\nVNPixelBufferObservation\nSee Also\nSaliency Analysis\nCropping Images Using Saliency\nIsolate regions in an image that are most likely to draw people's attention.\nHighlighting Areas of Interest in an Image Using Saliency\nQuantify and visualize where people are likely to look in an image.\nclass VNGenerateAttentionBasedSaliencyImageRequest\nAn object that produces a heat map that identifies the parts of an image most likely to draw attention.\nclass VNGenerateObjectnessBasedSaliencyImageRequest\nA request that generates a heat map that identifies the parts of an image most likely to represent objects."
  },
  {
    "title": "VNGenerateAttentionBasedSaliencyImageRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateattentionbasedsaliencyimagerequest",
    "html": "Topics\nAccessing the Results\nvar results: [VNSaliencyImageObservation]?\nThe results of the image saliency request.\nIdentifying Request Revisions\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision1: Int\nA constant for specifying revision 1 of the image saliency request.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nSaliency Analysis\nCropping Images Using Saliency\nIsolate regions in an image that are most likely to draw people's attention.\nHighlighting Areas of Interest in an Image Using Saliency\nQuantify and visualize where people are likely to look in an image.\nclass VNGenerateObjectnessBasedSaliencyImageRequest\nA request that generates a heat map that identifies the parts of an image most likely to represent objects.\nclass VNSaliencyImageObservation\nAn observation that contains a grayscale heat map of important areas across an image."
  },
  {
    "title": "Cropping Images Using Saliency | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/cropping_images_using_saliency",
    "html": "Overview\n\nSaliency refers to what’s noticeable or important in an image. The Vision framework supports two types of saliency: object-based and attention-based.\n\nAttention-based saliency highlights what people are likely to look at. Object-based saliency highlights foreground objects and provides a coarse segmentation of the main subjects in an image. The range of saliency is [0,1], where higher values indicate higher potential for interest or inclusion in the foreground.\n\nSaliency has many applications:\n\nAutomate image cropping to fit key elements.\n\nGenerate thumbnails from an image set.\n\nGuide the camera to focus on a key area for blur estimation or white balance.\n\nGuide postprocessing by determining candidates for sharpening or lighting enhancement.\n\nPan the camera to relevant shots in a photo or video album.\n\nThis article describes one application of saliency: cropping an input image to fit a given aspect ratio while keeping the most interesting element in the image. This technique works by focusing on the region most likely to draw attention at a glance.\n\nSpecify the Type of Saliency\n\nBoth models of saliency are based on deep-learning neural networks. The model used for object-based saliency is trained on foreground objects that have been segmented from the background. The model used for attention-based saliency is trained using eye-tracking data from human subjects looking at images.\n\nThe class of the request to generate saliency differs depending on the type of saliency you want Vision to compute:\n\nswitch SaliencyType(rawValue: UserDefaults.standard.saliencyType)! {\n    case .attentionBased:\n        request = VNGenerateAttentionBasedSaliencyImageRequest()\n    case .objectnessBased:\n        request = VNGenerateObjectnessBasedSaliencyImageRequest()\n}\n\n\nObject-based saliency and attention-based saliency have different use cases. If you're deciding what to keep in an image thumbnail based on what's most interesting, use attention-based saliency.\n\nParse the Output Heatmap\n\nBoth types of saliency requests return their results as heatmaps, which are 68 x 68 pixel buffers of floating-point saliency values. Think of each entry in the heatmap as a cell region of your original image. The heatmap quantifies how salient the pixels in the cell are for the chosen saliency approach.\n\nNote\n\nSaliency is computed on individual images. While it’s possible to compute saliency on each frame of a video stream, keep in mind that the saliency of objects in the scene may vary due to subtle changes in image composition, such as image framing or new subject matter.\n\nIf your app overlays the saliency heatmap on the original input image, upsample the heatmap and apply a colormap before showing it to the user.\n\nMerge Salient Regions for Object-Based Saliency\n\nFor object-based saliency requests, which return up to three bounding boxes, you can use either the most salient bounding box directly to crop a region of your image, or the union of all boxes. Each bounding box comes with a score that you use to rank the relevance of regions within the image.\n\n// Create the union of all salient regions.\nvar unionOfSalientRegions = CGRect(x: 0, y: 0, width: 0, height: 0)\nlet errorPointer = NSErrorPointer(nilLiteral: ())\nlet salientObjects = saliencyObservation.salientObjectsAndReturnError(errorPointer)\nfor salientObject in salientObjects {\n    unionOfSalientRegions = unionOfSalientRegions.union(salientObject.boundingBox)\n}\nself.salientRect = VNImageRectForNormalizedRect(unionOfSalientRegions,\n                             originalImage.extent.size.width,\n                             originalImage.extent.size.height)\n\n\nIf the object-based saliency map value is close to zero everywhere, the image contains nothing that's salient.\n\nCrop the Image\n\nAttention-based saliency requests return only one bounding box, which you use directly to crop the image and drop uninteresting content. Use this bounding box to find the region on which to focus:\n\nDispatchQueue.global(qos: .userInitiated).async {\n    let croppedImage = originalImage.cropped(to: salientRect)\n    let thumbnail =  UIImage(ciImage:croppedImage)\n    // Bounce back to the main thread to update the UI:\n    DispatchQueue.main.async {\n        self.imageView.image = thumbnail\n    }\n}\n\n\nIf nothing is salient, the attention heatmap that's returned highlights the central part of the image. This behavior reflects the fact that people tend to look at the center of an image if nothing in particular stands out to them.\n\nFeed Saliency Results into Other Vision Requests\n\nOnce you know which regions of an image are interesting, you can use the output of a saliency request as the input to another Vision request, like text recognition. The bounding boxes from saliency requests also help you localize the regions you’d like to search, so you can prioritize recognition on the most salient parts of an image, like signs or posters.\n\nlet requestHandler = VNImageRequestHandler(url: imageURL, options: [:])\n\n\nlet request: VNRequest\nswitch type {\ncase .attentionBased:\n    request = VNGenerateAttentionBasedSaliencyImageRequest()\ncase .objectnessBased:\n    request = VNGenerateObjectnessBasedSaliencyImageRequest()\n}\ntry? requestHandler.perform([request])\n    \nreturn request.results?.first as? VNSaliencyImageObservation\n\n\nSee Also\nSaliency Analysis\nHighlighting Areas of Interest in an Image Using Saliency\nQuantify and visualize where people are likely to look in an image.\nclass VNGenerateAttentionBasedSaliencyImageRequest\nAn object that produces a heat map that identifies the parts of an image most likely to draw attention.\nclass VNGenerateObjectnessBasedSaliencyImageRequest\nA request that generates a heat map that identifies the parts of an image most likely to represent objects.\nclass VNSaliencyImageObservation\nAn observation that contains a grayscale heat map of important areas across an image."
  },
  {
    "title": "VNSequenceRequestHandler | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnsequencerequesthandler",
    "html": "Overview\n\nInstantiate this handler to perform Vision requests on a series of images. Unlike the VNImageRequestHandler, you don’t specify the image on creation. Instead, you supply each image frame one by one as you continue to call one of the perform methods.\n\nTopics\nInitializing a Sequence Request\ninit()\nInitializes a sequence request handler.\nPerforming a Sequence Request\nfunc perform([VNRequest], on: CGImage)\nSchedules Vision requests to be performed on a Core Graphics image.\nfunc perform([VNRequest], on: CGImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Graphics image with known orientation.\nfunc perform([VNRequest], on: CIImage)\nSchedules one or more Vision requests to be performed on CIImage data.\nfunc perform([VNRequest], on: CIImage, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on CIImage data with known orientation.\nfunc perform([VNRequest], on: CVPixelBuffer)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer.\nfunc perform([VNRequest], on: CVPixelBuffer, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on a Core Video pixel buffer with known orientation.\nfunc perform([VNRequest], on: CMSampleBuffer)\nPerforms one or more requests on an image contained within a sample buffer.\nfunc perform([VNRequest], on: CMSampleBuffer, orientation: CGImagePropertyOrientation)\nPerforms one or more requests on an image of a specified orientation contained within a sample buffer.\nfunc perform([VNRequest], onImageData: Data)\nSchedules one or more Vision requests to be performed on raw image data.\nfunc perform([VNRequest], onImageData: Data, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on raw data containing an image with known orientation.\nfunc perform([VNRequest], onImageURL: URL)\nSchedules one or more Vision requests to be performed on an image.\nfunc perform([VNRequest], onImageURL: URL, orientation: CGImagePropertyOrientation)\nSchedules one or more Vision requests to be performed on an image with known orientation, at a specific URL.\nRelationships\nInherits From\nNSObject\nSee Also\nImage Sequence Analysis\nApplying Matte Effects to People in Images and Video\nGenerate image masks for people automatically by using semantic person-segmentation.\nDetecting Human Actions in a Live Video Feed\nIdentify body movements by sending a person’s pose data from a series of video frames to an action-classification model.\nclass VNStatefulRequest\nAn abstract request type that builds evidence of a condition over time.\nclass VNGeneratePersonSegmentationRequest\nAn object that produces a matte image for a person it finds in the input image.\nclass VNGeneratePersonInstanceMaskRequest\nAn object that produces a mask of individual people it finds in the input image.\nclass VNDetectDocumentSegmentationRequest\nAn object that detects rectangular regions that contain text in the input image."
  },
  {
    "title": "Detecting Human Actions in a Live Video Feed | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/detecting_human_actions_in_a_live_video_feed",
    "html": "Overview\n\nThis sample app recognizes a person’s body moves, called actions, by analyzing a series of video frames with Vision and predicting the name of the movement by applying an action classifier. The action classifier in this sample recognizes three exercises:\n\nJumping jacks\n\nLunges\n\nBurpees\n\nNote\n\nSee Creating an Action Classifier Model for information about creating your own action classifier.\n\nThe app continually presents its current action prediction on top of a live, full-screen video feed from the device’s camera. When the app recognizes one or more people in the frame, it overlays a wireframe body pose on each person. At the same time, the app predicts the prominent person’s current action; typically this is the person closest to the camera.\n\nAt launch, the app configures the device’s camera to generate video frames and then directs the frames through a series of methods it chains together with Combine. These methods work together to analyze the frames and make action predictions by performing the following sequence of steps:\n\nLocate all human body poses in each frame.\n\nIsolate the prominent pose.\n\nAggregate the prominent pose’s position data over time.\n\nMake action predictions by sending the aggregate data to the action classifier.\n\nConfigure the Sample Code Project\n\nThis sample app uses a camera, so you can’t run it in Simulator — you need to run it on an iOS or iPadOS device.\n\nStart a Video Capture Session\n\nThe app’s VideoCapture class configures the device’s camera to generate video frames by creating an AVCaptureSession.\n\nWhen the app first launches, or when the user rotates the device or switches between cameras, video capture configures a camera input, a frame output, and the connection between them in its configureCaptureSession() method.\n\n// Set the video camera to run at the action classifier's frame rate.\nlet modelFrameRate = ExerciseClassifier.frameRate\n\n\nlet input = AVCaptureDeviceInput.createCameraInput(position: cameraPosition,\n                                                   frameRate: modelFrameRate)\n\n\nlet output = AVCaptureVideoDataOutput.withPixelFormatType(kCVPixelFormatType_32BGRA)\n\n\nlet success = configureCaptureConnection(input, output)\nreturn success ? output : nil\n\n\nThe createCameraInput(position:frameRate:) method selects the front- or rear-facing camera and configures its frame rate so it matches that of the action classifier.\n\nImportant\n\nIf you replace the ExerciseClassifier.mlmodel file with your own action classifier model, set the frameRate property to match the Frame Rate training parameter you used in the Create ML developer tool.\n\nThe AVCaptureVideoDataOutput.withPixelFormatType(_:) method creates an AVCaptureVideoDataOutput that produces frames with a specific pixel format.\n\nThe configureCaptureConnection(_:_:) method configures the relationship between the capture session’s camera input and video output by:\n\nSelecting a video orientation\n\nDeciding whether to horizontally flip the video\n\nEnabling image stabilization when applicable\n\nif connection.isVideoOrientationSupported {\n    // Set the video capture's orientation to match that of the device.\n    connection.videoOrientation = orientation\n}\n\n\nif connection.isVideoMirroringSupported {\n    connection.isVideoMirrored = horizontalFlip\n}\n\n\nif connection.isVideoStabilizationSupported {\n    if videoStabilizationEnabled {\n        connection.preferredVideoStabilizationMode = .standard\n    } else {\n        connection.preferredVideoStabilizationMode = .off\n    }\n}\n\n\nThe method keeps the app operating in real time — and avoids building up a frame backlog — by setting the video output’s alwaysDiscardsLateVideoFrames property to true.\n\n// Discard newer frames if the app is busy with an earlier frame.\noutput.alwaysDiscardsLateVideoFrames = true\n\n\nSee Setting Up a Capture Session for more information on how to configure capture sessions and connect their inputs and outputs.\n\nCreate a Frame Publisher\n\nThe video capture publishes frames from its capture session by creating a PassthroughSubject in its createVideoFramePublisher() method.\n\n// Create a new passthrough subject that publishes frames to subscribers.\nlet passthroughSubject = PassthroughSubject<Frame, Never>()\n\n\n// Keep a reference to the publisher.\nframePublisher = passthroughSubject\n\n\nA passthrough subject is a concrete implementation of Subject that adapts imperative code to work with Combine. It immediately publishes the instance you pass to its send(_:) method, if it has a subscriber at that time.\n\nNext, the video capture registers itself as the video output’s delegate so it receives the video frames from the capture session by calling the output’s setSampleBufferDelegate(_:queue:) method.\n\n// Set the video capture as the video output's delegate.\nvideoDataOutput.setSampleBufferDelegate(self, queue: videoCaptureQueue)\n\n\nThe video capture forwards each frame it receives to its framePublisher by passing the frame to its send(_:) method.\n\nextension VideoCapture: AVCaptureVideoDataOutputSampleBufferDelegate {\n    func captureOutput(_ output: AVCaptureOutput,\n                       didOutput frame: Frame,\n                       from connection: AVCaptureConnection) {\n\n\n        // Forward the frame through the publisher.\n        framePublisher?.send(frame)\n    }\n}\n\nBuild a Publisher Chain\n\nThe sample processes each video frame, and its derivative data, with a series of methods that it connects together into a chain of Combine publishers in the VideoProcessingChain class.\n\nEach time the video capture creates a new frame publisher it notifies the main view controller, which then assigns the publisher to the video-processing chain’s upstreamFramePublisher property:\n\nfunc videoCapture(_ videoCapture: VideoCapture,\n                  didCreate framePublisher: FramePublisher) {\n    updateUILabelsWithPrediction(.startingPrediction)\n    \n    // Build a new video-processing chain by assigning the new frame publisher.\n    videoProcessingChain.upstreamFramePublisher = framePublisher\n}\n\n\nEach time the property’s value changes, the video-processing chain creates a new daisy chain of publishers by calling its buildProcessingChain() method.\n\nThe method creates each new publisher by calling one of the following Publisher methods:\n\nmap(_:),\n\ncompactMap(_:),\n\nscan(_:_:).\n\nfilter(_:)\n\nFor example, the publisher that subscribes to the initial frame publisher is a Publishers.CompactMap that converts each Frame (a type alias of CMSampleBuffer) it receives into a CGImage by calling the video-processing chain’s imageFromFrame(_:) method.\n\n// Create the chain of publisher-subscribers that transform the raw video\n// frames from upstreamFramePublisher.\nframeProcessingChain = upstreamFramePublisher\n    // ---- Frame (aka CMSampleBuffer) -- Frame ----\n\n\n    // Convert each frame to a CGImage, skipping any that don't convert.\n    .compactMap(imageFromFrame)\n\n\n    // ---- CGImage -- CGImage ----\n\n\n    // Detect any human body poses (or lack of them) in the frame.\n    .map(findPosesInFrame)\n\n\n    // ---- [Pose]? -- [Pose]? ----\n\n\nThe next sections explain the remaining publishers in the chain and the methods they use to transform their inputs.\n\nAnalyze Each Frame for Body Poses\n\nThe next publisher in the chain is a Publishers.Map that receives each CGImage from the previous publisher (the compact map) by subscribing to it. The map publisher locates any human body poses in the frame by using the video-processing chain’s findPosesInFrame(_:) method. The method invokes a VNDetectHumanBodyPoseRequest by creating a VNImageRequestHandler with the image and submitting the video-processing chain’s humanBodyPoseRequest property to the handler’s perform(_:) method.\n\nImportant\n\nImprove your app’s efficiency by creating and reusing a single VNDetectHumanBodyPoseRequest instance.\n\n// Create a request handler for the image.\nlet visionRequestHandler = VNImageRequestHandler(cgImage: frame)\n\n\n// Use Vision to find human body poses in the frame.\ndo { try visionRequestHandler.perform([humanBodyPoseRequest]) } catch {\n    assertionFailure(\"Human Pose Request failed: \\(error)\")\n}\n\n\nWhen the request completes, the method creates and returns a Pose array that contains one pose for every VNHumanBodyPoseObservation instance in the request’s results property.\n\nlet poses = Pose.fromObservations(humanBodyPoseRequest.results)\n\n\nThe Pose structure in this sample serves three main purposes:\n\nCalculating the observation’s area within a frame (see “Isolate A Body Pose”)\n\nStoring the the observation’s multiarray (see “Retrieve the Multiarray”)\n\nDrawing an observation as a wireframe of points and lines (see “Present the Poses to the User”)\n\nFor more information about using a VNDetectHumanBodyPoseRequest, see Detecting Human Body Poses in Images.\n\nIsolate a Body Pose\n\nThe next publisher in the chain is a map that chooses a single pose from the array of poses by using the video-processing chain’s isolateLargestPose(_:) method. This method selects the the most prominent pose by passing a closure to the pose array’s max(by:) method.\n\nprivate func isolateLargestPose(_ poses: [Pose]?) -> Pose? {\n    return poses?.max(by:) { pose1, pose2 in pose1.area < pose2.area }\n}\n\n\nThe closure compares the poses’ area estimates, with the goal of consistently selecting the same person’s pose over time, when multiple people are in frame.\n\nImportant\n\nGet the most accurate predictions from an action classifier by using whatever technique you think best tracks a person from frame to frame, and use the multiarray from that person’s VNHumanBodyPoseObservation result.\n\nRetrieve the Multiarray\n\nThe next publisher in the chain is a map that publishes the MLMultiArray from the pose’s multiArray property by using the video processing chain’s multiArrayFromPose(_:) method.\n\nprivate func multiArrayFromPose(_ item: Pose?) -> MLMultiArray? {\n    return item?.multiArray\n}\n\n\nThe Pose initializer copies the multiarray from its VNHumanBodyPoseObservation parameter by calling the observation’s keypointsMultiArray() method.\n\n// Save the multiarray from the observation.\nmultiArray = try? observation.keypointsMultiArray()\n\nGather a Window of Multiarrays\n\nThe next publisher in the chain is a Publishers.Scan that receives each multiarray from its upstream publisher and gathers them into an array by providing two arguments:\n\nAn empty multiarray-optional array ([MLMultiArray?]) as the scan publisher’s initial value\n\nThe video-processing chain’s gatherWindow(previousWindow:multiArray:) method as the scan publisher’s transform.\n\n// ---- MLMultiArray? -- MLMultiArray? ----\n\n\n// Gather a window of multiarrays, starting with an empty window.\n.scan([MLMultiArray?](), gatherWindow)\n\n\n// ---- [MLMultiArray?] -- [MLMultiArray?] ----\n\n\nA scan publisher behaves similarly to a map, but it also maintains a state. The following scan publisher’s state is an array of multiarray optionals that’s initially empty. As the scan publisher receives multiarray optionals from its upstream publisher, the scan publisher passes its previous state and the incoming multiarray optional as arguments to its transform.\n\nprivate func gatherWindow(previousWindow: [MLMultiArray?],\n                          multiArray: MLMultiArray?) -> [MLMultiArray?] {\n    var currentWindow = previousWindow\n\n\n    // If the previous window size is the target size, it\n    // means sendWindowWhenReady() just published an array window.\n    if previousWindow.count == predictionWindowSize {\n        // Advance the sliding array window by stride elements.\n        currentWindow.removeFirst(windowStride)\n    }\n\n\n    // Add the newest multiarray to the window.\n    currentWindow.append(multiArray)\n\n\n    // Publish the array window to the next subscriber.\n    // The currentWindow becomes this method's next previousWindow when\n    // it receives the next multiarray from the upstream publisher.\n    return currentWindow\n}\n\n\nThe method:\n\nCopies the previousWindow parameter to currentWindow\n\nRemoves windowStride elements from the front of currentWindow, if it’s full\n\nAppends the multiArray parameter to the end of currentWindow\n\nThe video-processing chain considers a window to be full if it contains predictionWindowSize elements. When the window is full, this method removes (in step 2) the oldest elements to make room for newer elements, effectively sliding the window forward in time.\n\nThe Exercise Classifier’s calculatePredictionWindowSize() method determines the value of the prediction window size at runtime by inspecting the model’s modelDescription property.\n\nMonitor the Window Size\n\nThe next publisher in the chain is a Publishers.Filter, which only publishes an array window when the gateWindow(_:) method returns true.\n\n// Only publish a window when it grows to the correct size.\n.filter(gateWindow)\n\n\n// ---- [MLMultiArray?] -- [MLMultiArray?] ----\n\n\nThe method returns true if the window array contains exactly the number of elements defined in predictionWindowSize. Otherwise, the method returns false, which instructs the filter publisher to discard the current window and not publish it.\n\nprivate func gateWindow(_ currentWindow: [MLMultiArray?]) -> Bool {\n    return currentWindow.count == predictionWindowSize\n}\n\n\nThis filter publisher, in combination with its upstream scan publisher, publishes an array of multiarray optionals ([MLMultiArray?]) once per each number of frames defined in windowStride.\n\nPredict the Person’s Action\n\nThe next publisher in the chain makes an ActionPrediction from the multiarray window by using the predictActionWithWindow(_:) method as its transform.\n\n// Make an activity prediction from the window.\n.map(predictActionWithWindow)\n\n\n// ---- ActionPrediction -- ActionPrediction ----\n\n\nThe method’s input array contains multiarray optionals where each nil element represents a frame in which Vision wasn’t able to find any human body poses. An action classifier requires a valid, non-nil multiarray for every frame. To remove the nil elements in the array, the method creates a new multiarray, filledWindow, by:\n\nCopying each each valid element in currentWindow\n\nReplacing each nil element in currentWindow with an emptyPoseMultiArray\n\nvar poseCount = 0\n\n\n// Fill the nil elements with an empty pose array.\nlet filledWindow: [MLMultiArray] = currentWindow.map { multiArray in\n    if let multiArray = multiArray {\n        poseCount += 1\n        return multiArray\n    } else {\n        return Pose.emptyPoseMultiArray\n    }\n}\n\n\nThe empty pose multiarray has:\n\nEvery element set to zero\n\nThe same value for its shape property as a multiarray from a human body-pose observation\n\nAs the method iterates through each element in currentWindow, it tallies the number of non-nil elements with poseCount.\n\nIf the value of poseCount is too low, the method directly creates a noPersonPrediction action prediction.\n\n// Only use windows with at least 60% real data to make a prediction\n// with the action classifier.\nlet minimum = predictionWindowSize * 60 / 100\nguard poseCount >= minimum else {\n    return ActionPrediction.noPersonPrediction\n}\n\n\nOtherwise, the method merges the array of multiarrays into a single, combined multiarray by calling the init(concatenating:axis:dataType:) initializer.\n\n// Merge the array window of multiarrays into one multiarray.\nlet mergedWindow = MLMultiArray(concatenating: filledWindow,\n                                axis: 0,\n                                dataType: .float)\n\n\nThe method generates an action prediction by passing the combined multiarray to the action classifier’s predictActionFromWindow(_:) helper method.\n\n// Make a genuine prediction with the action classifier.\nlet prediction = actionClassifier.predictActionFromWindow(mergedWindow)\n\n\n// Return the model's prediction if the confidence is high enough.\n// Otherwise, return a \"Low Confidence\" prediction.\nreturn checkConfidence(prediction)\n\n\nThe method checks the prediction’s confidence by passing the prediction to the checkConfidence(_:) helper method, which returns the same prediction if its confidence is high enough; otherwise lowConfidencePrediction.\n\nPresent the Prediction to the User\n\nThe final component in the chain is a subscriber that notifies the video-processing chain’s delegate with the prediction using the sendPrediction(_:) method.\n\n// Send the action prediction to the delegate.\n.sink(receiveValue: sendPrediction)\n\n\nThe method sends the action prediction and the number of frames the prediction represents (windowStride) to the video-processing chain’s delegate, the main view controller.\n\n// Send the prediction to the delegate on the main queue.\nDispatchQueue.main.async {\n    self.delegate?.videoProcessingChain(self,\n                                        didPredict: actionPrediction,\n                                        for: windowStride)\n}\n\n\nEach time the main view controller receives an action prediction, it updates the app’s UI with the prediction and confidence in a helper method.\n\nfunc videoProcessingChain(_ chain: VideoProcessingChain,\n                          didPredict actionPrediction: ActionPrediction,\n                          for frameCount: Int) {\n\n\n    if actionPrediction.isModelLabel {\n        // Update the total number of frames for this action.\n        addFrameCount(frameCount, to: actionPrediction.label)\n    }\n\n\n    // Present the prediction in the UI.\n    updateUILabelsWithPrediction(actionPrediction)\n}\n\n\nThe main view controller also updates its actionFrameCounts property for action labels that come from the model, which it later sends to the Summary View Controller when the user taps the Summary button.\n\nPresent the Poses to the User\n\nThe app visualizes the result of each human body-pose request by drawing the poses on top of the frame in which Vision found them. Each time the video-processing chain’s findPosesInFrame(_:) creates an array of Pose instances, it sends the poses to its delegate, the main view controller.\n\n// Send the frame and poses, if any, to the delegate on the main queue.\nDispatchQueue.main.async {\n    self.delegate?.videoProcessingChain(self, didDetect: poses, in: frame)\n}\n\n\nThe main view controller’s drawPoses(_:onto:) method uses the frame as the background by first drawing the frame.\n\n// Draw the camera image first as the background.\nlet imageRectangle = CGRect(origin: .zero, size: frameSize)\ncgContext.draw(frame, in: imageRectangle)\n\n\nNext, the method draws the poses by calling their drawWireframeToContext(_:applying:) method, which draws the pose as a wireframe of lines and circles.\n\n// Draw all the poses Vision found in the frame.\nfor pose in poses {\n    // Draw each pose as a wireframe at the scale of the image.\n    pose.drawWireframeToContext(cgContext, applying: pointTransform)\n}\n\n\nThe main view controller presents the finished image to the user by assigning it to its full-screen image view.\n\n// Update the UI's full-screen image view on the main thread.\nDispatchQueue.main.async { self.imageView.image = frameWithPosesRendering }\n\nSee Also\nImage Sequence Analysis\nApplying Matte Effects to People in Images and Video\nGenerate image masks for people automatically by using semantic person-segmentation.\nclass VNStatefulRequest\nAn abstract request type that builds evidence of a condition over time.\nclass VNGeneratePersonSegmentationRequest\nAn object that produces a matte image for a person it finds in the input image.\nclass VNGeneratePersonInstanceMaskRequest\nAn object that produces a mask of individual people it finds in the input image.\nclass VNDetectDocumentSegmentationRequest\nAn object that detects rectangular regions that contain text in the input image.\nclass VNSequenceRequestHandler\nAn object that processes image analysis requests for each frame in a sequence."
  },
  {
    "title": "Highlighting Areas of Interest in an Image Using Saliency | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/highlighting_areas_of_interest_in_an_image_using_saliency",
    "html": "Overview\n\nNote\n\nFor more information about this sample code project, see WWDC 2019 Session 222: Understanding Images in Vision Framework.\n\nSee Also\nSaliency Analysis\nCropping Images Using Saliency\nIsolate regions in an image that are most likely to draw people's attention.\nclass VNGenerateAttentionBasedSaliencyImageRequest\nAn object that produces a heat map that identifies the parts of an image most likely to draw attention.\nclass VNGenerateObjectnessBasedSaliencyImageRequest\nA request that generates a heat map that identifies the parts of an image most likely to represent objects.\nclass VNSaliencyImageObservation\nAn observation that contains a grayscale heat map of important areas across an image."
  },
  {
    "title": "VNDetectDocumentSegmentationRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectdocumentsegmentationrequest",
    "html": "Overview\n\nPerform this request to detect a document in an image. The result that the request generates contains the four corner points of a document’s quadrilateral and saliency mask.\n\nTopics\nAccessing the Results\nvar results: [VNRectangleObservation]?\nThe results of a document segmentation request.\nclass VNRectangleObservation\nAn object that represents the four vertices of a detected rectangle.\nIdentifying Request Revisions\nlet VNDetectDocumentSegmentationRequestRevision1: Int\nA constant for specifying revision 1 of the document segmentation request.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nImage Sequence Analysis\nApplying Matte Effects to People in Images and Video\nGenerate image masks for people automatically by using semantic person-segmentation.\nDetecting Human Actions in a Live Video Feed\nIdentify body movements by sending a person’s pose data from a series of video frames to an action-classification model.\nclass VNStatefulRequest\nAn abstract request type that builds evidence of a condition over time.\nclass VNGeneratePersonSegmentationRequest\nAn object that produces a matte image for a person it finds in the input image.\nclass VNGeneratePersonInstanceMaskRequest\nAn object that produces a mask of individual people it finds in the input image.\nclass VNSequenceRequestHandler\nAn object that processes image analysis requests for each frame in a sequence."
  },
  {
    "title": "VNGeneratePersonInstanceMaskRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeneratepersoninstancemaskrequest",
    "html": "Topics\nAccessing the Results\nvar results: [VNInstanceMaskObservation]?\nThe results of the instance mask request.\nIdentifying Request Revisions\nlet VNGeneratePersonInstanceMaskRequestRevision1: Int\nA constant for specifying revision 1 of the person instance mask request.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nImage Sequence Analysis\nApplying Matte Effects to People in Images and Video\nGenerate image masks for people automatically by using semantic person-segmentation.\nDetecting Human Actions in a Live Video Feed\nIdentify body movements by sending a person’s pose data from a series of video frames to an action-classification model.\nclass VNStatefulRequest\nAn abstract request type that builds evidence of a condition over time.\nclass VNGeneratePersonSegmentationRequest\nAn object that produces a matte image for a person it finds in the input image.\nclass VNDetectDocumentSegmentationRequest\nAn object that detects rectangular regions that contain text in the input image.\nclass VNSequenceRequestHandler\nAn object that processes image analysis requests for each frame in a sequence."
  },
  {
    "title": "VNGeneratePersonSegmentationRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngeneratepersonsegmentationrequest",
    "html": "Overview\n\nPerform this request to detect and generate an image mask for a person in an image. The request returns the resulting image mask in an instance of VNPixelBufferObservation.\n\nTopics\nConfiguring the Request\nvar outputPixelFormat: OSType\nThe pixel format of the output image.\nvar qualityLevel: VNGeneratePersonSegmentationRequest.QualityLevel\nA value that indicates how the request balances accuracy and performance.\nenum VNGeneratePersonSegmentationRequest.QualityLevel\nConstants that define the levels of quality for a person segmentation request.\nAccessing the Results\nvar results: [VNPixelBufferObservation]?\nThe results of the segmentation request.\nclass VNPixelBufferObservation\nAn object that represents an image that an image analysis request produces.\nIdentifying Request Revisions\nlet VNGeneratePersonSegmentationRequestRevision1: Int\nA constant for specifying revision 1 of the person segmentation generation request.\nInitializers\ninit()\nCreates a generate person segmentation request.\ninit(completionHandler: VNRequestCompletionHandler?)\nCreates a generate person segmentation request with a completion handler.\nRelationships\nInherits From\nVNStatefulRequest\nSee Also\nImage Sequence Analysis\nApplying Matte Effects to People in Images and Video\nGenerate image masks for people automatically by using semantic person-segmentation.\nDetecting Human Actions in a Live Video Feed\nIdentify body movements by sending a person’s pose data from a series of video frames to an action-classification model.\nclass VNStatefulRequest\nAn abstract request type that builds evidence of a condition over time.\nclass VNGeneratePersonInstanceMaskRequest\nAn object that produces a mask of individual people it finds in the input image.\nclass VNDetectDocumentSegmentationRequest\nAn object that detects rectangular regions that contain text in the input image.\nclass VNSequenceRequestHandler\nAn object that processes image analysis requests for each frame in a sequence."
  },
  {
    "title": "Applying Matte Effects to People in Images and Video | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/applying_matte_effects_to_people_in_images_and_video",
    "html": "Overview\n\nYou can use image masks to make selective adjustments to particular areas of an image. For example, the illustration below shows how to use a mask to perform a blend operation to replace the background behind the subject.\n\nIn iOS and tvOS 15 and macOS 12, Vision makes it simple for you to generate image masks for people it finds in images and video by using the VNGeneratePersonSegmentationRequest class. This new request type brings the semantic person-segmentation capabilities found in frameworks like ARKit and AVFoundation to the Vision framework.\n\nThe sample app uses the front-facing camera to capture video of the user. It runs a person-segmentation request on each video frame to produce an image mask. Then, it uses Core Image to perform a blend operation that replaces the original video frame’s background with an image that the app dynamically generates and colors based on the user’s head movements. Finally, it renders the image on screen using Metal.\n\nNote\n\nYou must run the sample app on a physical device with iOS 15 or later.\n\nPrepare the Requests\n\nThe app uses two Vision requests to perform its logic: VNDetectFaceRectanglesRequest and VNGeneratePersonSegmentationRequest. It uses VNDetectFaceRectanglesRequest to detect a bounding rectangle around a person’s face. The observation the request produces also includes the roll, yaw, and new in iOS and tvOS 15 and macOS 12, the pitch angles of the rectangle. The app uses the angles to dynamically calculate background colors as the user moves their head.\n\n// Create a request to detect face rectangles.\nfacePoseRequest = VNDetectFaceRectanglesRequest { [weak self] request, _ in\n    guard let face = request.results?.first as? VNFaceObservation else { return }\n    // Generate RGB color intensity values for the face rectangle angles.\n    self?.colors = AngleColors(roll: face.roll, pitch: face.pitch, yaw: face.yaw)\n}\nfacePoseRequest.revision = VNDetectFaceRectanglesRequestRevision3\n\n\nVNGeneratePersonSegmentationRequest generates an image mask for a person it detects in an image. The app can set the value for request’s qualityLevel property to .fast, .balanced, or .accurate; this value determines the quality of the generated mask as shown in the illustration below.\n\nBecause the sample processes live video, it chooses .balanced, which provides a mixture of accuracy and performance. It also sets the format of the mask image that the request generates to an 8-bit, one-component format where 0 represents black.\n\n// Create a request to segment a person from an image.\nsegmentationRequest = VNGeneratePersonSegmentationRequest()\nsegmentationRequest.qualityLevel = .balanced\nsegmentationRequest.outputPixelFormat = kCVPixelFormatType_OneComponent8\n\nPerform the Requests on a Video Frame\n\nThe sample captures video from the front-facing camera and performs the requests on each frame. After the requests finish processing, the app retrieves the image mask from result of the segmentation request and passes it and original frame to the the app’s blend(original:mask:) method for further processing.\n\nprivate func processVideoFrame(_ framePixelBuffer: CVPixelBuffer) {\n    // Perform the requests on the pixel buffer that contains the video frame.\n    try? requestHandler.perform([facePoseRequest, segmentationRequest],\n                                on: framePixelBuffer,\n                                orientation: .right)\n    \n    // Get the pixel buffer that contains the mask image.\n    guard let maskPixelBuffer =\n            segmentationRequest.results?.first?.pixelBuffer else { return }\n    \n    // Process the images.\n    blend(original: framePixelBuffer, mask: maskPixelBuffer)\n}\n\nRender the Results\n\nThe sample processes the results of the requests by taking the original frame, the mask image, and a background image that it dynamically generates based on the roll, pitch, and yaw angles of the user’s face. It creates a CIImage object for each image.\n\n// Create CIImage objects for the video frame and the segmentation mask.\nlet originalImage = CIImage(cvPixelBuffer: framePixelBuffer).oriented(.right)\nvar maskImage = CIImage(cvPixelBuffer: maskPixelBuffer)\n\n\n// Scale the mask image to fit the bounds of the video frame.\nlet scaleX = originalImage.extent.width / maskImage.extent.width\nlet scaleY = originalImage.extent.height / maskImage.extent.height\nmaskImage = maskImage.transformed(by: .init(scaleX: scaleX, y: scaleY))\n\n\n// Define RGB vectors for CIColorMatrix filter.\nlet vectors = [\n    \"inputRVector\": CIVector(x: 0, y: 0, z: 0, w: colors.red),\n    \"inputGVector\": CIVector(x: 0, y: 0, z: 0, w: colors.green),\n    \"inputBVector\": CIVector(x: 0, y: 0, z: 0, w: colors.blue)\n]\n\n\n// Create a colored background image.\nlet backgroundImage = maskImage.applyingFilter(\"CIColorMatrix\",\n                                               parameters: vectors)\n\n\nThe app then scales the mask image to fit the bounds of the captured video frame, and dynamically generates a background image using a CIColorMatrix filter.\n\nIt blends the images and sets the result as the current image, which causes the view to render it on screen.\n\n// Blend the original, background, and mask images.\nlet blendFilter = CIFilter.blendWithRedMask()\nblendFilter.inputImage = originalImage\nblendFilter.backgroundImage = backgroundImage\nblendFilter.maskImage = maskImage\n\n\n// Set the new, blended image as current.\ncurrentCIImage = blendFilter.outputImage?.oriented(.left)\n\nSee Also\nImage Sequence Analysis\nDetecting Human Actions in a Live Video Feed\nIdentify body movements by sending a person’s pose data from a series of video frames to an action-classification model.\nclass VNStatefulRequest\nAn abstract request type that builds evidence of a condition over time.\nclass VNGeneratePersonSegmentationRequest\nAn object that produces a matte image for a person it finds in the input image.\nclass VNGeneratePersonInstanceMaskRequest\nAn object that produces a mask of individual people it finds in the input image.\nclass VNDetectDocumentSegmentationRequest\nAn object that detects rectangular regions that contain text in the input image.\nclass VNSequenceRequestHandler\nAn object that processes image analysis requests for each frame in a sequence."
  },
  {
    "title": "VNObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnobservation",
    "html": "Overview\n\nObservations resulting from Vision image analysis requests inherit from this abstract base class. Don’t use this abstract superclass directly.\n\nTopics\nTracking Observations\nvar uuid: UUID\nA unique identifier assigned to the Vision observation.\nEvaluating Observations\nvar timeRange: CMTimeRange\nThe time range of the reported observation.\nvar confidence: VNConfidence\nThe level of confidence in the observation’s accuracy.\ntypealias VNConfidence\nA type alias for the confidence value of an observation.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nNSSecureCoding\nVNRequestRevisionProviding\nSee Also\nStill Image Analysis\nDetecting Objects in Still Images\nLocate and demarcate rectangles, faces, barcodes, and text in images using the Vision framework.\nClassifying Images for Categorization and Search\nAnalyze and label images using a Vision classification request.\nAnalyzing Image Similarity with Feature Print\nGenerate a feature print to compute distance between images.\nclass VNRequest\nThe abstract superclass for analysis requests.\nclass VNImageBasedRequest\nThe abstract superclass for image analysis requests that focus on a specific part of an image.\nclass VNClassifyImageRequest\nA request to classify an image.\nclass VNGenerateImageFeaturePrintRequest\nAn image-based request to generate feature prints from an image.\nclass VNImageRequestHandler\nAn object that processes one or more image analysis requests pertaining to a single image."
  },
  {
    "title": "VNImageRequestHandler | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagerequesthandler",
    "html": "Overview\n\nInstantiate this handler to perform Vision requests on a single image. You specify the image and, optionally, a completion handler at the time of creation, and call perform(_:) to begin executing the request.\n\nTopics\nCreating a Request Handler\ninit(cgImage: CGImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on Core Graphics images.\ninit(cgImage: CGImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on a Core Graphics image with known orientation.\ninit(ciImage: CIImage, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data.\ninit(ciImage: CIImage, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on CIImage data of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer.\ninit(cvPixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler for performing requests on a Core Video pixel buffer of a known orientation.\ninit(cvPixelBuffer: CVPixelBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\ninit(cmSampleBuffer: CMSampleBuffer, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image of a specified orientation contained within a sample buffer.\ninit(cmSampleBuffer: CMSampleBuffer, depthData: AVDepthData, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a request handler that performs requests on an image in a sample buffer that contains depth data.\ninit(data: Data, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image contained in an NSData object.\ninit(data: Data, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image of known orientation, contained in an NSData object.\ninit(url: URL, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image at the specified URL.\ninit(url: URL, orientation: CGImagePropertyOrientation, options: [VNImageOption : Any])\nCreates a handler to be used for performing requests on an image with known orientation, at the specified URL.\nExecuting a Request Handler\nfunc perform([VNRequest])\nSchedules Vision requests to perform.\nSetting Image Options\nstruct VNImageOption\nAn option key passed into VNImageRequestHandler creations or requests that take an auxiliary image.\nRelationships\nInherits From\nNSObject\nSee Also\nStill Image Analysis\nDetecting Objects in Still Images\nLocate and demarcate rectangles, faces, barcodes, and text in images using the Vision framework.\nClassifying Images for Categorization and Search\nAnalyze and label images using a Vision classification request.\nAnalyzing Image Similarity with Feature Print\nGenerate a feature print to compute distance between images.\nclass VNRequest\nThe abstract superclass for analysis requests.\nclass VNImageBasedRequest\nThe abstract superclass for image analysis requests that focus on a specific part of an image.\nclass VNClassifyImageRequest\nA request to classify an image.\nclass VNGenerateImageFeaturePrintRequest\nAn image-based request to generate feature prints from an image.\nclass VNObservation\nThe abstract superclass for analysis results."
  },
  {
    "title": "VNRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrequest",
    "html": "Overview\n\nOther Vision request handlers that perform image analysis inherit from this abstract base class. Instantiate one of its subclasses to perform image analysis.\n\nImportant\n\nA VNRequest discards the alpha channel of input images, so don’t rely on it.\n\nTopics\nInitializing a Request\ninit()\nCreates a new Vision request with no completion handler.\ninit(completionHandler: VNRequestCompletionHandler?)\nCreates a new Vision request with an optional completion handler.\nConfiguring a Request\nvar completionHandler: VNRequestCompletionHandler?\nThe completion handler the system invokes after the request finishes processing.\nvar preferBackgroundProcessing: Bool\nA hint to minimize the resource burden of the request.\nvar results: [VNObservation]?\nThe collection of VNObservation results generated by request processing.\nvar revision: Int\nThe specific algorithm or implementation revision that’s used to perform the request.\nvar usesCPUOnly: Bool\nA Boolean signifying that the Vision request should execute exclusively on the CPU.\nDeprecated\nConfiguring the Compute Device\nfunc setComputeDevice(MLComputeDevice?, for: VNComputeStage)\nAssigns a compute device for a compute stage.\nfunc computeDevice(for: VNComputeStage) -> MLComputeDevice?\nReturns the compute device for a compute stage.\nvar supportedComputeStageDevices: [VNComputeStage : [MLComputeDevice]]\nThe collection of compute devices per stage that a request supports.\nCanceling a Request\nfunc cancel()\nCancels the request before it can finish executing.\nExecuting a Completion Handler\ntypealias VNRequestCompletionHandler\nA type alias to encapsulate the syntax for the completion handler the system calls after the request finishes processing.\nDetermining the Revision\nprotocol VNRequestRevisionProviding\nA protocol for specifying the revision number of Vision algorithms.\nclass var currentRevision: Int\nThe current revison supported by the request.\nclass var defaultRevision: Int\nThe revision of the latest request for the particular SDK linked with the client application.\nclass var supportedRevisions: IndexSet\nThe collection of currently-supported algorithm versions for the class of request.\nRelationships\nInherits From\nNSObject\nConforms To\nNSCopying\nSee Also\nStill Image Analysis\nDetecting Objects in Still Images\nLocate and demarcate rectangles, faces, barcodes, and text in images using the Vision framework.\nClassifying Images for Categorization and Search\nAnalyze and label images using a Vision classification request.\nAnalyzing Image Similarity with Feature Print\nGenerate a feature print to compute distance between images.\nclass VNImageBasedRequest\nThe abstract superclass for image analysis requests that focus on a specific part of an image.\nclass VNClassifyImageRequest\nA request to classify an image.\nclass VNGenerateImageFeaturePrintRequest\nAn image-based request to generate feature prints from an image.\nclass VNImageRequestHandler\nAn object that processes one or more image analysis requests pertaining to a single image.\nclass VNObservation\nThe abstract superclass for analysis results."
  },
  {
    "title": "VNClassifyImageRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnclassifyimagerequest",
    "html": "Overview\n\nThis type of request produces a collection of VNClassificationObservation objects that describe an image. Access the classifications through knownClassifications(forRevision:).\n\nTopics\nAccessing Results\nfunc supportedIdentifiers() -> [String]\nReturns the classification identifiers that the request supports in its current configuration.\nvar results: [VNClassificationObservation]?\nThe results of the image classification request.\nclass VNClassificationObservation\nAn object that represents classification information that an image analysis request produces.\nclass func knownClassifications(forRevision: Int) -> [VNClassificationObservation]\nRequests the collection of classifications that the Vision framework recognizes.\nDeprecated\nSpecifying Algorithm Revision\nlet VNClassifyImageRequestRevision1: Int\nA constant for specifying revision 1 of the image classification request.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nStill Image Analysis\nDetecting Objects in Still Images\nLocate and demarcate rectangles, faces, barcodes, and text in images using the Vision framework.\nClassifying Images for Categorization and Search\nAnalyze and label images using a Vision classification request.\nAnalyzing Image Similarity with Feature Print\nGenerate a feature print to compute distance between images.\nclass VNRequest\nThe abstract superclass for analysis requests.\nclass VNImageBasedRequest\nThe abstract superclass for image analysis requests that focus on a specific part of an image.\nclass VNGenerateImageFeaturePrintRequest\nAn image-based request to generate feature prints from an image.\nclass VNImageRequestHandler\nAn object that processes one or more image analysis requests pertaining to a single image.\nclass VNObservation\nThe abstract superclass for analysis results."
  },
  {
    "title": "VNGenerateImageFeaturePrintRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vngenerateimagefeatureprintrequest",
    "html": "Overview\n\nThis request returns the feature print data it generates as an array of VNFeaturePrintObservation objects.\n\nTopics\nScaling and Cropping Images\nvar imageCropAndScaleOption: VNImageCropAndScaleOption\nAn optional setting that tells the algorithm how to scale an input image before generating the feature print.\nenum VNImageCropAndScaleOption\nOptions that define how Vision crops and scales an input-image.\nAccessing the Results\nvar results: [VNFeaturePrintObservation]?\nThe results of the feature print request.\nclass VNFeaturePrintObservation\nAn observation that provides the recognized feature print.\nIdentifying Request Revisions\nlet VNGenerateImageFeaturePrintRequestRevision1: Int\nA constant for specifying revision 1 of the feature print request.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nStill Image Analysis\nDetecting Objects in Still Images\nLocate and demarcate rectangles, faces, barcodes, and text in images using the Vision framework.\nClassifying Images for Categorization and Search\nAnalyze and label images using a Vision classification request.\nAnalyzing Image Similarity with Feature Print\nGenerate a feature print to compute distance between images.\nclass VNRequest\nThe abstract superclass for analysis requests.\nclass VNImageBasedRequest\nThe abstract superclass for image analysis requests that focus on a specific part of an image.\nclass VNClassifyImageRequest\nA request to classify an image.\nclass VNImageRequestHandler\nAn object that processes one or more image analysis requests pertaining to a single image.\nclass VNObservation\nThe abstract superclass for analysis results."
  },
  {
    "title": "Classifying Images for Categorization and Search | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/classifying_images_for_categorization_and_search",
    "html": "Overview\n\nNote\n\nFor more information about this sample code project, see WWDC 2019 Session 222: Understanding Images in Vision Framework.\n\nSee Also\nStill Image Analysis\nDetecting Objects in Still Images\nLocate and demarcate rectangles, faces, barcodes, and text in images using the Vision framework.\nAnalyzing Image Similarity with Feature Print\nGenerate a feature print to compute distance between images.\nclass VNRequest\nThe abstract superclass for analysis requests.\nclass VNImageBasedRequest\nThe abstract superclass for image analysis requests that focus on a specific part of an image.\nclass VNClassifyImageRequest\nA request to classify an image.\nclass VNGenerateImageFeaturePrintRequest\nAn image-based request to generate feature prints from an image.\nclass VNImageRequestHandler\nAn object that processes one or more image analysis requests pertaining to a single image.\nclass VNObservation\nThe abstract superclass for analysis results."
  },
  {
    "title": "Detecting Objects in Still Images | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/detecting_objects_in_still_images",
    "html": "Overview\n\nThe Vision framework can detect rectangles, faces, text, and barcodes at any orientation. This sample code shows how to create requests to detect these types of objects, and how to interpret the results of those requests. To help you visualize where an observation occurs, and how it looks, this code uses Core Animation layers to draw paths around detected features in images. For example, the following mock gift card has a QR code and rectangles that surface through the detector. The sample highlights not only text blocks (shown in red) but also individual characters within text (shown in purple):\n\nThis sample code project runs on iOS 11. However, you can also use Vision in your own apps on macOS 10.13, iOS 11, or tvOS 11.\n\nTo see this sample in action, build and run the project, then use the toggle switches to choose which kinds of objects (any combination of rectangles, faces, barcodes, and text) to detect. Tapping anywhere else prompts the sample to request a picture, which you either capture by camera or select from your photo library. The sample then applies computer vision algorithms to find the desired features in the provided image. Finally, the sample draws colored paths around observed features on Core Animation layers.\n\nPrepare an Input Image for Vision\n\nVision handles still image-based requests using a VNImageRequestHandler and assumes that images are oriented upright, so pass your image with orientation in mind. CGImage, CIImage, and CVPixelBuffer objects don’t carry orientation, so provide it as part of the initializer.\n\nYou can initialize a VNImageRequestHandler from image data in the following formats:\n\nCGImage: The Core Graphics image format, obtainable from any UIImage through its helper method cgImage. Specify orientation through the initializer using CGImagePropertyOrientation.\n\nCIImage: The Core Image format, best used if you already have Core Image in your image processing pipeline. CIImage objects don’t contain orientation, so supply it in the initializer init(ciImage:orientation:options:).\n\nCVPixelBuffer: The Core Video image format for data from a live feed and movies. CVPixelBuffer objects don’t contain orientation, so supply it in the initializer init(cvPixelBuffer:orientation:options:).\n\nNSData: Image data compressed or held in memory, as you might receive over a network connection. For example, photos downloaded from a website or the cloud fall into this category. Check that any images downloaded from the web have upright orientation; if they don’t, inform Vision about the orientation through the initializer init(data:orientation:options:).\n\nNSURL: A URL path to the image on disk.\n\nVision may not detect sideways or upside-down features properly if it assumes the wrong orientation. Photos selected in the sample’s image picker contain orientation information. Access this data through the UIImage property imageOrientation. If you acquire your photos through other means, such as from the web or other apps, be sure to check for orientation and provide it separately if it doesn’t come baked into the image.\n\nCreate Vision Requests\n\nCreate a VNImageRequestHandler object with the image to be processed.\n\n// Create a request handler.\nlet imageRequestHandler = VNImageRequestHandler(cgImage: image,\n                                                orientation: orientation,\n                                                options: [:])\n\n\nIf you’re making multiple requests from the same image (for example, detecting facial features as well as faces), create and bundle all requests to pass into the image request handler. Vision runs each request and executes its completion handler on its own thread.\n\nYou can pair each request with a completion handler to run request-specific code after Vision finishes all requests. The sample draws boxes differently based on the type of request, so this code differs from request to request. Specify your completion handler when initializing each request.\n\nlazy var rectangleDetectionRequest: VNDetectRectanglesRequest = {\n    let rectDetectRequest = VNDetectRectanglesRequest(completionHandler: self.handleDetectedRectangles)\n    // Customize & configure the request to detect only certain rectangles.\n    rectDetectRequest.maximumObservations = 8 // Vision currently supports up to 16.\n    rectDetectRequest.minimumConfidence = 0.6 // Be confident.\n    rectDetectRequest.minimumAspectRatio = 0.3 // height / width\n    return rectDetectRequest\n}()\n\n\nAfter you’ve created all your requests, pass them as an array to the request handler’s synchronous perform(_:). Vision computations may consume resources and take time, so use a background queue to avoid blocking the main queue as it executes.\n\n// Send the requests to the request handler.\nDispatchQueue.global(qos: .userInitiated).async {\n    do {\n        try imageRequestHandler.perform(requests)\n    } catch let error as NSError {\n        print(\"Failed to perform image request: \\(error)\")\n        self.presentAlert(\"Image Request Failed\", error: error)\n        return\n    }\n}\n\nInterpret Detection Results\n\nThe method perform(_:) returns a Boolean representing whether the requests succeeded or resulted in an error. If it succeeded, its results property contains observation or tracking data, such as a detected object’s location and bounding box.\n\nYou can access results in two ways:\n\nCheck the results property after calling perform(_:).\n\nIn the VNImageBasedRequest object’s completion handler, use the callback’s observation parameter to retrieve detection information. The callback results may contain multiple observations, so loop through the observations array to process each one.\n\nFor example, the sample uses facial observations and their landmarks’ bounding boxes to locate the features and draw a rectangle around them.\n\n// Perform drawing on the main thread.\nDispatchQueue.main.async {\n    guard let drawLayer = self.pathLayer,\n        let results = request?.results as? [VNFaceObservation] else {\n            return\n    }\n    self.draw(faces: results, onImageWithBounds: drawLayer.bounds)\n    drawLayer.setNeedsDisplay()\n}\n\n\nEven when Vision calls its completion handlers on a background thread, always dispatch UI calls like the path-drawing code to the main thread. Access to UIKit, AppKit & resources must be serialized, so changes that affect the app’s immediate appearance belong on the main thread.\n\nCATransaction.begin()\nfor observation in faces {\n    let faceBox = boundingBox(forRegionOfInterest: observation.boundingBox, withinImageBounds: bounds)\n    let faceLayer = shapeLayer(color: .yellow, frame: faceBox)\n    \n    // Add to pathLayer on top of image.\n    pathLayer?.addSublayer(faceLayer)\n}\nCATransaction.commit()\n\n\nFor face landmark requests, the detector provides VNFaceObservation results with greater detail, such as VNFaceLandmarkRegion2D.\n\nFor text observations, you can locate individual characters by checking the characterBoxes property.\n\nFor barcode observations, some supported symbologies contain payload information in the payloadStringValue property, allowing you to parse the content of detected barcodes. Like a supermarket scanner, barcode detection is optimized for finding one barcode per image.\n\nIt’s up to your app to use or store data from the observations before exiting the completion handler. Instead of drawing paths like the sample does, write custom code to extract what your app needs from each observation.\n\nFollow Best Practices\n\nTo reduce unnecessary computation, don’t create multiple request handlers and submit them multiple times on the same image. Instead, create all your requests before querying Vision, bundle them inside a requests array, and submit that array in a single call.\n\nTo perform detection across multiple, unrelated images, create a separate image handler for each image and make requests to each handler on separate threads, so they run in parallel. Each image request handler costs additional processing time and memory, so try not to run them on the main thread. Dispatch these handlers on additional background threads, calling back to the main thread only for UI updates such as displaying images or paths.\n\nThe image-based handler that this sample introduces works for detection across any number of images, but it doesn’t track objects. To perform object tracking, use a VNSequenceRequestHandler instead. For more information about object tracking, see Tracking the User’s Face in Real Time.\n\nSee Also\nStill Image Analysis\nClassifying Images for Categorization and Search\nAnalyze and label images using a Vision classification request.\nAnalyzing Image Similarity with Feature Print\nGenerate a feature print to compute distance between images.\nclass VNRequest\nThe abstract superclass for analysis requests.\nclass VNImageBasedRequest\nThe abstract superclass for image analysis requests that focus on a specific part of an image.\nclass VNClassifyImageRequest\nA request to classify an image.\nclass VNGenerateImageFeaturePrintRequest\nAn image-based request to generate feature prints from an image.\nclass VNImageRequestHandler\nAn object that processes one or more image analysis requests pertaining to a single image.\nclass VNObservation\nThe abstract superclass for analysis results."
  },
  {
    "title": "VNImageHomographicAlignmentObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnimagehomographicalignmentobservation",
    "html": "Overview\n\nThis type of observation results from a VNHomographicImageRegistrationRequest, informing the warpTransform performed to align the input images.\n\nTopics\nAccessing the Transform\nvar warpTransform: matrix_float3x3\nThe warp transform matrix to morph the floating image into the reference image.\nRelationships\nInherits From\nVNImageAlignmentObservation\nSee Also\nImage Alignment\nAligning Similar Images\nConstruct a composite image from images that capture the same scene.\nclass VNTargetedImageRequest\nThe abstract superclass for image analysis requests that operate on both the processed image and a secondary image.\nclass VNImageRegistrationRequest\nThe abstract superclass for image analysis requests that align images according to their content.\nclass VNTranslationalImageRegistrationRequest\nAn image analysis request that determines the affine transform necessary to align the content of two images.\nclass VNTrackTranslationalImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the affine transform necessary to align the content of two images.\nclass VNHomographicImageRegistrationRequest\nAn image analysis request that determines the perspective warp matrix necessary to align the content of two images.\nclass VNTrackHomographicImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the perspective warp matrix necessary to align the content of two images.\nclass VNImageAlignmentObservation\nThe abstract superclass for image analysis results that describe the relative alignment of two images.\nclass VNImageTranslationAlignmentObservation\nAffine transform information that an image alignment request produces."
  },
  {
    "title": "VNRecognizedTextObservation | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizedtextobservation",
    "html": "Overview\n\nThis type of observation results from a VNRecognizeTextRequest. It contains information about both the location and content of text and glyphs that Vision recognized in the input image.\n\nTopics\nObtaining Recognized Text\nfunc topCandidates(Int) -> [VNRecognizedText]\nRequests the n top candidates for a recognized text string.\nclass VNRecognizedText\nText recognized in an image through a text recognition request.\nRelationships\nInherits From\nVNRectangleObservation\nSee Also\nText Recognition\nRecognizing Text in Images\nAdd text-recognition features to your app using the Vision framework.\nStructuring Recognized Text on a Document\nDetect, recognize, and structure text on a business card or receipt using Vision and VisionKit.\nExtracting phone numbers from text in images\nAnalyze and filter phone numbers from text in live capture by using Vision.\nLocating and Displaying Recognized Text\nConfigure and perform text recognition on images to identify their textual content.\nclass VNRecognizeTextRequest\nAn image analysis request that finds and recognizes text in an image."
  },
  {
    "title": "VNBarcodeCompositeType | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnbarcodecompositetype",
    "html": "Topics\nComposite Types\ncase gs1TypeA\nA type that represents trade items in bulk.\ncase gs1TypeB\nA type that represents trade items by piece.\ncase gs1TypeC\nA type that represents trade items in varying quantity.\ncase linked\nA type that represents a linked composite type.\ncase none\nA type that represents no composite type.\nRelationships\nConforms To\nSendable\nSee Also\nBarcode Detection\nclass VNDetectBarcodesRequest\nA request that detects barcodes in an image."
  },
  {
    "title": "VNHumanBodyPose3DObservation.JointName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointname",
    "html": "Topics\nGetting the Head Joint Names\nstatic let topHead: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the top of the head.\nstatic let centerHead: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the center of the head.\nGetting the Arm Joint Names\nstatic let centerShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the point between the shoulders.\nstatic let leftShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left shoulder.\nstatic let rightShoulder: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right shoulder.\nstatic let leftElbow: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left elbow.\nstatic let rightElbow: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right elbow.\nstatic let leftWrist: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left wrist.\nstatic let rightWrist: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right wrist.\nGetting the Leg Joint Names\nstatic let leftHip: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left hip.\nstatic let rightHip: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right hip.\nstatic let leftKnee: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left knee.\nstatic let rightKnee: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right knee.\nstatic let leftAnkle: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the left ankle.\nstatic let rightAnkle: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the right ankle.\nGetting the Root Joint Name\nstatic let root: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the point between the left hip and right hip.\nGetting the Spine\nstatic let spine: VNHumanBodyPose3DObservation.JointName\nA joint name that represents the spine.\nCreating a Joint Name\ninit(rawValue: VNRecognizedPointKey)\nCreates a joint name with the key you specify.\nRelationships\nConforms To\nHashable\nRawRepresentable\nSendable\nSee Also\nAccessing Points\nvar availableJointNames: [VNHumanBodyPose3DObservation.JointName]\nThe names of the available joints in the observation.\nvar availableJointsGroupNames: [VNHumanBodyPose3DObservation.JointsGroupName]\nThe available joint group names in the observation.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose.\nfunc recognizedPoint(VNHumanBodyPose3DObservation.JointName) -> VNHumanBodyRecognizedPoint3D\nReturns the point for a joint name that the observation recognizes.\nfunc recognizedPoints(VNHumanBodyPose3DObservation.JointsGroupName) -> [VNHumanBodyPose3DObservation.JointName : VNHumanBodyRecognizedPoint3D]\nReturns a collection of points for the group name you specify."
  },
  {
    "title": "VNHumanBodyPose3DObservation.JointsGroupName | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnhumanbodypose3dobservation/jointsgroupname",
    "html": "Topics\nGetting the Group Names\nstatic let all: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents all joints.\nstatic let head: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the head joints.\nstatic let leftArm: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the left arm joints.\nstatic let leftLeg: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the left leg joints.\nstatic let rightArm: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the right arm joints.\nstatic let rightLeg: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the right leg joints.\nstatic let torso: VNHumanBodyPose3DObservation.JointsGroupName\nA group name that represents the torso joints.\nCreating a Group Name\ninit(rawValue: VNRecognizedPointGroupKey)\nCreates a joint name with the key you specify.\nRelationships\nConforms To\nHashable\nRawRepresentable\nSendable\nSee Also\nAccessing Points\nvar availableJointNames: [VNHumanBodyPose3DObservation.JointName]\nThe names of the available joints in the observation.\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nvar availableJointsGroupNames: [VNHumanBodyPose3DObservation.JointsGroupName]\nThe available joint group names in the observation.\nfunc recognizedPoint(VNHumanBodyPose3DObservation.JointName) -> VNHumanBodyRecognizedPoint3D\nReturns the point for a joint name that the observation recognizes.\nfunc recognizedPoints(VNHumanBodyPose3DObservation.JointsGroupName) -> [VNHumanBodyPose3DObservation.JointName : VNHumanBodyRecognizedPoint3D]\nReturns a collection of points for the group name you specify."
  },
  {
    "title": "VNDetectAnimalBodyPoseRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vndetectanimalbodyposerequest",
    "html": "Topics\nDetermining Supported Joints\nvar supportedJointNames: [VNAnimalBodyPoseObservation.JointName]\nRetrieves the joint names the request supports.\nvar supportedJointsGroupNames: [VNAnimalBodyPoseObservation.JointsGroupName]\nRetrieves the joint group names the request supports.\nAccessing the Results\nvar results: [VNAnimalBodyPoseObservation]?\nThe animal body pose the request observes.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nAnimal Body Pose Detection\nDetecting animal body poses with Vision\nDraw the skeleton of an animal by using Vision’s capability to detect animal body poses.\nclass VNAnimalBodyPoseObservation\nAn observation that provides the animal body points the analysis recognizes."
  },
  {
    "title": "VNRecognizeAnimalsRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnrecognizeanimalsrequest",
    "html": "Overview\n\nUse the knownAnimalIdentifiers(forRevision:) method to determine which animals the request supports.\n\nTopics\nAccessing the Results\nvar results: [VNRecognizedObjectObservation]?\nThe results of the request to recognize animals.\nIdentifying Animals\nfunc supportedIdentifiers() -> [VNAnimalIdentifier]\nReturns the identifiers of the animals that the request detects.\nstruct VNAnimalIdentifier\nAn animal identifier string.\nclass func knownAnimalIdentifiers(forRevision: Int) -> [VNAnimalIdentifier]\nReturns a list of animal identifiers the recognition algorithm supports for the specified revision.\nDeprecated\nIdentifying Request Revisions\nlet VNRecognizeAnimalsRequestRevision2: Int\nA constant for specifying revision 2 of the animal recognition request.\nlet VNRecognizeAnimalsRequestRevision1: Int\nA constant for specifying revision 1 of the animal recognition request.\nRelationships\nInherits From\nVNImageBasedRequest"
  },
  {
    "title": "Detecting animal body poses with Vision | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/detecting_animal_body_poses_with_vision",
    "html": "Overview\n\nNote\n\nThis sample code project is associated with WWDC23 session 10045: Detect animal poses in Vision.\n\nConfigure the sample code project\n\nThis sample app doesn’t run in Simulator, so you need to run it on a physical device with iOS 17 or later, or iPadOS 17 or later. To run the app, start a live video feed from the back camera.\n\nSee Also\nAnimal Body Pose Detection\nclass VNDetectAnimalBodyPoseRequest\nA request that detects an animal body pose.\nclass VNAnimalBodyPoseObservation\nAn observation that provides the animal body points the analysis recognizes."
  },
  {
    "title": "VNStatefulRequest | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/vision/vnstatefulrequest",
    "html": "Topics\nInitializing a Request\ninit(frameAnalysisSpacing: CMTime, completionHandler: VNRequestCompletionHandler?)\nInitializes a video-based request.\nConfiguring the Request\nvar minimumLatencyFrameCount: Int\nThe minimum number of frames a request processes before reporting an observation.\nvar frameAnalysisSpacing: CMTime\nA time value that indicates the interval between analysis operations.\nRelationships\nInherits From\nVNImageBasedRequest\nSee Also\nImage Sequence Analysis\nApplying Matte Effects to People in Images and Video\nGenerate image masks for people automatically by using semantic person-segmentation.\nDetecting Human Actions in a Live Video Feed\nIdentify body movements by sending a person’s pose data from a series of video frames to an action-classification model.\nclass VNGeneratePersonSegmentationRequest\nAn object that produces a matte image for a person it finds in the input image.\nclass VNGeneratePersonInstanceMaskRequest\nAn object that produces a mask of individual people it finds in the input image.\nclass VNDetectDocumentSegmentationRequest\nAn object that detects rectangular regions that contain text in the input image.\nclass VNSequenceRequestHandler\nAn object that processes image analysis requests for each frame in a sequence."
  },
  {
    "title": "Vision | Apple Developer Documentation",
    "url": "https://developer.apple.com/documentation/Vision",
    "html": "Overview\n\nThe Vision framework performs face and face landmark detection, text detection, barcode recognition, image registration, and general feature tracking. Vision also allows the use of custom Core ML models for tasks like classification or object detection.\n\nTopics\nEssentials\nBuilding a feature-rich app for sports analysis\nDetect and classify human activity in real time using computer vision and machine learning.\nStill Image Analysis\nDetecting Objects in Still Images\nLocate and demarcate rectangles, faces, barcodes, and text in images using the Vision framework.\nClassifying Images for Categorization and Search\nAnalyze and label images using a Vision classification request.\nAnalyzing Image Similarity with Feature Print\nGenerate a feature print to compute distance between images.\nclass VNRequest\nThe abstract superclass for analysis requests.\nclass VNImageBasedRequest\nThe abstract superclass for image analysis requests that focus on a specific part of an image.\nclass VNClassifyImageRequest\nA request to classify an image.\nclass VNGenerateImageFeaturePrintRequest\nAn image-based request to generate feature prints from an image.\nclass VNImageRequestHandler\nAn object that processes one or more image analysis requests pertaining to a single image.\nclass VNObservation\nThe abstract superclass for analysis results.\nImage Sequence Analysis\nApplying Matte Effects to People in Images and Video\nGenerate image masks for people automatically by using semantic person-segmentation.\nDetecting Human Actions in a Live Video Feed\nIdentify body movements by sending a person’s pose data from a series of video frames to an action-classification model.\nclass VNStatefulRequest\nAn abstract request type that builds evidence of a condition over time.\nclass VNGeneratePersonSegmentationRequest\nAn object that produces a matte image for a person it finds in the input image.\nclass VNGeneratePersonInstanceMaskRequest\nAn object that produces a mask of individual people it finds in the input image.\nclass VNDetectDocumentSegmentationRequest\nAn object that detects rectangular regions that contain text in the input image.\nclass VNSequenceRequestHandler\nAn object that processes image analysis requests for each frame in a sequence.\nSaliency Analysis\nCropping Images Using Saliency\nIsolate regions in an image that are most likely to draw people's attention.\nHighlighting Areas of Interest in an Image Using Saliency\nQuantify and visualize where people are likely to look in an image.\nclass VNGenerateAttentionBasedSaliencyImageRequest\nAn object that produces a heat map that identifies the parts of an image most likely to draw attention.\nclass VNGenerateObjectnessBasedSaliencyImageRequest\nA request that generates a heat map that identifies the parts of an image most likely to represent objects.\nclass VNSaliencyImageObservation\nAn observation that contains a grayscale heat map of important areas across an image.\nObject Tracking\nTracking the User’s Face in Real Time\nDetect and track faces from the selfie cam feed in real time.\nTracking Multiple Objects or Rectangles in Video\nApply Vision algorithms to track objects or rectangles throughout a video.\nclass VNTrackingRequest\nThe abstract superclass for image analysis requests that track unique features across multiple images or video frames.\nclass VNTrackRectangleRequest\nAn image analysis request that tracks movement of a previously identified rectangular object across multiple images or video frames.\nclass VNTrackObjectRequest\nAn image analysis request that tracks the movement of a previously identified object across multiple images or video frames.\nclass VNDetectedObjectObservation\nAn observation that provides the position and extent of an image feature that an image analysis request detects.\nRectangle Detection\nA rectangle detector finds rectangular features, like cards and signs, in images.\nclass VNDetectRectanglesRequest\nAn image analysis request that finds projected rectangular regions in an image.\nFace and Body Detection\nSelecting a selfie based on capture quality\nCompare face-capture quality in a set of images by using Vision.\nclass VNDetectFaceCaptureQualityRequest\nA request that produces a floating-point number that represents the capture quality of a face in a photo.\nclass VNDetectFaceLandmarksRequest\nAn image analysis request that finds facial features like eyes and mouth in an image.\nclass VNDetectFaceRectanglesRequest\nA request that finds faces within an image.\nclass VNDetectHumanRectanglesRequest\nA request that finds rectangular regions that contain people in an image.\nclass VNHumanObservation\nAn object that represents a person that the request detects.\nBody and Hand Pose Detection\nDetecting Human Body Poses in Images\nAdd the capability to detect human body poses to your app using the Vision framework.\nDetecting Hand Poses with Vision\nCreate a virtual drawing app by using Vision’s capability to detect hand poses.\nclass VNDetectHumanBodyPoseRequest\nA request that detects a human body pose.\nclass VNDetectHumanHandPoseRequest\nA request that detects a human hand pose.\nclass VNRecognizedPointsObservation\nAn observation that provides the points the analysis recognized.\nclass VNHumanBodyPoseObservation\nAn observation that provides the body points the analysis recognized.\nclass VNHumanHandPoseObservation\nAn observation that provides the hand points the analysis recognized.\nclass VNPoint\nAn immutable object that represents a single, two-dimensional point in an image.\nclass VNDetectedPoint\nAn object that represents a normalized point in an image, along with a confidence value.\nclass VNRecognizedPoint\nAn object that represents a normalized point in an image, along with an identifier label and a confidence value.\nstruct VNRecognizedPointKey\nThe data type for all recognized point keys.\nstruct VNRecognizedPointGroupKey\nThe data type for all recognized point group keys.\n3D Body Pose Detection\nIdentifying 3D human body poses in images\nDetect three-dimensional human body poses using the Vision framework.\nDetecting human body poses in 3D with Vision\nRender skeletons of 3D body pose points in a scene overlaying the input image.\nclass VNDetectHumanBodyPose3DRequest\nA request that detects points on human bodies in three-dimensional space, relative to the camera.\nclass VNHumanBodyPose3DObservation\nAn observation that provides the three-dimensional body points the request recognizes.\nclass VNRecognizedPoints3DObservation\nAn observation that provides the three-dimensional points for a request.\nclass VNHumanBodyRecognizedPoint3D\nA recognized three-dimensional point that includes a parent joint.\nclass VNPoint3D\nAn object that represents a three-dimensional point in an image.\nclass VNRecognizedPoint3D\nA three-dimensional point that includes an identifier to the point.\nstruct VNHumanBodyPose3DObservation.JointName\nThe joint names for a 3D body pose.\nstruct VNHumanBodyPose3DObservation.JointsGroupName\nThe joint group names for a 3D body pose.\nAnimal Detection\nclass VNRecognizeAnimalsRequest\nA request that recognizes animals in an image.\nAnimal Body Pose Detection\nDetecting animal body poses with Vision\nDraw the skeleton of an animal by using Vision’s capability to detect animal body poses.\nclass VNDetectAnimalBodyPoseRequest\nA request that detects an animal body pose.\nclass VNAnimalBodyPoseObservation\nAn observation that provides the animal body points the analysis recognizes.\nTrajectory Detection\nIdentifying Trajectories in Video\nGain new insights into your video data by using Vision to detect trajectories.\nDetecting moving objects in a video\nIdentify the trajectory of a thrown object by using Vision.\nclass VNDetectTrajectoriesRequest\nA request that detects the trajectories of shapes moving along a parabolic path.\nContour Detection\nclass VNDetectContoursRequest\nA request that detects the contours of the edges of an image.\nOptical Flow\nclass VNGenerateOpticalFlowRequest\nAn object that generates directional change vectors for each pixel in the targeted image.\nclass VNTrackOpticalFlowRequest\nAn object that determines the direction change of vectors for each pixel from a previous to current image.\nBarcode Detection\nclass VNDetectBarcodesRequest\nA request that detects barcodes in an image.\nenum VNBarcodeCompositeType\nComposite types for barcode requests.\nText Detection\nclass VNDetectTextRectanglesRequest\nAn image analysis request that finds regions of visible text in an image.\nclass VNTextObservation\nInformation about regions of text that an image analysis request detects.\nText Recognition\nRecognizing Text in Images\nAdd text-recognition features to your app using the Vision framework.\nStructuring Recognized Text on a Document\nDetect, recognize, and structure text on a business card or receipt using Vision and VisionKit.\nExtracting phone numbers from text in images\nAnalyze and filter phone numbers from text in live capture by using Vision.\nLocating and Displaying Recognized Text\nConfigure and perform text recognition on images to identify their textual content.\nclass VNRecognizeTextRequest\nAn image analysis request that finds and recognizes text in an image.\nclass VNRecognizedTextObservation\nA request that detects and recognizes regions of text in an image.\nRequest Progress Tracking\nprotocol VNRequestProgressProviding\nA protocol for providing progress information on long-running tasks in Vision.\ntypealias VNRequestProgressHandler\nA block executed at intervals during the processing of a Vision request.\nHorizon Detection\nclass VNDetectHorizonRequest\nAn image analysis request that determines the horizon angle in an image.\nclass VNHorizonObservation\nThe horizon angle information that an image analysis request detects.\nImage Alignment\nAligning Similar Images\nConstruct a composite image from images that capture the same scene.\nclass VNTargetedImageRequest\nThe abstract superclass for image analysis requests that operate on both the processed image and a secondary image.\nclass VNImageRegistrationRequest\nThe abstract superclass for image analysis requests that align images according to their content.\nclass VNTranslationalImageRegistrationRequest\nAn image analysis request that determines the affine transform necessary to align the content of two images.\nclass VNTrackTranslationalImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the affine transform necessary to align the content of two images.\nclass VNHomographicImageRegistrationRequest\nAn image analysis request that determines the perspective warp matrix necessary to align the content of two images.\nclass VNTrackHomographicImageRegistrationRequest\nAn image analysis request, as a stateful request you track over time, that determines the perspective warp matrix necessary to align the content of two images.\nclass VNImageAlignmentObservation\nThe abstract superclass for image analysis results that describe the relative alignment of two images.\nclass VNImageTranslationAlignmentObservation\nAffine transform information that an image alignment request produces.\nclass VNImageHomographicAlignmentObservation\nAn object that represents a perspective warp transformation.\nImage Background Removal\nApplying visual effects to foreground subjects\nSegment the foreground subjects of an image and composite them to a new background with visual effects.\nclass VNInstanceMaskObservation\nAn observation that contains an instance mask that labels instances in the mask.\nclass VNGenerateForegroundInstanceMaskRequest\nA request that generates an instance mask of noticable objects to separate from the background.\nlet VNGenerateForegroundInstanceMaskRequestRevision1: Int\nA constant for specifying revision 1 of the foreground instance mask request.\nObject Recognition\nRecognizing Objects in Live Capture\nApply Vision algorithms to identify objects in real-time video.\nclass VNRecognizedObjectObservation\nA detected object observation with an array of classification labels that classify the recognized object.\nUnderstanding a Dice Roll with Vision and Object Detection\nDetect dice position and values shown in a camera frame, and determine the end of a roll by leveraging a dice detection model.\nMachine Learning Image Analysis\nClassifying Images with Vision and Core ML\nCrop and scale photos using the Vision framework and classify them with a Core ML model.\nTraining a Create ML Model to Classify Flowers\nTrain a flower classifier using Create ML in Swift Playgrounds, and apply the resulting model to real-time image classification using Vision.\nclass VNCoreMLRequest\nAn image analysis request that uses a Core ML model to process images.\nclass VNClassificationObservation\nAn object that represents classification information that an image analysis request produces.\nclass VNPixelBufferObservation\nAn object that represents an image that an image analysis request produces.\nclass VNCoreMLFeatureValueObservation\nAn object that represents a collection of key-value information that a Core ML image analysis request produces.\nCoordinate Conversion\nVision uses a normalized coordinate space from 0.0 to 1.0 with lower left origin. For observations like landmarks in a face rect, these coordinates are relative to parent observations.\nfunc VNImagePointForNormalizedPoint(CGPoint, Int, Int) -> CGPoint\nProjects a point in normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePoint(CGPoint, Int, Int) -> CGPoint\nProjects a point from image coordinates into normalized coordinates.\nfunc VNImagePointForNormalizedPointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedPointForImagePointUsingRegionOfInterest(CGPoint, Int, Int, CGRect) -> CGPoint\nProjects a point from a region of interest within the image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRect(CGRect, Int, Int) -> CGRect\nProjects a rectangle from image coordinates into normalized coordinates.\nfunc VNImageRectForNormalizedRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the normalized coordinates into image coordinates.\nfunc VNNormalizedRectForImageRectUsingRegionOfInterest(CGRect, Int, Int, CGRect) -> CGRect\nProjects a rectangle from a region of interest within the image coordinates space into normalized coordinates.\nlet VNNormalizedIdentityRect: CGRect\nA normalized identity rectangle with an origin of zero and unit length and width.\nfunc VNNormalizedRectIsIdentityRect(CGRect) -> Bool\nReturns a Boolean value that indicates whether the rectangle has an origin of zero and unit length and width.\nfunc VNImagePointForFaceLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the image coordinates of a specified face landmark point.\nfunc VNNormalizedFaceBoundingBoxPointForLandmarkPoint(vector_float2, CGRect, Int, Int) -> CGPoint\nReturns the coordinates of a specified face landmark point, in bounding box coordinates.\nUtilities\nstruct VNComputeStage\nTypes that represent the compute stage.\nclass VNGeometryUtils\nUtility methods to determine the geometries of various Vision types.\nclass VNVideoProcessor\nAn object that performs offline analysis of video content.\nCommon Data Types\nclass VNCircle\nAn immutable, two-dimensional circle represented by its center point and radius.\nclass VNVector\nAn immutable, two-dimensional vector represented by its x-axis and y-axis projections.\nErrors\nlet VNErrorDomain: String\nThe domain of errors that the framework generates.\nenum VNErrorCode\nConstants that identify errors from the framework.\nVision Framework Version and Revision Numbers\nvar VNVisionVersionNumber: Double\nThe current version number of the Vision framework.\nlet VNDetectAnimalBodyPoseRequestRevision1: Int\nA value that indicates the first revision for an animal body pose request.\nlet VNDetectHumanBodyPose3DRequestRevision1: Int\nA value that indicates the first revision for a human three-dimensional body pose request.\nlet VNTrackHomographicImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a homographic image registration request.\nlet VNTrackTranslationalImageRegistrationRequestRevision1: Int\nA value that indicates the first revision for a translational image registration request.\nlet VNTrackOpticalFlowRequestRevision1: Int\nA value that indicates the first revision for an optial flow request.\nlet VNClassifyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateObjectnessBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an image classification request.\nlet VNGenerateAttentionBasedSaliencyImageRequestRevision2: Int\nA value that indicates the second revision for an attention saliency image request.\nlet VNGenerateImageFeaturePrintRequestRevision2: Int\nA value that indicates the second revision for a feature print request.\nlet VNDetectFaceCaptureQualityRequestRevision3: Int\nA value that indicates the third revision for a face capture quality request.\nlet VNDetectBarcodesRequestRevision4: Int\nA value that indicates the fourth revision for a barcode request."
  }
]